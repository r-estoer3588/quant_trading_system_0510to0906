"""
JSONL ã‚¤ãƒ™ãƒ³ãƒˆãƒ»ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆãƒ»è¨ºæ–­ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã®åŒæœŸåˆ†æãƒ„ãƒ¼ãƒ«ï¼ˆ3ç‚¹åŒæœŸï¼‰

ã‚„ã‚‹ã“ã¨:
1) ã‚¹ã‚¯ã‚·ãƒ§ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã¨ JSONL ã® system_* ã‚¤ãƒ™ãƒ³ãƒˆã‚’ç…§åˆï¼ˆæ—¢å­˜ï¼‰
2) JSONL ã®å€™è£œæ•°ï¼ˆsystem_complete.candidatesï¼‰ã¨ è¨ºæ–­ JSON ã® ranked_top_n_count ã‚’çªãåˆã‚ã›
3) ä»£è¡¨ã‚¹ã‚¯ã‚·ãƒ§ï¼ˆstart/complete è¿‘å‚ï¼‰ã‚’æ·»ä»˜ã—ã¦ã€3ç‚¹ã®ä¸€è²«æ€§ã‚’ãƒã‚§ãƒƒã‚¯ã—å¯è¦–åŒ–

Usage:
    python tools/sync_analysis.py
å‡ºåŠ›:
    - screenshots/progress_tracking/sync_analysis.json
    - screenshots/progress_tracking/ANALYSIS_REPORT.mdï¼ˆ3ç‚¹åŒæœŸã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å«ã‚€ï¼‰
    - screenshots/progress_tracking/sync_summary.json
    - screenshots/progress_tracking/tri_sync_report.jsonï¼ˆ3ç‚¹åŒæœŸè©³ç´°ï¼‰
"""

from datetime import datetime, timedelta
import json
from pathlib import Path
import re
from typing import Any, Iterable, cast


def load_jsonl_events(jsonl_path: Path) -> list[dict]:
    """JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¤ãƒ™ãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰"""
    events: list[dict] = []
    if not jsonl_path.exists():
        return events

    with open(jsonl_path, "r", encoding="utf-8") as f:
        for line in f:
            try:
                events.append(json.loads(line))
            except Exception:
                continue
    return events


def parse_screenshot_timestamp(filename: str) -> datetime | None:
    """
    ã‚¹ã‚¯ã‚·ãƒ§ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç”Ÿæˆ
    ä¾‹: progress_20251013_065209_856.png -> 2025-10-13 06:52:09
    """
    match = re.match(r"progress_(\d{8})_(\d{6})_\d+\.png", filename)
    if match:
        date_str = match.group(1)
        time_str = match.group(2)
        dt_str = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]} {time_str[:2]}:{time_str[2:4]}:{time_str[4:6]}"
        return datetime.strptime(dt_str, "%Y-%m-%d %H:%M:%S")
    return None


def find_nearest_event(screenshot_time: datetime, events: list[dict]) -> dict | None:
    """ã‚¹ã‚¯ã‚·ãƒ§ã®æ™‚åˆ»ã«æœ€ã‚‚è¿‘ã„JSONLã‚¤ãƒ™ãƒ³ãƒˆã‚’æ¤œç´¢"""
    min_delta = None
    nearest = None

    for event in events:
        try:
            # JSONLã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å½¢å¼: "2025/10/13 6:52:07" ã¾ãŸã¯ ISOå½¢å¼
            ts_str = event.get("timestamp", "")
            if "/" in ts_str:
                # "2025/10/13 6:52:07" å½¢å¼
                event_time = datetime.strptime(ts_str, "%Y/%m/%d %H:%M:%S")
            else:
                # ISOå½¢å¼
                event_time = datetime.fromisoformat(ts_str)

            delta = abs((screenshot_time - event_time).total_seconds())
            if min_delta is None or delta < min_delta:
                min_delta = delta
                nearest = event
        except Exception:
            continue

    return nearest


def analyze_sync():
    """ãƒ¡ã‚¤ãƒ³åˆ†æå‡¦ç†"""
    # JSONLã‚¤ãƒ™ãƒ³ãƒˆãƒ­ãƒ¼ãƒ‰
    jsonl_path = Path("logs/progress_today.jsonl")
    events: list[dict] = load_jsonl_events(jsonl_path)

    print(f"ğŸ“‹ JSONLã‚¤ãƒ™ãƒ³ãƒˆ: {len(events)} ä»¶")

    # system_start/complete ã‚¤ãƒ™ãƒ³ãƒˆã®ã¿æŠ½å‡º
    system_events = [
        e for e in events if e.get("event_type") in ["system_start", "system_complete"]
    ]
    print(f"   ã†ã¡systemé–¢é€£: {len(system_events)} ä»¶\n")

    # ã‚¹ã‚¯ã‚·ãƒ§ä¸€è¦§å–å¾—
    screenshot_dir = Path("screenshots/progress_tracking")
    screenshots = sorted(screenshot_dir.glob("progress_*.png"))

    print(f"ğŸ“¸ ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ: {len(screenshots)} æš\n")

    # åŒæœŸåˆ†æ
    print("=" * 80)
    print("åŒæœŸåˆ†æçµæœ")
    print("=" * 80)

    sync_results = []

    for screenshot in screenshots:
        ss_time = parse_screenshot_timestamp(screenshot.name)
        if not ss_time:
            continue

        # æœ€ã‚‚è¿‘ã„ã‚¤ãƒ™ãƒ³ãƒˆã‚’æ¤œç´¢
        nearest = find_nearest_event(ss_time, system_events)

        if nearest:
            event_type = nearest.get("event_type")
            data = nearest.get("data", {})
            system = data.get("system", "unknown")
            candidates = data.get("candidates")

            result = {
                "screenshot": screenshot.name,
                "screenshot_time": ss_time.strftime("%H:%M:%S"),
                "event_type": event_type,
                "system": system,
                "candidates": candidates,
                "event_timestamp": nearest.get("timestamp"),
            }
            sync_results.append(result)

    # çµæœå‡ºåŠ›
    print(f"\n{'æ™‚åˆ»':<10} {'ã‚¹ã‚¯ã‚·ãƒ§':<35} {'ã‚¤ãƒ™ãƒ³ãƒˆ':<18} {'System':<8} {'å€™è£œ':<6}")
    print("-" * 80)

    for r in sync_results[:20]:  # æœ€åˆã®20ä»¶ã‚’è¡¨ç¤º
        cand_str = (
            str(r.get("candidates", "-")) if r.get("candidates") is not None else "-"
        )
        print(
            f"{r['screenshot_time']:<10} {r['screenshot']:<35} {r['event_type']:<18} {r['system']:<8} {cand_str:<6}"
        )

    if len(sync_results) > 20:
        print(f"\n   ... ä»– {len(sync_results) - 20} ä»¶\n")

    # JSONå‡ºåŠ›
    output_path = Path("screenshots/progress_tracking/sync_analysis.json")
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(sync_results, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ“Š è©³ç´°çµæœ: {output_path}")

    # å„ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¹ã‚¯ã‚·ãƒ§æšæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    print("\nã€ã‚·ã‚¹ãƒ†ãƒ åˆ¥ã‚¹ã‚¯ã‚·ãƒ§æšæ•°ã€‘")
    system_counts: dict[str, int] = {}
    for r in sync_results:
        system = r.get("system", "unknown")
        system_counts[system] = system_counts.get(system, 0) + 1

    for system in sorted(system_counts.keys()):
        print(f"   {system}: {system_counts[system]} æš")

    # ä»£è¡¨ã‚¹ã‚¯ã‚·ãƒ§æŠ½å‡ºã¨Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ + 3ç‚¹åŒæœŸ
    md_path = Path("screenshots/progress_tracking/ANALYSIS_REPORT.md")
    try:
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ–‡å­—åˆ—ã‚’ datetime ã«å¤‰æ›
        def _to_dt(ts: str | None) -> datetime | None:
            try:
                if ts is None:
                    return None
                if "/" in ts:
                    return datetime.strptime(ts, "%Y/%m/%d %H:%M:%S")
                return datetime.fromisoformat(ts)
            except Exception:
                return None

        systems = [f"system{i}" for i in range(1, 8)]
        lines: list[str] = []
        lines.append("# åŒæœŸåˆ†æãƒ¬ãƒãƒ¼ãƒˆ\n")
        lines.append(f"ç”Ÿæˆæ™‚åˆ»: {datetime.now().isoformat()}\n")
        lines.append(f"- JSONLã‚¤ãƒ™ãƒ³ãƒˆ: {len(events)} ä»¶")
        lines.append(f"- ã‚·ã‚¹ãƒ†ãƒ é–¢é€£ã‚¤ãƒ™ãƒ³ãƒˆ: {len(system_events)} ä»¶")
        lines.append(f"- ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ: {len(screenshots)} æš\n")

        # ã‚¹ã‚¯ã‚·ãƒ§ã®æ™‚åˆ»é…åˆ—
        ss_times: list[tuple[datetime, Path]] = []
        for ss in screenshots:
            dt = parse_screenshot_timestamp(ss.name)
            if dt:
                ss_times.append((dt, ss))
        ss_times.sort(key=lambda x: x[0])

        def nearest_ss(ts: datetime) -> Path | None:
            best = None
            best_delta = None
            for dt, p in ss_times:
                delta = abs((dt - ts).total_seconds())
                if best_delta is None or delta < best_delta:
                    best_delta = delta
                    best = p
            if best and best_delta is not None and best_delta <= 5.0:
                return best
            return best

        lines.append("## ä»£è¡¨ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆï¼ˆå„Systemã®é–‹å§‹/å®Œäº† è¿‘å‚ï¼‰\n")
        lines.append(
            "| System | Start Event | Start SS | Complete Event | Complete SS |\n"
        )
        lines.append("|---|---|---|---|---|\n")

        def _find_evt(evt_type: str, sys_name: str) -> dict | None:
            return next(
                (
                    e
                    for e in system_events
                    if e.get("event_type") == evt_type
                    and e.get("data", {}).get("system") == sys_name
                ),
                None,
            )

        # ä»£è¡¨ã‚¹ã‚¯ã‚·ãƒ§ã‚’ä¿å­˜ã—ã¦å¾Œã§ per_system ã«ã‚‚å…¥ã‚Œã‚‹
        rep_ss: dict[str, dict[str, str | None]] = {}
        for sys_name in systems:
            st_e = _find_evt("system_start", sys_name)
            cm_e = _find_evt("system_complete", sys_name)
            st_ts = _to_dt(st_e.get("timestamp")) if st_e else None
            cm_ts = _to_dt(cm_e.get("timestamp")) if cm_e else None
            st_ss = nearest_ss(st_ts) if st_ts else None
            cm_ss = nearest_ss(cm_ts) if cm_ts else None
            rep_ss[sys_name] = {
                "start_ss": st_ss.name if st_ss else None,
                "complete_ss": cm_ss.name if cm_ss else None,
            }

            st_evt = st_e.get("timestamp") if st_e else "-"
            cm_evt = cm_e.get("timestamp") if cm_e else "-"
            st_link = f"![{st_ss.name}](./{st_ss.name})" if st_ss else "-"
            cm_link = f"![{cm_ss.name}](./{cm_ss.name})" if cm_ss else "-"
            lines.append(
                f"| {sys_name} | {st_evt} | {st_link} | {cm_evt} | {cm_link} |\n"
            )

        # overall start/end ã‚’æ¨å®š
        def _ev_time(e: dict | None) -> datetime | None:
            if not e:
                return None
            return _to_dt(e.get("timestamp"))

        ev_phase4 = next(
            (
                e
                for e in events
                if e.get("event_type") == "phase4_signal_generation_start"
            ),
            None,
        )
        overall_start = _ev_time(ev_phase4)
        if overall_start is None:
            ev_s_first = next(
                (e for e in events if e.get("event_type") == "system_start"),
                None,
            )
            overall_start = _ev_time(ev_s_first)

        ev_pipeline = next(
            (e for e in reversed(events) if e.get("event_type") == "pipeline_complete"),
            None,
        )
        ev_ui_wrap = next(
            (
                e
                for e in reversed(events)
                if e.get("event_type") == "phase_ui_wrapper_complete"
            ),
            None,
        )
        ev_alloc_done = next(
            (
                e
                for e in reversed(events)
                if e.get("event_type") == "phase5_allocation_complete"
            ),
            None,
        )
        overall_end = (
            _ev_time(ev_pipeline) or _ev_time(ev_ui_wrap) or _ev_time(ev_alloc_done)
        )

        # per-system start/end/candidates (+ ä»£è¡¨ã‚¹ã‚¯ã‚·ãƒ§å)
        per_system: dict[str, dict] = {}
        for sys_name in systems:
            st_e = _find_evt("system_start", sys_name)
            cm_e = _find_evt("system_complete", sys_name)
            st_t = _ev_time(st_e)
            cm_t = _ev_time(cm_e)
            cand = None
            try:
                if cm_e and isinstance(cm_e.get("data"), dict):
                    cand_val = cm_e["data"].get("candidates")
                    if isinstance(cand_val, (int, float)):
                        cand = int(cand_val)
            except Exception:
                cand = None
            dur = None
            if st_t and cm_t:
                dur = int((cm_t - st_t).total_seconds())
            per_system[sys_name] = {
                "start": st_t.isoformat() if st_t else None,
                "end": cm_t.isoformat() if cm_t else None,
                "duration_sec": dur,
                "candidates": cand,
                "start_ss": rep_ss.get(sys_name, {}).get("start_ss"),
                "complete_ss": rep_ss.get(sys_name, {}).get("complete_ss"),
            }

        # Markdown: æ¦‚è¦ã¨æ‰€è¦æ™‚é–“
        pc_flag = ev_pipeline is not None
        lines.append("\n## ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã¨æ‰€è¦æ™‚é–“\n")
        lines.append(f"- pipeline_complete: {'ã‚ã‚Š' if pc_flag else 'ãªã—'}\n")
        if overall_start:
            lines.append(f"- overall_start: {overall_start.isoformat()}\n")
        if overall_end:
            lines.append(f"- overall_end: {overall_end.isoformat()}\n")
        if overall_start and overall_end:
            dur_all = int((overall_end - overall_start).total_seconds())
            lines.append(f"- overall_duration: {dur_all} ç§’\n")

        lines.append("\n### Systemåˆ¥ æ‰€è¦æ™‚é–“ãƒ»å€™è£œæ•°\n")
        lines.append("| System | start | end | duration(s) | candidates |\n")
        lines.append("|---|---|---|---:|---:|\n")
        for sys_name in systems:
            info = per_system.get(sys_name, {})
            lines.append(
                f"| {sys_name} | {info.get('start', '-')} | {info.get('end', '-')} | "
                f"{info.get('duration_sec', '-')} | {info.get('candidates', '-')} |\n"
            )

        # ===== 3ç‚¹åŒæœŸ: è¨ºæ–­ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’æ¢ç´¢ã—ã€JSONLå€™è£œæ•°ã¨çªãåˆã‚ã› =====
        def _iter_diag_files() -> Iterable[Path]:
            candidates_paths = [
                Path("results_csv/diagnostics_test"),
                Path("results_csv_test/diagnostics_test"),
            ]
            for base in candidates_paths:
                if base.exists():
                    yield from base.glob("diagnostics_snapshot_*.json")

        def _load_json(path: Path) -> dict[str, Any] | None:
            try:
                return cast(
                    dict[str, Any],
                    json.loads(path.read_text(encoding="utf-8")),
                )
            except Exception:
                return None

        # overall_end è¿‘å‚ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’é¸ã¶ï¼ˆÂ±45åˆ†ã€åŒæ—¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å„ªå…ˆï¼‰ã€‚ç„¡ã‘ã‚Œã°æœ€æ–°ã‚’æ¡ç”¨ã€‚
        chosen_diag_path: Path | None = None
        chosen_diag: dict[str, Any] | None = None
        if overall_end is None:
            diag_files = sorted(_iter_diag_files())
            if diag_files:
                chosen_diag_path = diag_files[-1]
                chosen_diag = _load_json(chosen_diag_path)
        else:
            best_dt_diff: float | None = None
            window = timedelta(minutes=45)
            # å…¨å€™è£œã‚’é›†ã‚ã¦åŒæ—¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å„ªå…ˆ
            all_files = list(_iter_diag_files())
            same_day_files: list[Path] = []
            for diag_file in all_files:
                data0 = _load_json(diag_file)
                if not isinstance(data0, dict):
                    continue
                exp0 = data0.get("export_date")
                try:
                    exp_dt0 = (
                        datetime.fromisoformat(exp0) if isinstance(exp0, str) else None
                    )
                except Exception:
                    exp_dt0 = None
                if exp_dt0 and exp_dt0.date() == overall_end.date():
                    same_day_files.append(diag_file)

            scan_list = same_day_files or all_files
            for diag_file in scan_list:
                data = _load_json(diag_file)
                if not isinstance(data, dict):
                    continue
                exp = data.get("export_date")
                try:
                    exp_dt = (
                        datetime.fromisoformat(exp) if isinstance(exp, str) else None
                    )
                except Exception:
                    exp_dt = None
                if exp_dt is None:
                    continue
                delta = abs((exp_dt - overall_end).total_seconds())
                if delta <= window.total_seconds() and (
                    best_dt_diff is None or delta < best_dt_diff
                ):
                    best_dt_diff = delta
                    chosen_diag_path = diag_file
                    chosen_diag = data
            if chosen_diag is None:
                diag_files = sorted(_iter_diag_files())
                if diag_files:
                    chosen_diag_path = diag_files[-1]
                    chosen_diag = _load_json(chosen_diag_path)

        # è¨ºæ–­ã‹ã‚‰ per-system ã® ranked_top_n_count ã‚’æŠ½å‡º
        diag_counts: dict[str, int | None] = {s: None for s in systems}
        diag_mode: str | None = None
        diag_export: str | None = None
        if isinstance(chosen_diag, dict):
            diag_mode = chosen_diag.get("mode")
            diag_export = chosen_diag.get("export_date")
            try:
                for item in chosen_diag.get("systems", []) or []:
                    sid = item.get("system_id")
                    diag_obj = item.get("diagnostics") or {}
                    if sid in diag_counts:
                        val = diag_obj.get("ranked_top_n_count")
                        if isinstance(val, (int, float)):
                            diag_counts[sid] = int(val)
            except Exception:
                pass

        # JSONL åˆè¨ˆ vs allocation_start ã® total_candidates ã‚’ç¢ºèª
        sum_jsonl = 0
        for s in systems:
            v = per_system.get(s, {}).get("candidates")
            if isinstance(v, int):
                sum_jsonl += v
        alloc_evt = next(
            (e for e in events if e.get("event_type") == "phase5_allocation_start"),
            None,
        )
        alloc_total = None
        if alloc_evt and isinstance(alloc_evt.get("data"), dict):
            try:
                alloc_total = int(alloc_evt["data"].get("total_candidates"))
            except Exception:
                alloc_total = None

        # 3ç‚¹åŒæœŸã®çµæœã‚’çµ„ã¿ç«‹ã¦
        tri_rows: list[dict[str, Any]] = []
        for s in systems:
            row = {
                "system": s,
                "jsonl_candidates": per_system.get(s, {}).get("candidates"),
                "diagnostics_ranked_top_n": diag_counts.get(s),
                "start_ss": per_system.get(s, {}).get("start_ss"),
                "complete_ss": per_system.get(s, {}).get("complete_ss"),
            }
            a = row["jsonl_candidates"]
            b = row["diagnostics_ranked_top_n"]
            if a is None and b is None:
                status = "missing"
            elif isinstance(a, int) and isinstance(b, int):
                status = "match" if a == b else "mismatch"
            else:
                status = "partial"
            row["status"] = status
            tri_rows.append(row)

        tri_report = {
            "selected_diagnostics": str(chosen_diag_path) if chosen_diag_path else None,
            "diagnostics_mode": diag_mode,
            "diagnostics_export_date": diag_export,
            "overall_start": overall_start.isoformat() if overall_start else None,
            "overall_end": overall_end.isoformat() if overall_end else None,
            "allocation_total_candidates": alloc_total,
            "sum_jsonl_candidates": sum_jsonl,
            "sum_jsonl_vs_alloc_match": (
                (alloc_total == sum_jsonl) if isinstance(alloc_total, int) else None
            ),
            "systems": tri_rows,
            "generated_at": datetime.now().isoformat(),
        }

        tri_path = md_path.parent / "tri_sync_report.json"
        tri_path.write_text(
            json.dumps(tri_report, indent=2, ensure_ascii=False), encoding="utf-8"
        )

        # Markdown: 3ç‚¹åŒæœŸã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½è¨˜
        lines.append("\n## 3ç‚¹åŒæœŸãƒã‚§ãƒƒã‚¯ï¼ˆJSONL Ã— è¨ºæ–­ Ã— ã‚¹ã‚¯ã‚·ãƒ§ä»£è¡¨ï¼‰\n")
        if chosen_diag_path:
            lines.append(
                f"- ä½¿ç”¨è¨ºæ–­ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ: `{chosen_diag_path}`"
                f" ï¼ˆexport: {diag_export or '-'}, mode: {diag_mode or '-'}ï¼‰\n"
            )
        if isinstance(alloc_total, int):
            lines.append(
                f"- allocation_total_candidates: {alloc_total} / "
                f"sum(jsonl per-system): {sum_jsonl}"
                f" â†’ {'ä¸€è‡´' if alloc_total == sum_jsonl else 'ä¸ä¸€è‡´'}\n"
            )
        lines.append(
            "\n| System | JSONL candidates | Diagnostics ranked_top_n | status | start SS | complete SS |\n"
        )
        lines.append("|---|---:|---:|---|---|---|\n")
        for r in tri_rows:
            lines.append(
                f"| {r['system']} | {r.get('jsonl_candidates', '-')} | "
                f"{r.get('diagnostics_ranked_top_n', '-')} | "
                f"{r.get('status', '-')} | {r.get('start_ss', '-')} | "
                f"{r.get('complete_ss', '-')} |\n"
            )

        # æœ«å°¾ã«JSONãƒªãƒ³ã‚¯
        lines.append("\n---\n")
        lines.append(f"è©³ç´°JSON: [sync_analysis.json](./{output_path.name})\n")
        lines.append("3ç‚¹åŒæœŸ: [tri_sync_report.json](./tri_sync_report.json)\n")

        md_path.write_text("".join(lines), encoding="utf-8")
        print(f"ğŸ“ Markdownãƒ¬ãƒãƒ¼ãƒˆ: {md_path}")

        # è¿½åŠ ã®è¦ç´„JSONã‚’æ›¸ãå‡ºã—
        summary = {
            "events_total": len(events),
            "system_events": len(system_events),
            "screenshots": len(screenshots),
            "pipeline_complete": pc_flag,
            "overall": {
                "start": overall_start.isoformat() if overall_start else None,
                "end": overall_end.isoformat() if overall_end else None,
                "duration_sec": (
                    int((overall_end - overall_start).total_seconds())
                    if (overall_start and overall_end)
                    else None
                ),
            },
            "systems": per_system,
        }
        summary_path = md_path.parent / "sync_summary.json"
        summary_path.write_text(
            json.dumps(summary, indent=2, ensure_ascii=False), encoding="utf-8"
        )
        print(f"ğŸ§¾ ã‚µãƒãƒªãƒ¼JSON: {summary_path}")
    except Exception as e:
        print(f"âš ï¸ Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã«å¤±æ•—: {e}")


if __name__ == "__main__":
    analyze_sync()
