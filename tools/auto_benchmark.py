"""
è‡ªå‹•ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œãƒ„ãƒ¼ãƒ«

ã‚³ãƒŸãƒƒãƒˆå‰ã«ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œã—ã€å›å¸°ã‚’æ¤œå‡ºã€‚

ä½¿ã„æ–¹:
    python tools/auto_benchmark.py
    python tools/auto_benchmark.py --threshold 0.15  # 15%ã®åŠ£åŒ–ã¾ã§è¨±å®¹
"""

import json
import subprocess
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

BENCHMARK_HISTORY = Path("benchmarks/history.jsonl")
BENCHMARK_DIR = Path("benchmarks")


def run_benchmark() -> Optional[Dict]:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
    print("ğŸ” Running performance benchmark...")

    result = subprocess.run(
        [
            sys.executable,
            "scripts/run_all_systems_today.py",
            "--test-mode",
            "mini",
            "--skip-external",
            "--benchmark",
        ],
        capture_output=True,
        text=True,
        encoding="utf-8",
    )

    if result.returncode != 0:
        print("âŒ Benchmark failed:")
        print(result.stderr)
        return None

    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’èª­ã¿è¾¼ã¿
    benchmark_files = list(Path("results_csv_test").glob("benchmark_*.json"))
    if not benchmark_files:
        print("âš ï¸  No benchmark file found")
        return None

    latest_benchmark = max(benchmark_files, key=lambda p: p.stat().st_mtime)
    with open(latest_benchmark, encoding="utf-8") as f:
        data = json.load(f)

    return data


def compare_with_baseline(
    current: Dict, baseline: Dict, threshold: float = 0.10
) -> List[Dict]:
    """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨æ¯”è¼ƒï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ10%ä»¥ä¸Šã®åŠ£åŒ–ã‚’æ¤œå‡ºï¼‰"""
    regressions = []

    # ãƒ•ã‚§ãƒ¼ã‚ºã”ã¨ã®æ™‚é–“ã‚’æ¯”è¼ƒ
    current_phases = current.get("phase_times", {})
    baseline_phases = baseline.get("phase_times", {})

    for phase, current_time in current_phases.items():
        baseline_time = baseline_phases.get(phase, 0)

        if baseline_time > 0:
            regression_pct = (current_time - baseline_time) / baseline_time

            if regression_pct > threshold:
                regressions.append(
                    {
                        "phase": phase,
                        "baseline": baseline_time,
                        "current": current_time,
                        "regression_pct": regression_pct,
                    }
                )

    # å…¨ä½“æ™‚é–“ã®æ¯”è¼ƒ
    current_total = current.get("total_time", 0)
    baseline_total = baseline.get("total_time", 0)

    if baseline_total > 0:
        total_regression = (current_total - baseline_total) / baseline_total

        if total_regression > threshold:
            regressions.append(
                {
                    "phase": "TOTAL",
                    "baseline": baseline_total,
                    "current": current_total,
                    "regression_pct": total_regression,
                }
            )

    return regressions


def get_baseline() -> Optional[Dict]:
    """éå»7æ—¥é–“ã®ä¸­å¤®å€¤ã‚’ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã—ã¦ä½¿ç”¨"""
    if not BENCHMARK_HISTORY.exists():
        return None

    # å±¥æ­´èª­ã¿è¾¼ã¿
    history = []
    with open(BENCHMARK_HISTORY, encoding="utf-8") as f:
        for line in f:
            try:
                history.append(json.loads(line))
            except json.JSONDecodeError:
                continue

    if len(history) < 3:
        return None

    # ç›´è¿‘7æ—¥é–“
    recent = history[-7:]

    # ãƒ•ã‚§ãƒ¼ã‚ºã”ã¨ã®ä¸­å¤®å€¤ã‚’è¨ˆç®—
    baseline = {"phase_times": {}, "total_time": 0}

    # å…¨ãƒ•ã‚§ãƒ¼ã‚ºåã‚’åé›†
    all_phases = set()
    for h in recent:
        all_phases.update(h.get("phase_times", {}).keys())

    # ä¸­å¤®å€¤è¨ˆç®—
    for phase in all_phases:
        times = [h["phase_times"].get(phase, 0) for h in recent]
        times = [t for t in times if t > 0]  # 0ã‚’é™¤å¤–

        if times:
            times.sort()
            mid = len(times) // 2
            if len(times) % 2 == 0:
                baseline["phase_times"][phase] = (times[mid - 1] + times[mid]) / 2
            else:
                baseline["phase_times"][phase] = times[mid]

    # å…¨ä½“æ™‚é–“ã®ä¸­å¤®å€¤
    total_times = [h.get("total_time", 0) for h in recent if h.get("total_time", 0) > 0]
    if total_times:
        total_times.sort()
        mid = len(total_times) // 2
        if len(total_times) % 2 == 0:
            baseline["total_time"] = (total_times[mid - 1] + total_times[mid]) / 2
        else:
            baseline["total_time"] = total_times[mid]

    return baseline


def save_benchmark(data: Dict):
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’å±¥æ­´ã«ä¿å­˜"""
    BENCHMARK_DIR.mkdir(parents=True, exist_ok=True)

    # Git commit å–å¾—ï¼ˆå¤±æ•—ã—ã¦ã‚‚ç¶šè¡Œï¼‰
    try:
        git_commit = (
            subprocess.check_output(
                ["git", "rev-parse", "HEAD"], stderr=subprocess.DEVNULL
            )
            .decode()
            .strip()
        )
    except Exception:
        git_commit = "unknown"

    record = {
        "timestamp": datetime.now().isoformat(),
        "git_commit": git_commit,
        "phase_times": data.get("phase_times", {}),
        "total_time": data.get("total_time", 0),
    }

    with open(BENCHMARK_HISTORY, "a", encoding="utf-8") as f:
        f.write(json.dumps(record, ensure_ascii=False) + "\n")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    import argparse

    parser = argparse.ArgumentParser(description="è‡ªå‹•ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ")
    parser.add_argument(
        "--threshold",
        type=float,
        default=0.10,
        help="è¨±å®¹ã™ã‚‹åŠ£åŒ–ç‡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.10 = 10%%ï¼‰",
    )
    parser.add_argument(
        "--skip-interactive", action="store_true", help="å¯¾è©±çš„ç¢ºèªã‚’ã‚¹ã‚­ãƒƒãƒ—"
    )
    args = parser.parse_args()

    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    current = run_benchmark()
    if not current:
        return 1

    print(f"âœ… Benchmark completed: {current.get('total_time', 0):.2f}s\n")

    # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å–å¾—
    baseline = get_baseline()

    if baseline:
        # å›å¸°æ¤œå‡º
        regressions = compare_with_baseline(current, baseline, threshold=args.threshold)

        if regressions:
            print(
                f"âš ï¸  Performance regressions detected (>{args.threshold:.0%} slower):\n"
            )
            for reg in regressions:
                print(f"  â€¢ {reg['phase']}: {reg['regression_pct']:+.1%} slower")
                print(
                    f"    Baseline: {reg['baseline']:.2f}s â†’ Current: {reg['current']:.2f}s"
                )

            print(
                f"\nâŒ Performance degraded by >{args.threshold:.0%}. Review changes."
            )

            if not args.skip_interactive:
                response = input("\nContinue anyway? (y/N): ")
                if response.lower() != "y":
                    return 1
            else:
                return 1
        else:
            print(
                f"âœ… No performance regressions detected (threshold: {args.threshold:.0%})"
            )
    else:
        print("â„¹ï¸  No baseline available (need 3+ historical runs)")
        print("   This run will be used as baseline for future comparisons.")

    # çµæœã‚’å±¥æ­´ã«ä¿å­˜
    save_benchmark(current)
    print(f"\nğŸ“Š Benchmark saved to: {BENCHMARK_HISTORY}")

    return 0


if __name__ == "__main__":
    sys.exit(main())
