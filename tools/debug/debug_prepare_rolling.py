#!/usr/bin/env python3
"""Debug tool for analyzing the _prepare_rolling_frame function step by step."""

import sys
from pathlib import Path

import pandas as pd

# Add project root to path
ROOT_DIR = Path(__file__).resolve().parent.parent.parent
if str(ROOT_DIR) not in sys.path:
    sys.path.insert(0, str(ROOT_DIR))

from common.cache_manager import CacheManager
from common.indicators_common import add_indicators
from config.settings import get_settings


def debug_prepare_rolling_frame():
    """Debug the _prepare_rolling_frame function step by step."""
    print("ğŸ” _prepare_rolling_frame é–¢æ•°ã®å‹•ä½œè§£æ")

    settings = get_settings(create_dirs=True)
    cm = CacheManager(settings)

    # Get a sample symbol - skip if no data available
    csv_files = list(cm.full_dir.glob("*.csv"))
    if not csv_files:
        print("âŒ CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ¼ã‚¿ãŒãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        return

    csv_path = csv_files[0]
    symbol = csv_path.stem
    print(f"\nğŸ“Š ã‚·ãƒ³ãƒœãƒ«: {symbol}")

    # Read raw data
    df = cm.read(symbol, "full")
    if df is None or df.empty:
        print(f"âŒ ã‚·ãƒ³ãƒœãƒ« {symbol} ã®ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    print(f"ğŸ”´ Raw ãƒ‡ãƒ¼ã‚¿: {len(df.columns)} åˆ—")
    print(f"ğŸ“ Raw åˆ—å: {list(df.columns)}")

    # Step 1: Copy and basic processing
    work = df.copy()
    print(f"\n1ï¸âƒ£ Copyå¾Œ: {len(work.columns)} åˆ—")

    # Step 2: Date processing
    if "date" not in work.columns:
        if "Date" in work.columns:
            work = work.rename(columns={"Date": "date"})
        print(f"2ï¸âƒ£ Dateå‡¦ç†å¾Œ: {len(work.columns)} åˆ—")

    # Step 3: Date normalization
    work["date"] = pd.to_datetime(work["date"], errors="coerce")
    work = work.dropna(subset=["date"])
    work = (
        work.sort_values("date")
        .drop_duplicates("date", keep="last")
        .reset_index(drop=True)
    )
    print(f"3ï¸âƒ£ Dateæ­£è¦åŒ–å¾Œ: {len(work.columns)} åˆ—")

    # Step 4: Create calc copy
    calc = work.copy()
    calc["Date"] = pd.to_datetime(calc["date"], errors="coerce").dt.normalize()
    print(f"4ï¸âƒ£ calcä½œæˆ(Dateè¿½åŠ )å¾Œ: {len(calc.columns)} åˆ—")
    print(f"   ğŸ“ åˆ—å: {list(calc.columns)}")

    # Step 5: Column conversion (this is where the problem likely occurs)
    print("\n5ï¸âƒ£ OHLCV åˆ—å¤‰æ›å‡¦ç†:")
    col_pairs = (
        ("open", "Open"),
        ("high", "High"),
        ("low", "Low"),
        ("close", "Close"),
        ("volume", "Volume"),
    )

    for src, dst in col_pairs:
        if src in calc.columns and dst not in calc.columns:
            print(f"   âœ… {src} -> {dst} (å¤‰æ›å®Ÿè¡Œ)")
            calc[dst] = calc[src]
        elif src in calc.columns and dst in calc.columns:
            print(f"   âš ï¸  {src} ã¨ {dst} ä¸¡æ–¹å­˜åœ¨ -> {src}å‰Šé™¤")
            calc = calc.drop(columns=[src])
        else:
            print(
                f"   â­ï¸ {src}({src in calc.columns}) -> {dst}({dst in calc.columns}) ã‚¹ã‚­ãƒƒãƒ—"
            )

    print(f"   ğŸ”„ å¤‰æ›å¾Œ: {len(calc.columns)} åˆ—")
    print(f"   ğŸ“ åˆ—å: {list(calc.columns)}")

    # Step 6: AdjClose processing
    if "AdjClose" not in calc.columns:
        for cand in ("adjusted_close", "adj_close", "adjclose"):
            if cand in calc.columns:
                print(f"6ï¸âƒ£ AdjCloseå¤‰æ›: {cand} -> AdjClose")
                calc["AdjClose"] = calc[cand]
                calc = calc.drop(columns=[cand])
                break

    print(f"6ï¸âƒ£ AdjCloseå‡¦ç†å¾Œ: {len(calc.columns)} åˆ—")

    # Step 7: Indicator calculation
    print(f"\n7ï¸âƒ£ æŒ‡æ¨™è¨ˆç®—å‰: {len(calc.columns)} åˆ—")
    try:
        enriched = add_indicators(calc)
        print(f"7ï¸âƒ£ æŒ‡æ¨™è¨ˆç®—å¾Œ: {len(enriched.columns)} åˆ—")
    except Exception as e:
        print(f"âŒ æŒ‡æ¨™è¨ˆç®—ã§ã‚¨ãƒ©ãƒ¼: {e}")
        return

    # Find what changed
    added = set(enriched.columns) - set(calc.columns)
    removed = set(calc.columns) - set(enriched.columns)

    if added:
        print(f"   ğŸ†• è¿½åŠ åˆ—: {sorted(added)}")
    if removed:
        print(f"   ğŸ—‘ï¸ å‰Šé™¤åˆ—: {sorted(removed)}")

    # Step 8: Final cleanup (simulate _clean_duplicate_columns)
    print("\n8ï¸âƒ£ é‡è¤‡ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‰ã®åˆ—:")
    all_cols = enriched.columns.tolist()
    col_mapping = {}
    for col in all_cols:
        key = col.lower()
        if key not in col_mapping:
            col_mapping[key] = []
        col_mapping[key].append(col)

    duplicates_to_remove = []
    for key, similar_cols in col_mapping.items():
        if len(similar_cols) > 1:
            print(f"   ğŸ”„ {key}: {similar_cols}")
            # Keep the best one
            priority_scores = []
            for col in similar_cols:
                if col.isupper():
                    score = 3
                elif col[0].isupper():
                    score = 2
                elif "_" in col:
                    score = 1
                else:
                    score = 0
                priority_scores.append((score, col))

            priority_scores.sort(reverse=True)
            best_col = priority_scores[0][1]
            print(f"     âœ… ä¿æŒ: {best_col}")

            for _, col in priority_scores[1:]:
                duplicates_to_remove.append(col)
                print(f"     âŒ å‰Šé™¤: {col}")

    print(f"\nğŸ§¹ å‰Šé™¤å¯¾è±¡: {duplicates_to_remove}")

    # Summary
    print("\nğŸ“Š è¦ç´„:")
    print(f"   â€¢ Raw ãƒ‡ãƒ¼ã‚¿: {len(df.columns)} åˆ—")
    print(f"   â€¢ æŒ‡æ¨™è¨ˆç®—å¾Œ: {len(enriched.columns)} åˆ—")
    print(f"   â€¢ è¿½åŠ ã•ã‚ŒãŸåˆ—: {len(added)}")
    print(f"   â€¢ é‡è¤‡å‰Šé™¤å¯¾è±¡: {len(duplicates_to_remove)}")

    if len(enriched) > 0:
        print(f"   â€¢ ãƒ‡ãƒ¼ã‚¿è¡Œæ•°: {len(enriched)}")
        print("âœ… ãƒ‡ãƒãƒƒã‚°å®Œäº†")
    else:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")


if __name__ == "__main__":
    debug_prepare_rolling_frame()
