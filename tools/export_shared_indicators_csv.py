"""CLI ãƒ„ãƒ¼ãƒ«: å…±æœ‰æŒ‡æ¨™ã‚’å‰è¨ˆç®—ã— CSV ã¨ã—ã¦æ›¸ãå‡ºã™ã€‚

å½“æ—¥ã‚·ã‚°ãƒŠãƒ«å®Ÿè¡Œã®å‰ã« ATR/SMA/RSI ãªã©ã®å…±æœ‰æŒ‡æ¨™ãŒå¿…è¦ã«ãªã‚‹å ´åˆã€
ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ã£ã¦äº‹å‰è¨ˆç®—ã¨ CSV ã¸ã®æ›¸ãå‡ºã—ã‚’è¡Œãˆã‚‹ã€‚
æ—¢å­˜ã® ``precompute_shared_indicators`` ã‚’åˆ©ç”¨ã—ã€è¨ˆç®—çµæœã¯
``data_cache/signals/shared_indicators``ï¼ˆæ—¢å®šï¼‰ã«ä¿å­˜ã•ã‚Œã‚‹ã€‚
"""

from __future__ import annotations

import argparse
from collections.abc import Iterable
from pathlib import Path

import pandas as pd

from common.cache_manager import round_dataframe
from common.data_loader import load_price
from common.indicators_precompute import precompute_shared_indicators
from common.universe import load_universe_file
from config.settings import get_settings


def _parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description=("Precompute shared indicators (ATR/SMA/RSI ç­‰) and export them to CSV")
    )
    parser.add_argument(
        "--symbols",
        nargs="+",
        help="å‡¦ç†å¯¾è±¡ã®éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ã€‚æŒ‡å®šãŒç„¡ã‘ã‚Œã°ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§ã€‚",
    )
    parser.add_argument(
        "--universe-file",
        type=Path,
        help="ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã€‚symbols ãŒæœªæŒ‡å®šã®ã¨ãã«ä½¿ç”¨ã€‚",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        help=(
            "CSV ã‚’ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€‚æœªæŒ‡å®šã®å ´åˆã¯ <signals_dir>/shared_indicators ã‚’åˆ©ç”¨ã€‚"
        ),
    )
    parser.add_argument(
        "--cache-profile",
        choices=("full", "rolling"),
        default="full",
        help="OHLCV ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿å…ƒã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¨®åˆ¥ (default: full)",
    )
    parser.add_argument(
        "--no-parallel",
        action="store_true",
        help="ä¸¦åˆ—å®Ÿè¡Œã‚’å¼·åˆ¶çš„ã«ç„¡åŠ¹åŒ–ã™ã‚‹ã€‚",
    )
    parser.add_argument(
        "--max-workers",
        type=int,
        help="ä¸¦åˆ—å®Ÿè¡Œæ™‚ã®æœ€å¤§ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã€‚æœªæŒ‡å®šæ™‚ã¯è¨­å®šå€¤ã‚’ä½¿ç”¨ã€‚",
    )
    parser.add_argument(
        "--combined-output",
        type=Path,
        help="å…¨éŠ˜æŸ„ã‚’çµåˆã—ãŸå˜ä¸€ CSV ã®å‡ºåŠ›å…ˆ (ä»»æ„)ã€‚",
    )
    return parser.parse_args()


def _unique_upper_symbols(symbols: Iterable[str]) -> list[str]:
    ordered = []
    for sym in symbols:
        key = sym.strip().upper()
        if not key:
            continue
        if key not in ordered:
            ordered.append(key)
    return ordered


def _prepare_output_frame(df: pd.DataFrame) -> pd.DataFrame:
    """CSV æ›¸ãå‡ºã—å‘ã‘ã«åˆ—ã‚’æ•´å½¢ã™ã‚‹ã€‚"""

    frame = df.copy()

    if "Date" in frame.columns:
        date_series = pd.to_datetime(frame["Date"], errors="coerce")
    elif "date" in frame.columns:
        date_series = pd.to_datetime(frame["date"], errors="coerce")
    else:
        date_series = pd.to_datetime(frame.index, errors="coerce")

    frame["Date"] = date_series.dt.normalize()
    frame = frame.dropna(subset=["Date"])
    frame = frame.sort_values("Date")

    if "date" in frame.columns:
        frame = frame.drop(columns=["date"])

    preferred_order = [
        "Date",
        "open",
        "high",
        "low",
        "close",
        "adjusted_close",
        "volume",
        "Open",
        "High",
        "Low",
        "Close",
        "Volume",
    ]
    ordered_cols: list[str] = []
    for col in preferred_order:
        if col in frame.columns and col not in ordered_cols:
            ordered_cols.append(col)
    for col in frame.columns:
        if col not in ordered_cols:
            ordered_cols.append(col)
    frame = frame.loc[:, ordered_cols]

    return frame


def main() -> None:
    args = _parse_args()
    settings = get_settings(create_dirs=True)

    if args.symbols:
        symbols = _unique_upper_symbols(args.symbols)
    else:
        universe_path = args.universe_file
        if universe_path is not None:
            symbols = load_universe_file(str(universe_path))
        else:
            symbols = load_universe_file()

    if not symbols:
        print("ğŸ›‘ éŠ˜æŸ„ãƒªã‚¹ãƒˆãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚symbols/universe ã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return

    output_dir = (
        args.output_dir
        if args.output_dir is not None
        else Path(settings.outputs.signals_dir) / "shared_indicators"
    )
    output_dir.mkdir(parents=True, exist_ok=True)

    basic_data: dict[str, pd.DataFrame] = {}
    skipped: list[str] = []
    for sym in symbols:
        try:
            df = load_price(sym, cache_profile=args.cache_profile)
        except Exception:
            df = pd.DataFrame()
        if df is None or getattr(df, "empty", True):
            skipped.append(sym)
            continue
        work = df.copy()
        if "date" in work.columns and "Date" not in work.columns:
            work["Date"] = pd.to_datetime(work["date"], errors="coerce")
        basic_data[sym] = work

    if not basic_data:
        print("ğŸ›‘ æŒ‡æ¨™è¨ˆç®—ç”¨ã®ãƒ‡ãƒ¼ã‚¿ãŒ1ä»¶ã‚‚å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        if skipped:
            print("  ã‚¹ã‚­ãƒƒãƒ—éŠ˜æŸ„:", ", ".join(skipped))
        return

    use_parallel = not args.no_parallel
    if use_parallel:
        if args.max_workers is not None:
            max_workers = max(1, int(args.max_workers))
        else:
            max_workers = int(getattr(settings, "THREADS_DEFAULT", 12))
    else:
        max_workers = None

    enriched = precompute_shared_indicators(
        basic_data,
        log=lambda msg: print(msg),
        parallel=use_parallel,
        max_workers=max_workers if use_parallel else None,
    )

    combined_frames: list[pd.DataFrame] = []
    written = 0
    for sym, df in enriched.items():
        if df is None or getattr(df, "empty", True):
            continue
        prepared = _prepare_output_frame(df)
        if prepared.empty:
            continue
        prepared.insert(0, "Symbol", sym)
        try:
            round_dec = getattr(settings.cache, "round_decimals", None)
        except Exception:
            round_dec = None
        try:
            prepared_to_write = round_dataframe(prepared, round_dec)
        except Exception:
            prepared_to_write = prepared
        prepared_to_write.to_csv(output_dir / f"{sym}.csv", index=False)
        combined_frames.append(prepared)
        written += 1

    if args.combined_output and combined_frames:
        combined_path = Path(args.combined_output)
        combined_path.parent.mkdir(parents=True, exist_ok=True)
        try:
            round_dec = getattr(settings.cache, "round_decimals", None)
        except Exception:
            round_dec = None
        try:
            combined = pd.concat(combined_frames, ignore_index=True)
            combined = round_dataframe(combined, round_dec)
        except Exception:
            combined = pd.concat(combined_frames, ignore_index=True)
        combined.to_csv(combined_path, index=False)
        print(f"ğŸ“¦ çµåˆ CSV ã‚’å‡ºåŠ›ã—ã¾ã—ãŸ: {combined_path}")

    if skipped:
        print("âš ï¸ ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã«ã‚ˆã‚Šã‚¹ã‚­ãƒƒãƒ—ã—ãŸéŠ˜æŸ„:", ", ".join(skipped))

    print(f"âœ… å…±æœ‰æŒ‡æ¨™ã®CSVå‡ºåŠ›ãŒå®Œäº†ã—ã¾ã—ãŸ: {written} éŠ˜æŸ„ (ä¿å­˜å…ˆ: {output_dir.resolve()})")


if __name__ == "__main__":  # pragma: no cover - CLI ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
    main()
