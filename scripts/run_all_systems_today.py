"""Daily multi-system signal pipeline (repaired minimal bootstrap section).

NOTE: This file experienced prior encoding corruption. Incremental repairs are
being applied. The current patch introduces:
 1. Explicit project root insertion into sys.path so that running the script
     via ``python scripts/run_all_systems_today.py`` correctly resolves top-level
     modules like ``common``.
 2. Use of ``get_settings(create_dirs=False)`` inside ``_initialize_run_context``
     to avoid potential hangs during strategy initialization (directory
     creation is performed lazily elsewhere if needed).

Further clean-up (mojibake in log strings/docstrings) will follow in later
patches without altering CLI flags or public behavior.
"""

from __future__ import annotations

# flake8: noqa: E501
import argparse
from collections.abc import Callable, Mapping, Sequence
from concurrent.futures import ThreadPoolExecutor, as_completed
from contextvars import ContextVar
from dataclasses import dataclass, field
from datetime import datetime
import json
import logging
import multiprocessing
import os
from pathlib import Path
import sys
import threading
from threading import Lock
from typing import Any, cast, no_type_check
from zoneinfo import ZoneInfo

# --- ensure repository root on sys.path
# (script executed from repo root or elsewhere)
try:  # noqa: SIM105
    _project_root = Path(__file__).resolve().parents[1]
    if str(_project_root) not in sys.path:
        sys.path.insert(0, str(_project_root))
except Exception:  # pragma: no cover - defensive; failure is non-fatal
    pass

import pandas as pd

from common import broker_alpaca as ba
from common.alpaca_order import submit_orders_df
from common.cache_manager import CacheManager, load_base_cache
from common.dataframe_utils import round_dataframe  # noqa: E402
from common.indicator_access import get_indicator, is_true, to_float
from common.notification import notify_zero_trd_all_systems
from common.notifier import create_notifier
from common.position_age import load_entry_dates, save_entry_dates
from common.signal_merge import Signal, merge_signals
from common.stage_metrics import GLOBAL_STAGE_METRICS, StageEvent, StageSnapshot
from common.structured_logging import MetricsCollector
from common.symbol_universe import build_symbol_universe_from_settings
from common.system_diagnostics import get_diagnostics_with_fallback

# æŠ½å‡º: ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€é–¢æ•°ã¯ common.today_data_loader ã¸åˆ†é›¢
from common.today_data_loader import load_basic_data

# æŠ½å‡º: ãƒ•ã‚£ãƒ«ã‚¿/æ¡ä»¶/ä½ãƒ¬ãƒ™ãƒ«ãƒ˜ãƒ«ãƒ‘ã¯ common.today_filters ã¸åˆ†é›¢
from common.today_filters import (
    _system1_conditions,
    _system2_conditions,
    _system3_conditions,
    _system4_conditions,
    _system5_conditions,
    _system6_conditions,
    filter_system1,
    filter_system2,
    filter_system3,
    filter_system4,
    filter_system5,
    filter_system6,
)
from common.utils_spy import (
    get_latest_nyse_trading_day,
    get_signal_target_trading_day,
    get_spy_with_indicators,
)
from config.settings import get_settings
from core.final_allocation import finalize_allocation, load_symbol_system_map
from core.system1 import summarize_system1_diagnostics
from core.system5 import DEFAULT_ATR_PCT_THRESHOLD

# strategies
from strategies.system1_strategy import System1Strategy
from strategies.system2_strategy import System2Strategy
from strategies.system3_strategy import System3Strategy
from strategies.system4_strategy import System4Strategy
from strategies.system5_strategy import System5Strategy
from strategies.system6_strategy import System6Strategy
from strategies.system7_strategy import System7Strategy
from tools.notify_metrics import send_metrics_notification  # noqa: E402
from tools.verify_trd_length import get_expected_max_for_mode, verify_trd_length  # noqa: E402

# --- Console encoding helpers (to mitigate mojibake on Windows terminals) ---
_NO_EMOJI_ENV = (
    os.environ.get("NO_EMOJI") or os.environ.get("DISABLE_EMOJI") or ""
).strip().lower() in {"1", "true", "yes", "on"}


def _console_supports_utf8() -> bool:
    try:
        enc = (getattr(sys.stdout, "encoding", None) or "").lower()
        return "utf-8" in enc or "65001" in enc  # CP65001 is UTF-8 on Windows
    except Exception:
        return False


def _strip_emojis(text: str) -> str:
    try:
        import re as _re

        # Remove characters outside BMP (common emojis etc.)
        return _re.sub(r"[\U00010000-\U0010FFFF]", "", str(text))
    except Exception:
        # Fallback: best-effort ASCII replacement
        try:
            enc = getattr(sys.stdout, "encoding", "utf-8") or "utf-8"
            return str(text).encode(enc, errors="ignore").decode(enc, errors="ignore")
        except Exception:
            return str(text)


_LOG_CALLBACK = None

# Progress event settings
ENABLE_PROGRESS_EVENTS = os.getenv("ENABLE_PROGRESS_EVENTS", "false").lower() == "true"

# Global log file variables (initialized by setup_logging)
_LOG_FILE_PATH: Path | None = None
_LOG_FILE_MODE: str | None = None

# Global metrics collector for performance tracking
_GLOBAL_METRICS = MetricsCollector()


def emit_progress_event(event_type: str, data: dict) -> None:
    """Emit a progress event with given type and data."""
    if not ENABLE_PROGRESS_EVENTS:
        return

    try:
        logger = logging.getLogger(__name__)
        # Use lazy logging formatting to avoid building the string when DEBUG disabled
        logger.debug("Progress event [%s]: %s", event_type, data)
    except Exception:
        pass


_LOG_FORWARDING: ContextVar[bool] = ContextVar("_LOG_FORWARDING", default=False)


# NOTE: StrategyProtocol ä¸€æ™‚æ’¤å»ï¼ˆæˆ¦ç•¥å´ã®å®Ÿè£…å·®ç•°ãŒå¤§ãã attr-defined å•é¡Œã‚’èª˜ç™ºã®ãŸã‚ï¼‰
_LOG_START_TS: float | None = None  # CLI ç”¨ã®çµŒéæ™‚é–“æ¸¬å®šé–‹å§‹æ™‚åˆ»

# Structured UI logging state (initialized lazily inside _emit_ui_log)
_STRUCTURED_LOG_START_TS: float | None = None  # monotonic-ish epoch seconds
_STRUCTURED_LAST_PHASE: dict[str, str] | None = None  # {system: last_phase}

# ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®šï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å›ºå®šãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ã€‚å¿…è¦ã«å¿œã˜ã¦æ—¥ä»˜ä»˜ãã¸åˆ‡æ›¿ã€‚
# ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒ­ã‚¬ãƒ¼
_rate_limited_logger = None


# --- stage progress bridging helpers -----------------------------------------------------

_PER_SYSTEM_STAGE = None
_PER_SYSTEM_EXIT = None
_SET_STAGE_UNIVERSE_TARGET = None

_STAGE_EVENT_PUMP_THREAD: threading.Thread | None = None
_STAGE_EVENT_PUMP_STOP: threading.Event | None = None
_STAGE_EVENT_PUMP_INTERVAL = 0.25  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ250ms

# æœ€é©åŒ–ç”¨ãƒ•ãƒ©ã‚°ï¼ˆã‚¢ã‚¯ãƒ†ã‚£ãƒ–å‡¦ç†æ™‚ã¯é »ç¹ã«ã€ã‚¢ã‚¤ãƒ‰ãƒ«æ™‚ã¯è² è·è»½æ¸›ï¼‰
_STAGE_EVENT_PUMP_ADAPTIVE = True
_STAGE_EVENT_PUMP_MIN_INTERVAL = 0.1  # æœ€å°100msï¼ˆé«˜è² è·æ™‚ï¼‰
_STAGE_EVENT_PUMP_MAX_INTERVAL = 1.0  # æœ€å¤§1ç§’ï¼ˆã‚¢ã‚¤ãƒ‰ãƒ«æ™‚ï¼‰
_STAGE_EVENT_PUMP_IDLE_THRESHOLD = 5  # 5å›é€£ç¶šã§ã‚¤ãƒ™ãƒ³ãƒˆãªã—ã§ã‚¢ã‚¤ãƒ‰ãƒ«åˆ¤å®š


class StageReporter:
    """Callable wrapper that forwards stage progress with an associated system name."""

    __slots__ = ("system", "_queue")

    def __init__(self, system: str, queue: Any | None = None) -> None:
        self.system = str(system or "").strip().lower() or "unknown"
        self._queue = queue

    def __call__(
        self,
        progress: int,
        filter_count: int | None = None,
        setup_count: int | None = None,
        candidate_count: int | None = None,
        entry_count: int | None = None,
    ) -> None:
        if self._queue is not None:
            try:
                self._queue.put(
                    (
                        self.system,
                        progress,
                        filter_count,
                        setup_count,
                        candidate_count,
                        entry_count,
                    ),
                    block=False,
                )
            except Exception:
                pass
            return
        _stage(
            self.system,
            progress,
            filter_count,
            setup_count,
            candidate_count,
            entry_count,
        )


def register_stage_callback(callback: Callable[..., None] | None) -> None:
    """Register per-system stage callback and ensure the event pump is running."""

    globals()["_PER_SYSTEM_STAGE"] = callback
    if callable(callback):
        _ensure_stage_event_pump()
    else:
        _stop_stage_event_pump()


def register_stage_exit_callback(callback: Callable[[str, int], None] | None) -> None:
    """Register per-system exit callback (UI integration helper)."""

    globals()["_PER_SYSTEM_EXIT"] = callback


def register_universe_target_callback(
    callback: Callable[[int | None], None] | None,
) -> None:
    """Register callback to update the shared universe target in the UI."""

    globals()["_SET_STAGE_UNIVERSE_TARGET"] = callback


def _ensure_stage_event_pump(interval: float | None = None) -> None:
    """Start a background thread that periodically drains stage events for the UI.

    ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–é–“éš”èª¿æ•´æ©Ÿèƒ½:
    - ã‚¤ãƒ™ãƒ³ãƒˆãŒé »ç¹ãªæ™‚ã¯é«˜é »åº¦ï¼ˆ100msï¼‰
    - ã‚¢ã‚¤ãƒ‰ãƒ«æ™‚ã¯ä½é »åº¦ï¼ˆ1ç§’ï¼‰ã§CPUè² è·è»½æ¸›
    """

    cb = globals().get("_PER_SYSTEM_STAGE")
    if not cb or not callable(cb):
        return

    thread = globals().get("_STAGE_EVENT_PUMP_THREAD")
    if isinstance(thread, threading.Thread) and thread.is_alive():
        return

    stop_event = threading.Event()
    globals()["_STAGE_EVENT_PUMP_STOP"] = stop_event

    base_interval = float(interval if interval is not None else _STAGE_EVENT_PUMP_INTERVAL)

    def _pump() -> None:
        current_interval = base_interval
        idle_count = 0

        while not stop_event.is_set():
            events_processed = False
            try:
                # ã‚¤ãƒ™ãƒ³ãƒˆæ•°ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–èª¿æ•´
                queue_obj = globals().get("_PROGRESS_QUEUE")
                queue_size = 0
                if queue_obj is not None:
                    try:
                        # ã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚ºã®æ¦‚ç®—ï¼ˆå®Ÿéš›ã«ã¯éç ´å£Šçš„ã«ãƒã‚§ãƒƒã‚¯ä¸å¯ï¼‰
                        queue_size = queue_obj.qsize() if hasattr(queue_obj, "qsize") else 0
                    except Exception:
                        queue_size = 0

                _drain_stage_event_queue()

                # GLOBAL_STAGE_METRICS ã‹ã‚‰ã‚‚ã‚¤ãƒ™ãƒ³ãƒˆæ•°ã‚’ãƒã‚§ãƒƒã‚¯
                try:
                    metrics_events = len(GLOBAL_STAGE_METRICS.drain_events())
                    if metrics_events > 0 or queue_size > 0:
                        events_processed = True
                except Exception:
                    pass

                # ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–é–“éš”èª¿æ•´
                if _STAGE_EVENT_PUMP_ADAPTIVE:
                    if events_processed:
                        # ã‚¤ãƒ™ãƒ³ãƒˆãŒã‚ã£ãŸå ´åˆã€é–“éš”ã‚’çŸ­ç¸®
                        current_interval = max(
                            _STAGE_EVENT_PUMP_MIN_INTERVAL, current_interval * 0.8
                        )
                        idle_count = 0
                    else:
                        # ã‚¤ãƒ™ãƒ³ãƒˆãŒãªã‹ã£ãŸå ´åˆã€ã‚¢ã‚¤ãƒ‰ãƒ«ã‚«ã‚¦ãƒ³ãƒˆå¢—åŠ 
                        idle_count += 1
                        if idle_count >= _STAGE_EVENT_PUMP_IDLE_THRESHOLD:
                            # ã‚¢ã‚¤ãƒ‰ãƒ«çŠ¶æ…‹ã§ã¯é–“éš”ã‚’å»¶é•·ã—ã¦CPUè² è·è»½æ¸›
                            current_interval = min(
                                _STAGE_EVENT_PUMP_MAX_INTERVAL, current_interval * 1.2
                            )

            except Exception:
                pass

            stop_event.wait(current_interval)

    pump_thread = threading.Thread(target=_pump, name="stage-event-pump", daemon=True)
    globals()["_STAGE_EVENT_PUMP_THREAD"] = pump_thread
    pump_thread.start()


def _stop_stage_event_pump(timeout: float = 1.0) -> None:
    """Stop the background event pump thread if it is running."""

    stop_event = globals().get("_STAGE_EVENT_PUMP_STOP")
    thread = globals().get("_STAGE_EVENT_PUMP_THREAD")

    if isinstance(stop_event, threading.Event):
        stop_event.set()

    if isinstance(thread, threading.Thread) and thread.is_alive():
        if threading.current_thread() is not thread:
            thread.join(timeout)

    globals().pop("_STAGE_EVENT_PUMP_STOP", None)
    globals().pop("_STAGE_EVENT_PUMP_THREAD", None)


def _get_rate_limited_logger():
    """ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒ­ã‚¬ãƒ¼ã‚’å–å¾—ã€‚"""
    global _rate_limited_logger
    if _rate_limited_logger is None:
        from common.rate_limited_logging import create_rate_limited_logger

        _rate_limited_logger = create_rate_limited_logger("run_all_systems_today", 3.0)
    return _rate_limited_logger


def _prepare_concat_frames(
    frames: Sequence[pd.DataFrame | None],
) -> list[pd.DataFrame]:
    """Dropå…¨NAåˆ—ã‚’é™¤å»ã—ã€ç©ºãƒ‡ãƒ¼ã‚¿ã‚’é€£çµå¯¾è±¡ã‹ã‚‰å¤–ã™ã€‚"""

    cleaned: list[pd.DataFrame] = []
    for frame in frames:
        if frame is None or getattr(frame, "empty", True):
            continue
        try:
            cleaned_frame = frame.dropna(axis=1, how="all")
        except Exception:
            cleaned_frame = frame
        if getattr(cleaned_frame, "empty", True):
            continue
        cleaned.append(cleaned_frame)
    return cleaned


@dataclass(slots=True)
class BaseCachePool:
    """base ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å…±æœ‰è¾æ›¸ã‚’ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ã«ç®¡ç†ã™ã‚‹è£œåŠ©ã‚¯ãƒ©ã‚¹ã€‚"""

    cache_manager: CacheManager
    shared: dict[str, pd.DataFrame] | None = None
    hits: int = 0
    loads: int = 0
    failures: int = 0
    _lock: Lock = field(default_factory=Lock, init=False, repr=False)

    def __post_init__(self) -> None:
        if self.shared is None:
            self.shared = {}

    def get(
        self,
        symbol: str,
        *,
        rebuild_if_missing: bool = True,
        min_last_date: pd.Timestamp | None = None,
        allowed_recent_dates: set[pd.Timestamp] | None = None,
    ) -> tuple[pd.DataFrame | None, bool]:
        """base ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰éŠ˜æŸ„ã‚·ãƒ³ãƒœãƒ«ã® DataFrame ã‚’å–å¾—ã™ã‚‹ã€‚

        Returns (df, from_cache):
            - df: å–å¾—ã¾ãŸã¯å†æ§‹ç¯‰ã•ã‚ŒãŸ DataFrameï¼ˆå­˜åœ¨ã—ãªã‘ã‚Œã° Noneï¼‰
            - from_cache: True=å…±æœ‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥å‘½ä¸­ / False=æ–°è¦ãƒ­ãƒ¼ãƒ‰

        ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶:
            rebuild_if_missing: ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¬ ææ™‚ã«ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å†æ§‹ç¯‰ã™ã‚‹ã‹
            min_last_date: æœ«å°¾æ—¥ä»˜ãŒã“ã®æ—¥ä»˜(æ­£è¦åŒ–)æœªæº€ãªã‚‰ stale ã¨ã¿ãªã™
            allowed_recent_dates: è¨±å¯ã•ã‚ŒãŸæœ€çµ‚æ—¥ä»˜é›†åˆï¼ˆå­˜åœ¨ã—ã€ã‹ã¤ä¸€è‡´ã—ãªã‘ã‚Œã° staleï¼‰
        stale åˆ¤å®šæ™‚ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç ´æ£„ã—ã¦å†ãƒ­ãƒ¼ãƒ‰ã‚’è©¦ã¿ã‚‹ã€‚
        """

        allowed_set = set(allowed_recent_dates or ())
        if min_last_date is not None:
            try:
                min_norm: pd.Timestamp | None = pd.Timestamp(min_last_date).normalize()
            except Exception:
                min_norm = None
        else:
            min_norm = None

        def _detect_last(frame: pd.DataFrame | None) -> pd.Timestamp | None:
            if frame is None or getattr(frame, "empty", True):
                return None
            try:
                if isinstance(frame.index, pd.DatetimeIndex) and len(frame.index):
                    # Ensure we pass a scalar-compatible value into Timestamp
                    idx_last = frame.index[-1]
                    if hasattr(idx_last, "to_pydatetime"):
                        scalar_dt = idx_last.to_pydatetime()
                    else:
                        scalar_dt = str(idx_last)
                    return pd.Timestamp(scalar_dt).normalize()
            except Exception:
                pass
            try:
                idx = pd.to_datetime(frame.index.to_numpy(), errors="coerce")
                idx = idx[~pd.isna(idx)]
                if len(idx):
                    idx_last = idx[-1]
                    if hasattr(idx_last, "to_pydatetime"):
                        scalar_dt = idx_last.to_pydatetime()
                    else:
                        scalar_dt = str(idx_last)
                    return pd.Timestamp(scalar_dt).normalize()
            except Exception:
                pass
            try:
                series = frame.get("Date") if frame is not None else None
                if series is not None:
                    # convert to numpy array to match pandas.to_datetime overloads
                    arr = pd.to_datetime(pd.Series(series).to_numpy(), errors="coerce")
                    arr = arr[~pd.isna(arr)]
                    if arr.size:
                        try:
                            arr_last = arr[-1]
                            if hasattr(arr_last, "to_pydatetime"):
                                scalar_dt = arr_last.to_pydatetime()
                            else:
                                scalar_dt = str(arr_last)
                            return pd.Timestamp(scalar_dt).normalize()
                        except Exception:
                            pass
            except Exception:
                pass
            return None

        with self._lock:
            if self.shared is not None and symbol in self.shared:
                value = self.shared[symbol]
                last_date = _detect_last(value)
                stale = False
                if allowed_set and (last_date is None or last_date not in allowed_set):
                    stale = True
                if not stale and min_norm is not None:
                    if last_date is None or last_date < min_norm:
                        stale = True
                if not stale:
                    self.hits += 1
                    return value, True
                try:
                    if self.shared is not None:
                        self.shared.pop(symbol, None)
                except Exception:
                    pass

        df = load_base_cache(
            symbol,
            rebuild_if_missing=rebuild_if_missing,
            cache_manager=self.cache_manager,
            min_last_date=min_last_date,
            allowed_recent_dates=allowed_set or None,
            prefer_precomputed_indicators=True,
        )

        with self._lock:
            if self.shared is not None and df is not None:
                # only store when df is a real DataFrame
                self.shared[symbol] = df
            self.loads += 1
            if df is None or getattr(df, "empty", True):
                self.failures += 1

        return df, False

    def sync_to(self, target: dict[str, pd.DataFrame] | None) -> None:
        """æ—¢å­˜ã®å¤–éƒ¨è¾æ›¸ã¸å…±æœ‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’åæ˜ ã™ã‚‹ã€‚"""

        if target is None or self.shared is None or target is self.shared:
            return
        with self._lock:
            try:
                target.update(self.shared)
            except Exception:
                pass

    def snapshot_stats(self) -> dict[str, int]:
        with self._lock:
            size = len(self.shared or {})
            return {
                "hits": self.hits,
                "loads": self.loads,
                "failures": self.failures,
                "size": size,
            }


@dataclass(slots=True)
class TodayRunContext:
    """ä¿æŒå…±æœ‰çŠ¶æ…‹ã¨ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’é›†ç´„ã—ãŸå½“æ—¥ã‚·ã‚°ãƒŠãƒ«å®Ÿè¡Œç”¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€‚"""

    settings: Any
    cache_manager: CacheManager
    signals_dir: Path
    cache_dir: Path
    slots_long: int | None = None
    slots_short: int | None = None
    capital_long: float | None = None
    capital_short: float | None = None
    save_csv: bool = False
    csv_name_mode: str | None = None
    notify: bool = True
    log_callback: Callable[[str], None] | None = None
    progress_callback: Callable[[int, int, str], None] | None = None
    per_system_progress: Callable[[str, str], None] | None = None
    symbol_data: dict[str, pd.DataFrame] | None = None
    parallel: bool = False
    run_start_time: datetime = field(default_factory=datetime.now)
    start_equity: float = 0.0
    run_id: str = ""
    today: pd.Timestamp | None = None
    symbol_universe: list[str] = field(default_factory=list)
    basic_data: dict[str, pd.DataFrame] = field(default_factory=dict)
    base_cache: dict[str, pd.DataFrame] = field(default_factory=dict)
    system_filters: dict[str, list[str]] = field(default_factory=dict)
    per_system_frames: dict[str, pd.DataFrame] = field(default_factory=dict)
    final_signals: pd.DataFrame | None = None
    system_diagnostics: dict[str, dict[str, Any]] = field(default_factory=dict)
    # ãƒ†ã‚¹ãƒˆé«˜é€ŸåŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    test_mode: str | None = None  # mini/quick/sample
    skip_external: bool = False  # å¤–éƒ¨APIå‘¼ã³å‡ºã—ã‚’ã‚¹ã‚­ãƒƒãƒ—
    # latest_only ã‚°ãƒ­ãƒ¼ãƒãƒ«åˆ¶å¾¡: "ãƒ‡ãƒ¼ã‚¿åŸºæº–æ—¥"ï¼ˆä¾‹: é€±æœ«ã¯é‡‘æ›œã€å¹³æ—¥ã¯å½“æ—¥ï¼‰
    signal_base_day: pd.Timestamp | None = None
    # å®Ÿè¡Œé–‹å§‹æ™‚ã«ç¢ºå®šã™ã‚‹ã€Œã‚¨ãƒ³ãƒˆãƒªãƒ¼äºˆå®šæ—¥ã€ï¼ˆåŸºæº–æ—¥ã®ç¿Œå–¶æ¥­æ—¥ï¼‰
    entry_day: pd.Timestamp | None = None


def _get_account_equity() -> float:
    """Return current account equity via Alpaca API.

    å¤±æ•—ã—ãŸå ´åˆã¯ 0.0 ã‚’è¿”ã™ï¼ˆãƒ†ã‚¹ãƒˆç’°å¢ƒãªã© API æœªè¨­å®šæ™‚ã®å®‰å…¨å¯¾ç­–ï¼‰ã€‚
    """
    try:
        client = ba.get_client(paper=True)
        acct = client.get_account()
        return float(getattr(acct, "equity", 0.0) or 0.0)
    except Exception:
        return 0.0


def _configure_today_logger(*, mode: str = "single", _run_id: str | None = None) -> None:
    """today_signals ç”¨ã®ãƒ­ã‚¬ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ§‹æˆã™ã‚‹ã€‚

    mode:
      - "single": å›ºå®šãƒ•ã‚¡ã‚¤ãƒ« `today_signals.log`
      - "dated":  æ—¥ä»˜ä»˜ã `today_signals_YYYYMMDD_HHMM.log`ï¼ˆJSTï¼‰
    run_id: äºˆç´„ï¼ˆç¾çŠ¶æœªä½¿ç”¨ï¼‰ã€‚å°†æ¥ã€ãƒ•ã‚¡ã‚¤ãƒ«åã«å«ã‚ãŸã„å ´åˆã«åˆ©ç”¨ã€‚
    """
    global _LOG_FILE_PATH, _LOG_FILE_MODE
    _LOG_FILE_MODE = mode or "single"
    try:
        settings = get_settings(create_dirs=True)
        log_dir = Path(settings.LOGS_DIR)
    except Exception:
        log_dir = Path("logs")
    try:
        log_dir.mkdir(parents=True, exist_ok=True)
    except Exception:
        pass

    if _LOG_FILE_MODE == "dated":
        try:
            jst_now = datetime.now(ZoneInfo("Asia/Tokyo"))
        except Exception:
            jst_now = datetime.now()
        stamp = jst_now.strftime("%Y%m%d_%H%M")
        filename = f"today_signals_{stamp}.log"
    else:
        filename = "today_signals.log"

    _LOG_FILE_PATH = log_dir / filename
    # ãƒãƒ³ãƒ‰ãƒ©ã‚’æœ€æ–°ãƒ‘ã‚¹ã«åˆã‚ã›ã¦å¼µã‚Šæ›¿ãˆã‚‹
    try:
        logger = logging.getLogger("today_signals")
        for h in list(logger.handlers):
            try:
                if isinstance(h, logging.FileHandler) and getattr(h, "baseFilename", None):
                    if Path(h.baseFilename) != _LOG_FILE_PATH:
                        logger.removeHandler(h)
                        try:
                            h.close()
                        except Exception:
                            pass
            except Exception:
                # ãƒãƒ³ãƒ‰ãƒ©æƒ…å ±å–å¾—ã«å¤±æ•—ã—ãŸå ´åˆã¯ç„¡è¦–
                pass
        # ä»¥é™ã€_get_today_logger() ãŒé©åˆ‡ãªãƒãƒ³ãƒ‰ãƒ©ã‚’è¿½åŠ ã™ã‚‹
    except Exception:
        pass


def _get_today_logger() -> logging.Logger:
    """today_signals ç”¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ã‚¬ãƒ¼ã‚’å–å¾—ã€‚

    ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ `logs/today_signals.log`ã€‚
    `_configure_today_logger(mode="dated")` é©ç”¨æ™‚ã¯æ—¥ä»˜ä»˜ããƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›ã€‚
    UI æœ‰ç„¡ã«é–¢ä¿‚ãªãã€å®Œå…¨ãªå®Ÿè¡Œãƒ­ã‚°ã‚’å¸¸ã«ãƒ•ã‚¡ã‚¤ãƒ«ã¸æ®‹ã™ã€‚
    """
    logger = logging.getLogger("today_signals")
    logger.setLevel(logging.INFO)
    # ãƒ«ãƒ¼ãƒˆãƒ­ã‚¬ãƒ¼ã¸ã®ä¼æ’­ã‚’æ­¢ã‚ã¦é‡è¤‡å‡ºåŠ›ã‚’é˜²æ­¢
    try:
        logger.propagate = False
    except Exception:
        pass
    # ãƒ«ãƒ¼ãƒˆãƒ­ã‚¬ãƒ¼ã¸ã®ä¼æ’­ã‚’æ­¢ã‚ã€ã‚³ãƒ³ã‚½ãƒ¼ãƒ«äºŒé‡å‡ºåŠ›ã‚’é˜²æ­¢
    try:
        logger.propagate = False
    except Exception:
        pass
    # ç›®æ¨™ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æ±ºå®š
    try:
        # ç’°å¢ƒå¤‰æ•°ã§ã‚‚æ—¥ä»˜åˆ¥ãƒ­ã‚°æŒ‡å®šã‚’è¨±å¯ï¼ˆUI å®Ÿè¡Œãªã© main() ã‚’çµŒãªã„å ´åˆï¼‰
        if globals().get("_LOG_FILE_PATH") is None:
            try:
                _mode_env = (os.environ.get("TODAY_SIGNALS_LOG_MODE") or "").strip().lower()
                if _mode_env == "dated":
                    try:
                        _jst_now = datetime.now(ZoneInfo("Asia/Tokyo"))
                    except Exception:
                        _jst_now = datetime.now()
                    _stamp = _jst_now.strftime("%Y%m%d_%H%M")
                    try:
                        settings = get_settings(create_dirs=True)
                        _log_dir = Path(settings.LOGS_DIR)
                    except Exception:
                        _log_dir = Path("logs")
                    try:
                        _log_dir.mkdir(parents=True, exist_ok=True)
                    except Exception:
                        pass
                    globals()["_LOG_FILE_PATH"] = _log_dir / f"today_signals_{_stamp}.log"
            except Exception:
                pass

        if globals().get("_LOG_FILE_PATH") is not None:
            log_path = globals().get("_LOG_FILE_PATH")
        else:
            try:
                settings = get_settings(create_dirs=True)
                log_dir = Path(settings.LOGS_DIR)
            except Exception:
                log_dir = Path("logs")
            try:
                log_dir.mkdir(parents=True, exist_ok=True)
            except Exception:
                pass
            log_path = log_dir / "today_signals.log"
    except Exception:
        log_path = Path("logs") / "today_signals.log"

    # æ—¢å­˜ã®åŒä¸€ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãŒã‚ã‚‹ã‹ç¢ºèª
    has_handler = False
    for h in list(logger.handlers):
        try:
            if isinstance(h, logging.FileHandler):
                base = getattr(h, "baseFilename", None)
                if base:
                    if Path(base).resolve() == Path(str(log_path)).resolve():
                        has_handler = True
                        break
        except Exception:
            continue
    if not has_handler:
        try:
            fh = logging.FileHandler(str(log_path), encoding="utf-8")
            fmt = logging.Formatter("%(asctime)s [%(levelname)s] %(message)s", "%Y-%m-%d %H:%M:%S")
            fh.setFormatter(fmt)
            logger.addHandler(fh)
        except Exception:
            pass
    return logger


def _emit_ui_log(message: str) -> None:
    """UI ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¸ãƒ­ã‚°ã‚’é€ä¿¡ã€‚

    ç’°å¢ƒå¤‰æ•° `STRUCTURED_UI_LOGS=1` ã®å ´åˆã¯ JSON æ–‡å­—åˆ—ã‚’é€ã‚Šã€
    `{"ts": epoch_ms, "iso": iso8601, "msg": message}` å½¢å¼ã«ã™ã‚‹ã€‚
    æ—¢å­˜ãƒ†ã‚¹ãƒˆäº’æ›ã®ãŸã‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å¾“æ¥ã®ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€‚
    """
    # 1) ãƒ•ãƒ©ã‚°åˆ¤å®šï¼ˆUIæ§‹é€ åŒ– ã¨ NDJSONï¼‰
    try:
        structured_ui = (os.environ.get("STRUCTURED_UI_LOGS") or "").lower() in {
            "1",
            "true",
            "yes",
        }
    except Exception:
        structured_ui = False
    try:
        ndjson_flag = (os.environ.get("STRUCTURED_LOG_NDJSON") or "").lower() in {
            "1",
            "true",
            "yes",
        }
    except Exception:
        ndjson_flag = False

    obj = None
    json_payload = None
    if structured_ui or ndjson_flag:
        try:
            import json as _json
            import re as _re
            import time as _t

            # é–‹å§‹åŸºæº–æ™‚åˆ»ï¼ˆãƒ—ãƒ­ã‚»ã‚¹èµ·å‹•å¾Œæœ€åˆã®å‘¼ã³å‡ºã—ã§åˆæœŸåŒ–ï¼‰
            global _STRUCTURED_LOG_START_TS
            if _STRUCTURED_LOG_START_TS is None:
                _STRUCTURED_LOG_START_TS = _t.time()
            now = _t.time()
            iso = datetime.utcfromtimestamp(now).strftime("%Y-%m-%dT%H:%M:%S.%fZ")
            # elapsed_ms was unused; keep timestamp in iso only
            raw_msg = str(message)
            lower = raw_msg.lower()
            # system æŠ½å‡º: System1..System7 (å¤§æ–‡å­—å°æ–‡å­—ãã®ã¾ã¾æƒ³å®š)
            m_sys = _re.search(r"\bSystem([1-9]|1[0-9])\b", raw_msg)
            system = f"system{m_sys.group(1)}" if m_sys else None

            # phase ãƒãƒƒãƒè¾æ›¸ (é †åºé‡è¦: ã‚ˆã‚Šç‰¹æ®Šãªèªã‚’å‰ã«)
            phase_patterns = [
                ("universe", [r"universe", r"load symbols", r"symbol universe"]),
                ("indicators", [r"indicator", r"precompute", r"adx", r"rsi"]),
                ("filter", [r"filter", r"phase2 filter", r"screening"]),
                ("setup", [r"setup", r"prepare setup"]),
                ("ranking", [r"ranking", r"rank "]),
                ("signals", [r" signal", r"signals", r"generate signal"]),
                (
                    "allocation",
                    [r"allocation", r"alloc ", r"allocating", r"final allocation"],
                ),
            ]
            phase = None
            for ph, pats in phase_patterns:
                if any(pat in lower for pat in pats):
                    phase = ph
                    break

            # é–‹å§‹/çµ‚äº†ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ¨å®š
            phase_status = None
            if phase:
                if _re.search(r"\b(start|begin|é–‹å§‹)\b", lower):
                    phase_status = "start"
                elif _re.search(r"\b(done|complete|completed|çµ‚äº†|end|finished)\b", lower):
                    phase_status = "end"

            # å‰å› phase ã®è£œå¼·: system å˜ä½ã§ç›´å‰ phase ã‚’è¦šãˆã€end/done ã ã‘ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ã‚‚ä»˜ä¸
            global _STRUCTURED_LAST_PHASE
            if _STRUCTURED_LAST_PHASE is None:
                _STRUCTURED_LAST_PHASE = {}
            if system:
                if phase:
                    _STRUCTURED_LAST_PHASE[system] = phase
                else:
                    # æ˜ç¤º phase ãªã— ã‹ã¤ done/complete èªãŒã‚ã‚Œã°ç›´å‰ã‚’å‚ç…§
                    if _re.search(r"\b(done|complete|completed|çµ‚äº†|end|finished)\b", lower):
                        last = _STRUCTURED_LAST_PHASE.get(system)
                        if last:
                            phase = last
                            phase_status = phase_status or "end"

            # v: ã‚¹ã‚­ãƒ¼ãƒãƒãƒ¼ã‚¸ãƒ§ãƒ³ / lvl: å°†æ¥ã®ãƒ¬ãƒ™ãƒ«æ‹¡å¼µ (ç¾çŠ¶ INFO å›ºå®š)
            obj = {
                "v": 1,
                "ts": int(now * 1000),
                "iso": iso,
                "lvl": "INFO",
                "msg": raw_msg,
            }
            if system:
                obj["system"] = system
            if phase:
                obj["phase"] = phase
            if phase_status:
                obj["phase_status"] = phase_status
            if structured_ui:
                json_payload = _json.dumps(obj)
        except Exception:
            obj = None
            json_payload = None

    # 2) NDJSON æ›¸ãå‡ºã—ï¼ˆUIã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯æœ‰ç„¡ã«é–¢ä¿‚ãªãï¼‰
    if ndjson_flag and obj is not None:
        try:
            from common.structured_log_ndjson import maybe_init_global_writer

            writer = maybe_init_global_writer()
            if writer:
                writer.write(obj)
        except Exception:
            pass

    # 3) UI ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¸é€ä¿¡ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ï¼‰
    try:
        cb = globals().get("_LOG_CALLBACK")
    except Exception:
        cb = None
    if not (cb and callable(cb)):
        return

    payload = json_payload if (structured_ui and json_payload) else str(message)
    try:
        token = _LOG_FORWARDING.set(True)
        try:
            cb(payload)
        finally:
            _LOG_FORWARDING.reset(token)
    except Exception:
        pass


def _drain_stage_event_queue() -> None:
    """ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰ã§ã‚¹ãƒ†ãƒ¼ã‚¸é€²æ—ã‚¤ãƒ™ãƒ³ãƒˆã‚’å‡¦ç†ã—ã€UI è¡¨ç¤ºã‚’æ›´æ–°ã™ã‚‹ã€‚"""

    try:
        cb2 = globals().get("_PER_SYSTEM_STAGE")
    except Exception:
        cb2 = None

    def _normalize_stage_value(value: object | None) -> int | None:
        """å€¤ã‚’ int ã«å®‰å…¨å¤‰æ›ã€‚æ–‡å­—åˆ—/æ•°å€¤ä»¥å¤–ã¯ Noneã€‚

        mypy: object ã‹ã‚‰ç›´æ¥ int(value) ã™ã‚‹ã¨ overload ä¸ä¸€è‡´ã«ãªã‚‹ãŸã‚
        å‹åˆ†å²ã‚’æ˜ç¢ºåŒ–ã—ã¦ Any åŒ–ã‚’é¿ã‘ã‚‹ã€‚
        """
        if value is None:
            return None
        # æ—¢ã« int
        if isinstance(value, int):
            return value
        # bool ã¯ int ã®ã‚µãƒ–ã‚¯ãƒ©ã‚¹ãªã®ã§é™¤å¤–ï¼ˆé€²æ—å€¤ã«ä½¿ã‚ãªã„ï¼‰
        if isinstance(value, bool):
            return int(value)
        # float -> åˆ‡ã‚Šæ¨ã¦ (æ„å›³çš„)
        if isinstance(value, float):
            try:
                return int(value)
            except Exception:
                return None
        # æ–‡å­—åˆ—ã¯ç©ºç™½é™¤å»å¾Œ æ•°å€¤åˆ¤å®š
        if isinstance(value, str):
            txt = value.strip()
            if not txt:
                return None
            # ã¾ãšæ•´æ•°è¡¨ç¾
            if txt.isdigit() or (txt[0] == "-" and txt[1:].isdigit()):
                try:
                    return int(txt)
                except Exception:
                    return None
            # float è¡¨ç¾ã‚’è¨±å®¹
            try:
                fl = float(txt)
                return int(fl)
            except Exception:
                return None
        # ãã®ä»–ã®å‹ã¯æœªå¯¾å¿œï¼ˆmypyæ•´åˆã®ãŸã‚ç„¡å¤‰æ›ï¼‰
        return None

    events: list[StageEvent] = []

    queue_obj = globals().get("_PROGRESS_QUEUE")
    if queue_obj is not None:
        while True:
            try:
                item = queue_obj.get_nowait()
            except Exception:
                break
            if not isinstance(item, (list, tuple)) or len(item) < 2:
                continue
            system = str(item[0] or "").strip().lower() or "unknown"
            try:
                progress = int(item[1])
            except Exception:
                progress = 0
            filter_count = _normalize_stage_value(item[2] if len(item) > 2 else None)
            setup_count = _normalize_stage_value(item[3] if len(item) > 3 else None)
            candidate_count = _normalize_stage_value(item[4] if len(item) > 4 else None)
            entry_count = _normalize_stage_value(item[5] if len(item) > 5 else None)
            try:
                GLOBAL_STAGE_METRICS.record_stage(
                    system,
                    progress,
                    filter_count,
                    setup_count,
                    candidate_count,
                    entry_count,
                    emit_event=False,
                )
            except Exception:
                continue
            events.append(
                StageEvent(
                    system,
                    progress,
                    filter_count,
                    setup_count,
                    candidate_count,
                    entry_count,
                )
            )

    try:
        events.extend(GLOBAL_STAGE_METRICS.drain_events())
    except Exception:
        pass

    if not events:
        return

    if not cb2 or not callable(cb2):
        return

    for event in events:
        try:
            cb2(
                event.system,
                event.progress,
                event.filter_count,
                event.setup_count,
                event.candidate_count,
                event.entry_count,
            )
        except Exception:
            continue


def _get_stage_snapshot(system: str) -> StageSnapshot | None:
    try:
        return GLOBAL_STAGE_METRICS.get_snapshot(system)
    except Exception:
        return None


def _log(
    msg: str,
    ui: bool = True,
    no_timestamp: bool = False,
    phase_id: str | None = None,
    level: str = "INFO",
    error_code: str | None = None,
) -> None:
    """CLI å‡ºåŠ›ã«ã¯ [HH:MM:SS | måˆ†sç§’] ã‚’ä»˜ä¸ã€‚å¿…è¦ã«å¿œã˜ã¦ UI ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’æŠ‘åˆ¶ã€‚

    Args:
        msg: ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        ui: UIè¡¨ç¤ºãƒ•ãƒ©ã‚°
        no_timestamp: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ç„¡åŠ¹åŒ–ãƒ•ãƒ©ã‚°
        phase_id: ãƒ•ã‚§ãƒ¼ã‚ºID
        level: ãƒ­ã‚°ãƒ¬ãƒ™ãƒ« (INFO, WARNING, ERROR, DEBUG)
        error_code: ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰ (ã‚¨ãƒ©ãƒ¼æ™‚ã«æŒ‡å®š)
    """
    import time as _t

    # åˆå›å‘¼ã³å‡ºã—ã§é–‹å§‹æ™‚åˆ»ã‚’è¨­å®š
    try:
        global _LOG_START_TS
        if _LOG_START_TS is None:
            _LOG_START_TS = _t.time()
    except Exception:
        _LOG_START_TS = None

    # ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’ä½œæˆï¼ˆç¾åœ¨æ™‚åˆ» + åˆ†ç§’çµŒé + ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰ï¼‰
    try:
        if no_timestamp:
            prefix = ""
        else:
            now = _t.strftime("%H:%M:%S")
            elapsed = 0 if _LOG_START_TS is None else max(0, _t.time() - _LOG_START_TS)
            m, s = divmod(int(elapsed), 60)
            # ç§’ã¯2æ¡ã‚¼ãƒ­åŸ‹ã‚ã§æ•´å½¢ï¼ˆä¾‹: 0åˆ†05ç§’ï¼‰
            prefix = f"[{now} | {m}åˆ†{s:02d}ç§’] "

        # ã‚¨ãƒ©ãƒ¼ãƒ¬ãƒ™ãƒ«ã¨ã‚³ãƒ¼ãƒ‰ã‚’å«ã‚€ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
        if level != "INFO":
            prefix += f"[{level}] "
        if error_code:
            prefix += f"[{error_code}] "
    except Exception:
        prefix = ""

    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ã‚ˆã‚‹é™¤å¤–åˆ¤å®šï¼ˆå…¨ä½“ï¼‰
    try:
        # SHOW_INDICATOR_LOGS ãŒçœŸã§ãªã„é™ã‚Šã€ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼ç³»ã®é€²æ—ãƒ­ã‚°ã‚’æŠ‘åˆ¶
        _show_ind_logs = (os.environ.get("SHOW_INDICATOR_LOGS") or "").strip().lower()
        _hide_indicator_logs = _show_ind_logs not in {"1", "true", "yes", "on"}
        _indicator_skip = (
            "ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼è¨ˆç®—",
            "æŒ‡æ¨™è¨ˆç®—",
            "å…±æœ‰æŒ‡æ¨™",
            "æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰",
            "ğŸ“Š æŒ‡æ¨™è¨ˆç®—",
            "ğŸ§® å…±æœ‰æŒ‡æ¨™",
        )
        _skip_all = _GLOBAL_SKIP_KEYWORDS + (_indicator_skip if _hide_indicator_logs else ())
        if any(k in str(msg) for k in _skip_all):
            return
        ui_allowed = ui and not any(k in str(msg) for k in _UI_ONLY_SKIP_KEYWORDS)
    except Exception:
        ui_allowed = ui

    # CLI ã¸ã¯æ•´å½¢ã—ã¦å‡ºåŠ›ï¼ˆéUTF-8ç«¯æœ«ã§ã¯çµµæ–‡å­—ç­‰ã‚’å®‰å…¨åŒ–ï¼‰
    try:
        display_msg = str(msg)
        if _NO_EMOJI_ENV or not _console_supports_utf8():
            try:
                import unicodedata as _ud

                display_msg = _strip_emojis(display_msg)
                display_msg = _ud.normalize("NFKC", display_msg)
            except Exception:
                display_msg = _strip_emojis(display_msg)
    except Exception:
        display_msg = str(msg)
    out = f"{prefix}{display_msg}"
    try:
        print(out, flush=True)
    except UnicodeEncodeError:
        try:
            encoding = getattr(sys.stdout, "encoding", "") or "utf-8"
            safe = out.encode(encoding, errors="replace").decode(encoding, errors="replace")
            print(safe, flush=True)
        except Exception:
            try:
                safe = out.encode("ascii", errors="replace").decode("ascii", errors="replace")
                print(safe, flush=True)
            except Exception:
                pass

    # UI å´ã¸ã®é€šçŸ¥
    if ui_allowed:
        try:
            _emit_ui_log(str(msg))
        except Exception:
            pass

    # ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒ­ã‚°ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
    try:
        logger = _get_today_logger()
        log_msg = str(msg)
        if error_code:
            log_msg = f"[{error_code}] {log_msg}"
        if level == "ERROR":
            logger.error(log_msg)
        elif level == "WARNING":
            logger.warning(log_msg)
        elif level == "DEBUG":
            logger.debug(log_msg)
        else:
            logger.info(log_msg)
    except Exception:
        pass


class _PerfTimer:
    """è»½é‡ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆæ¸¬ (ç’°å¢ƒå¤‰æ•° ENABLE_STEP_TIMINGS=1 ã®æ™‚ã®ã¿æœ‰åŠ¹)"""

    def __init__(self, label: str, level: str = "DEBUG") -> None:
        self.label = label
        self.level = level
        self.enabled = (os.environ.get("ENABLE_STEP_TIMINGS") or "").lower() in {
            "1",
            "true",
            "yes",
        }
        self._t0: float | None = None

    def __enter__(self):  # noqa: D401
        if self.enabled:
            try:
                import time as _t

                self._t0 = _t.perf_counter()
            except Exception:
                self.enabled = False
        return self

    def __exit__(self, exc_type, exc, tb):  # noqa: D401
        if not self.enabled or self._t0 is None:
            return False
        try:
            import time as _t

            dt = _t.perf_counter() - self._t0
            _log(f"â± {self.label} {dt*1000:.1f}ms", ui=False, level=self.level)
        except Exception:
            pass
        return False


def _log_error(msg: str, error_code: str, ui: bool = True, phase_id: str | None = None) -> None:
    """ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã®ç°¡ä¾¿é–¢æ•°ã€‚"""
    _log(msg, ui=ui, phase_id=phase_id, level="ERROR", error_code=error_code)


def _log_warning(
    msg: str,
    error_code: str | None = None,
    ui: bool = True,
    phase_id: str | None = None,
) -> None:
    """è­¦å‘Šãƒ­ã‚°ã®ç°¡ä¾¿é–¢æ•°ã€‚"""
    _log(msg, ui=ui, phase_id=phase_id, level="WARNING", error_code=error_code)


def _asc_by_score_key(score_key: str | None) -> bool:
    return bool(score_key and score_key.upper() in {"RSI4"})


_SYSTEM1_REASON_LABELS = {
    "filter": "ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ (filter)",
    "setup": "ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ¡ä»¶ (setup)",
    "roc200": "ROC200â‰¤0",
}


def _log_zero_candidate_diagnostics(
    system_name: str,
    candidate_count: int,
    diag_payload: Mapping[str, Any] | None,
) -> None:
    """Emit helpful diagnostics when a system ends up with zero candidates."""

    if str(system_name).strip().lower() != "system1":
        return
    if candidate_count != 0:
        return

    summary = summarize_system1_diagnostics(diag_payload)
    if not summary:
        return

    top_n = summary.get("top_n")
    prefix = f"æŠ½å‡ºä¸Šé™ {top_n} ä»¶, " if isinstance(top_n, int) and top_n > 0 else ""
    message_parts = [
        f"ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é€šé {summary.get('filter_pass', 0)} ä»¶",
        f"ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æˆç«‹ {summary.get('setup_flag_true', 0)} ä»¶",
        f"ä»£æ›¿åˆ¤å®šæˆç«‹ {summary.get('fallback_pass', 0)} ä»¶",
        f"ROC200>0 {summary.get('roc200_positive', 0)} ä»¶",
        f"æœ€çµ‚é€šé {summary.get('final_pass', 0)} ä»¶",
    ]
    detail_line = f"[system1] å€™è£œ0ä»¶ç†ç”±: {prefix}{', '.join(message_parts)}ã€‚"
    _log(detail_line)

    reasons = summary.get("exclude_reasons")
    if isinstance(reasons, Mapping) and reasons:
        reason_parts: list[str] = []
        for key, count in reasons.items():
            if not isinstance(count, int) or count <= 0:
                continue
            label = _SYSTEM1_REASON_LABELS.get(str(key), str(key))
            reason_parts.append(f"{label} {count} ä»¶")
        if reason_parts:
            _log("[system1] å€™è£œ0ä»¶ã®é™¤å¤–å†…è¨³: " + ", ".join(reason_parts))


def _export_diagnostics_snapshot(ctx: TodayRunContext, final_df: pd.DataFrame | None) -> None:
    """Export a minimal diagnostics snapshot (JSON) for Phase2 verification.

    - Test modes only (mini/quick/sample)
    - Output path: <RESULTS_DIR>/diagnostics_test/diagnostics_snapshot_YYYYMMDD_HHMMSS.json
    - Content: export_date, mode, systems[{system_id, diagnostics, final_candidate_count}]
    """
    try:
        mode = getattr(ctx, "test_mode", None)
    except Exception:
        mode = None
    if not mode:
        return  # production ã§ã¯å‡ºåŠ›ã—ãªã„

    try:
        settings = ctx.settings
        # test_mode ã®ã¨ãã¯ results_csv_test é…ä¸‹ã«å‡ºåŠ›ã—ã€é‹ç”¨çµæœã¨åˆ†é›¢
        if mode:
            base_dir = Path("results_csv_test")
        else:
            base_dir = Path(getattr(settings, "RESULTS_DIR", Path("results_csv")))
        out_dir = base_dir / "diagnostics_test"
        out_dir.mkdir(parents=True, exist_ok=True)

        now = datetime.now()
        stamp = now.strftime("%Y%m%d_%H%M%S")
        out_path = out_dir / f"diagnostics_snapshot_{stamp}.json"

        # per-system final candidate counts
        final_counts: dict[str, int] = {}
        try:
            if final_df is not None and not final_df.empty and "system" in final_df.columns:
                final_counts = final_df.groupby("system").size().astype(int).to_dict()
        except Exception:
            final_counts = {}

        systems_payload: list[dict[str, Any]] = []
        try:
            diag_map = getattr(ctx, "system_diagnostics", {}) or {}
            for sys_id in sorted(diag_map.keys()):
                raw_diag = diag_map.get(sys_id) or {}
                # Phase5: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é©ç”¨ã§æ¬ æå€¤ã‚’ -1 / unknown ã«æ­£è¦åŒ–
                safe_diag = get_diagnostics_with_fallback(raw_diag, sys_id)
                systems_payload.append(
                    {
                        "system_id": sys_id,
                        "diagnostics": safe_diag,
                        "final_candidate_count": int(final_counts.get(sys_id, 0)),
                    }
                )
        except Exception:
            systems_payload = []

        snapshot = {
            "export_date": now.isoformat(),
            "mode": mode,
            "systems": systems_payload,
        }

        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(snapshot, f, indent=2, ensure_ascii=False, default=str)

        _log(
            f"ğŸ§ª Diagnostics snapshot exported: {out_path.relative_to(base_dir)}",
            ui=True,
        )
    except Exception as e:
        _log_warning(f"diagnostics ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã®å‡ºåŠ›ã«å¤±æ•—: {e}", error_code="SNAP-FAIL")


def _export_discrepancy_triage(ctx: TodayRunContext) -> None:
    """Discrepancy triage çµæœã‚’ JSON ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã€‚

    - Test modes only (mini/quick/sample)
    - Output path: <RESULTS_DIR>/diagnostics_test/discrepancy_triage_YYYYMMDD_HHMMSS.json
    - Content: export_date, mode, triage_results, unexpected_systems
    """
    try:
        mode = getattr(ctx, "test_mode", None)
    except Exception:
        mode = None
    if not mode:
        return  # production ã§ã¯å‡ºåŠ›ã—ãªã„

    try:
        from common.system_diagnostics import (
            format_triage_summary,
            get_unexpected_systems,
            triage_all_systems,
        )

        settings = ctx.settings
        base_dir = Path(getattr(settings, "RESULTS_DIR", Path("results_csv")))
        out_dir = base_dir / "diagnostics_test"
        out_dir.mkdir(parents=True, exist_ok=True)

        now = datetime.now()
        stamp = now.strftime("%Y%m%d_%H%M%S")
        out_path = out_dir / f"discrepancy_triage_{stamp}.json"

        # ã‚·ã‚¹ãƒ†ãƒ è¨ºæ–­æƒ…å ±ã‚’å–å¾—
        diag_map = getattr(ctx, "system_diagnostics", {}) or {}

        # Triage å®Ÿæ–½
        triage_results = triage_all_systems(diag_map)
        unexpected = get_unexpected_systems(triage_results)

        # ã‚µãƒãƒªãƒ¼ãƒ­ã‚°å‡ºåŠ›
        summary_text = format_triage_summary(triage_results)
        _log("ğŸ“‹ Discrepancy Triage Results:")
        for line in summary_text.split("\n"):
            _log(f"  {line}")

        # JSON ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        export_payload = {
            "export_date": now.isoformat(),
            "mode": mode,
            "triage_results": triage_results,
            "unexpected_systems": unexpected,
            "summary": summary_text,
        }

        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(export_payload, f, indent=2, ensure_ascii=False, default=str)

        _log(f"ğŸ§ª Discrepancy triage exported: {out_path.relative_to(base_dir)}", ui=True)

        # Unexpected ã‚·ã‚¹ãƒ†ãƒ ãŒã‚ã‚Œã°è­¦å‘Š
        if unexpected:
            _log_warning(
                f"âš ï¸ Unexpected discrepancies detected in: {', '.join(unexpected)}",
                error_code="TRIAGE-UNEXPECTED",
            )

    except Exception as e:
        _log_warning(f"discrepancy triage ã®å‡ºåŠ›ã«å¤±æ•—: {e}", error_code="TRIAGE-FAIL")


# ãƒ­ã‚°å‡ºåŠ›ã‹ã‚‰é™¤å¤–ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
# ãƒ­ã‚°å…¨ä½“ã‹ã‚‰é™¤å¤–ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆCLI/UI å…±é€šï¼‰
# ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼è¨ˆç®—è‡ªä½“ã¯ CLI ã«å‡ºã—ãŸã„ã®ã§é™¤å¤–ã—ãªã„ã€‚
_GLOBAL_SKIP_KEYWORDS = (
    "ãƒãƒƒãƒæ™‚é–“",
    "batch time",
    # éŠ˜æŸ„ã®é•·ã„ãƒ€ãƒ³ãƒ—ã¯ CLI ã§ã‚‚éè¡¨ç¤ºã«ã™ã‚‹
    "éŠ˜æŸ„:",
)
# UI è¡¨ç¤ºã‹ã‚‰ã®ã¿é™¤å¤–ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
_UI_ONLY_SKIP_KEYWORDS = (
    "é€²æ—",
    "å€™è£œæŠ½å‡º",
    "å€™è£œæ—¥æ•°",
)


def _filter_logs(lines: list[str], ui: bool = False) -> list[str]:
    """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«åŸºã¥ã„ã¦ãƒ­ã‚°è¡Œã‚’é™¤å¤–ã™ã‚‹ã€‚

    Args:
        lines: å¯¾è±¡ãƒ­ã‚°è¡Œã®ãƒªã‚¹ãƒˆã€‚
        ui: True ã®å ´åˆã¯ UI é™å®šã®é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚‚é©ç”¨ã€‚
    """

    skip_keywords = _GLOBAL_SKIP_KEYWORDS + (_UI_ONLY_SKIP_KEYWORDS if ui else ())
    return [ln for ln in lines if not any(k in ln for k in skip_keywords)]


def _prev_counts_path(signals_dir: Path) -> Path:
    try:
        return signals_dir / "previous_per_system_counts.json"
    except Exception:
        return Path("signals/previous_per_system_counts.json")


def _load_prev_counts(signals_dir: Path) -> dict[str, int]:
    fp = _prev_counts_path(signals_dir)
    if not fp.exists():
        return {}
    try:
        data = json.loads(fp.read_text(encoding="utf-8"))
        counts = data.get("counts", {}) if isinstance(data, dict) else {}
        out: dict[str, int] = {}
        for i in range(1, 8):
            key = f"system{i}"
            try:
                out[key] = int(counts.get(key, 0))
            except Exception:
                out[key] = 0
        return out
    except Exception:
        return {}


def _save_prev_counts(signals_dir: Path, per_system_map: dict[str, pd.DataFrame]) -> None:
    try:
        counts = {
            k: (0 if (v is None or v.empty) else int(len(v))) for k, v in per_system_map.items()
        }
        data = {"timestamp": datetime.utcnow().isoformat() + "Z", "counts": counts}
        fp = _prev_counts_path(signals_dir)
        try:
            fp.parent.mkdir(parents=True, exist_ok=True)
        except Exception:
            pass
        fp.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")
    except Exception:
        pass


def _normalize_ohlcv(df: pd.DataFrame) -> pd.DataFrame:
    """åˆ—åã‚’å¤§æ–‡å­—OHLCVã«çµ±ä¸€"""
    col_map = {
        "open": "Open",
        "high": "High",
        "low": "Low",
        "close": "Close",
        "volume": "Volume",
        "adj_close": "AdjClose",
        "adjusted_close": "AdjClose",
    }
    try:
        return df.rename(columns={k: v for k, v in col_map.items() if k in df.columns})
    except Exception:
        return df


def _extract_last_cache_date(df: pd.DataFrame) -> pd.Timestamp | None:
    if df is None or getattr(df, "empty", True):
        return None
    for col in ("date", "Date"):
        if col in df.columns:
            try:
                values = pd.to_datetime(df[col].to_numpy(), errors="coerce")
                values = values[~pd.isna(values)]
                if values.size:
                    # wrap in array and extract scalar for type safety
                    ts = pd.to_datetime([values[-1]], errors="coerce")
                    if ts.size and not pd.isna(ts[0]):
                        return pd.Timestamp(ts[0]).normalize()
            except Exception:
                continue
    try:
        idx = pd.to_datetime(df.index.to_numpy(), errors="coerce")
        mask = ~pd.isna(idx)
        if mask.any():
            ts = pd.to_datetime([idx[mask][-1]], errors="coerce")
            if ts.size and not pd.isna(ts[0]):
                return pd.Timestamp(ts[0]).normalize()
    except Exception:
        pass
    return None


def _recent_trading_days(today: pd.Timestamp | None, max_back: int) -> list[pd.Timestamp]:
    if today is None:
        return []
    out: list[pd.Timestamp] = []
    seen: set[pd.Timestamp] = set()
    current = pd.Timestamp(today).normalize()
    steps = max(0, int(max_back))
    for _ in range(steps + 1):
        if current in seen:
            break
        out.append(current)
        seen.add(current)
        prev_candidate = get_latest_nyse_trading_day(current - pd.Timedelta(days=1))
        prev_candidate = pd.Timestamp(prev_candidate).normalize()
        if prev_candidate == current:
            break
        current = prev_candidate
    return out


def _build_rolling_from_base(
    symbol: str,
    base_df: pd.DataFrame,
    target_len: int,
    cache_manager: CacheManager | None = None,
) -> pd.DataFrame | None:
    """Convert base cache dataframe to rolling window and optionally persist it."""

    if base_df is None or getattr(base_df, "empty", True):
        return None
    try:
        work = base_df.copy()
    except Exception:
        work = base_df
    if work.index.name is not None:
        work = work.reset_index()
    if "Date" in work.columns:
        work["date"] = pd.to_datetime(work["Date"].to_numpy(), errors="coerce")
    elif "date" in work.columns:
        work["date"] = pd.to_datetime(work["date"].to_numpy(), errors="coerce")
    else:
        return None
    work = work.dropna(subset=["date"]).sort_values("date")
    col_map = {
        "Open": "open",
        "High": "high",
        "Low": "low",
        "Close": "close",
        "AdjClose": "adjusted_close",
        "Adj Close": "adjusted_close",
        "Volume": "volume",
    }
    try:
        for src, dst in list(col_map.items()):
            if src in work.columns:
                work = work.rename(columns={src: dst})
    except Exception:
        pass
    sliced = work.tail(int(target_len)).reset_index(drop=True)
    if sliced.empty:
        return None
    if cache_manager is not None:
        try:
            cache_manager.write_atomic(sliced, symbol, "rolling")
        except Exception:
            pass
    return sliced


def _load_basic_data(
    symbols: list[str],
    cache_manager: CacheManager,
    settings: Any,
    symbol_data: dict[str, pd.DataFrame] | None,
    *,
    today: pd.Timestamp | None = None,
    freshness_tolerance: int | None = None,
    _base_cache: dict[str, pd.DataFrame] | None = None,
) -> dict[str, pd.DataFrame]:
    from time import perf_counter

    data: dict[str, pd.DataFrame] = {}
    total_syms = len(symbols)
    start_ts = perf_counter()
    chunk = 500

    if freshness_tolerance is None:
        try:
            freshness_tolerance = int(settings.cache.rolling.max_staleness_days)
        except Exception:
            freshness_tolerance = 2
    freshness_tolerance = max(0, int(freshness_tolerance))

    try:
        target_len = int(
            settings.cache.rolling.base_lookback_days + settings.cache.rolling.buffer_days
        )
    except Exception:
        target_len = 0

    stats_lock = Lock()
    stats: dict[str, int] = {}

    def _record_stat(key: str) -> None:
        with stats_lock:
            stats[key] = stats.get(key, 0) + 1

    recent_allowed: set[pd.Timestamp] = set()
    if today is not None and freshness_tolerance >= 0:
        try:
            recent_allowed = {
                pd.Timestamp(d).normalize()
                for d in _recent_trading_days(pd.Timestamp(today), freshness_tolerance)
            }
        except Exception:
            recent_allowed = set()

    if recent_allowed:
        try:
            _ = min(recent_allowed)
        except Exception:
            pass

    gap_probe_days = max(freshness_tolerance + 5, 10)

    def _estimate_gap_days(
        today_dt: pd.Timestamp | None, last_dt: pd.Timestamp | None
    ) -> int | None:
        if today_dt is None or last_dt is None:
            return None
        try:
            recent = _recent_trading_days(pd.Timestamp(today_dt), gap_probe_days)
        except Exception:
            recent = []
        for offset, dt in enumerate(recent):
            if dt == last_dt:
                return offset
        try:
            return max(0, int((pd.Timestamp(today_dt) - pd.Timestamp(last_dt)).days))
        except Exception:
            return None

    def _pick_symbol_data(sym: str) -> pd.DataFrame | None:
        try:
            if not symbol_data or sym not in symbol_data:
                return None
            df = symbol_data.get(sym)
            if df is None or getattr(df, "empty", True):
                return None
            x = df.copy()
            if x.index.name is not None:
                x = x.reset_index()
            if "date" in x.columns:
                x["date"] = pd.to_datetime(x["date"].to_numpy(), errors="coerce")
            elif "Date" in x.columns:
                x["date"] = pd.to_datetime(x["Date"].to_numpy(), errors="coerce")
            else:
                return None
            col_map = {
                "Open": "open",
                "High": "high",
                "Low": "low",
                "Close": "close",
                "Adj Close": "adjusted_close",
                "AdjClose": "adjusted_close",
                "Volume": "volume",
            }
            for k, v in list(col_map.items()):
                if k in x.columns:
                    x = x.rename(columns={k: v})
            required = {"date", "close"}
            if not required.issubset(set(x.columns)):
                return None
            x = x.dropna(subset=["date"]).sort_values("date")
            return x
        except Exception:
            return None

    def _normalize_loaded(df: pd.DataFrame | None) -> pd.DataFrame | None:
        if df is None or getattr(df, "empty", True):
            return None
        try:
            if "Date" not in df.columns:
                work = df.copy()
                if "date" in work.columns:
                    work["Date"] = pd.to_datetime(work["date"].to_numpy(), errors="coerce")
                else:
                    work["Date"] = pd.to_datetime(work.index.to_numpy(), errors="coerce")
                df = work
            df["Date"] = pd.to_datetime(df["Date"].to_numpy(), errors="coerce").normalize()
        except Exception:
            pass
        normalized = _normalize_ohlcv(df)
        try:
            fill_cols = [
                c for c in ("Open", "High", "Low", "Close", "Volume") if c in normalized.columns
            ]
            if fill_cols:
                normalized = normalized.copy()
                try:
                    filled = normalized[fill_cols].apply(pd.to_numeric, errors="coerce")
                except Exception:
                    filled = normalized[fill_cols]
                normalized.loc[:, fill_cols] = filled.ffill().bfill()
        except Exception:
            pass
        try:
            if "Date" in normalized.columns:
                normalized = normalized.dropna(subset=["Date"])
        except Exception:
            pass
        return normalized

    env_parallel = (os.environ.get("BASIC_DATA_PARALLEL", "") or "").strip().lower()
    try:
        env_parallel_threshold = int(os.environ.get("BASIC_DATA_PARALLEL_THRESHOLD", "200"))
    except Exception:
        env_parallel_threshold = 200
    if env_parallel in ("1", "true", "yes"):
        use_parallel = total_syms > 1
    elif env_parallel in ("0", "false", "no"):
        use_parallel = False
    else:
        use_parallel = total_syms >= max(0, env_parallel_threshold)

    max_workers: int | None = None
    if use_parallel and total_syms > 0:
        try:
            env_workers = (os.environ.get("BASIC_DATA_MAX_WORKERS", "") or "").strip()
            if env_workers:
                max_workers = int(env_workers)
        except Exception:
            max_workers = None
        if max_workers is None:
            try:
                cfg_workers = getattr(settings.cache.rolling, "load_max_workers", None)
                if cfg_workers:
                    max_workers = int(cfg_workers)
            except Exception:
                pass
        if max_workers is None:
            cpu_count = os.cpu_count() or 4
            max_workers = max(4, cpu_count * 2)
        max_workers = max(1, min(int(max_workers), total_syms))
        try:
            _log(f"ğŸ§µ åŸºç¤ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ä¸¦åˆ—åŒ–: workers={max_workers}")
        except Exception:
            pass

    def _load_one(sym: str) -> tuple[str, pd.DataFrame | None]:
        try:
            source: str | None = None
            df = _pick_symbol_data(sym)
            rebuild_reason: str | None = None
            last_seen_date: pd.Timestamp | None = None
            gap_days: int | None = None
            if df is None or getattr(df, "empty", True):
                df = cache_manager.read(sym, "rolling")
            else:
                source = "prefetched"
            if df is None or getattr(df, "empty", True):
                source = None
            if df is None or getattr(df, "empty", True):
                needs_rebuild = True
            else:
                needs_rebuild = False
            if df is not None and not getattr(df, "empty", True) and source is None:
                source = "rolling"
            if df is not None and not getattr(df, "empty", True):
                # ãƒ‡ãƒ¼ã‚¿é•·ã•ãƒã‚§ãƒƒã‚¯ã‚’è¿½åŠ 
                if len(df) < target_len:
                    if len(df) < 100:  # æ˜ã‚‰ã‹ã«æ–°è¦ä¸Šå ´
                        _log(
                            f"ğŸ“Š æ–°è¦ä¸Šå ´éŠ˜æŸ„ {sym}: len={len(df)}/{target_len} (æ­£å¸¸)",
                            ui=False,
                        )
                        # çŸ­ã„ãƒ‡ãƒ¼ã‚¿ã§ã‚‚å‡¦ç†ã‚’ç¶™ç¶šï¼ˆrebuildã—ãªã„ï¼‰
                    else:
                        rebuild_reason = "length"
                        needs_rebuild = True
                last_seen_date = _extract_last_cache_date(df)
                if last_seen_date is None:
                    rebuild_reason = rebuild_reason or "missing_date"
                    needs_rebuild = True
                else:
                    last_seen_date = pd.Timestamp(last_seen_date).normalize()
                    if (
                        today is not None
                        and recent_allowed
                        and last_seen_date not in recent_allowed
                    ):
                        rebuild_reason = "stale"
                        gap_days = _estimate_gap_days(pd.Timestamp(today), last_seen_date)
                        # æ—¥ä»˜ãŒå¤ã„ãŒãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã€è­¦å‘Šã®ã¿ã§å‡¦ç†ã‚’ç¶™ç¶š
                        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ®µéšã§å„ã‚·ã‚¹ãƒ†ãƒ ãŒå¿…è¦ãªæ¡ä»¶ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹
                        _log(
                            f"âš ï¸ ãƒ‡ãƒ¼ã‚¿é®®åº¦æ³¨æ„: {sym} (æœ€çµ‚æ—¥={last_seen_date.date()}, ã‚®ãƒ£ãƒƒãƒ—={gap_days if gap_days else 'ä¸æ˜'}å–¶æ¥­æ—¥)",
                            ui=False,
                        )
                        # needs_rebuild = True  # ã“ã®è¡Œã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ã¦é™¤å¤–ã‚’å›é¿
            if needs_rebuild:
                # å€‹åˆ¥ãƒ­ã‚°ã‚’æŠ‘åˆ¶ï¼ˆã‚µãƒãƒªãƒ¼è¡¨ç¤ºã«çµ±åˆï¼‰
                _record_stat("manual_rebuild_required")
                _record_stat("failed")
                return sym, None
            normalized = _normalize_loaded(df)
            if normalized is not None and not getattr(normalized, "empty", True):
                _record_stat(source or "rolling")
                return sym, normalized
            _record_stat("failed")
            return sym, None
        except Exception:
            _record_stat("failed")
            return sym, None

    def _report_progress(done: int) -> None:
        if done <= 0 or chunk <= 0:
            return
        if done % chunk != 0:
            return
        try:
            elapsed = max(0.001, perf_counter() - start_ts)
            rate = done / elapsed
            remain = max(0, total_syms - done)
            eta_sec = int(remain / rate) if rate > 0 else 0
            m, s = divmod(eta_sec, 60)
            # å›ºå®šå¹…æ•´å½¢ï¼ˆæ¡æ•°æºã‚Œå¯¾ç­–ï¼‰
            w = max(1, len(str(total_syms)))
            cur_s = f"{done:>{w}d}"
            tot_s = f"{total_syms:>{w}d}"
            mm = f"{m:02d}"
            ss = f"{s:02d}"
            msg = f"ğŸ“¦ åŸºç¤ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰é€²æ—: {cur_s}/{tot_s} | ETA {mm}åˆ†{ss}ç§’"

            # é€²æ—ãƒ­ã‚°ã¯DEBUGãƒ¬ãƒ™ãƒ«ã§ãƒ¬ãƒ¼ãƒˆåˆ¶é™é©ç”¨
            rate_logger = _get_rate_limited_logger()
            rate_logger.debug_rate_limited(
                f"ğŸ“¦ åŸºç¤ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰é€²æ—: {cur_s}/{tot_s}",
                interval=2.0,
                message_key="åŸºç¤ãƒ‡ãƒ¼ã‚¿é€²æ—",
            )
            _emit_ui_log(msg)
        except Exception:
            try:
                w = max(1, len(str(total_syms)))
                cur_s = f"{done:>{w}d}"
                tot_s = f"{total_syms:>{w}d}"
            except Exception:
                cur_s, tot_s = str(done), str(total_syms)
            _log(f"ğŸ“¦ åŸºç¤ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰é€²æ—: {cur_s}/{tot_s}", ui=False)
            _emit_ui_log(f"ğŸ“¦ åŸºç¤ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰é€²æ—: {cur_s}/{tot_s}")

    processed = 0
    if use_parallel and max_workers and total_syms > 1:
        # æ–°ã—ã„ä¸¦åˆ—ãƒãƒƒãƒèª­ã¿è¾¼ã¿ã‚’ä½¿ç”¨ï¼ˆPhase2æœ€é©åŒ–ï¼‰
        try:
            _log(f"ğŸš€ ä¸¦åˆ—ãƒãƒƒãƒèª­ã¿è¾¼ã¿é–‹å§‹: {total_syms}ã‚·ãƒ³ãƒœãƒ«, workers={max_workers}")

            def progress_callback_internal(loaded, _total):
                nonlocal processed
                processed = loaded
                _report_progress(processed)

            # CacheManagerã®ä¸¦åˆ—èª­ã¿è¾¼ã¿æ©Ÿèƒ½ã‚’æ´»ç”¨
            parallel_data = cache_manager.read_batch_parallel(
                symbols=symbols,
                profile="rolling",
                max_workers=max_workers,
                fallback_profile="full",
                progress_callback=progress_callback_internal,
            )

            # çµæœã‚’æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«åˆã‚ã›ã¦å‡¦ç†
            for sym, df in parallel_data.items():
                if df is not None and not getattr(df, "empty", True):
                    # æ—¢å­˜ã®_normalize_loadedã¨åŒæ§˜ã®å‡¦ç†ã‚’é©ç”¨
                    normalized = _normalize_loaded(df)
                    if normalized is not None and not getattr(normalized, "empty", True):
                        data[sym] = normalized
                        _record_stat("rolling")
                    else:
                        _record_stat("failed")
                else:
                    _record_stat("failed")

            _log(f"âœ… ä¸¦åˆ—ãƒãƒƒãƒèª­ã¿è¾¼ã¿å®Œäº†: {len(data)}/{total_syms}ä»¶æˆåŠŸ")

        except Exception as e:
            # ä¸¦åˆ—å‡¦ç†å¤±æ•—æ™‚ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            _log(f"âš ï¸ ä¸¦åˆ—ãƒãƒƒãƒèª­ã¿è¾¼ã¿å¤±æ•—ã€å¾“æ¥å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {e}")
            data.clear()
            processed = 0
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = {executor.submit(_load_one, sym): sym for sym in symbols}
                for fut in as_completed(futures):
                    try:
                        sym, df = fut.result()
                    except Exception:
                        sym, df = futures[fut], None
                    if df is not None and not getattr(df, "empty", True):
                        data[sym] = df
                    processed += 1
                    _report_progress(processed)
    else:
        for sym in symbols:
            sym, df = _load_one(sym)
            if df is not None and not getattr(df, "empty", True):
                data[sym] = df
            processed += 1
            _report_progress(processed)

    try:
        total_elapsed = max(0.0, perf_counter() - start_ts)
        total_int = int(total_elapsed)
        m, s = divmod(total_int, 60)
        done_msg = f"ğŸ“¦ åŸºç¤ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰å®Œäº†: {len(data)}/{total_syms} | æ‰€è¦ {m}åˆ†{s}ç§’" + (
            " | ä¸¦åˆ—=ON" if use_parallel and max_workers else " | ä¸¦åˆ—=OFF"
        )
        _log(done_msg)
        _emit_ui_log(done_msg)
    except Exception:
        _log(f"ğŸ“¦ åŸºç¤ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰å®Œäº†: {len(data)}/{total_syms}")
        _emit_ui_log(f"ğŸ“¦ åŸºç¤ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰å®Œäº†: {len(data)}/{total_syms}")

    try:
        summary_map = {
            "prefetched": "äº‹å‰ä¾›çµ¦",
            "rolling": "rollingå†åˆ©ç”¨",
            "manual_rebuild_required": "æ‰‹å‹•å¯¾å¿œ",
            "failed": "å¤±æ•—",
        }
        summary_parts = [
            f"{label}={stats.get(key, 0)}" for key, label in summary_map.items() if stats.get(key)
        ]
        if summary_parts:
            rate_logger = _get_rate_limited_logger()
            rate_logger.debug_rate_limited(
                "ğŸ“Š åŸºç¤ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰å†…è¨³: " + " / ".join(summary_parts),
                interval=5.0,
                message_key="åŸºç¤ãƒ‡ãƒ¼ã‚¿å†…è¨³",
            )
    except Exception:
        pass

    return data


def _load_indicator_data(
    symbols: list[str],
    cache_manager: CacheManager,
    settings: Any,
    symbol_data: dict[str, pd.DataFrame] | None,
) -> dict[str, pd.DataFrame]:
    import time as _t

    data: dict[str, pd.DataFrame] = {}
    total_syms = len(symbols)
    start_ts = _t.time()
    chunk = 500
    for idx, sym in enumerate(symbols, start=1):
        try:
            df = None
            try:
                if symbol_data and sym in symbol_data:
                    df = symbol_data.get(sym)
                    if df is not None and not df.empty:
                        x = df.copy()
                        if x.index.name is not None:
                            x = x.reset_index()
                        if "date" in x.columns:
                            x["date"] = pd.to_datetime(x["date"].to_numpy(), errors="coerce")
                        elif "Date" in x.columns:
                            x["date"] = pd.to_datetime(x["Date"].to_numpy(), errors="coerce")
                        col_map = {
                            "Open": "open",
                            "High": "high",
                            "Low": "low",
                            "Close": "close",
                            "Adj Close": "adjusted_close",
                            "AdjClose": "adjusted_close",
                            "Volume": "volume",
                        }
                        for k, v in list(col_map.items()):
                            if k in x.columns:
                                x = x.rename(columns={k: v})
                        required = {"date", "close"}
                        if required.issubset(set(x.columns)):
                            x = x.dropna(subset=["date"]).sort_values("date")
                            df = x
                        else:
                            df = None
                    else:
                        df = None
            except Exception:
                df = None
            if df is None or df.empty:
                df = cache_manager.read(sym, "rolling")
            needs_rebuild = df is None or getattr(df, "empty", True)
            if needs_rebuild:
                # å€‹åˆ¥éŠ˜æŸ„ã”ã¨ã® "â›” rollingæœªæ•´å‚™" ãƒ­ã‚°ã¯å†—é•·ãªãŸã‚å®Œå…¨ã«å‰Šé™¤ã€‚
                # ãƒ«ãƒ¼ãƒ—çµ‚äº†å¾Œã®ã‚µãƒãƒªãƒ¼ãƒ­ã‚°ï¼ˆâš ï¸ rollingæœªæ•´å‚™ï¼‰ã§ä¸€æ‹¬è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚
                continue
            if df is not None and not df.empty:
                try:
                    if "Date" not in df.columns:
                        if "date" in df.columns:
                            df = df.copy()
                            df["Date"] = pd.to_datetime(df["date"].to_numpy(), errors="coerce")
                        else:
                            df = df.copy()
                            df["Date"] = pd.to_datetime(df.index.to_numpy(), errors="coerce")
                    df["Date"] = pd.to_datetime(df["Date"].to_numpy(), errors="coerce").normalize()
                except Exception:
                    pass
                df = _normalize_ohlcv(df)
                data[sym] = df
        except Exception:
            continue
        if total_syms > 0 and idx % chunk == 0:
            try:
                elapsed = max(0.001, _t.time() - start_ts)
                rate = idx / elapsed
                remain = max(0, total_syms - idx)
                eta_sec = int(remain / rate) if rate > 0 else 0
                m, s = divmod(eta_sec, 60)
                msg = f"ğŸ§® æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰é€²æ—: {idx}/{total_syms} | ETA {m}åˆ†{s}ç§’"

                # é€²æ—ãƒ­ã‚°ã¯DEBUGãƒ¬ãƒ™ãƒ«ã§ãƒ¬ãƒ¼ãƒˆåˆ¶é™é©ç”¨
                rate_logger = _get_rate_limited_logger()
                rate_logger.debug_rate_limited(
                    f"ğŸ§® æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰é€²æ—: {idx}/{total_syms}",
                    interval=2.0,
                    message_key="æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿é€²æ—",
                )
                _emit_ui_log(msg)
            except Exception:
                rate_logger = _get_rate_limited_logger()
                rate_logger.debug_rate_limited(
                    f"ğŸ§® æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰é€²æ—: {idx}/{total_syms}",
                    interval=2.0,
                    message_key="æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿é€²æ—",
                )
                _emit_ui_log(f"ğŸ§® æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰é€²æ—: {idx}/{total_syms}")
    try:
        total_elapsed = int(max(0, _t.time() - start_ts))
        m, s = divmod(total_elapsed, 60)
        done_msg = f"ğŸ§® æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰å®Œäº†: {len(data)}/{total_syms} | æ‰€è¦ {m}åˆ†{s}ç§’"
        _log(done_msg)
        _emit_ui_log(done_msg)
    except Exception:
        _log(f"ğŸ§® æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰å®Œäº†: {len(data)}/{total_syms}")
        _emit_ui_log(f"ğŸ§® æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰å®Œäº†: {len(data)}/{total_syms}")
    return data


def _subset_data(basic_data: dict[str, pd.DataFrame], keys: list[str]) -> dict[str, pd.DataFrame]:
    out = {}
    for s in keys or []:
        v = basic_data.get(s)
        if v is not None and not getattr(v, "empty", True):
            out[s] = v
    return out


def _fetch_positions_and_symbol_map() -> tuple[list[Any], dict[str, str]]:
    """Fetch Alpaca positions and cached symbol-to-system mapping once."""

    try:
        client = ba.get_client(paper=True)
        positions = list(client.get_all_positions())
    except Exception:
        positions = []

    try:
        symbol_system_map = load_symbol_system_map()
    except Exception:
        symbol_system_map = {}

    return positions, symbol_system_map


def _submit_orders(
    final_df: pd.DataFrame,
    *,
    paper: bool = True,
    order_type: str = "market",
    tif: str = "GTC",
    retries: int = 2,
    delay: float = 0.5,
) -> pd.DataFrame:
    """final_df ã‚’ã‚‚ã¨ã« Alpaca ã¸æ³¨æ–‡é€ä¿¡ï¼ˆshares å¿…é ˆï¼‰ã€‚
    è¿”ã‚Šå€¤: å®Ÿè¡Œçµæœã® DataFrameï¼ˆorder_id/status/error ã‚’å«ã‚€ï¼‰
    """
    if final_df is None or final_df.empty:
        _log("(submit) final_df is empty; skip")
        return pd.DataFrame()
    if "shares" not in final_df.columns:
        _log("(submit) shares åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚è³‡é‡‘é…åˆ†ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        return pd.DataFrame()
    try:
        client = ba.get_client(paper=paper)
    except Exception as e:
        _log(f"(submit) Alpacaæ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
        return pd.DataFrame()

    results = []
    for _, r in final_df.iterrows():
        sym = str(r.get("symbol"))
        qty = int(r.get("shares") or 0)
        side = "buy" if str(r.get("side")).lower() == "long" else "sell"
        system = str(r.get("system"))
        entry_date = r.get("entry_date")
        if not sym or qty <= 0:
            continue
        # safely parse limit price
        limit_price = None
        if order_type == "limit":
            try:
                val = r.get("entry_price")
                if val is not None and val != "":
                    limit_price = float(val)
            except Exception:
                limit_price = None
        # estimate price for notification purposes
        price_val = None
        try:
            val = r.get("entry_price")
            if val is not None and val != "":
                price_val = float(val)
        except Exception:
            price_val = None
        if limit_price is not None:
            price_val = limit_price
        try:
            order = ba.submit_order_with_retry(
                client,
                sym,
                qty,
                side=side,
                order_type=order_type,
                limit_price=limit_price,
                time_in_force=tif,
                retries=max(0, int(retries)),
                backoff_seconds=max(0.0, float(delay)),
                rate_limit_seconds=max(0.0, float(delay)),
                log_callback=_log,
            )
            results.append(
                {
                    "symbol": sym,
                    "side": side,
                    "qty": qty,
                    "price": price_val,
                    "system": system,
                    "entry_date": entry_date,
                    # Streamlit/Arrow äº’æ›ã®ãŸã‚ UUID ã‚’æ–‡å­—åˆ—åŒ–
                    "order_id": (
                        str(getattr(order, "id", ""))
                        if getattr(order, "id", None) is not None
                        else ""
                    ),
                    "status": getattr(order, "status", None),
                }
            )
        except Exception as e:
            results.append(
                {
                    "symbol": sym,
                    "side": side,
                    "qty": qty,
                    "price": price_val,
                    "system": system,
                    "entry_date": entry_date,
                    "error": str(e),
                }
            )
    if results:
        out = pd.DataFrame(results)
        # å¿µã®ãŸã‚ order_id åˆ—ãŒå­˜åœ¨ã™ã‚Œã°æ–‡å­—åˆ—åŒ–ï¼ˆä»–çµŒè·¯ã§ UUID å‹ãŒæ··ã˜ã‚‹ã®ã‚’é˜²ãï¼‰
        try:
            if "order_id" in out.columns:
                out["order_id"] = out["order_id"].apply(
                    lambda x: str(x) if x not in (None, "") else ""
                )
        except Exception:
            pass
        _log("\n=== Alpaca submission results ===")
        _log(out.to_string(index=False))
        # record entry dates for future day-based rules
        entry_map = load_entry_dates()
        for _, row in out.iterrows():
            sym = str(row.get("symbol"))
            side_val = str(row.get("side", "")).lower()
            if side_val == "buy" and row.get("entry_date"):
                entry_map[sym] = str(row["entry_date"])
            elif side_val == "sell":
                entry_map.pop(sym, None)
        save_entry_dates(entry_map)

        # Emit progress event for notification
        if ENABLE_PROGRESS_EVENTS:
            emit_progress_event(
                "notification_complete",
                {"notifications_sent": 1, "results_count": len(results)},
            )

        notifier = create_notifier(platform="auto", fallback=True)
        notifier.send_trade_report("integrated", results)
        return out
    return pd.DataFrame()


def _apply_filters(
    df: pd.DataFrame,
    *,
    only_long: bool = False,
    only_short: bool = False,
    top_per_system: int = 0,
) -> pd.DataFrame:
    if df is None or df.empty:
        return df
    out = df.copy()
    if "side" in out.columns:
        if only_long and not only_short:
            out = out[out["side"].str.lower() == "long"]
        if only_short and not only_long:
            out = out[out["side"].str.lower() == "short"]
    if top_per_system and top_per_system > 0 and "system" in out.columns:
        by = ["system"] + (["side"] if "side" in out.columns else [])
        out = out.groupby(by, as_index=False, group_keys=False).head(
            int(top_per_system)
        )  # noqa: E501
    return out


def _initialize_run_context(
    *,
    slots_long: int | None = None,
    slots_short: int | None = None,
    capital_long: float | None = None,
    capital_short: float | None = None,
    save_csv: bool = False,
    csv_name_mode: str | None = None,
    notify: bool = True,
    log_callback: Callable[[str], None] | None = None,
    progress_callback: Callable[[int, int, str], None] | None = None,
    per_system_progress: Callable[[str, str], None] | None = None,
    symbol_data: dict[str, pd.DataFrame] | None = None,
    parallel: bool = False,
    test_mode: str | None = None,
    skip_external: bool = False,
) -> TodayRunContext:
    """å½“æ—¥ã‚·ã‚°ãƒŠãƒ«å®Ÿè¡Œå‰ã«å…±æœ‰è¨­å®šãƒ»çŠ¶æ…‹ã‚’ã¾ã¨ã‚ãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã€‚"""

    # Avoid directory creation side-effects during initialization; directories
    # are expected to exist or be created lazily by CacheManager/write ops.
    settings = get_settings(create_dirs=False)
    cache_manager = CacheManager(settings)
    signals_dir = Path(settings.outputs.signals_dir)
    signals_dir.mkdir(parents=True, exist_ok=True)

    ctx = TodayRunContext(
        settings=settings,
        cache_manager=cache_manager,
        signals_dir=signals_dir,
        cache_dir=cache_manager.rolling_dir,
        slots_long=slots_long,
        slots_short=slots_short,
        capital_long=capital_long,
        capital_short=capital_short,
        save_csv=save_csv,
        csv_name_mode=csv_name_mode,
        notify=notify,
        log_callback=log_callback,
        progress_callback=progress_callback,
        per_system_progress=per_system_progress,
        symbol_data=symbol_data,
        parallel=parallel,
        test_mode=test_mode,
        skip_external=skip_external,
    )
    ctx.run_start_time = datetime.now()
    ctx.start_equity = _get_account_equity()
    try:
        import uuid as _uuid

        ctx.run_id = str(_uuid.uuid4())[:8]
    except Exception:
        ctx.run_id = "--------"
    return ctx


def _prepare_symbol_universe(ctx: TodayRunContext, initial_symbols: list[str] | None) -> list[str]:
    """Determine today's symbol universe and emit initial run banners."""

    cache_dir = ctx.cache_dir
    log_callback = ctx.log_callback
    progress_callback = ctx.progress_callback

    if initial_symbols and len(initial_symbols) > 0:
        symbols = [s.upper() for s in initial_symbols]
    else:
        from common.universe import build_universe_from_cache, load_universe_file

        settings = getattr(ctx, "settings", None)
        log = _get_today_logger()
        skip_external = getattr(ctx, "skip_external", False)

        # å…ˆã« test_symbols ãƒ¢ãƒ¼ãƒ‰ã‚’å„ªå…ˆåˆ¤å®šï¼ˆskip_external ã«ä¾å­˜ã›ãšèª­ã¿è¾¼ã‚ã‚‹ï¼‰
        fetched = []
        test_mode = getattr(ctx, "test_mode", None)
        if test_mode == "test_symbols":
            try:
                from config.settings import get_settings

                settings_local = get_settings()
                test_symbols_dir = settings_local.DATA_CACHE_DIR / "test_symbols"
                if test_symbols_dir.exists():
                    feather_files = list(test_symbols_dir.glob("*.feather"))
                    fetched = [f.stem for f in feather_files]
                    _log(
                        f"ğŸ§ª æ¶ç©ºéŠ˜æŸ„ãƒ¢ãƒ¼ãƒ‰: {len(fetched)}éŠ˜æŸ„ã‚’ä½¿ç”¨ (skip_external={skip_external})"
                    )
                else:
                    _log(f"âŒ æ¶ç©ºéŠ˜æŸ„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {test_symbols_dir}")
                    _log("å…ˆã« 'python tools/generate_test_symbols.py' ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
            except Exception as exc:
                _log(f"âŒ æ¶ç©ºéŠ˜æŸ„èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {exc}")
                fetched = []
        if not fetched:  # é€šå¸¸çµŒè·¯
            try:
                if skip_external:
                    _log("âš¡ å¤–éƒ¨APIå‘¼ã³å‡ºã—ã‚’ã‚¹ã‚­ãƒƒãƒ— - ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰éŠ˜æŸ„ãƒªã‚¹ãƒˆã‚’æ§‹ç¯‰")
                    fetched = []
                else:
                    fetched = build_symbol_universe_from_settings(settings, logger=log)
            except Exception as exc:  # pragma: no cover - ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¾‹å¤–ã®ã¿ãƒ­ã‚°
                fetched = []
                msg = f"âš ï¸ NASDAQ/EODHDéŠ˜æŸ„ãƒªã‚¹ãƒˆã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {exc}"
                _log(msg)
                if log_callback:
                    try:
                        log_callback(msg)
                    except Exception:
                        pass

        if fetched:
            limit_val: int | None = None
            limit_src = ""

            # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã®åˆ¶é™ãƒã‚§ãƒƒã‚¯
            if test_mode:
                test_limits = {"mini": 10, "quick": 50, "sample": 100}
                if test_mode in test_limits and test_mode != "test_symbols":
                    limit_val = test_limits[test_mode]
                    limit_src = f"test-mode={test_mode}"

            # ç’°å¢ƒå¤‰æ•°ã«ã‚ˆã‚‹åˆ¶é™ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ãŒæœªæŒ‡å®šã®å ´åˆï¼‰
            if limit_val is None:
                try:
                    env_limit = os.getenv("TODAY_SYMBOL_LIMIT", "").strip()
                    if env_limit:
                        parsed = int(env_limit)
                        if parsed > 0:
                            limit_val = parsed
                            limit_src = "TODAY_SYMBOL_LIMIT"
                except Exception:
                    limit_val = None

            if limit_val is not None and len(fetched) > limit_val:
                fetched = fetched[:limit_val]
                label = limit_src or "TODAY_SYMBOL_LIMIT"
                info = f"ğŸ¯ ã‚·ãƒ³ãƒœãƒ«æ•°ã‚’åˆ¶é™ ({label}={limit_val})"
                _log(info)
                if log_callback:
                    try:
                        log_callback(info)
                    except Exception:
                        pass
            symbols = [s.upper() for s in fetched]
        else:
            universe = load_universe_file()
            if not universe:
                universe = build_universe_from_cache(limit=None)
            symbols = [s.upper() for s in universe]
            if not symbols:
                try:
                    files = list(cache_dir.glob("*.*"))
                    primaries = [p.stem for p in files if p.stem.upper() == "SPY"]
                    others = sorted({p.stem for p in files if len(p.stem) <= 5})[:200]
                    symbols = list(dict.fromkeys(primaries + others))
                except Exception:
                    symbols = []

    # Ensure SPY is the first symbol in today's universe (required by some systems)
    try:
        symbols = [s.upper() for s in symbols]
    except Exception:
        symbols = [str(s).upper() for s in symbols]
    if "SPY" in symbols:
        try:
            symbols.remove("SPY")
        except Exception:
            pass
        symbols.insert(0, "SPY")
    else:
        symbols.insert(0, "SPY")
    ctx.symbol_universe = list(symbols)

    try:
        universe_total = sum(1 for s in symbols if str(s).upper() != "SPY")
    except Exception:
        universe_total = len(symbols)

    try:
        target_cb = globals().get("_SET_STAGE_UNIVERSE_TARGET")
    except Exception:
        target_cb = None
    if target_cb and callable(target_cb):
        try:
            target_cb(universe_total)
        except Exception:
            pass
    try:
        GLOBAL_STAGE_METRICS.set_universe_target(universe_total)
    except Exception:
        pass

    _log(f"ğŸ¯ å¯¾è±¡ã‚·ãƒ³ãƒœãƒ«æ•°: {len(symbols)} | éŠ˜æŸ„æ•°ï¼š{universe_total}")
    # ãƒ˜ãƒƒãƒ€ãƒ¼éƒ¨åˆ†ã«è¿½åŠ ã§éŠ˜æŸ„æ•°ã‚’è¡¨ç¤º
    _log(f"# ğŸ“Š éŠ˜æŸ„æ•°ï¼š{universe_total}", ui=False, no_timestamp=True)
    _log(f"ğŸ“‹ ã‚µãƒ³ãƒ—ãƒ«: {', '.join(symbols[:10])}" f"{'...' if len(symbols) > 10 else ''}")

    if log_callback:
        try:
            log_callback("ğŸ§­ ã‚·ãƒ³ãƒœãƒ«æ±ºå®šå®Œäº†ã€‚åŸºç¤ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ‰ã¸â€¦")
        except Exception:
            pass
    if progress_callback:
        try:
            progress_callback(1, 8, "å¯¾è±¡èª­ã¿è¾¼ã¿:start")
        except Exception:
            pass

    return symbols


def _load_universe_basic_data(ctx: TodayRunContext, symbols: list[str]) -> dict[str, pd.DataFrame]:
    """Load rolling cache data for the prepared universe and ensure coverage."""

    cache_manager = ctx.cache_manager
    settings = ctx.settings
    progress_callback = ctx.progress_callback
    symbol_data = ctx.symbol_data

    # In test modes, allow older rolling caches by widening freshness tolerance
    # to avoid skipping symbols due to staleness when validating the pipeline.
    try:
        test_mode_active = bool(getattr(ctx, "test_mode", None))
    except Exception:
        test_mode_active = False
    freshness_tolerance: int | None = None
    if test_mode_active:
        try:
            # Allow override via env; default to 365 trading days for safety in tests
            freshness_tolerance = int(os.environ.get("BASIC_DATA_TEST_FRESHNESS_TOLERANCE", "365"))
        except Exception:
            freshness_tolerance = 365
        # Informative warning to make relaxed freshness explicit during tests
        try:
            _log_warning(
                f"ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ã®é®®åº¦è¨±å®¹ã‚’ {freshness_tolerance} å–¶æ¥­æ—¥ã¸ç·©å’Œã—ã¾ã™ (rolling cache æ¤œè¨¼)",
                error_code="TST-FRESHNESS",
                ui=True,
            )
        except Exception:
            pass

    basic_data = load_basic_data(
        symbols,
        cache_manager,
        settings,
        symbol_data,
        today=ctx.today,
        freshness_tolerance=freshness_tolerance,
        base_cache=ctx.base_cache,
        log_callback=lambda msg, ui=True: None,
        ui_log_callback=lambda msg: None,
    )
    # ensure precise type for type-checker
    ctx.basic_data = cast(dict[str, pd.DataFrame], basic_data)

    if progress_callback:
        try:
            progress_callback(2, 8, "load_basic")
        except Exception:
            pass

    try:
        cov_have = len(basic_data)
        cov_total = len(symbols)
        cov_missing = max(0, cov_total - cov_have)
        _log(
            "ğŸ§® ãƒ‡ãƒ¼ã‚¿ã‚«ãƒãƒ¬ãƒƒã‚¸: "
            + f"rollingå–å¾—æ¸ˆã¿ {cov_have}/{cov_total} | missing={cov_missing}"
        )
        if cov_missing > 0:
            missing_syms = [s for s in symbols if s not in basic_data]
            # 10%ã”ã¨ã«ãƒãƒƒãƒè¡¨ç¤º
            batch_size = max(1, int(cov_total * 0.1))
            for i in range(0, len(missing_syms), batch_size):
                batch = missing_syms[i : i + batch_size]
                symbols_str = ", ".join(batch)
                _log(
                    f"âš ï¸ rollingæœªæ•´å‚™ ({i+1}ã€œ{min(i+batch_size, len(missing_syms))}/{len(missing_syms)}): {symbols_str}",
                    ui=False,
                )
            # æœ€å¾Œã«é›†è¨ˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            _log(
                f"ğŸ’¡ rollingæœªæ•´å‚™ã®è¨ˆ{cov_missing}éŠ˜æŸ„ã¯è‡ªå‹•çš„ã«ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸï¼ˆbase/full_backupã‹ã‚‰ã®å†è©¦è¡Œã¯ä¸è¦ï¼‰",
                ui=False,
            )
    except Exception:
        pass

    return cast(dict[str, pd.DataFrame], basic_data)


def _ensure_cli_logger_configured() -> None:
    """CLI å®Ÿè¡Œæ™‚ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ã‚¬ãƒ¼è¨­å®šã‚’ä¿è¨¼ã™ã‚‹ã€‚"""
    try:
        if globals().get("_LOG_FILE_PATH") is None:
            _mode_env = (os.environ.get("TODAY_SIGNALS_LOG_MODE") or "").strip().lower()
            _configure_today_logger(mode=("single" if _mode_env == "single" else "dated"))
    except Exception:
        pass


def _silence_streamlit_cli_warnings() -> None:
    """CLI ã§ã®å®Ÿè¡Œæ™‚ã€Streamlit ã® bare mode è­¦å‘Šã‚’æŠ‘åˆ¶ã™ã‚‹ã€‚"""
    try:
        if os.environ.get("STREAMLIT_SERVER_ENABLED"):
            return

        class _SilenceBareModeWarnings(logging.Filter):
            def filter(self, record: logging.LogRecord) -> bool:
                msg = str(record.getMessage())
                if "missing ScriptRunContext" in msg:
                    return False
                if "Session state does not function" in msg:
                    return False
                return True

        _names = [
            "streamlit",
            "streamlit.runtime",
            "streamlit.runtime.scriptrunner_utils.script_run_context",
            "streamlit.runtime.state.session_state_proxy",
        ]
        for _name in _names:
            _logger = logging.getLogger(_name)
            _logger.addFilter(_SilenceBareModeWarnings())
            try:
                _logger.setLevel(logging.ERROR)
            except Exception:
                pass
    except Exception:
        pass


def _safe_progress_call(
    callback: Callable[[int, int, str], None] | None,
    current: int,
    total: int,
    label: str,
) -> None:
    """UI é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å®‰å…¨ã«å‘¼ã³å‡ºã™ï¼ˆä¾‹å¤–ã¯æ¡ã‚Šã¤ã¶ã™ï¼‰ã€‚"""
    if not callback:
        return
    try:
        callback(current, total, label)
    except Exception:
        pass


def _save_and_notify_phase(
    ctx: TodayRunContext,
    *,
    final_df: pd.DataFrame | None,
    per_system: Mapping[str, pd.DataFrame],
    order_1_7: Sequence[str],
    metrics_summary_context: Mapping[str, Any] | None,
) -> None:
    """ä¿å­˜ãŠã‚ˆã³é€šçŸ¥ãƒ•ã‚§ãƒ¼ã‚ºã‚’æ‹…å½“ã™ã‚‹è£œåŠ©é–¢æ•°ã€‚"""

    signals_dir = ctx.signals_dir
    notify = ctx.notify
    save_csv = ctx.save_csv
    csv_name_mode = ctx.csv_name_mode or "date"
    progress_callback = ctx.progress_callback
    run_start_time = ctx.run_start_time
    start_equity = ctx.start_equity
    today = ctx.today or get_latest_nyse_trading_day().normalize()
    run_id = ctx.run_id

    try:
        final_counts: dict[str, int] = {}
        if (
            final_df is not None
            and not getattr(final_df, "empty", True)
            and "system" in final_df.columns
        ):
            final_counts = final_df.groupby("system").size().to_dict()
    except Exception:
        final_counts = {}
    for name in order_1_7:
        cand_cnt: int | None
        try:
            snapshot = _get_stage_snapshot(name)
            cand_cnt = (
                None
                if snapshot is None or snapshot.candidate_count is None
                else int(snapshot.candidate_count)
            )
        except Exception:
            cand_cnt = None
        if cand_cnt is None:
            df_sys = per_system.get(name)
            cand_cnt = int(0 if df_sys is None or getattr(df_sys, "empty", True) else len(df_sys))
        final_cnt = int(final_counts.get(name, 0))
        try:
            _stage(name, 100, None, None, cand_cnt, final_cnt)
        except Exception:
            pass

    if metrics_summary_context:
        try:
            prefilter_map = dict(metrics_summary_context.get("prefilter_map", {}))
            exit_counts_map_ctx = metrics_summary_context.get("exit_counts_map", {})
            exit_counts_map = (
                {k: v for k, v in exit_counts_map_ctx.items()}
                if isinstance(exit_counts_map_ctx, dict)
                else {}
            )
            setup_map = dict(metrics_summary_context.get("setup_map", {}))
            tgt_base = int(metrics_summary_context.get("tgt_base", 0))
            final_counts = {}
            if (
                final_df is not None
                and not getattr(final_df, "empty", True)
                and "system" in final_df.columns
            ):
                final_counts = final_df.groupby("system").size().to_dict()
            lines: list[dict[str, str]] = []
            for sys_name in order_1_7:
                tgt = tgt_base if sys_name != "system7" else 1
                fil = int(prefilter_map.get(sys_name, 0))
                stu = int(setup_map.get(sys_name, 0))
                try:
                    df_trd = per_system.get(sys_name, pd.DataFrame())
                    trd = int(
                        0 if df_trd is None or getattr(df_trd, "empty", True) else len(df_trd)
                    )
                except Exception:
                    trd = 0
                ent = int(final_counts.get(sys_name, 0))
                exv = exit_counts_map.get(sys_name)
                ex_txt = "-" if exv is None else str(int(exv))
                value = (
                    f"Tgt {tgt} / FIL {fil} / STU {stu} / TRD {trd} / Entry {ent} / Exit {ex_txt}"
                )
                lines.append({"name": sys_name, "value": value})
            title = "ğŸ“ˆ æœ¬æ—¥ã®æœ€çµ‚ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆsystemåˆ¥ï¼‰"
            td = ctx.today
            try:
                td_str = str(getattr(td, "date", lambda: None)() or td)
            except Exception:
                td_str = ""
            run_end_time = datetime.now()
            end_equity = _get_account_equity()
            start_equity_val = float(start_equity or 0.0)
            end_equity_val = float(end_equity or 0.0)
            profit_amt = max(end_equity_val - start_equity_val, 0.0)
            loss_amt = max(start_equity_val - end_equity_val, 0.0)
            try:
                total_entries = int(sum(int(v) for v in final_counts.values()))
            except Exception:
                total_entries = 0
            try:
                total_exits = int(sum(int(v) for v in exit_counts_map.values() if v is not None))
            except Exception:
                total_exits = 0
            start_time_str = run_start_time.strftime("%H:%M:%S")
            end_time_str = run_end_time.strftime("%H:%M:%S")
            duration_seconds = max(0, int((run_end_time - run_start_time).total_seconds()))
            hours, remainder = divmod(duration_seconds, 3600)
            minutes, seconds = divmod(remainder, 60)
            duration_str = f"{hours:02d}:{minutes:02d}:{seconds:02d}"
            summary_pairs = [
                ("æŒ‡å®šéŠ˜æŸ„ç·æ•°", f"{int(tgt_base):,}"),
                (
                    "é–‹å§‹æ™‚é–“/å®Œäº†æ™‚é–“",
                    f"{start_time_str} / {end_time_str} (æ‰€è¦: {duration_str})",
                ),
                (
                    "é–‹å§‹æ™‚è³‡ç”£/å®Œäº†æ™‚è³‡ç”£",
                    f"${start_equity_val:,.2f} / ${end_equity_val:,.2f}",
                ),
                (
                    "ã‚¨ãƒ³ãƒˆãƒªãƒ¼éŠ˜æŸ„æ•°/ã‚¨ã‚°ã‚¸ãƒƒãƒˆéŠ˜æŸ„æ•°",
                    f"{total_entries} / {total_exits}",
                ),
                ("åˆ©ç›Šé¡/æå¤±é¡", f"${profit_amt:,.2f} / ${loss_amt:,.2f}"),
            ]
            summary_fields: list[dict[str, str | bool]] = [
                {"name": key, "value": value, "inline": True} for key, value in summary_pairs
            ]
            send_metrics_notification(
                day_str=str(td_str),
                fields=summary_fields + lines,
                summary_pairs=summary_pairs,
                title=title,
            )
        except Exception:
            pass

    if notify:
        try:
            from tools.notify_signals import send_signal_notification

            # Guard against None being passed where a DataFrame is required
            if final_df is not None and not getattr(final_df, "empty", True):
                send_signal_notification(final_df)
        except Exception:
            _log("âš ï¸ é€šçŸ¥ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

    if save_csv and final_df is not None and not final_df.empty:
        mode = (csv_name_mode or "date").lower()
        date_str = today.strftime("%Y-%m-%d")
        suffix = date_str
        if mode == "datetime":
            try:
                jst_now = datetime.now(ZoneInfo("Asia/Tokyo"))
            except Exception:
                jst_now = datetime.now()
            suffix = f"{date_str}_{jst_now.strftime('%H%M')}"
        elif mode == "runid":
            suffix = f"{date_str}_{run_id}" if run_id else date_str

        out_all = signals_dir / f"signals_final_{suffix}.csv"
        try:
            try:
                round_dec = getattr(get_settings(create_dirs=True).cache, "round_decimals", None)
            except Exception:
                round_dec = None
            out_df = round_dataframe(final_df, round_dec)
        except Exception:
            out_df = final_df
        out_df.to_csv(out_all, index=False)
        for name, df in per_system.items():
            if df is None or getattr(df, "empty", True):
                continue
            out = signals_dir / f"signals_{name}_{suffix}.csv"
            try:
                try:
                    round_dec = getattr(
                        get_settings(create_dirs=True).cache, "round_decimals", None
                    )
                except Exception:
                    round_dec = None
                out_df = round_dataframe(df, round_dec)
            except Exception:
                out_df = df
            out_df.to_csv(out, index=False)
        _log(f"ğŸ’¾ ä¿å­˜: {signals_dir} ã«CSVã‚’æ›¸ãå‡ºã—ã¾ã—ãŸ")

    _safe_progress_call(progress_callback, 8, 8, "done")

    try:
        cnt = 0 if final_df is None else len(final_df)
        _log(f"âœ… ã‚·ã‚°ãƒŠãƒ«æ¤œå‡ºå‡¦ç† çµ‚äº† | æœ€çµ‚å€™è£œ {cnt} ä»¶")
    except Exception:
        pass

    try:
        import time as _time

        end_txt = _time.strftime("%Y-%m-%d %H:%M:%S")
    except Exception:
        end_txt = ""
    try:
        print("#" * 68, flush=True)
    except Exception:
        pass
    _log(
        "# ğŸğŸğŸ  æœ¬æ—¥ã®ã‚·ã‚°ãƒŠãƒ« å®Ÿè¡Œçµ‚äº† (Engine)  ğŸğŸğŸ",
        ui=False,
        no_timestamp=True,
    )
    _log(f"# â±ï¸ {end_txt} | RUN-ID: {run_id}", ui=False, no_timestamp=True)
    try:
        print("#" * 68 + "\n", flush=True)
    except Exception:
        pass


def _log_previous_counts_summary(signals_dir: Path) -> None:
    """å‰å›å®Ÿè¡Œã®ã‚·ã‚¹ãƒ†ãƒ åˆ¥å€™è£œä»¶æ•°ã‚’ç°¡æ˜“è¡¨ç¤ºã™ã‚‹ã€‚"""
    try:
        prev = _load_prev_counts(signals_dir)
        if prev:
            for i in range(1, 8):
                key = f"system{i}"
                v = int(prev.get(key, 0))
                icon = "âœ…" if v > 0 else "â€”"
                suffix = " (0ä»¶)" if v == 0 else ""
                _log(f"å‰å› {icon} {key}: {v}{suffix}")
    except Exception:
        pass


def _apply_system_filters_and_update_ctx(
    ctx: TodayRunContext,
    symbols: list[str],
    basic_data: dict[str, pd.DataFrame],
) -> dict[str, list[str]]:
    """ã‚·ã‚¹ãƒ†ãƒ åˆ¥ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’é©ç”¨ã—ã€ctx.system_filters ã‚’æ›´æ–°ã™ã‚‹ã€‚"""
    system1_syms = filter_system1(symbols, basic_data)
    system2_syms = filter_system2(symbols, basic_data)
    system3_syms = filter_system3(symbols, basic_data)
    system4_syms = filter_system4(symbols, basic_data)
    system5_syms = filter_system5(symbols, basic_data)
    system6_syms = filter_system6(symbols, basic_data)
    filters = {
        "system1": system1_syms,
        "system2": system2_syms,
        "system3": system3_syms,
        "system4": system4_syms,
        "system5": system5_syms,
        "system6": system6_syms,
    }
    ctx.system_filters = filters
    for system_name, syms in filters.items():
        try:
            total_len = len(syms)
        except Exception:
            total_len = 0
        try:
            _stage(system_name, 25, total_len, None, None, None)
        except Exception:
            pass
    # System7 ã¯ SPY å°‚ç”¨
    try:
        spy_total = 1 if "SPY" in (basic_data or {}) else 0
        _stage("system7", 25, spy_total, None, None, None)
    except Exception:
        pass
    return filters


def _log_system1_filter_stats(symbols: list[str], basic_data: dict[str, pd.DataFrame]) -> None:
    """System1 ã®äº‹å‰æ¡ä»¶ãƒ’ãƒƒãƒˆæ•°ã‚’è¡¨ç¤ºã™ã‚‹ã€‚"""
    try:
        s1_total = len(symbols)
        s1_price = 0
        s1_dv = 0
        for _sym in symbols:
            _df = basic_data.get(_sym)
            if _df is None or getattr(_df, "empty", True):
                continue
            try:
                price_ok, dv_ok = _system1_conditions(_df)
            except Exception:
                continue
            if price_ok:
                s1_price += 1
            else:
                continue
            if dv_ok:
                s1_dv += 1
        _log(
            "system1 äº‹å‰æ¡ä»¶ã‚µãƒãƒªãƒ¼: "
            + f"ç·æ•°={s1_total}, ä¾¡æ ¼>=5: {s1_price}, DV20>=50M: {s1_dv}"
        )
    except Exception:
        pass


def _log_system2_filter_stats(symbols: list[str], basic_data: dict[str, pd.DataFrame]) -> None:
    """System2 ã®äº‹å‰æ¡ä»¶ãƒ’ãƒƒãƒˆæ•°ã‚’è¡¨ç¤ºã™ã‚‹ã€‚"""
    try:
        s2_total = len(symbols)
        c_price = 0
        c_dv = 0
        c_atr = 0
        for _sym in symbols:
            _df = basic_data.get(_sym)
            if _df is None or getattr(_df, "empty", True):
                continue
            try:
                price_ok, dv_ok, atr_ok = _system2_conditions(_df)
            except Exception:
                continue
            if price_ok:
                c_price += 1
            else:
                continue
            if dv_ok:
                c_dv += 1
            else:
                continue
            if atr_ok:
                c_atr += 1
        _log(
            "system2 äº‹å‰æ¡ä»¶ã‚µãƒãƒªãƒ¼: "
            + f"ç·æ•°={s2_total}, ä¾¡æ ¼>=5: {c_price}, DV20>=25M: {c_dv}, ATRæ¯”ç‡>=3%: {c_atr}"
        )
    except Exception:
        pass


def _log_system3_filter_stats(symbols: list[str], basic_data: dict[str, pd.DataFrame]) -> None:
    """System3 ã®äº‹å‰æ¡ä»¶ãƒ’ãƒƒãƒˆæ•°ã‚’è¡¨ç¤ºã™ã‚‹ã€‚"""
    try:
        s3_total = len(symbols)
        s3_low = 0
        s3_av = 0
        s3_atr = 0
        for _sym in symbols:
            _df = basic_data.get(_sym)
            if _df is None or getattr(_df, "empty", True):
                continue
            try:
                low_ok, av_ok, atr_ok = _system3_conditions(_df)
            except Exception:
                continue
            if low_ok:
                s3_low += 1
            else:
                continue
            if av_ok:
                s3_av += 1
            else:
                continue
            if atr_ok:
                s3_atr += 1
        _log(
            "system3 äº‹å‰æ¡ä»¶ã‚µãƒãƒªãƒ¼: "
            + f"ç·æ•°={s3_total}, Low>=1: {s3_low}, AvgVol50>=1M: {s3_av}, ATR_Ratio>=5%: {s3_atr}"
        )
    except Exception:
        pass


def _log_system4_filter_stats(symbols: list[str], basic_data: dict[str, pd.DataFrame]) -> None:
    """System4 ã®äº‹å‰æ¡ä»¶ãƒ’ãƒƒãƒˆæ•°ã‚’è¡¨ç¤ºã™ã‚‹ã€‚"""
    try:
        s4_total = len(symbols)
        s4_dv = 0
        s4_hv = 0
        for _sym in symbols:
            _df = basic_data.get(_sym)
            if _df is None or getattr(_df, "empty", True):
                continue
            try:
                dv_ok, hv_ok = _system4_conditions(_df)
            except Exception:
                continue
            if dv_ok:
                s4_dv += 1
            else:
                continue
            if hv_ok:
                s4_hv += 1
        _log(
            "system4 äº‹å‰æ¡ä»¶ã‚µãƒãƒªãƒ¼: "
            + f"ç·æ•°={s4_total}, DV50>=100M: {s4_dv}, HV50 10ã€œ40: {s4_hv}"
        )
    except Exception:
        pass


def _log_system5_filter_stats(symbols: list[str], basic_data: dict[str, pd.DataFrame]) -> None:
    """System5 ã®äº‹å‰æ¡ä»¶ãƒ’ãƒƒãƒˆæ•°ã‚’è¡¨ç¤ºã™ã‚‹ã€‚"""
    try:
        threshold_label = f"ATR_Pct>{DEFAULT_ATR_PCT_THRESHOLD*100:.1f}%"
        s5_total = len(symbols)
        s5_av = 0
        s5_dv = 0
        s5_atr = 0
        for _sym in symbols:
            _df = basic_data.get(_sym)
            if _df is None or getattr(_df, "empty", True):
                continue
            try:
                av_ok, dv_ok, atr_ok = _system5_conditions(_df)
            except Exception:
                continue
            if av_ok:
                s5_av += 1
            else:
                continue
            if dv_ok:
                s5_dv += 1
            else:
                continue
            if atr_ok:
                s5_atr += 1
        _log(
            "system5 äº‹å‰æ¡ä»¶ã‚µãƒãƒªãƒ¼: "
            + f"ç·æ•°={s5_total}, AvgVol50>500k: {s5_av}, DV50>2.5M: {s5_dv}"
            + f", {threshold_label}: {s5_atr}"
        )
    except Exception:
        pass


def _log_system6_filter_stats(symbols: list[str], basic_data: dict[str, pd.DataFrame]) -> None:
    """System6 ã®äº‹å‰æ¡ä»¶ãƒ’ãƒƒãƒˆæ•°ã‚’è¡¨ç¤ºã™ã‚‹ã€‚"""
    try:
        s6_total = len(symbols)
        s6_low = 0
        s6_dv = 0
        s6_hv = 0
        for _sym in symbols:
            _df = basic_data.get(_sym)
            if _df is None or getattr(_df, "empty", True):
                continue
            try:
                low_ok, dv_ok, hv_ok = _system6_conditions(_df)
            except Exception:
                continue
            if not low_ok:
                continue
            s6_low += 1
            if not dv_ok:
                continue
            s6_dv += 1
            if hv_ok:
                s6_hv += 1
        _log(
            "system6 äº‹å‰æ¡ä»¶ã‚µãƒãƒªãƒ¼: "
            + f"ç·æ•°={s6_total}, Low>=5: {s6_low}, DV50>10M: {s6_dv}, HV50 10ã€œ40: {s6_hv}"
        )
    except Exception:
        pass


def _log_system7_filter_stats(basic_data: dict[str, pd.DataFrame]) -> None:
    """System7 (SPY) ã®äº‹å‰æ¡ä»¶ãƒ’ãƒƒãƒˆæ•°ã‚’è¡¨ç¤ºã™ã‚‹ã€‚"""
    try:
        spyp = (
            1 if ("SPY" in basic_data and not getattr(basic_data.get("SPY"), "empty", True)) else 0
        )
        _log("system7 äº‹å‰æ¡ä»¶ã‚µãƒãƒªãƒ¼: SPYã®æœ‰ç„¡ | SPY=" + str(spyp))
    except Exception:
        pass


def _log_system_filter_stats(
    symbols: list[str],
    basic_data: dict[str, pd.DataFrame],
    filters: dict[str, list[str]],
) -> None:
    """å„ã‚·ã‚¹ãƒ†ãƒ ã®äº‹å‰æ¡ä»¶ã‚µãƒãƒªãƒ¼ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é€šéä»¶æ•°ã‚’è¡¨ç¤ºã™ã‚‹ã€‚"""
    _log("å„ã‚·ã‚¹ãƒ†ãƒ ã®äº‹å‰æ¡ä»¶ã‚µãƒãƒªãƒ¼ (system1ã€œsystem6)")
    _log_system1_filter_stats(symbols, basic_data)
    _log_system2_filter_stats(symbols, basic_data)
    _log_system3_filter_stats(symbols, basic_data)
    _log_system4_filter_stats(symbols, basic_data)
    _log_system5_filter_stats(symbols, basic_data)
    _log_system6_filter_stats(symbols, basic_data)
    _log_system7_filter_stats(basic_data)
    system1_syms = filters.get("system1", [])
    system2_syms = filters.get("system2", [])
    system3_syms = filters.get("system3", [])
    system4_syms = filters.get("system4", [])
    system5_syms = filters.get("system5", [])
    system6_syms = filters.get("system6", [])
    _log(
        "ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é€šéä»¶æ•°: "
        + f"system1={len(system1_syms)}ä»¶, "
        + f"system2={len(system2_syms)}ä»¶, "
        + f"system3={len(system3_syms)}ä»¶, "
        + f"system4={len(system4_syms)}ä»¶, "
        + f"system5={len(system5_syms)}ä»¶, "
        + f"system6={len(system6_syms)}ä»¶"
    )


def _ensure_rolling_cache_fresh(
    symbol: str,
    rolling_df: pd.DataFrame,
    today: pd.Timestamp,
    cache_manager: CacheManager,
    base_rows: int = 320,
    max_lag_days: int = 2,
) -> pd.DataFrame:
    """
    rolling_dfã®æœ€çµ‚æ—¥ä»˜ãŒtodayã‹ã‚‰max_lag_daysä»¥ä¸Šã‚ºãƒ¬ã¦ã„ã‚‹å ´åˆã€
    baseã‹ã‚‰rollingã‚’å†ç”Ÿæˆã—ã€rollingã¸æ›¸ãæˆ»ã™ã€‚
    """
    if rolling_df is None or getattr(rolling_df, "empty", True):
        # æ¬ ææ™‚ã¯baseã‹ã‚‰å†ç”Ÿæˆ
        base_df = cast(Any, cache_manager).read(symbol, layer="base", rows=base_rows)
        if base_df is not None and not getattr(base_df, "empty", True):
            rolling_new = base_df.tail(base_rows).copy()
            cast(Any, cache_manager).write_atomic(symbol, rolling_new, layer="rolling")
            return cast(pd.DataFrame, rolling_new)
        return rolling_df
    try:
        last_idx = rolling_df.index[-1]
        if isinstance(last_idx, str):
            last_ts = pd.to_datetime(last_idx)
        else:
            last_ts = (
                pd.Timestamp(last_idx.to_pydatetime())
                if hasattr(last_idx, "to_pydatetime")
                else pd.Timestamp(last_idx)
            )
    except Exception:
        return rolling_df
    try:
        lag_days = int((today - last_ts).days)
    except Exception:
        lag_days = 0
    if lag_days > max_lag_days:
        # é®®åº¦ä¸è¶³: baseã‹ã‚‰rollingå†ç”Ÿæˆ
        base_df = cast(Any, cache_manager).read(symbol, layer="base", rows=base_rows)
        if base_df is not None and not getattr(base_df, "empty", True):
            rolling_new = base_df.tail(base_rows).copy()
            cast(Any, cache_manager).write_atomic(symbol, rolling_new, layer="rolling")
            return cast(pd.DataFrame, rolling_new)
    return rolling_df


def _prepare_system2_data(
    basic_data: dict[str, pd.DataFrame],
    system_symbols: list[str],
) -> tuple[dict[str, pd.DataFrame], int, int, int]:
    """System2 ã®æº–å‚™ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é€šéé›†åˆãªã©ï¼‰ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚"""
    _log("System2 æº–å‚™ãƒ‡ãƒ¼ã‚¿ã®é›†è¨ˆ")
    raw_data = _subset_data(basic_data, system_symbols)
    _log(f"æŠ½å‡ºå¯¾è±¡ã®ä»¶æ•°: system2={len(raw_data)}ä»¶")
    s2_filter = int(len(system_symbols))
    s2_rsi = 0
    s2_combo = 0
    try:
        for _sym in system_symbols or []:
            _df = raw_data.get(_sym)
            if _df is None or getattr(_df, "empty", True):
                continue
            try:
                last = _df.iloc[-1]
            except Exception:
                continue
            try:
                rsi_pass = float(last.get("RSI3", 0)) > 90
            except Exception:
                rsi_pass = False
            if not rsi_pass:
                continue
            s2_rsi += 1
            try:
                if bool(last.get("TwoDayUp", False)):
                    s2_combo += 1
            except Exception:
                pass
        _log(
            "system2 ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ¡ä»¶: "
            + f"å€™è£œæ•°={s2_filter}, RSI3>90: {s2_rsi}, "
            + f"TwoDayUp: {s2_combo}"
        )
        try:
            _stage(
                "system2",
                50,
                filter_count=int(s2_filter),
                setup_count=int(s2_combo),
            )
        except Exception:
            pass
    except Exception:
        pass
    return raw_data, s2_filter, s2_rsi, s2_combo


def _prepare_system3_data(
    basic_data: dict[str, pd.DataFrame],
    system_symbols: list[str],
) -> tuple[dict[str, pd.DataFrame], int, int, int]:
    """System3 ã®æº–å‚™ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é€šéé›†åˆãªã©ï¼‰ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚"""
    _log("System3 æº–å‚™ãƒ‡ãƒ¼ã‚¿ã®é›†è¨ˆ")
    raw_data = _subset_data(basic_data, system_symbols)
    _log(f"æŠ½å‡ºå¯¾è±¡ã®ä»¶æ•°: system3={len(raw_data)}ä»¶")
    s3_filter = int(len(system_symbols))
    s3_close = 0
    s3_combo = 0
    try:
        for _sym in system_symbols or []:
            _df = raw_data.get(_sym)
            if _df is None or getattr(_df, "empty", True):
                continue
            try:
                last = _df.iloc[-1]
            except Exception:
                continue
            try:
                close_pass = float(last.get("Close", 0)) > float(last.get("SMA150", float("inf")))
            except Exception:
                close_pass = False
            if not close_pass:
                continue
            s3_close += 1
            try:
                if float(last.get("Drop3D", 0)) >= 0.125:
                    s3_combo += 1
            except Exception:
                pass
        _log(
            "system3 ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ¡ä»¶: "
            + f"å€™è£œæ•°={s3_filter}, Close>SMA150: {s3_close}, "
            + f"3æ—¥ä¸‹è½>=12.5%: {s3_combo}"
        )
        try:
            _stage(
                "system3",
                50,
                filter_count=int(s3_filter),
                setup_count=int(s3_combo),
                candidate_count=None,
                entry_count=None,
            )
        except Exception:
            pass
    except Exception:
        pass
    return raw_data, s3_filter, s3_close, s3_combo


def _prepare_system4_data(
    basic_data: dict[str, pd.DataFrame],
    system_symbols: list[str],
) -> tuple[dict[str, pd.DataFrame], int, int]:
    """System4 ã®æº–å‚™ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é€šéé›†åˆãªã©ï¼‰ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚"""
    _log("System4 æº–å‚™ãƒ‡ãƒ¼ã‚¿ã®é›†è¨ˆ")
    raw_data = _subset_data(basic_data, system_symbols)
    _log(f"æŠ½å‡ºå¯¾è±¡ã®ä»¶æ•°: system4={len(raw_data)}ä»¶")
    s4_filter = int(len(system_symbols))
    s4_close = 0
    try:
        for _sym in system_symbols or []:
            _df = raw_data.get(_sym)
            if _df is None or getattr(_df, "empty", True):
                continue
            try:
                last = _df.iloc[-1]
            except Exception:
                continue
            try:
                if float(last.get("Close", 0)) > float(last.get("SMA200", float("inf"))):
                    s4_close += 1
            except Exception:
                pass
        _log(f"system4 ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ¡ä»¶: å€™è£œæ•°={s4_filter}, Close>SMA200: {s4_close}")
        try:
            _stage(
                "system4",
                50,
                filter_count=int(s4_filter),
                setup_count=int(s4_close),
                candidate_count=None,
                entry_count=None,
            )
        except Exception:
            pass
    except Exception:
        pass
    return raw_data, s4_filter, s4_close


def _prepare_system5_data(
    basic_data: dict[str, pd.DataFrame],
    system_symbols: list[str],
) -> tuple[dict[str, pd.DataFrame], int, int, int, int]:
    """System5 ã®æº–å‚™ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é€šéé›†åˆãªã©ï¼‰ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚"""
    _log("System5 æº–å‚™ãƒ‡ãƒ¼ã‚¿ã®é›†è¨ˆ")
    raw_data = _subset_data(basic_data, system_symbols)
    _log(f"æŠ½å‡ºå¯¾è±¡ã®ä»¶æ•°: system5={len(raw_data)}ä»¶")
    s5_filter = int(len(system_symbols))
    s5_close = 0
    s5_adx = 0
    s5_combo = 0
    try:
        for _sym in system_symbols or []:
            _df = raw_data.get(_sym)
            if _df is None or getattr(_df, "empty", True):
                continue
            try:
                last = _df.iloc[-1]
            except Exception:
                continue
            try:
                price_pass = float(last.get("Close", 0)) > float(last.get("SMA100", 0)) + float(
                    last.get("ATR10", 0)
                )
            except Exception:
                price_pass = False
            if not price_pass:
                continue
            s5_close += 1
            try:
                adx_pass = float(last.get("ADX7", 0)) > 55
            except Exception:
                adx_pass = False
            if not adx_pass:
                continue
            s5_adx += 1
            try:
                if float(last.get("RSI3", 100)) < 50:
                    s5_combo += 1
            except Exception:
                pass
        _log(
            "system5 ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ¡ä»¶: "
            + f"å€™è£œæ•°={s5_filter}, Close>SMA100+ATR10: {s5_close}, "
            + f"ADX7>55: {s5_adx}, RSI3<50: {s5_combo}"
        )
        try:
            _stage(
                "system5",
                50,
                filter_count=int(s5_filter),
                setup_count=int(s5_combo),
                candidate_count=None,
                entry_count=None,
            )
        except Exception:
            pass
    except Exception:
        pass
    return raw_data, s5_filter, s5_close, s5_adx, s5_combo


def _prepare_system6_data(
    basic_data: dict[str, pd.DataFrame],
    system_symbols: list[str],
) -> tuple[dict[str, pd.DataFrame], int, int, int]:
    """System6 ã®æº–å‚™ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é€šéé›†åˆãªã©ï¼‰ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚"""
    _log("System6 æº–å‚™ãƒ‡ãƒ¼ã‚¿ã®é›†è¨ˆ")
    raw_data = _subset_data(basic_data, system_symbols)
    _log(f"æŠ½å‡ºå¯¾è±¡ã®ä»¶æ•°: system6={len(raw_data)}ä»¶")
    s6_filter = int(len(system_symbols))
    s6_ret = 0
    s6_combo = 0
    try:
        for _sym in system_symbols or []:
            _df = raw_data.get(_sym)
            if _df is None or getattr(_df, "empty", True):
                continue
            try:
                last = _df.iloc[-1]
            except Exception:
                continue
            try:
                # return_6d: æ—§ç§° Return6D (å‘½åçµ±ä¸€æ¸ˆ)
                ret_val = to_float(get_indicator(cast(Mapping[str, Any], last), "return_6d"))
                ret_pass = (ret_val > 0.20) if not pd.isna(ret_val) else False
            except Exception:
                ret_pass = False
            if not ret_pass:
                continue
            s6_ret += 1
            try:
                # UpTwoDays ã¯åˆ—åæºã‚Œã«å¯¾å¿œï¼ˆUpTwoDayâ€¦ï¼‰
                if is_true(get_indicator(cast(Mapping[str, Any], last), "uptwodays")):
                    s6_combo += 1
            except Exception:
                pass
        _log(
            "system6 ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ¡ä»¶: "
            + f"å€™è£œæ•°={s6_filter}, return_6d>20%: {s6_ret}, "
            + f"UpTwoDays: {s6_combo}"
        )
        try:
            _stage(
                "system6",
                50,
                filter_count=int(s6_filter),
                setup_count=int(s6_combo),
                candidate_count=None,
                entry_count=None,
            )
        except Exception:
            pass
    except Exception:
        pass
    return raw_data, s6_filter, s6_ret, s6_combo


def _resolve_spy_dataframe(basic_data: dict[str, pd.DataFrame]) -> pd.DataFrame | None:
    """SPY ã® DataFrame ã‚’æŒ‡æ¨™ä»˜ãã§å–å¾—ã™ã‚‹ã€‚"""
    if "SPY" in basic_data:
        try:
            return cast(pd.DataFrame | None, get_spy_with_indicators(basic_data["SPY"]))
        except Exception:
            return None
    _log(
        "SPY ã®åŸºç¤ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ (base/full_backup/rolling ã®ã„ãšã‚Œã«ã‚‚å­˜åœ¨ã—ã¾ã›ã‚“)ã€‚"
        + " SPY.csv ã¾ãŸã¯ data_cache/base ãªã‚‰ã³ã« data_cache/full_backup ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
    )
    return None


@no_type_check
def compute_today_signals(
    symbols: list[str] | None,
    *,
    slots_long: int | None = None,
    slots_short: int | None = None,
    capital_long: float | None = None,
    capital_short: float | None = None,
    save_csv: bool = False,
    csv_name_mode: str | None = None,
    notify: bool = True,
    log_callback: Callable[[str], None] | None = None,
    progress_callback: Callable[[int, int, str], None] | None = None,
    # è¿½åŠ : ä¸¦åˆ—å®Ÿè¡Œæ™‚ãªã©ã« system ã”ã¨ã®é–‹å§‹/å®Œäº†ã‚’é€šçŸ¥ã™ã‚‹è»½é‡ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
    # phase ã¯ "start" | "done" ã‚’æƒ³å®š
    per_system_progress: Callable[[str, str], None] | None = None,
    symbol_data: dict[str, pd.DataFrame] | None = None,
    parallel: bool = False,
    test_mode: str | None = None,
    skip_external: bool = False,
) -> tuple[pd.DataFrame, dict[str, pd.DataFrame]]:
    """å½“æ—¥ã‚·ã‚°ãƒŠãƒ«æŠ½å‡ºï¼‹é…åˆ†ã®æœ¬ä½“ã€‚

    Args:
        symbols: å¯¾è±¡ã‚·ãƒ³ãƒœãƒ«ãƒªã‚¹ãƒˆã€‚
        parallel: True ã®å ´åˆã¯ã‚·ã‚¹ãƒ†ãƒ ã”ã¨ã®ã‚·ã‚°ãƒŠãƒ«æŠ½å‡ºã‚’ä¸¦è¡Œå®Ÿè¡Œã™ã‚‹ã€‚

    æˆ»ã‚Šå€¤: (final_df, per_system_df_dict)
    """

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæˆ»ã‚Šå€¤ã‚’äº‹å‰ã«è¨­å®šï¼ˆã‚·ã‚°ãƒŠãƒ«0ä»¶ã‚„æ—©æœŸreturnã®å ´åˆã«ä½¿ç”¨ï¼‰
    # final_df = pd.DataFrame()  # Unused variable removed
    per_system: dict[str, pd.DataFrame] = {}

    # å®Ÿè¡Œé–‹å§‹æ™‚ã«ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ãƒªã‚»ãƒƒãƒˆï¼ˆStreamlit UI ã‹ã‚‰ä½•åº¦ã‚‚å®Ÿè¡Œã•ã‚Œã‚‹å ´åˆã«å¯¾å¿œï¼‰
    import time as _t

    global _LOG_START_TS
    _LOG_START_TS = _t.time()

    _log("ğŸ”§ ãƒ‡ãƒãƒƒã‚°: compute_today_signalsé–‹å§‹")

    ctx = _initialize_run_context(
        slots_long=slots_long,
        slots_short=slots_short,
        capital_long=capital_long,
        capital_short=capital_short,
        save_csv=save_csv,
        csv_name_mode=csv_name_mode,
        notify=notify,
        log_callback=log_callback,
        progress_callback=progress_callback,
        per_system_progress=per_system_progress,
        symbol_data=symbol_data,
        parallel=parallel,
        test_mode=test_mode,
        skip_external=skip_external,
    )

    try:
        GLOBAL_STAGE_METRICS.reset()
    except Exception:
        pass

    # CLI çµŒç”±ã§æœªè¨­å®šã®å ´åˆï¼ˆUI ç­‰ï¼‰ã€æ—¢å®šã§æ—¥ä»˜åˆ¥ãƒ­ã‚°ã«åˆ‡æ›¿
    try:
        if globals().get("_LOG_FILE_PATH") is None:
            _mode_env = (os.environ.get("TODAY_SIGNALS_LOG_MODE") or "").strip().lower()
            _configure_today_logger(mode=("single" if _mode_env == "single" else "dated"))
    except Exception:
        pass

    _run_id = ctx.run_id
    # settings = ctx.settings  # Unused variable removed
    # install log callback for helpers
    globals()["_LOG_CALLBACK"] = ctx.log_callback
    signals_dir = ctx.signals_dir

    # run_start_time = ctx.run_start_time  # Unused variable removed
    # start_equity = ctx.start_equity  # Unused variable removed
    slots_long = ctx.slots_long
    slots_short = ctx.slots_short
    capital_long = ctx.capital_long
    capital_short = ctx.capital_short
    save_csv = ctx.save_csv
    csv_name_mode = ctx.csv_name_mode
    notify = ctx.notify
    log_callback = ctx.log_callback
    progress_callback = ctx.progress_callback
    per_system_progress = ctx.per_system_progress
    parallel = ctx.parallel

    # CLIå®Ÿè¡Œæ™‚ã®Streamlitè­¦å‘Šã‚’æŠ‘åˆ¶ï¼ˆUIã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒç„¡ã„å ´åˆã®ã¿ï¼‰
    try:
        if not os.environ.get("STREAMLIT_SERVER_ENABLED"):

            class _SilenceBareModeWarnings(logging.Filter):
                def filter(self, record: logging.LogRecord) -> bool:
                    msg = str(record.getMessage())
                    if "missing ScriptRunContext" in msg:
                        return False
                    if "Session state does not function" in msg:
                        return False
                    return True

            _names = [
                "streamlit",
                "streamlit.runtime",
                "streamlit.runtime.scriptrunner_utils.script_run_context",
                "streamlit.runtime.state.session_state_proxy",
            ]
            for _name in _names:
                _logger = logging.getLogger(_name)
                _logger.addFilter(_SilenceBareModeWarnings())
                try:
                    _logger.setLevel(logging.ERROR)
                except Exception:
                    pass
    except Exception:
        pass

    # å¯¾è±¡ã¨ã™ã‚‹NYSEå–¶æ¥­æ—¥ï¼ˆå®Ÿè¡Œé–‹å§‹æ™‚ã«ä¸€åº¦ã ã‘ç¢ºå®šï¼‰
    entry_day = get_signal_target_trading_day().normalize()
    ctx.today = entry_day  # äº’æ›ã®ãŸã‚ today ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯ã‚¨ãƒ³ãƒˆãƒªãƒ¼äºˆå®šæ—¥ã‚’æŒ‡ã™
    ctx.entry_day = entry_day
    try:
        prev_trading = get_latest_nyse_trading_day(entry_day - pd.Timedelta(days=1))
        ctx.signal_base_day = pd.Timestamp(prev_trading).normalize()
    except Exception:
        ctx.signal_base_day = entry_day

    # Run start banner (CLI only) - æœ€åˆã«å®Ÿè¡Œé–‹å§‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
    try:
        print("#" * 68, flush=True)
    except Exception:
        pass
    _log(
        "# ğŸš€ğŸš€ğŸš€  æœ¬æ—¥ã®ã‚·ã‚°ãƒŠãƒ« å®Ÿè¡Œé–‹å§‹ (Engine)  ğŸš€ğŸš€ğŸš€",
        ui=False,
        no_timestamp=True,
    )
    try:
        import time as _time

        now_str = _time.strftime("%Y-%m-%d %H:%M:%S")
    except Exception:
        now_str = ""
    _log(f"# â±ï¸ {now_str} | RUN-ID: {_run_id}", ui=False, no_timestamp=True)
    try:
        print("#" * 68 + "\n", flush=True)
    except Exception:
        pass

    try:
        _log(
            f"ğŸ“… ã‚¨ãƒ³ãƒˆãƒªãƒ¼äºˆå®šæ—¥ï¼ˆNYSEï¼‰: {entry_day.date()}",
            no_timestamp=True,
        )
        base_day_disp = getattr(ctx, "signal_base_day", None)
        if base_day_disp is not None:
            _log(
                f"ğŸ“Œ ã‚·ã‚°ãƒŠãƒ«åŸºæº–æ—¥ï¼ˆå‰å–¶æ¥­æ—¥ï¼‰: {pd.Timestamp(base_day_disp).date()}",
                no_timestamp=True,
            )
    except Exception:
        pass
    _log(
        "â„¹ï¸ æ³¨: EODHDã¯å½“æ—¥çµ‚å€¤ãŒæœªåæ˜ ã®ãŸã‚ã€ç›´è¿‘å–¶æ¥­æ—¥ãƒ™ãƒ¼ã‚¹ã§è¨ˆç®—ã—ã¾ã™ã€‚",
        no_timestamp=True,
    )
    _log("", no_timestamp=True)  # ç©ºè¡Œã‚’è¿½åŠ 
    # é–‹å§‹ç›´å¾Œã«å‰å›çµæœã‚’ã¾ã¨ã‚ã¦è¡¨ç¤º
    try:
        prev = _load_prev_counts(signals_dir)
        if prev:
            for i in range(1, 8):
                key = f"system{i}"
                v = int(prev.get(key, 0))
                icon = "âœ…" if v > 0 else "âŒ"
                _log(f"ğŸ§¾ {icon} (å‰å›çµæœ) {key}: {v} ä»¶{' ğŸš«' if v == 0 else ''}")
    except Exception:
        pass
    if progress_callback:
        try:
            progress_callback(0, 8, "init")
        except Exception:
            pass

    symbols = _prepare_symbol_universe(ctx, symbols)
    basic_data = _load_universe_basic_data(ctx, symbols)

    # é‡è¦: SPY ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å­˜åœ¨ã¨æœ€ä½é™ã®å¥å…¨æ€§ã‚’èµ·å‹•ç›´å¾Œã«ãƒã‚§ãƒƒã‚¯ã—ã€NGãªã‚‰å³åœæ­¢
    try:
        spy_df_check = basic_data.get("SPY") if isinstance(basic_data, dict) else None
    except Exception:
        spy_df_check = None
    if spy_df_check is None or getattr(spy_df_check, "empty", True):
        _log(
            "âŒ SPYã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆã¾ãŸã¯ç©ºã§ã™ï¼‰ã€‚å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã™ã€‚",
            ui=False,
        )
        _log(
            "ğŸ’¡ å¯¾ç­–: data_cache/rolling ã¾ãŸã¯ base/full_backup ã« SPY.csv ã‚’é…ç½®ã—ã€"
            "å¿…è¦ãªã‚‰ scripts/recover_spy_cache.py ã§å¾©æ—§ã—ã¦ãã ã•ã„ã€‚",
            ui=False,
        )
        raise SystemExit(1)
    try:
        last_dt = _extract_last_cache_date(spy_df_check)
    except Exception:
        last_dt = None
    if last_dt is None:
        _log(
            "âŒ SPYã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æ—¥ä»˜åˆ—ï¼ˆdate/Date/indexï¼‰ãŒè§£é‡ˆã§ãã¾ã›ã‚“ã€‚å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã™ã€‚",
            ui=False,
        )
        raise SystemExit(1)

    # latest_only ã®åŸºæº–æ—¥ã¯é–‹å§‹æ™‚ã«ç¢ºå®šæ¸ˆã¿ï¼ˆctx.signal_base_dayï¼‰ã€‚SPYã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨ç›¸é•ã—ã¦ã‚‚è­¦å‘Šã®ã¿ã€‚
    try:
        spy_df = basic_data.get("SPY") if isinstance(basic_data, dict) else None
        anchor_last = _extract_last_cache_date(spy_df) if spy_df is not None else None
        if anchor_last is not None:
            frozen_base = pd.Timestamp(getattr(ctx, "signal_base_day", None)).normalize()
            if pd.Timestamp(anchor_last).normalize() != frozen_base:
                _log(
                    "âš ï¸ SPYã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ€çµ‚æ—¥ãŒå›ºå®šã—ãŸã‚·ã‚°ãƒŠãƒ«åŸºæº–æ—¥ã¨ç•°ãªã‚Šã¾ã™: "
                    f"cache={pd.Timestamp(anchor_last).date()} / frozen={frozen_base.date()}"
                )
    except Exception:
        pass

    # âœ¨ NEW: æŒ‡æ¨™äº‹å‰è¨ˆç®—ãƒã‚§ãƒƒã‚¯ï¼ˆä¸è¶³æ™‚ã¯å³åº§åœæ­¢ï¼‰
    try:
        from common.indicators_validation import (
            IndicatorValidationError,
            validate_precomputed_indicators,
        )

        target_systems = [1, 2, 3, 4, 5, 6, 7]  # å…¨Systemå¯¾è±¡
        _log("ğŸ” æŒ‡æ¨™äº‹å‰è¨ˆç®—çŠ¶æ³ã‚’ãƒã‚§ãƒƒã‚¯ä¸­...")

        validate_precomputed_indicators(
            basic_data,
            systems=target_systems,
            strict_mode=True,  # ä¸è¶³æ™‚ã¯å³åº§åœæ­¢
            log_callback=_log,
        )

    except IndicatorValidationError as e:
        _log(f"âŒ æŒ‡æ¨™ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
        _log("ğŸ’¡ è§£æ±ºæ–¹æ³•: python scripts/build_rolling_with_indicators.py --workers 4")
        raise SystemExit(1) from e
    except Exception as e:
        _log(f"âš ï¸  æŒ‡æ¨™ãƒã‚§ãƒƒã‚¯å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}")
        # ãƒã‚§ãƒƒã‚¯å‡¦ç†è‡ªä½“ã®ã‚¨ãƒ©ãƒ¼ã¯ç¶™ç¶šï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰

    _log("ğŸ§ª äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å®Ÿè¡Œä¸­ (system1ã€œsystem6)â€¦")

    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é–‹å§‹å‰ã«å„ã‚·ã‚¹ãƒ†ãƒ ã®é€²æ—ã‚’0%ã«ãƒªã‚»ãƒƒãƒˆ
    try:
        for system_name in [
            "system1",
            "system2",
            "system3",
            "system4",
            "system5",
            "system6",
            "system7",
        ]:
            _stage(system_name, 0, filter_count=len(symbols))
    except Exception:
        pass

    filter_stats: dict[str, dict[str, int]] = {
        "system1": {},
        "system2": {},
        "system3": {},
        "system4": {},
        "system5": {},
        "system6": {},
    }
    system1_syms = filter_system1(symbols, basic_data, stats=filter_stats["system1"])
    system2_syms = filter_system2(symbols, basic_data, stats=filter_stats["system2"])
    system3_syms = filter_system3(symbols, basic_data, stats=filter_stats["system3"])
    system4_syms = filter_system4(symbols, basic_data, stats=filter_stats["system4"])
    system5_syms = filter_system5(symbols, basic_data, stats=filter_stats["system5"])
    system6_syms = filter_system6(symbols, basic_data, stats=filter_stats["system6"])
    ctx.system_filters = {
        "system1": system1_syms,
        "system2": system2_syms,
        "system3": system3_syms,
        "system4": system4_syms,
        "system5": system5_syms,
        "system6": system6_syms,
    }

    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å‡¦ç†å®Œäº†å¾Œã«å„ã‚·ã‚¹ãƒ†ãƒ ã®é€²æ—ã‚’25%ã«æ›´æ–°
    try:
        stage_targets = (
            ("system1", system1_syms),
            ("system2", system2_syms),
            ("system3", system3_syms),
            ("system4", system4_syms),
            ("system5", system5_syms),
            ("system6", system6_syms),
        )
        for system_name, items in stage_targets:
            _stage(system_name, 25, filter_count=len(items or []))
        # System7 ã¯ SPY å°‚ç”¨
        _stage("system7", 25, filter_count=1 if "SPY" in (basic_data or {}) else 0)
    except Exception:
        pass
    # System1 ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å†…è¨³ï¼ˆä¾¡æ ¼ãƒ»å£²è²·ä»£é‡‘ï¼‰
    try:
        stats1 = filter_stats.get("system1", {})
        s1_total = stats1.get("total", len(symbols or []))
        s1_price = stats1.get("price_pass", 0)
        s1_dv = stats1.get("dv_pass", 0)
        _log("ğŸ§ª system1å†…è¨³: " + f"å…ƒ={s1_total}, ä¾¡æ ¼>=5: {s1_price}, DV20>=50M: {s1_dv}")
    except Exception:
        pass
    # System2 ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å†…è¨³ã®å¯è¦–åŒ–ï¼ˆä¾¡æ ¼ãƒ»å£²è²·ä»£é‡‘ãƒ»ATRæ¯”ç‡ã®æ®µéšé€šéæ•°ï¼‰
    try:
        stats2 = filter_stats.get("system2", {})
        s2_total = stats2.get("total", len(symbols or []))
        c_price = stats2.get("price_pass", 0)
        c_dv = stats2.get("dv_pass", 0)
        c_atr = stats2.get("atr_pass", 0)
        _log(
            "ğŸ§ª system2å†…è¨³: "
            + f"å…ƒ={s2_total}, ä¾¡æ ¼>=5: {c_price}, DV20>=25M: {c_dv}, ATRæ¯”ç‡>=3%: {c_atr}"
        )
    except Exception:
        pass
    # System3 ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å†…è¨³ï¼ˆLow>=1 â†’ AvgVol50>=1M â†’ ATR_Ratio>=5%ï¼‰
    try:
        stats3 = filter_stats.get("system3", {})
        s3_total = stats3.get("total", len(symbols or []))
        s3_low = stats3.get("low_pass", 0)
        s3_av = stats3.get("avgvol_pass", 0)
        s3_atr = stats3.get("atr_pass", 0)
        _log(
            "ğŸ§ª system3å†…è¨³: "
            + (
                f"å…ƒ={s3_total}, Low>=1: {s3_low}, "
                f"AvgVol50>=1M: {s3_av}, ATR_Ratio>=5%: {s3_atr}"
            )
        )
    except Exception:
        pass
    # System4 ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å†…è¨³ï¼ˆDV50>=100M â†’ HV50 10ã€œ40ï¼‰
    try:
        stats4 = filter_stats.get("system4", {})
        s4_total = stats4.get("total", len(symbols or []))
        s4_dv = stats4.get("dv_pass", 0)
        s4_hv = stats4.get("hv_pass", 0)
        _log("ğŸ§ª system4å†…è¨³: " + f"å…ƒ={s4_total}, DV50>=100M: {s4_dv}, HV50 10ã€œ40: {s4_hv}")
    except Exception:
        pass
    # System5 ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å†…è¨³ï¼ˆAvgVol50>500k â†’ DV50>2.5M â†’ ATR_Pct>é–¾å€¤ï¼‰
    try:
        threshold_label = f"ATR_Pct>{DEFAULT_ATR_PCT_THRESHOLD*100:.1f}%"
        stats5 = filter_stats.get("system5", {})
        s5_total = stats5.get("total", len(symbols or []))
        s5_av = stats5.get("avgvol_pass", 0)
        s5_dv = stats5.get("dv_pass", 0)
        s5_atr = stats5.get("atr_pass", 0)
        _log(
            "ğŸ§ª system5å†…è¨³: "
            + f"å…ƒ={s5_total}, AvgVol50>500k: {s5_av}, DV50>2.5M: {s5_dv}, "
            + f"{threshold_label}: {s5_atr}"
        )
    except Exception:
        pass
    # System6 ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å†…è¨³ï¼ˆLow>=5 â†’ DV50>10Mï¼‰
    try:
        stats6 = filter_stats.get("system6", {})
        s6_total = stats6.get("total", len(symbols or []))
        s6_low = stats6.get("low_pass", 0)
        s6_dv = stats6.get("dv_pass", 0)
        _log("ğŸ§ª system6å†…è¨³: " + f"å…ƒ={s6_total}, Low>=5: {s6_low}, DV50>10M: {s6_dv}")
    except Exception:
        pass
    # System7 ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å†…è¨³ï¼ˆSPYå›ºå®šï¼‰
    try:
        spyp = (
            1 if ("SPY" in basic_data and not getattr(basic_data.get("SPY"), "empty", True)) else 0
        )
        _log(f"ğŸ§ª system7å†…è¨³: SPYå›ºå®š | SPYå­˜åœ¨={spyp}")
    except Exception:
        pass
    _log(
        "ğŸ§ª ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœ: "
        + f"system1={len(system1_syms)}ä»¶, "
        + f"system2={len(system2_syms)}ä»¶, "
        + f"system3={len(system3_syms)}ä»¶, "
        + f"system4={len(system4_syms)}ä»¶, "
        + f"system5={len(system5_syms)}ä»¶, "
        + f"system6={len(system6_syms)}ä»¶, "
        + f"system7={spyp}ä»¶"
    )
    if progress_callback:
        try:
            progress_callback(3, 8, "filter")
        except Exception:
            pass

    # å„ã‚·ã‚¹ãƒ†ãƒ ç”¨ã®ç”Ÿãƒ‡ãƒ¼ã‚¿è¾æ›¸ã‚’äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¾Œã®éŠ˜æŸ„ã§æ§‹ç¯‰
    _log("ğŸ§® æŒ‡æ¨™è¨ˆç®—ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ä¸­ (system1)â€¦")
    raw_data_system1 = _subset_data(basic_data, system1_syms)
    _log(f"ğŸ§® æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿: system1={len(raw_data_system1)}éŠ˜æŸ„")
    # System1 ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å†…è¨³ï¼ˆæœ€æ–°æ—¥ã® setup åˆ¤å®šæ•°ï¼‰ã‚’ CLI ã«å‡ºåŠ›
    s1_setup = None
    s1_setup_eff = None
    # s1_spy_gate = None  # Unused variable removed
    try:
        # ãƒ•ã‚£ãƒ«ã‚¿é€šéã¯äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœï¼ˆsystem1_symsï¼‰ç”±æ¥ã§ç¢ºå®š
        s1_filter = int(len(system1_syms))
        # ç›´è¿‘æ—¥ã® SMA25>SMA50 ã‚’é›†è¨ˆï¼ˆäº‹å‰è¨ˆç®—æ¸ˆã¿åˆ—ã‚’å‚ç…§ï¼‰
        s1_setup_calc = 0
        # å¸‚å ´æ¡ä»¶ï¼ˆSPYã®Close>SMA100ï¼‰ã‚’å…ˆã«åˆ¤å®š
        _spy_ok = None
        try:
            if "SPY" in (basic_data or {}):
                _spy_df = get_spy_with_indicators(basic_data["SPY"])
                if _spy_df is not None and not getattr(_spy_df, "empty", True):
                    _last = _spy_df.iloc[-1]
                    _spy_ok = int(float(_last.get("Close", 0)) > float(_last.get("SMA100", 0)))
        except Exception:
            _spy_ok = None
        # system1 ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼ˆæœ€åˆã®1éŠ˜æŸ„åˆ†ï¼‰ã‚’ä¸€æ™‚çš„ã«ä¿æŒã—ã€ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å†…è¨³ã®å¾Œã«ã¾ã¨ã‚ã¦å‡ºåŠ›
        s1_debug_cols_line = None
        s1_debug_once_line = None
        for _sym, _df in (raw_data_system1 or {}).items():
            if _df is None or getattr(_df, "empty", True):
                continue
            try:
                last = _df.iloc[-1]
            except Exception:
                continue
            # æœ€åˆã®1ä»¶ã ã‘ãƒ‡ãƒãƒƒã‚°è¡Œã‚’æº–å‚™ï¼ˆã™ãã«å‡ºåŠ›ã›ãšã€å†…è¨³ãƒ­ã‚°ã®å¾Œã«ã¾ã¨ã‚ã¦å‡ºã™ï¼‰
            if s1_setup_calc == 0 and s1_debug_cols_line is None:
                try:
                    s1_debug_cols_line = (
                        f"[DEBUG_S1_COLS] sym={_sym} df_cols={list(_df.columns)[:40]}"
                    )
                except Exception:
                    s1_debug_cols_line = None
            if s1_setup_calc == 0 and s1_debug_once_line is None:
                try:
                    _cols_preview = (
                        list(last.index)
                        if hasattr(last, "index")
                        else list(getattr(last, "keys", lambda: [])())
                    )
                except Exception:
                    _cols_preview = []
                try:
                    _s25_raw = get_indicator(last, "sma25")
                    _s50_raw = get_indicator(last, "sma50")
                    s1_debug_once_line = (
                        f"[DEBUG_S1_ONCE] sym={_sym} "
                        f"sma25_raw={_s25_raw} sma50_raw={_s50_raw} "
                        f"cols_sample={_cols_preview[:25]}"
                    )
                except Exception:
                    s1_debug_once_line = f"[DEBUG_S1_ONCE] sym={_sym} å–å¾—å¤±æ•—"
            try:
                a = to_float(get_indicator(last, "sma25"))
                b = to_float(get_indicator(last, "sma50"))
                if (not pd.isna(a)) and (not pd.isna(b)) and a > b:
                    s1_setup_calc += 1
            except Exception:
                pass
        s1_setup = int(s1_setup_calc)
        # å‡ºåŠ›é †: ãƒ•ã‚£ãƒ«ã‚¿é€šé â†’ SPY>SMA100 â†’ SMA25>SMA50
        if _spy_ok is None:
            _log(
                f"ğŸ§© system1ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å†…è¨³: ãƒ•ã‚£ãƒ«ã‚¿é€šé={s1_filter}, SPY>SMA100: -, "
                f"SMA25>SMA50: {s1_setup}"
            )
        else:
            _log(
                f"ğŸ§© system1ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å†…è¨³: ãƒ•ã‚£ãƒ«ã‚¿é€šé={s1_filter}, SPY>SMA100: {_spy_ok}, "
                f"SMA25>SMA50: {s1_setup}"
            )
        # ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å†…è¨³ã®å¾Œã«ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’é †ã«å‡ºåŠ›ï¼ˆäº¤äº’ã®ç¾è¦³ã‚’å´©ã•ãªã„ãŸã‚ï¼‰
        # COMPACT_TODAY_LOGS=1ã®å ´åˆã¯DEBUGãƒ­ã‚°ã‚’æŠ‘åˆ¶
        try:
            if not os.getenv("COMPACT_TODAY_LOGS"):
                if s1_debug_cols_line:
                    print(s1_debug_cols_line)
                if s1_debug_once_line:
                    print(s1_debug_once_line)
        except Exception:
            pass
        # UI ã® STUpass ã¸åæ˜ ï¼ˆ50%æ™‚ç‚¹ï¼‰
        try:
            s1_setup_eff = int(s1_setup)
            try:
                if isinstance(_spy_ok, int) and _spy_ok == 0:
                    s1_setup_eff = 0
            except Exception:
                pass
            _stage(
                "system1",
                50,
                filter_count=int(s1_filter),
                setup_count=int(s1_setup_eff),
                candidate_count=None,
                entry_count=None,
            )
        except Exception:
            pass
        # å‚è€ƒ: System1 ã® SPY gate çŠ¶æ…‹ã‚’ UI ã«è£œè¶³è¡¨ç¤º
        try:
            cb_note = globals().get("_PER_SYSTEM_NOTE")
            if cb_note and callable(cb_note):
                try:
                    if _spy_ok is None:
                        cb_note("system1", "SPY>SMA100: -")
                    else:
                        cb_note(
                            "system1",
                            "SPY>SMA100: OK" if int(_spy_ok) == 1 else "SPY>SMA100: NG",
                        )
                except Exception:
                    pass
        except Exception:
            pass
        if s1_setup_eff is None:
            s1_setup_eff = s1_setup
        # s1_spy_gate = _spy_ok  # Unused variable removed
    except Exception:
        pass
    _log("ğŸ§® æŒ‡æ¨™è¨ˆç®—ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ä¸­ (system2)â€¦")
    raw_data_system2 = _subset_data(basic_data, system2_syms)
    _log(f"ğŸ§® æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿: system2={len(raw_data_system2)}éŠ˜æŸ„")
    # System2 ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å†…è¨³: ãƒ•ã‚£ãƒ«ã‚¿é€šé, RSI3>90, TwoDayUp
    s2_setup = None
    try:
        s2_filter = int(len(system2_syms))
        s2_rsi = 0
        s2_combo = 0
        for _sym in system2_syms or []:
            _df = raw_data_system2.get(_sym)
            if _df is None or getattr(_df, "empty", True):
                continue
            try:
                last = _df.iloc[-1]
            except Exception:
                continue
            try:
                rv = to_float(get_indicator(last, "rsi3"))
                rsi_pass = (not pd.isna(rv)) and rv > 90
            except Exception:
                rsi_pass = False
            if not rsi_pass:
                continue
            s2_rsi += 1
            try:
                up = get_indicator(last, "twodayup") or get_indicator(last, "uptwodays")
                if bool(up):
                    s2_combo += 1
            except Exception:
                pass
        s2_setup = int(s2_combo)
        _log(
            "ğŸ§© system2ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å†…è¨³: "
            + f"ãƒ•ã‚£ãƒ«ã‚¿é€šé={s2_filter}, RSI3>90: {s2_rsi}, "
            + f"TwoDayUp: {s2_setup}"
        )
        try:
            _stage(
                "system2",
                50,
                filter_count=int(s2_filter),
                setup_count=int(s2_setup),
                candidate_count=None,
                entry_count=None,
            )
        except Exception:
            pass
    except Exception:
        pass
    _log("ğŸ§® æŒ‡æ¨™è¨ˆç®—ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ä¸­ (system3)â€¦")
    raw_data_system3 = _subset_data(basic_data, system3_syms)
    _log(f"ğŸ§® æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿: system3={len(raw_data_system3)}éŠ˜æŸ„")
    # System3 ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å†…è¨³: ãƒ•ã‚£ãƒ«ã‚¿é€šé, Close>SMA150, 3æ—¥ä¸‹è½ç‡>=12.5%
    s3_setup = None
    try:
        s3_filter = int(len(system3_syms))
        s3_close = 0
        s3_combo = 0
        for _sym in system3_syms or []:
            _df = raw_data_system3.get(_sym)
            if _df is None or getattr(_df, "empty", True):
                continue
            try:
                last = _df.iloc[-1]
            except Exception:
                continue
            try:
                cval = to_float(last.get("Close"))
                sval = to_float(get_indicator(last, "sma150"))
                close_pass = (not pd.isna(cval)) and (not pd.isna(sval)) and cval > sval
            except Exception:
                close_pass = False
            if not close_pass:
                continue
            s3_close += 1
            try:
                dv = to_float(get_indicator(last, "drop3d"))
                drop_pass = (not pd.isna(dv)) and dv >= 0.125
            except Exception:
                drop_pass = False
            if drop_pass:
                s3_combo += 1
        s3_setup = int(s3_combo)
        _log(
            "ğŸ§© system3ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å†…è¨³: "
            + f"ãƒ•ã‚£ãƒ«ã‚¿é€šé={s3_filter}, Close>SMA150: {s3_close}, "
            + f"3æ—¥ä¸‹è½ç‡>=12.5%: {s3_setup}"
        )
        try:
            _stage(
                "system3",
                50,
                filter_count=int(s3_filter),
                setup_count=int(s3_setup),
                candidate_count=None,
                entry_count=None,
            )
        except Exception:
            pass
    except Exception:
        pass
    _log("ğŸ§® æŒ‡æ¨™è¨ˆç®—ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ä¸­ (system4)â€¦")
    raw_data_system4 = _subset_data(basic_data, system4_syms)
    _log(f"ğŸ§® æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿: system4={len(raw_data_system4)}éŠ˜æŸ„")
    # System4 ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å†…è¨³: ãƒ•ã‚£ãƒ«ã‚¿é€šé, Close>SMA200
    try:
        s4_filter = int(len(system4_syms))
        s4_close = 0
        for _sym in system4_syms or []:
            _df = raw_data_system4.get(_sym)
            if _df is None or getattr(_df, "empty", True):
                continue
            try:
                last = _df.iloc[-1]
            except Exception:
                continue
            try:
                sval = to_float(get_indicator(last, "sma200"))
                cval = to_float(last.get("Close"))
                if (not pd.isna(sval)) and (not pd.isna(cval)) and cval > sval:
                    s4_close += 1
            except Exception:
                pass
        _log(f"ğŸ§© system4ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å†…è¨³: ãƒ•ã‚£ãƒ«ã‚¿é€šé={s4_filter}, Close>SMA200: {s4_close}")
        try:
            _stage(
                "system4",
                50,
                filter_count=int(s4_filter),
                setup_count=int(s4_close),
                candidate_count=None,
                entry_count=None,
            )
        except Exception:
            pass
    except Exception:
        pass
    _log("ğŸ§® æŒ‡æ¨™è¨ˆç®—ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ä¸­ (system5)â€¦")
    raw_data_system5 = _subset_data(basic_data, system5_syms)
    _log(f"ğŸ§® æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿: system5={len(raw_data_system5)}éŠ˜æŸ„")
    # System5 ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å†…è¨³: ãƒ•ã‚£ãƒ«ã‚¿é€šé, Close>SMA100+ATR10, ADX7>55, RSI3<50
    s5_setup = None
    try:
        s5_filter = int(len(system5_syms))
        s5_close = 0
        s5_adx = 0
        s5_combo = 0
        for _sym in system5_syms or []:
            _df = raw_data_system5.get(_sym)
            if _df is None or getattr(_df, "empty", True):
                continue
            try:
                last = _df.iloc[-1]
            except Exception:
                continue
            try:
                cval = to_float(last.get("Close"))
                sval = to_float(get_indicator(last, "sma100"))
                aval = to_float(get_indicator(last, "atr10"))
                price_pass = (
                    (not pd.isna(cval))
                    and (not pd.isna(sval))
                    and (not pd.isna(aval))
                    and (cval > sval + aval)
                )
            except Exception:
                price_pass = False
            if not price_pass:
                continue
            s5_close += 1
            try:
                adx_val = to_float(get_indicator(last, "adx7"))
                adx_pass = (not pd.isna(adx_val)) and adx_val > 55
            except Exception:
                adx_pass = False
            if not adx_pass:
                continue
            s5_adx += 1
            try:
                rsi_val = to_float(get_indicator(last, "rsi3"))
                rsi_pass = (not pd.isna(rsi_val)) and rsi_val < 50
            except Exception:
                rsi_pass = False
            if rsi_pass:
                s5_combo += 1
        s5_setup = int(s5_combo)
        _log(
            "ğŸ§© system5ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å†…è¨³: "
            + f"ãƒ•ã‚£ãƒ«ã‚¿é€šé={s5_filter}, Close>SMA100+ATR10: {s5_close}, "
            + f"ADX7>55: {s5_adx}, RSI3<50: {s5_setup}"
        )
        try:
            _stage(
                "system5",
                50,
                filter_count=int(s5_filter),
                setup_count=int(s5_setup),
                candidate_count=None,
                entry_count=None,
            )
        except Exception:
            pass
    except Exception:
        pass
    _log("ğŸ§® æŒ‡æ¨™è¨ˆç®—ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ä¸­ (system6)â€¦")
    raw_data_system6 = _subset_data(basic_data, system6_syms)
    _log(f"ğŸ§® æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿: system6={len(raw_data_system6)}éŠ˜æŸ„")
    # System6 ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å†…è¨³: å„æ¡ä»¶ã‚’ç‹¬ç«‹ã‚«ã‚¦ãƒ³ãƒˆ
    s6_setup = None
    try:
        s6_filter = int(len(system6_syms))
        s6_ret = 0
        s6_uptwo = 0
        s6_combo = 0
        for _sym in system6_syms or []:
            _df = raw_data_system6.get(_sym)
            if _df is None or getattr(_df, "empty", True):
                continue
            try:
                last = _df.iloc[-1]
            except Exception:
                continue
            # return_6d>20% åˆ¤å®šï¼ˆç‹¬ç«‹ï¼‰
            try:
                # æŒ‡æ¨™ã‚¢ã‚¯ã‚»ã‚¹APIã§åˆ—åæºã‚Œã«å¯¾å¿œï¼ˆreturn_6d/RETURN_6Dï¼‰
                r6v = to_float(get_indicator(last, "return_6d"))
                ret_pass = (not pd.isna(r6v)) and (r6v > 0.20)
            except Exception:
                ret_pass = False
            if ret_pass:
                s6_ret += 1
            # UpTwoDays åˆ¤å®šï¼ˆç‹¬ç«‹ï¼‰
            try:
                # åˆ—åæºã‚Œã«å¯¾å¿œï¼ˆUpTwoDays/TwoDayUp/twodayup/uptwodaysï¼‰
                up_pass = bool(is_true(get_indicator(last, "uptwodays")))
            except Exception:
                up_pass = False
            if up_pass:
                s6_uptwo += 1
            # AND æ¡ä»¶ï¼ˆreturn_6d>20% ã‹ã¤ UpTwoDaysï¼‰
            if ret_pass and up_pass:
                s6_combo += 1
        # ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—çµæœã¯ AND æ¡ä»¶ã§é›†è¨ˆ
        s6_setup = int(s6_combo)
        _log(
            "ğŸ§© system6ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å†…è¨³: "
            + f"ãƒ•ã‚£ãƒ«ã‚¿é€šé={s6_filter}, return_6d>20%: {s6_ret}, "
            + f"UpTwoDays: {s6_uptwo}"
        )
        try:
            _stage(
                "system6",
                50,
                filter_count=int(s6_filter),
                setup_count=int(s6_setup),
                candidate_count=None,
                entry_count=None,
            )
        except Exception:
            pass
    except Exception:
        pass
    # System7 ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å†…è¨³ï¼ˆSPYå›ºå®š: Low <= min_50ï¼‰
    s7_filter = 0
    s7_setup = 0
    try:
        if "SPY" in basic_data:
            s7_filter = 1
            spy_data = basic_data["SPY"]
            if not spy_data.empty:
                # æœ€æ–°è¡Œã‚’å–å¾—
                last_row = spy_data.iloc[-1] if hasattr(spy_data, "iloc") else spy_data
                # ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ¡ä»¶: Low <= min_50
                try:
                    low_val = to_float(get_indicator(cast(Mapping[str, Any], last_row), "Low"))
                    min50_val = to_float(get_indicator(cast(Mapping[str, Any], last_row), "min_50"))
                    if (not pd.isna(low_val)) and (not pd.isna(min50_val)) and low_val <= min50_val:
                        s7_setup = 1
                except Exception:
                    pass
        _log(f"ğŸ§© system7ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å†…è¨³: ãƒ•ã‚£ãƒ«ã‚¿é€šé={s7_filter}, Low<=min_50: {s7_setup}")
        try:
            _stage(
                "system7",
                50,
                filter_count=int(s7_filter),
                setup_count=int(s7_setup),
                candidate_count=None,
                entry_count=None,
            )
        except Exception:
            pass
    except Exception:
        pass
    try:
        # system1 ã¯ SPY ã‚²ãƒ¼ãƒˆé©ç”¨å¾Œã®å®ŸåŠ¹å€¤ã‚’å„ªå…ˆ
        try:
            _s1_base = (
                s1_setup_eff
                if ("s1_setup_eff" in locals() and s1_setup_eff is not None)
                else (s1_setup or 0)
            )
            s1_val = int(_s1_base)
        except Exception:
            s1_val = int(s1_setup or 0)
        s2_val = int(s2_setup or 0) if "s2_setup" in locals() else 0
        s3_val = int(s3_setup or 0) if "s3_setup" in locals() else 0
        # system4 ã¯ Close>SMA200 ä»¶æ•°ï¼ˆs4_closeï¼‰ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ç›¸å½“ã¨ã—ã¦æ‰±ã†
        s4_val = int(locals().get("s4_close", 0) or 0)
        s5_val = int(s5_setup or 0) if "s5_setup" in locals() else 0
        s6_val = int(s6_setup or 0) if "s6_setup" in locals() else 0
        s7_val = int(s7_setup or 0) if "s7_setup" in locals() else 0

        _log(
            "ğŸ§© ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—çµæœ: "
            + f"system1={s1_val}ä»¶, "
            + f"system2={s2_val}ä»¶, "
            + f"system3={s3_val}ä»¶, "
            + f"system4={s4_val}ä»¶, "
            + f"system5={s5_val}ä»¶, "
            + f"system6={s6_val}ä»¶, "
            + f"system7={s7_val}ä»¶"
        )
    except Exception:
        pass
    if progress_callback:
        try:
            progress_callback(4, 8, "load_indicators")
        except Exception:
            pass
    # ...raw_data_system...
    if "SPY" in basic_data:
        spy_df = get_spy_with_indicators(basic_data["SPY"])
    else:
        spy_df = None
        _log(
            "âš ï¸ SPY ãŒã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ (base/full_backup/rolling ã‚’ç¢ºèª)ã€‚"
            "SPY.csv ã‚’ data_cache/base ã‚‚ã—ãã¯ data_cache/full_backup ã«é…ç½®ã—ã¦ãã ã•ã„ã€‚"
        )

    # ã‚¹ãƒˆãƒ©ãƒ†ã‚¸åˆæœŸåŒ–
    strategy_objs = [
        System1Strategy(),
        System2Strategy(),
        System3Strategy(),
        System4Strategy(),
        System5Strategy(),
        # fixed_mode=True ã§äº‹å‰è¨ˆç®—æ¸ˆã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ã®ã¿åˆ©ç”¨ï¼ˆé«˜é€ŸçµŒè·¯ï¼‰
        System6Strategy(),
        System7Strategy(),
    ]
    strategies = {getattr(s, "SYSTEM_NAME", "").lower(): s for s in strategy_objs}

    # å„ã‚·ã‚¹ãƒ†ãƒ ã®å½“æ—¥ã‚·ã‚°ãƒŠãƒ«æŠ½å‡ºã‚’ä¸¦åˆ—å®Ÿè¡Œ
    _log("ğŸš€ å„ã‚·ã‚¹ãƒ†ãƒ ã®å½“æ—¥ã‚·ã‚°ãƒŠãƒ«æŠ½å‡ºã‚’é–‹å§‹")

    per_system = {}
    system_names = [f"system{i}" for i in range(1, 8)]

    for system_name in system_names:
        _log(f"â–¶ {system_name} é–‹å§‹")

        # ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹ã‚’UIã«é€šçŸ¥
        try:
            if per_system_progress and callable(per_system_progress):
                per_system_progress(system_name, "start")
        except Exception:
            pass

        try:
            if system_name == "system1":
                raw_data = raw_data_system1
            elif system_name == "system2":
                raw_data = raw_data_system2
            elif system_name == "system3":
                raw_data = raw_data_system3
            elif system_name == "system4":
                raw_data = raw_data_system4
            elif system_name == "system5":
                raw_data = raw_data_system5
            elif system_name == "system6":
                raw_data = raw_data_system6
            elif system_name == "system7":
                raw_data = {"SPY": basic_data.get("SPY")}
            else:
                raw_data = basic_data

            strategy = strategies.get(system_name)
            if strategy is None:
                _log(f"[{system_name}] âŒ strategy not found")
                per_system[system_name] = pd.DataFrame()
                continue

            # ã‚·ã‚¹ãƒ†ãƒ å›ºæœ‰ã®ãƒ­ã‚¸ãƒƒã‚¯å®Ÿè¡Œ
            if system_name == "system4" and spy_df is None:
                _log(
                    f"[{system_name}] âš ï¸ System4 ã¯ SPY æŒ‡æ¨™ãŒå¿…è¦ã§ã™ãŒ SPY ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
                )
                per_system[system_name] = pd.DataFrame()
                continue

            _log(f"[{system_name}] ğŸ” {system_name}: ã‚·ã‚°ãƒŠãƒ«æŠ½å‡ºã‚’é–‹å§‹")
            with _PerfTimer(f"{system_name}.prepare_data"):
                try:
                    prepared_data = strategy.prepare_data(raw_data)
                except Exception as prep_err:
                    _log_error(f"[{system_name}] ãƒ‡ãƒ¼ã‚¿æº–å‚™ã§ã‚¨ãƒ©ãƒ¼: {prep_err}", "DATA001")
                    per_system[system_name] = pd.DataFrame()
                    _log(f"[{system_name}] âŒ {system_name}: 0 ä»¶ ğŸš«")
                    _log(f"âœ… {system_name} å®Œäº†: 0ä»¶")
                    continue

            candidate_kwargs: dict[str, Any] = {}
            if system_name == "system4":
                candidate_kwargs["market_df"] = spy_df

            # today å®Ÿè¡Œã§ã¯æœ€æ–°æ—¥ã®ã¿ã‚’å¯¾è±¡ã¨ã—ãŸé«˜é€Ÿå€™è£œæŠ½å‡ºã‚’æœ‰åŠ¹åŒ–ï¼ˆãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆäº’æ›ä¿æŒã®ãŸã‚ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            # --full-scan-today ãƒ•ãƒ©ã‚°ï¼ˆã¾ãŸã¯ç’°å¢ƒå¤‰æ•° FULL_SCAN_TODAY=1ï¼‰æŒ‡å®šæ™‚ã¯ latest_only ã‚’ç„¡åŠ¹åŒ–ã—
            # å¾“æ¥ã©ãŠã‚Šå…¨å±¥æ­´ã‚’å¯¾è±¡ã«å€™è£œæŠ½å‡ºã™ã‚‹ã€‚
            try:
                _disable_fast = False
                # ç’°å¢ƒå¤‰æ•°å„ªå…ˆ: FULL_SCAN_TODAY=1/true/on ãªã‚‰ç„¡åŠ¹åŒ–
                _env_full = (os.environ.get("FULL_SCAN_TODAY") or "").strip().lower()
                if _env_full in {"1", "true", "yes", "on"}:
                    _disable_fast = True
                # argparse ã‹ã‚‰ã®ãƒ•ãƒ©ã‚°ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆã®ã¿ getattr ã§å®‰å…¨å–å¾—ï¼‰
                if not _disable_fast:
                    _args_obj = globals().get("_CLI_ARGS")
                    if _args_obj is not None:
                        try:
                            if getattr(_args_obj, "full_scan_today", False):
                                _disable_fast = True
                        except Exception:
                            pass
                if not _disable_fast and system_name in {
                    "system1",
                    "system2",
                    "system3",
                    "system4",
                    "system5",
                    "system6",
                    "system7",
                }:
                    candidate_kwargs.setdefault("latest_only", True)
                else:
                    # æ˜ç¤ºçš„ã«ç„¡åŠ¹åŒ–ã™ã‚‹å ´åˆã¯ latest_only=False ã‚’å…¥ã‚Œã¦ãŠãï¼ˆã‚¹ãƒˆãƒ©ãƒ†ã‚¸å´ã§åˆ†å²å®¹æ˜“ï¼‰
                    candidate_kwargs.setdefault("latest_only", False)
            except Exception:
                # å¤±æ•—æ™‚ã¯å¾“æ¥æŒ™å‹•ï¼ˆé«˜é€ŸçµŒè·¯ï¼‰
                if system_name in {
                    "system1",
                    "system2",
                    "system3",
                    "system4",
                    "system5",
                    "system6",
                    "system7",
                }:
                    candidate_kwargs.setdefault("latest_only", True)

            # ã“ã“ã‹ã‚‰: latest_only å¯¾è±¡æ—¥ã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«ä¸€åº¦ã ã‘æ±ºã‚ãŸ ctx.signal_base_day ã‚’ä½¿ç”¨
            try:
                if candidate_kwargs.get("latest_only", False):
                    base_day = getattr(ctx, "signal_base_day", None)
                    # ãƒ•ãƒ«ã‚¹ã‚­ãƒ£ãƒ³ã¸ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯è¡Œã‚ãšã€latest_only ã‚’ç¶­æŒã™ã‚‹
                    if base_day is not None:
                        # å…¨ã‚·ã‚¹ãƒ†ãƒ ã«ã‚°ãƒ­ãƒ¼ãƒãƒ«åŸºæº–æ—¥ã‚’æ³¨å…¥ï¼ˆsystem6 ã‚‚å¯¾å¿œæ¸ˆã¿ï¼‰
                        candidate_kwargs["latest_mode_date"] = pd.Timestamp(base_day).normalize()
            except Exception:
                pass
            # DEBUG: latest_only ãƒ•ãƒ©ã‚°ã¨ top_n ç›¸å½“ã‚’ãƒ­ã‚°ï¼ˆsystem1ã®ã¿å†—é•·ï¼‰
            try:
                if system_name == "system1":
                    _log(
                        "[system1] DEBUG call generate_candidates "
                        f"latest_only={candidate_kwargs.get('latest_only')} "
                        f"prepared_syms={len(prepared_data)}"
                    )
            except Exception:
                pass
            with _PerfTimer(f"{system_name}.generate_candidates"):
                candidates, _ = strategy.generate_candidates(prepared_data, **candidate_kwargs)

            # TRD ãƒªã‚¹ãƒˆé•·ã®æ¤œè¨¼ï¼ˆãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰æ™‚ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ï¼‰
            try:
                expected_max_trd = get_expected_max_for_mode(ctx.test_mode)
                trd_result = verify_trd_length(candidates or {}, system_name, expected_max_trd)
                if not trd_result["valid"]:
                    _log(f"[{system_name}] {trd_result['message']}")
                else:
                    # æ—¢å®šã¯ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆé‹ç”¨ã‚’å°Šé‡ã—ã¦ DEBUG ã«ç•™ã‚ã‚‹
                    # TRD_LOG_OK=1/true/yes/on ã®ã¨ãã¯ Info ã¨ã—ã¦ã‚‚å‡ºåŠ›
                    _ok_env = (os.environ.get("TRD_LOG_OK") or "").strip().lower()
                    if _ok_env in {"1", "true", "yes", "on"}:
                        _log(f"[{system_name}] {trd_result['message']}")
                    else:
                        rate_limited_logger = _get_rate_limited_logger()
                        rate_limited_logger.debug_rate_limited(
                            trd_result["message"],
                            message_key=f"trd_verify_{system_name}",
                            interval=10,
                        )
            except Exception as trd_err:
                # æ¤œè¨¼å¤±æ•—æ™‚ã‚‚ã‚¨ãƒ©ãƒ¼ã§æ­¢ã‚ãªã„ï¼ˆè­¦å‘Šã®ã¿ï¼‰
                _log(f"[{system_name}] âš ï¸ TRDæ¤œè¨¼ã§ã‚¨ãƒ©ãƒ¼: {trd_err}")

            try:
                if system_name == "system1":
                    _log(
                        "[system1] DEBUG returned candidate_dates="
                        f"{len(candidates) if candidates else 0}"
                    )
            except Exception:
                pass
            if candidates:
                # å€™è£œã‚’DataFrameã«å¤‰æ›
                rows = []
                for date_key, symbols_data in candidates.items():
                    if isinstance(symbols_data, dict):
                        for symbol, data in symbols_data.items():
                            rows.append(
                                {
                                    "system": system_name,
                                    "symbol": symbol,
                                    "entry_date": date_key,
                                    "side": (
                                        (
                                            "long"
                                            if int(system_name[-1]) in [1, 3, 4, 5]
                                            else "short"
                                        )
                                    ),
                                    **data,
                                }
                            )
                df = pd.DataFrame(rows) if rows else pd.DataFrame()
            else:
                df = pd.DataFrame()

            per_system[system_name] = df
            count = len(df) if not df.empty else 0
            if count > 0:
                # æˆåŠŸã‚¢ã‚¤ã‚³ãƒ³ï¼ˆå¾“æ¥ã¯å¸¸ã«âŒè¡¨ç¤ºã ã£ãŸç®‡æ‰€ã‚’æ¡ä»¶åˆ†å²ï¼‰
                _log(f"[{system_name}] âœ… {system_name}: {count} ä»¶")
            else:
                _log(f"[{system_name}] âŒ {system_name}: {count} ä»¶ ğŸš«")

            try:
                diag_payload = getattr(strategy, "last_diagnostics", None)
                if isinstance(diag_payload, dict):
                    ctx.system_diagnostics[system_name] = diag_payload
                    _log_zero_candidate_diagnostics(system_name, count, diag_payload)
            except Exception:
                pass

        except Exception as e:
            _log(f"[{system_name}] âš ï¸ {system_name}: ã‚·ã‚°ãƒŠãƒ«æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            per_system[system_name] = pd.DataFrame()
            _log(f"[{system_name}] âŒ {system_name}: 0 ä»¶ ğŸš«")

        _log(f"âœ… {system_name} å®Œäº†: {len(per_system[system_name])}ä»¶")

        # ã‚·ã‚¹ãƒ†ãƒ å®Œäº†ã‚’UIã«é€šçŸ¥
        try:
            if per_system_progress and callable(per_system_progress):
                per_system_progress(system_name, "done")
        except Exception:
            pass

    # é€²æ—é€šçŸ¥
    if progress_callback:
        try:
            progress_callback(6, 8, "strategies_done")
        except Exception:
            pass

    # ã‚·ã‚¹ãƒ†ãƒ åˆ¥ã®é †åºã‚’æ˜ç¤ºï¼ˆ1..7ï¼‰ã«å›ºå®š
    order_1_7 = [f"system{i}" for i in range(1, 8)]
    per_system = {k: per_system.get(k, pd.DataFrame()) for k in order_1_7 if k in per_system}
    ctx.per_system_frames = dict(per_system)
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¦‚è¦è¨ˆç®—

    # === Allocation & Final Assembly ===
    # ã“ã“ã§ per_system ã‹ã‚‰æœ€çµ‚å€™è£œ (final_df) ã‚’æ§‹ç¯‰ã— AllocationSummary ã‚’å–å¾—ã™ã‚‹ã€‚
    try:
        # ã‚·ãƒ³ãƒœãƒ«â†’system ãƒãƒƒãƒ—ï¼ˆå­˜åœ¨ã—ãªãã¦ã‚‚ç¶™ç¶šï¼‰
        try:
            symbol_system_map = load_symbol_system_map()
        except Exception:
            symbol_system_map = None

        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒã‚¸ã‚·ãƒ§ãƒ³æƒ…å ±ï¼ˆå°†æ¥: broker / ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—å¯èƒ½ãªã‚‰æ‹¡å¼µï¼‰
        active_positions = None  # NOTE: Could be retrieved via ctx if needed

        final_df, allocation_summary = finalize_allocation(
            per_system,
            strategies=strategies,
            positions=active_positions,
            symbol_system_map=symbol_system_map,
            slots_long=slots_long,
            slots_short=slots_short,
            capital_long=capital_long,
            capital_short=capital_short,
            system_diagnostics=ctx.system_diagnostics,
        )
    except Exception as e:
        _log(f"âŒ finalize_allocation å¤±æ•—: {e}")
        final_df = pd.DataFrame()
        from core.final_allocation import AllocationSummary as _AS  # local import to avoid cycle

        allocation_summary = _AS(
            mode="error",
            long_allocations={},
            short_allocations={},
            active_positions={},
            available_slots={},
            final_counts={},
        )

    # ä¸¦ã¹æ›¿ãˆ / é€£ç•ªä»˜ä¸ï¼ˆfinalize_allocation å†…éƒ¨ã§ä»˜ä¸ã•ã‚Œã‚‹ãŒå¿µã®ãŸã‚æœ€çµ‚å®‰å®šã‚½ãƒ¼ãƒˆï¼‰
    try:
        if not final_df.empty and "system" in final_df.columns:
            # systemç•ªå·æŠ½å‡º (system4 ç­‰)
            final_df["_system_no"] = (
                final_df["system"]
                .astype(str)
                .str.extract(r"(\d+)", expand=False)
                .fillna("0")
                .astype(int)
            )
            final_df = final_df.sort_values(["side", "_system_no"], kind="stable")
            final_df = final_df.drop(columns=["_system_no"], errors="ignore")
            if "no" not in final_df.columns:
                final_df.insert(0, "no", range(1, len(final_df) + 1))
    except Exception:
        pass

    # ã‚µãƒãƒªãƒ­ã‚°
    try:
        if final_df.empty:
            _log("ğŸ“­ æœ€çµ‚å€™è£œã¯0ä»¶ã§ã—ãŸ")
        else:
            _log(f"ğŸ“Š æœ€çµ‚å€™è£œä»¶æ•°: {len(final_df)}")
            try:
                if "system" in final_df.columns:
                    grp = final_df.groupby("system").size().to_dict()
                    for k, v in grp.items():
                        _log(f"âœ… {k}: {int(v)} ä»¶")
            except Exception:
                pass
    except Exception:
        pass

    if progress_callback:
        try:
            progress_callback(7, 8, "finalize")
        except Exception:
            pass

    # Phase5: Zero TRD escalation notification
    try:
        notify_zero_trd_all_systems(ctx, final_df)
    except Exception:
        pass

    # Phase2: Export diagnostics snapshot in test modes
    try:
        _export_diagnostics_snapshot(ctx, final_df)
    except Exception:
        pass

    # Phase4: Discrepancy triage in test modes
    try:
        _export_discrepancy_triage(ctx)
    except Exception:
        pass

    # æˆ»ã‚Šå€¤: final_df ã¨ AllocationSummary (å‘¼ã³å‡ºã—å´ã§ dict åŒ–å¯èƒ½)
    return final_df, allocation_summary


def _safe_stage_int(value: object | None) -> int:
    """å®‰å…¨ã«æ•´æ•°å€¤ã«å¤‰æ›ã™ã‚‹"""
    if value is None:
        return 0
    if isinstance(value, bool):
        return int(value)
    if isinstance(value, int):
        return value
    if isinstance(value, float):
        try:
            return int(value)
        except Exception:
            return 0
    if isinstance(value, str):
        txt = value.strip()
        if not txt:
            return 0
        try:
            return int(float(txt))
        except Exception:
            return 0
    # æœ€å¾Œã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: __int__ ã‚’å®Ÿè£…ã—ã¦ã„ã‚Œã°ä½¿ç”¨
    try:
        to_int = getattr(value, "__int__", None)
        if callable(to_int):
            v2 = to_int()
            return int(v2) if isinstance(v2, (int, float)) else 0
    except Exception:
        return 0
    return 0


def _format_stage_message(
    progress: int,
    filter_count: int | None = None,
    setup_count: int | None = None,
    candidate_count: int | None = None,
    entry_count: int | None = None,
) -> str | None:
    """é€²æ—æ®µéšã«å¿œã˜ãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    if progress < 0 or progress > 100:
        return None

    filter_int = _safe_stage_int(filter_count)
    setup_int = _safe_stage_int(setup_count)
    candidate_int = _safe_stage_int(candidate_count)
    entry_int = _safe_stage_int(entry_count)

    # ã‚·ã‚¹ãƒ†ãƒ åã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‹ã‚‰å–å¾—ï¼ˆã“ã®é–¢æ•°ã®å¤–ã§å®šç¾©ã•ã‚Œã¦ã„ã‚‹æƒ³å®šï¼‰
    name = "System"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤

    if progress == 0:
        if filter_int is not None:
            return f"ğŸ§ª {name}: ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒã‚§ãƒƒã‚¯é–‹å§‹ (å¯¾è±¡ {filter_int} éŠ˜æŸ„)"
        return f"ğŸ§ª {name}: ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒã‚§ãƒƒã‚¯ã‚’é–‹å§‹"
    if progress == 25:
        if filter_int is not None:
            return f"ğŸ§ª {name}: ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é€šé {filter_int} éŠ˜æŸ„"
        return f"ğŸ§ª {name}: ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å‡¦ç†ãŒå®Œäº†"
    if progress == 50:
        if filter_int is not None and setup_int is not None:
            return "ğŸ§© " + f"{name}: ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é€šé {setup_int}/{filter_int} éŠ˜æŸ„"
        if setup_int is not None:
            return f"ğŸ§© {name}: ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é€šé {setup_int} éŠ˜æŸ„"
        return f"ğŸ§© {name}: ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—åˆ¤å®šãŒå®Œäº†"
    if progress == 75:
        if candidate_int is not None:
            return f"ğŸ§® {name}: å€™è£œæŠ½å‡ºä¸­ (å½“æ—¥å€™è£œ {candidate_int} éŠ˜æŸ„)"
        return f"ğŸ§® {name}: å€™è£œæŠ½å‡ºã‚’å®Ÿè¡Œä¸­"
    if progress == 100:
        if entry_int is not None:
            parts: list[str] = []
            if candidate_int is not None:
                parts.append(f"å€™è£œ {candidate_int} éŠ˜æŸ„")
            parts.append(f"ã‚¨ãƒ³ãƒˆãƒªãƒ¼ {entry_int} éŠ˜æŸ„")
            joined = " / ".join(parts)
            return f"âœ… {name}: ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚¹ãƒ†ãƒ¼ã‚¸å®Œäº† ({joined})"
        return f"âœ… {name}: ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚¹ãƒ†ãƒ¼ã‚¸å®Œäº†"
    return None


def _format_phase_completion(
    prev_stage: int,
    filter_int: int | None,
    setup_int: int | None,
    candidate_int: int | None,
    final_int: int | None,
) -> str | None:
    """ãƒ•ã‚§ãƒ¼ã‚ºå®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    # phase_namesã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¹ã‚³ãƒ¼ãƒ—ã‹ã‚‰å–å¾—ã™ã‚‹æƒ³å®š
    phase_names = {
        0: "ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°",
        25: "ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—",
        50: "å€™è£œæŠ½å‡º",
        75: "æœ€çµ‚é¸å®š",
    }
    name = "System"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤

    label = phase_names.get(prev_stage)
    if not label:
        return None

    if prev_stage == 0:
        if filter_int is not None:
            return f"ğŸ {name}: {label}ã®ãƒ—ãƒ­ã‚»ã‚¹ãƒ—ãƒ¼ãƒ«ãŒå®Œäº† (é€šé {filter_int} éŠ˜æŸ„)"
        return f"ğŸ {name}: {label}ã®ãƒ—ãƒ­ã‚»ã‚¹ãƒ—ãƒ¼ãƒ«ãŒå®Œäº†"

    if prev_stage == 25:
        if setup_int is not None and filter_int is not None:
            return (
                f"ğŸ {name}: {label}ã®ãƒ—ãƒ­ã‚»ã‚¹ãƒ—ãƒ¼ãƒ«ãŒå®Œäº† "
                f"(ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é€šé {setup_int}/{filter_int} éŠ˜æŸ„)"
            )
        if setup_int is not None:
            return (
                f"ğŸ {name}: {label}ã®ãƒ—ãƒ­ã‚»ã‚¹ãƒ—ãƒ¼ãƒ«ãŒå®Œäº† " f"(ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é€šé {setup_int} éŠ˜æŸ„)"
            )
        return f"ğŸ {name}: {label}ã®ãƒ—ãƒ­ã‚»ã‚¹ãƒ—ãƒ¼ãƒ«ãŒå®Œäº†"

    if prev_stage == 50:
        if candidate_int is not None:
            return f"ğŸ {name}: {label}ã®ãƒ—ãƒ­ã‚»ã‚¹ãƒ—ãƒ¼ãƒ«ãŒå®Œäº† " f"(å½“æ—¥å€™è£œ {candidate_int} éŠ˜æŸ„)"
        return f"ğŸ {name}: {label}ã®ãƒ—ãƒ­ã‚»ã‚¹ãƒ—ãƒ¼ãƒ«ãŒå®Œäº†"

    if prev_stage == 75:
        if final_int is not None:
            parts: list[str] = [f"ã‚¨ãƒ³ãƒˆãƒªãƒ¼ {final_int} éŠ˜æŸ„"]
            if candidate_int is not None:
                parts.append(f"å€™è£œ {candidate_int} éŠ˜æŸ„")
            joined = " / ".join(parts)
            return f"ğŸ {name}: {label}ã®ãƒ—ãƒ­ã‚»ã‚¹ãƒ—ãƒ¼ãƒ«ãŒå®Œäº† ({joined})"
        return f"ğŸ {name}: {label}ã®ãƒ—ãƒ­ã‚»ã‚¹ãƒ—ãƒ¼ãƒ«ãŒå®Œäº†"

    return None


def _stage(
    system: str,
    progress: int,
    filter_count: int | None = None,
    setup_count: int | None = None,
    candidate_count: int | None = None,
    entry_count: int | None = None,
    # ã‚µãƒ–ã‚¹ãƒ†ãƒ¼ã‚¸æƒ…å ±ã®è¿½åŠ 
    substage_name: str | None = None,
    substage_progress: int | None = None,
    substage_total: int | None = None,
) -> None:
    """Record stage progress for ``system`` and flush pending UI events."""

    system_key = str(system or "").strip().lower() or "unknown"
    try:
        GLOBAL_STAGE_METRICS.record_stage(
            system_key,
            progress,
            filter_count,
            setup_count,
            candidate_count,
            entry_count,
            emit_event=True,
            substage_name=substage_name,
            substage_progress=substage_progress,
            substage_total=substage_total,
        )
    except Exception as e:
        # ãƒ­ã‚°ã‚’æ®‹ã—ã¦ãƒ‡ãƒãƒƒã‚°æ™‚ã®æ‰‹ãŒã‹ã‚Šã«ã™ã‚‹
        import logging

        logging.getLogger(__name__).debug(f"_stage failed for {system}: {e}")
        return
    _drain_stage_event_queue()


# ãƒ—ãƒ­ã‚»ã‚¹ãƒ—ãƒ¼ãƒ«åˆ©ç”¨å¯å¦ï¼ˆç’°å¢ƒå¤‰æ•°ã§ä¸Šæ›¸ãå¯ï¼‰
def _configure_process_pool_and_workers(
    name: str = "", _log: Callable[[str], None] = print
) -> tuple[bool, int | None]:
    """Configure process pool usage and worker count based on environment variables."""
    env_pp_raw = os.environ.get("USE_PROCESS_POOL", "")
    env_pp = env_pp_raw.strip().lower()
    if env_pp in {"1", "true", "yes", "on"}:
        use_process_pool = True
    elif env_pp in {"0", "false", "no", "off"}:
        use_process_pool = False
    else:
        use_process_pool = False
        if env_pp:
            _log(
                "âš ï¸ "
                + f"{name}: USE_PROCESS_POOL ã®å€¤ '{env_pp_raw}' ã‚’è§£é‡ˆã§ãã¾ã›ã‚“ã€‚"
                + "ãƒ—ãƒ­ã‚»ã‚¹ãƒ—ãƒ¼ãƒ«ã‚’ç„¡åŠ¹åŒ–ã—ã¾ã™ã€‚"
            )
    # ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã¯ç’°å¢ƒå¤‰æ•°ãŒã‚ã‚Œã°å„ªå…ˆã€ç„¡ã‘ã‚Œã°è¨­å®š(THREADS_DEFAULT)ã«é€£å‹•
    try:
        _env_workers = os.environ.get("PROCESS_POOL_WORKERS", "").strip()
        if _env_workers:
            max_workers = int(_env_workers) or None
        else:
            try:
                _st = get_settings(create_dirs=False)
                max_workers = int(getattr(_st, "THREADS_DEFAULT", 8)) or None
            except Exception:
                max_workers = None
    except Exception:
        max_workers = None
    return use_process_pool, max_workers


def _configure_lookback_days(
    name: str = "",
    stg: object | None = None,
    base: object | None = None,
) -> int:
    """Configure lookback days based on strategy requirements.

    The strategy object may optionally expose a ``get_total_days(base_df)`` method.
    We treat this attribute as ``Callable[[Any], Any] | None`` and validate at runtime
    before invoking, which prevents the E1102 (not-callable) warning once type hinted.
    """
    # ãƒ«ãƒƒã‚¯ãƒãƒƒã‚¯ã¯ã€å¿…è¦æŒ‡æ¨™ã®æœ€å¤§çª“ï¼‹Î±ã€ã‚’å‹•çš„æ¨å®š
    try:
        settings2 = get_settings(create_dirs=True)
        lb_default = int(
            settings2.cache.rolling.base_lookback_days + settings2.cache.rolling.buffer_days
        )
    except Exception:
        settings2 = None
        lb_default = 300
    # YAMLã®strategiesã‚»ã‚¯ã‚·ãƒ§ãƒ³ç­‰ã‹ã‚‰ãƒ’ãƒ³ãƒˆã‚’å–å¾—ï¼ˆãªã‘ã‚Œã°ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ï¼‰
    # ãƒ«ãƒƒã‚¯ãƒãƒƒã‚¯ã®ãƒãƒ¼ã‚¸ãƒ³/æœ€å°æ—¥æ•°ã¯ç’°å¢ƒå¤‰æ•°ã§ä¸Šæ›¸ãå¯èƒ½
    try:
        margin = float(os.environ.get("LOOKBACK_MARGIN", "0.15"))
    except Exception:
        margin = 0.15
    need_map: dict[str, int] = {
        "system1": int(220 * (1 + margin)),
        "system2": int(120 * (1 + margin)),
        # SMA150 ã‚’å®‰å®šã«è¨ˆç®—ã™ã‚‹ãŸã‚ 170 æ—¥ç¨‹åº¦ã‚’è¦æ±‚
        "system3": int(170 * (1 + margin)),
        # SMA200 ç³»ã®ãŸã‚ 220 æ—¥ç¨‹åº¦ã‚’è¦æ±‚
        "system4": int(220 * (1 + margin)),
        "system5": int(140 * (1 + margin)),
        "system6": int(80 * (1 + margin)),
        "system7": int(80 * (1 + margin)),
    }
    # æˆ¦ç•¥å´ãŒ get_total_days ã‚’å®Ÿè£…ã—ã¦ã„ã‚Œã°å„ªå…ˆ
    custom_need: int | None = None
    # Use collections.abc.Callable already imported at top for type hints.
    fn: Callable[[object], object] | None
    try:
        raw = getattr(stg, "get_total_days", None)
        fn = raw if callable(raw) else None
    except Exception:  # pragma: no cover - defensive
        fn = None
    if fn is not None:
        try:
            _val = fn(base)
            if isinstance(_val, (int, float)):
                custom_need = int(_val)
            elif isinstance(_val, str):
                try:
                    custom_need = int(float(_val))
                except Exception:
                    custom_need = None
        except Exception:  # pragma: no cover - strategy specific failures ignored
            custom_need = None
    try:
        min_floor = int(os.environ.get("LOOKBACK_MIN_DAYS", "80"))
    except Exception:
        min_floor = 80
    min_required = custom_need or need_map.get(name, lb_default)
    lookback_days = min(lb_default, max(min_floor, int(min_required)))
    return lookback_days


# Let's clean up from here and find the actual function that needs these variables
def _run_strategy_with_proper_scope(
    name: str,
    stg: object,
    base: object,
    spy_df: pd.DataFrame | None,
    today: datetime | None,
    _log: Callable[[str], None],
) -> tuple[str, pd.DataFrame, str, list[str]]:
    """Run strategy with properly scoped variables (ç¾åœ¨ã¯ç°¡ç•¥ç‰ˆ)."""
    logs: list[str] = []
    pool_outcome = "none"
    progress_q: Any | None = None
    mgr: Any | None = None

    # Configure process pool settings
    use_process_pool, max_workers = _configure_process_pool_and_workers(name=name, _log=_log)

    # Configure lookback days
    lookback_days = _configure_lookback_days(name=name, stg=stg, base=base)

    _t0 = __import__("time").time()
    # ãƒ—ãƒ­ã‚»ã‚¹ãƒ—ãƒ¼ãƒ«åˆ©ç”¨æ™‚ã‚‚ stage_progress ã‚’æ¸¡ã—ã€è¦æ‰€ã®é€²æ—ãƒ­ã‚°ã‚’å…±æœ‰ã™ã‚‹
    _log_cb = None if use_process_pool else _log
    # ãƒ—ãƒ­ã‚»ã‚¹ãƒ—ãƒ¼ãƒ«åˆ©ç”¨æ™‚ã¯ Manager().Queue ã‚’ç”Ÿæˆã—ã¦å­ãƒ—ãƒ­ã‚»ã‚¹ã‹ã‚‰
    # é€²æ—ã‚’é€ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚globals ã«ç½®ã„ã¦å­ãŒå‚ç…§ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚
    if use_process_pool:
        try:
            mgr = multiprocessing.Manager()  # noqa: F401 (kept for child access)
            progress_q = mgr.Queue()
            globals()["_PROGRESS_MANAGER"] = mgr
            globals()["_PROGRESS_QUEUE"] = progress_q
        except Exception:
            progress_q = None
            globals().pop("_PROGRESS_MANAGER", None)
            globals().pop("_PROGRESS_QUEUE", None)
    else:
        globals().pop("_PROGRESS_MANAGER", None)
        globals().pop("_PROGRESS_QUEUE", None)

    stage_reporter = StageReporter(name, progress_q)
    _stage_cb = stage_reporter
    if use_process_pool:
        workers_label = str(max_workers) if max_workers is not None else "auto"
        _log(
            f"âš™ï¸ {name}: USE_PROCESS_POOL=1 ã§ãƒ—ãƒ­ã‚»ã‚¹ãƒ—ãƒ¼ãƒ«å®Ÿè¡Œã‚’é–‹å§‹"
            + f" (workers={workers_label})"
            + " | ä¸¦åˆ—åŒ–: ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼è¨ˆç®—/å‰å‡¦ç†"
        )
        _log(
            f"ğŸ§­ {name}: ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ»ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãƒ»å€™è£œæŠ½å‡ºã¯"
            "ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã§é€²è¡ŒçŠ¶æ³ã‚’è¨˜éŒ²ã—ã¾ã™"
        )
    try:
        # æˆ¦ç•¥ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã¯çµ±ä¸€ã•ã‚Œã¦ã„ãªã„ãŸã‚ Any ã¨ã—ã¦æ‰±ã† (å¾Œç¶šæ®µéšã§æ•´å‚™äºˆå®š)
        stg_any: Any = stg
        df = stg_any.get_today_signals(
            base,
            market_df=spy_df,
            today=today,
            progress_callback=None,
            log_callback=_log_cb,
            stage_progress=_stage_cb,
            use_process_pool=use_process_pool,
            max_workers=max_workers,
            lookback_days=lookback_days,
        )
        # å­ãƒ—ãƒ­ã‚»ã‚¹ã‹ã‚‰ã‚­ãƒ¥ãƒ¼ã¸é€ã‚‰ã‚ŒãŸé€²æ—ã¯ä¸Šã§ä½œã‚‰ã‚ŒãŸ globals ä¸Šã®
        # _PROGRESS_QUEUE ã«è“„ç©ã•ã‚Œã‚‹ã€‚_drain_stage_event_queue ãŒãã‚Œã‚’
        # å®šæœŸçš„ã«å–ã‚Šå‡ºã—ã€UI æ›´æ–°ã«è»¢æ›ã™ã‚‹ã€‚
        if use_process_pool:
            pool_outcome = "success"
        _elapsed = int(max(0, __import__("time").time() - _t0))
        _m, _s = divmod(_elapsed, 60)
        _log(f"â±ï¸ {name}: çµŒé {_m}åˆ†{_s}ç§’")
        _drain_stage_event_queue()
    except Exception as e:  # noqa: BLE001
        _log(f"âš ï¸ {name}: ã‚·ã‚°ãƒŠãƒ«æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        # ãƒ—ãƒ­ã‚»ã‚¹ãƒ—ãƒ¼ãƒ«ç•°å¸¸æ™‚ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆéãƒ—ãƒ¼ãƒ«ï¼‰ã§ä¸€åº¦ã ã‘å†è©¦è¡Œ
        try:
            msg = str(e).lower()
        except Exception:
            msg = ""
        if use_process_pool and pool_outcome == "none":
            pool_outcome = "error"
        needs_fallback = any(
            k in msg
            for k in [
                "process pool",
                "a child process terminated",
                "terminated abruptly",
                "forkserver",
                "__main__",
            ]
        )
        if needs_fallback:
            _log("ğŸ›Ÿ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å†è©¦è¡Œ: ãƒ—ãƒ­ã‚»ã‚¹ãƒ—ãƒ¼ãƒ«ç„¡åŠ¹åŒ–ã§å®Ÿè¡Œã—ã¾ã™")
            try:
                _t0b = __import__("time").time()
                stg_fallback: Any = stg
                df = stg_fallback.get_today_signals(
                    base,
                    market_df=spy_df,
                    today=today,
                    progress_callback=None,
                    log_callback=_log,
                    stage_progress=StageReporter(name, None),
                    use_process_pool=False,
                    max_workers=None,
                    lookback_days=lookback_days,
                )
                _elapsed_b = int(max(0, __import__("time").time() - _t0b))
                _m2, _s2 = divmod(_elapsed_b, 60)
                _log(f"â±ï¸ {name} (fallback): çµŒé {_m2}åˆ†{_s2}ç§’")
                _drain_stage_event_queue()
                if use_process_pool:
                    pool_outcome = "fallback"
            except Exception as e2:  # noqa: BLE001
                _log(f"âŒ {name}: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚‚å¤±æ•—: {e2}")
                if use_process_pool:
                    pool_outcome = "error"
                df = pd.DataFrame()
        else:
            df = pd.DataFrame()
    finally:
        _drain_stage_event_queue()
        if use_process_pool:
            if pool_outcome == "success":
                _log(f"ğŸ {name}: ãƒ—ãƒ­ã‚»ã‚¹ãƒ—ãƒ¼ãƒ«å®Ÿè¡ŒãŒå®Œäº†ã—ã¾ã—ãŸ")
            elif pool_outcome == "fallback":
                _log(f"ğŸ {name}: ãƒ—ãƒ­ã‚»ã‚¹ãƒ—ãƒ¼ãƒ«å®Ÿè¡Œã‚’çµ‚äº†ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œæ¸ˆã¿ï¼‰")
            else:
                _log(f"ğŸ {name}: ãƒ—ãƒ­ã‚»ã‚¹ãƒ—ãƒ¼ãƒ«å®Ÿè¡Œã‚’çµ‚äº†ï¼ˆçµæœ: å¤±æ•—ï¼‰")
            globals().pop("_PROGRESS_QUEUE", None)
            globals().pop("_PROGRESS_MANAGER", None)
            if mgr is not None:
                try:
                    mgr.shutdown()
                except Exception:
                    pass
    if not df.empty:
        if "score_key" in df.columns and len(df):
            first_key = df["score_key"].iloc[0]
        else:
            first_key = None
        asc = _asc_by_score_key(first_key)
        df = df.sort_values("score", ascending=asc, na_position="last")
        df = df.reset_index(drop=True)
    if df is not None and not df.empty:
        msg = f"ğŸ“Š {name}: {len(df)} ä»¶"
    else:
        msg = f"âŒ {name}: 0 ä»¶ ğŸš«"
    _log(msg)
    logs = []  # Initialize logs list for return statement

    return name, df, msg, logs


def _run_strategy(name: str, _stg: object) -> tuple[str, pd.DataFrame, str, list[str]]:
    """
    Wrapper function for _run_strategy_with_proper_scope with appropriate defaults.
    """
    try:
        # This is a simplified wrapper - actual implementation depends on full context
        # For now, return a basic result structure
        df = pd.DataFrame()  # Empty dataframe as placeholder
        msg = f"ğŸ“Š {name}: 0 ä»¶ (placeholder)"
        logs: list[str] = []
        return name, df, msg, logs
    except Exception:
        return name, pd.DataFrame(), f"âŒ {name}: ã‚¨ãƒ©ãƒ¼", []


# Setup summary code that was after return - moved to proper location
# NOTE: This function and subsequent code have been temporarily commented out
# due to structural issues with undefined variables. The main functionality
# remains intact through other entry points.
#
# def _log_setup_summary():
#     """Log setup summary - this function should be called before strategy execution"""
#     try:
#         setup_summary = []
#         for name, val in (
#             ("system1", s1_setup_eff if s1_setup_eff is not None else s1_setup),
#             ("system2", s2_setup),
#             ("system3", s3_setup),
#             ("system4", locals().get("s4_close")),
#             ("system5", s5_setup),
#             ("system6", s6_setup),
#             ("system7", 1 if ("SPY" in (basic_data or {})) else 0),
#         ):
#             try:
#                 if val is not None:
#                     setup_summary.append(f"{name}={int(val)}")
#             except Exception:
#                 continue
#         if setup_summary:
#             _log("ğŸ§© ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é€šéã¾ã¨ã‚: " + ", ".join(setup_summary))
#     except Exception:
#         pass

#     _log("ğŸš€ å„ã‚·ã‚¹ãƒ†ãƒ ã®å½“æ—¥ã‚·ã‚°ãƒŠãƒ«æŠ½å‡ºã‚’é–‹å§‹")
#     per_system: dict[str, pd.DataFrame] = {}
#     total = len(strategies)
#     # (rest of the problematic code commented out)


def _placeholder_log_setup_summary() -> None:
    """æœ€å°ãƒ€ãƒŸãƒ¼: ç ´æã—ã¦ã„ãŸæ—§ _log_setup_summary / é‡è¤‡é…åˆ†ãƒ­ã‚¸ãƒƒã‚¯ã‚’æ’¤å»ã€‚

    å°†æ¥ã“ã“ã§ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—çµæœã‚µãƒãƒªã‚’å¾©æ´»ã•ã›ã‚‹å ´åˆã¯ã€
    (ctx, final_df ãªã©) å¿…è¦æƒ…å ±ã‚’å¼•æ•°ã¨ã—ã¦å—ã‘å–ã‚‹æ–°ã—ã„é–¢æ•°ã¨ã—ã¦å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚
    ç¾åœ¨ã¯å‰¯ä½œç”¨ãªã—ã§è»½ã„ãƒ­ã‚°ã®ã¿ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚
    """
    try:
        _log("ğŸ§© ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é€šéã¾ã¨ã‚æ©Ÿèƒ½: ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–ä¸­")
    except Exception:
        pass
    # ã“ã‚Œä»¥ä¸Šã®å‡¦ç†ã¯è¡Œã‚ãªã„ï¼ˆfinal_df ç­‰ã¯ã“ã®ã‚¹ã‚³ãƒ¼ãƒ—ã«å­˜åœ¨ã—ãªã„ãŸã‚å‚ç…§ç¦æ­¢ï¼‰
    return None


def build_cli_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="å…¨ã‚·ã‚¹ãƒ†ãƒ å½“æ—¥ã‚·ã‚°ãƒŠãƒ«æŠ½å‡ºãƒ»é›†ç´„")
    parser.add_argument(
        "--symbols",
        nargs="*",
        help="å¯¾è±¡ã‚·ãƒ³ãƒœãƒ«ã€‚æœªæŒ‡å®šãªã‚‰è¨­å®šã®auto_tickersã‚’ä½¿ç”¨",
    )
    parser.add_argument(
        "--slots-long",
        type=int,
        default=None,
        help="è²·ã„ã‚µã‚¤ãƒ‰ã®æœ€å¤§æ¡ç”¨æ•°ï¼ˆã‚¹ãƒ­ãƒƒãƒˆæ–¹å¼ï¼‰",
    )
    parser.add_argument(
        "--slots-short",
        type=int,
        default=None,
        help="å£²ã‚Šã‚µã‚¤ãƒ‰ã®æœ€å¤§æ¡ç”¨æ•°ï¼ˆã‚¹ãƒ­ãƒƒãƒˆæ–¹å¼ï¼‰",
    )
    parser.add_argument(
        "--capital-long",
        type=float,
        default=None,
        help=("è²·ã„ã‚µã‚¤ãƒ‰äºˆç®—ï¼ˆãƒ‰ãƒ«ï¼‰ã€‚æŒ‡å®šæ™‚ã¯é‡‘é¡é…åˆ†ãƒ¢ãƒ¼ãƒ‰"),
    )
    parser.add_argument(
        "--capital-short",
        type=float,
        default=None,
        help=("å£²ã‚Šã‚µã‚¤ãƒ‰äºˆç®—ï¼ˆãƒ‰ãƒ«ï¼‰ã€‚æŒ‡å®šæ™‚ã¯é‡‘é¡é…åˆ†ãƒ¢ãƒ¼ãƒ‰"),
    )
    parser.add_argument(
        "--save-csv",
        action="store_true",
        help="signalsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«CSVã‚’ä¿å­˜ã™ã‚‹",
    )
    parser.add_argument(
        "--parallel",
        action="store_true",
        help="ã‚·ã‚¹ãƒ†ãƒ ã”ã¨ã®å½“æ—¥ã‚·ã‚°ãƒŠãƒ«æŠ½å‡ºã‚’ä¸¦åˆ—å®Ÿè¡Œã™ã‚‹",
    )
    parser.add_argument(
        "--full-scan-today",
        action="store_true",
        help="å½“æ—¥ã‚·ã‚°ãƒŠãƒ«æŠ½å‡ºã§ latest_only æœ€é©åŒ–ã‚’ç„¡åŠ¹åŒ–ã—å…¨å±¥æ­´èµ°æŸ» (æ¤œè¨¼/ãƒ‡ãƒãƒƒã‚°ç”¨é€”)",
    )
    # Alpaca è‡ªå‹•ç™ºæ³¨ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    parser.add_argument(
        "--alpaca-submit",
        action="store_true",
        help="Alpaca ã«è‡ªå‹•ç™ºæ³¨ï¼ˆshares å¿…é ˆï¼‰",
    )
    parser.add_argument(
        "--order-type",
        choices=["market", "limit"],
        default="market",
        help="æ³¨æ–‡ç¨®åˆ¥",
    )
    parser.add_argument(
        "--tif",
        choices=["GTC", "DAY"],
        default="GTC",
        help="Time In Force",
    )
    parser.add_argument(
        "--live",
        action="store_true",
        help="ãƒ©ã‚¤ãƒ–å£åº§ã§ç™ºæ³¨ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Paperï¼‰",
    )
    parser.add_argument(
        "--log-file-mode",
        choices=["single", "dated"],
        default=None,
        help="ãƒ­ã‚°ä¿å­˜å½¢å¼: single=å›ºå®š today_signals.log / dated=æ—¥ä»˜åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«",
    )
    parser.add_argument(
        "--csv-name-mode",
        choices=["date", "datetime", "runid"],
        default=None,
        help=(
            "CSVãƒ•ã‚¡ã‚¤ãƒ«åã®å½¢å¼: date=YYYY-MM-DD / "
            "datetime=YYYY-MM-DD_HHMM / runid=YYYY-MM-DD_RUNID"
        ),
    )
    # è¨ˆç”» -> å®Ÿè¡Œãƒ–ãƒªãƒƒã‚¸ï¼ˆå®‰å…¨ã®ãŸã‚æ—¢å®šã¯ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼‰
    parser.add_argument(
        "--run-planned-exits",
        choices=["off", "open", "close", "auto"],
        default=None,
        help=(
            "æ‰‹ä»•èˆã„è¨ˆç”»ã®è‡ªå‹•å®Ÿè¡Œ: off=ç„¡åŠ¹ / open=å¯„ã‚Š(OPG) / "
            "close=å¼•ã‘(CLS) / auto=æ™‚é–“å¸¯ã§è‡ªå‹•åˆ¤å®š"
        ),
    )
    parser.add_argument(
        "--planned-exits-dry-run",
        action="store_true",
        help="æ‰‹ä»•èˆã„è¨ˆç”»ã®è‡ªå‹•å®Ÿè¡Œã‚’ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ã«ã™ã‚‹ï¼ˆæ—¢å®šã¯å®Ÿç™ºæ³¨ï¼‰",
    )
    parser.add_argument(
        "--benchmark",
        action="store_true",
        help="ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã®ãƒ•ã‚§ãƒ¼ã‚ºåˆ¥å®Ÿè¡Œæ™‚é–“ã‚’è¨ˆæ¸¬ã— logs/perf ã«ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜",
    )
    parser.add_argument(
        "--test-mode",
        choices=["mini", "quick", "sample", "test_symbols"],
        help="ãƒ†ã‚¹ãƒˆç”¨ãƒ¢ãƒ¼ãƒ‰: mini=10éŠ˜æŸ„ / quick=50éŠ˜æŸ„ / sample=100éŠ˜æŸ„ / test_symbols=æ¶ç©ºéŠ˜æŸ„",
    )
    parser.add_argument(
        "--skip-external",
        action="store_true",
        help="å¤–éƒ¨APIå‘¼ã³å‡ºã—ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆNASDAQ Trader, pandas_market_calendarsç­‰ï¼‰",
    )
    parser.add_argument(
        "--perf-snapshot",
        action="store_true",
        help="æ€§èƒ½ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ(JSON)ã‚’ logs/perf_snapshots ã«ä¿å­˜ (latest_only åˆ‡æ›¿æ¯”è¼ƒç”¨)",
    )
    parser.add_argument(
        "--filter-debug",
        action="store_true",
        help="ãƒ•ã‚£ãƒ«ã‚¿æ®µéšé€šéæ•°ã®FDBGãƒ­ã‚°ã‚’æœ‰åŠ¹åŒ– (ç’°å¢ƒå¤‰æ•° FILTER_DEBUG=1 ã‚’å†…éƒ¨è¨­å®š)",
    )
    return parser


def parse_cli_args() -> argparse.Namespace:
    parser = build_cli_parser()
    return parser.parse_args()


def configure_logging_for_cli(args: argparse.Namespace) -> None:
    env_mode = os.environ.get("TODAY_SIGNALS_LOG_MODE", "").strip().lower()
    mode = args.log_file_mode or (env_mode if env_mode in {"single", "dated"} else None) or "dated"
    _configure_today_logger(mode=mode)
    try:
        sel_path = globals().get("_LOG_FILE_PATH")
        _log(f"ğŸ“ ãƒ­ã‚°ä¿å­˜å…ˆ: {sel_path}", ui=False)
    except Exception:
        pass


def run_signal_pipeline(
    args: argparse.Namespace,
) -> tuple[pd.DataFrame, dict[str, pd.DataFrame]]:
    # latest_only æ¨å®š: --full-scan-today æŒ‡å®šã§ Falseã€ãã‚Œä»¥å¤– True (ã‚·ã‚¹ãƒ†ãƒ æ¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ­ã‚¸ãƒƒã‚¯ã¨æƒãˆã‚‹)
    latest_only_flag = False if getattr(args, "full_scan_today", False) else True

    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ‡ãƒãƒƒã‚°è¦æ±‚æ™‚ã«ç’°å¢ƒå¤‰æ•°è¨­å®šï¼ˆtoday_filters å´ã¯ç’°å¢ƒå‚ç…§ï¼‰
    try:
        if getattr(args, "filter_debug", False):
            os.environ.setdefault("FILTER_DEBUG", "1")
    except Exception:
        pass

    perf = None
    if getattr(args, "perf_snapshot", False):
        try:
            from common.perf_snapshot import enable_global_perf

            perf = enable_global_perf(True)
        except Exception:  # pragma: no cover - å®‰å…¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            perf = None

    from typing import ContextManager

    cm: ContextManager[Any]
    if perf is not None:
        cm = perf.run(latest_only=latest_only_flag)
    else:
        # ãƒ€ãƒŸãƒ¼ contextmanager: å¿…ãšã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ (é–¢æ•°å‚ç…§ã‚’ãã®ã¾ã¾ with ã—ãªã„)
        from contextlib import nullcontext

        cm = nullcontext()

    with cm:
        result = compute_today_signals(
            args.symbols,
            slots_long=args.slots_long,
            slots_short=args.slots_short,
            capital_long=args.capital_long,
            capital_short=args.capital_short,
            save_csv=args.save_csv,
            csv_name_mode=args.csv_name_mode,
            parallel=args.parallel,
            test_mode=getattr(args, "test_mode", None),
            skip_external=getattr(args, "skip_external", False),
        )
    # æˆ»ã‚Šå€¤ãŒNoneã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    if result is None:
        return pd.DataFrame(), {}

    # AllocationSummaryã‚’è¾æ›¸ã«å¤‰æ›ã™ã‚‹å¿…è¦ãŒã‚ã‚‹å ´åˆ
    final_df, allocation_summary = result
    if hasattr(allocation_summary, "__dict__"):
        # AllocationSummaryã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰é©åˆ‡ãªè¾æ›¸å½¢å¼ã«å¤‰æ›
        per_system_dict = {}
    else:
        # æ—¢ã«è¾æ›¸å½¢å¼ã®å ´åˆ
        per_system_dict = allocation_summary if isinstance(allocation_summary, dict) else {}

    return final_df, per_system_dict


def log_final_candidates(final_df: pd.DataFrame) -> list[Signal]:
    if final_df.empty:
        _log("ğŸ“­ æœ¬æ—¥ã®æœ€çµ‚å€™è£œã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        return []

    _log("\n=== æœ€çµ‚å€™è£œï¼ˆæ¨å¥¨ï¼‰ ===")
    cols = [
        "symbol",
        "system",
        "side",
        "signal_type",
        "entry_date",
        "entry_price",
        "stop_price",
        "shares",
        "position_value",
        "score_key",
        "score",
    ]
    show = [c for c in cols if c in final_df.columns]
    _log(final_df[show].to_string(index=False))
    signals_for_merge = [
        Signal(
            system_id=int(str(r.get("system")).replace("system", "") or 0),
            symbol=str(r.get("symbol")),
            side="BUY" if str(r.get("side")).lower() == "long" else "SELL",
            strength=float(r.get("score", 0.0)),
            meta={},
        )
        for _, r in final_df.iterrows()
    ]
    return signals_for_merge


def merge_signals_for_cli(signals_for_merge: list[Signal]) -> None:
    if not signals_for_merge:
        return
    merge_signals([signals_for_merge], portfolio_state={}, market_state={})


def maybe_submit_orders(final_df: pd.DataFrame, args: argparse.Namespace) -> None:
    if final_df.empty or not args.alpaca_submit:
        return
    submit_orders_df(
        final_df,
        paper=(not args.live),
        order_type=args.order_type,
        system_order_type=None,
        tif=args.tif,
        retries=2,
        delay=0.5,
        log_callback=_log,
        notify=True,
    )


def maybe_run_planned_exits(args: argparse.Namespace) -> None:
    try:
        from schedulers.next_day_exits import submit_planned_exits as _run_planned
    except Exception:
        _run_planned = None  # type: ignore[assignment]
    env_run = os.environ.get("RUN_PLANNED_EXITS", "").lower()
    run_mode = (
        args.run_planned_exits
        or (env_run if env_run in {"off", "open", "close", "auto"} else None)
        or "off"
    )
    dry_run = True if args.planned_exits_dry_run else False
    if _run_planned is not None and run_mode != "off":
        sel = run_mode
        if run_mode == "auto":
            now = datetime.now(ZoneInfo("America/New_York"))
            hhmm = now.strftime("%H%M")
            sel = (
                "open"
                if ("0930" <= hhmm <= "0945")
                else ("close" if ("1550" <= hhmm <= "1600") else "off")
            )
        if sel in {"open", "close"}:
            _log(f"â±ï¸ æ‰‹ä»•èˆã„è¨ˆç”»ã®è‡ªå‹•å®Ÿè¡Œ: {sel} (dry_run={dry_run})")
            try:
                df_exec = _run_planned(sel, dry_run=dry_run)
                if df_exec is not None and not df_exec.empty:
                    _log(df_exec.to_string(index=False), ui=False)
                else:
                    _log("å¯¾è±¡ã®æ‰‹ä»•èˆã„è¨ˆç”»ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚", ui=False)
            except Exception as e:
                _log(f"âš ï¸ æ‰‹ä»•èˆã„è¨ˆç”»ã®è‡ªå‹•å®Ÿè¡Œã«å¤±æ•—: {e}")


def main():
    args = parse_cli_args()
    configure_logging_for_cli(args)
    # ä»–ã‚¹ã‚³ãƒ¼ãƒ—ï¼ˆcompute_today_signals å†…ï¼‰ã§ --full-scan-today ã‚’å‚ç…§ã§ãã‚‹ã‚ˆã†ã«ä¸€æ™‚ä¿å­˜
    try:
        globals()["_CLI_ARGS"] = args
    except Exception:
        pass
    if getattr(args, "full_scan_today", False):
        # ç’°å¢ƒå¤‰æ•°ã§æ˜ç¤ºã—ã¦ãŠãã¨å°†æ¥ä»–ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§ã‚‚åˆ©ç”¨å¯èƒ½
        os.environ.setdefault("FULL_SCAN_TODAY", "1")
    final_df, _per_system = run_signal_pipeline(args)
    signals_for_merge = log_final_candidates(final_df)
    merge_signals_for_cli(signals_for_merge)
    maybe_submit_orders(final_df, args)
    maybe_run_planned_exits(args)


if __name__ == "__main__":
    main()
