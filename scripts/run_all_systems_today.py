from __future__ import annotations

import argparse
from collections.abc import Callable
from dataclasses import dataclass, field
from concurrent.futures import Future, ThreadPoolExecutor, as_completed
from threading import Lock
import json
import logging
from pathlib import Path
from datetime import datetime
from zoneinfo import ZoneInfo
from typing import Any, no_type_check
import os

import pandas as pd

from common import broker_alpaca as ba
from common.alpaca_order import submit_orders_df
from common.cache_manager import CacheManager, load_base_cache
from common.notifier import create_notifier
from common.position_age import load_entry_dates, save_entry_dates
from common.signal_merge import Signal, merge_signals
from common.system_groups import (
    format_group_counts,
    format_group_counts_and_values,
)
from common.utils_spy import get_latest_nyse_trading_day, get_spy_with_indicators
from config.settings import get_settings
from core.system5 import (
    DEFAULT_ATR_PCT_THRESHOLD,
    format_atr_pct_threshold_label,
)

# strategies
from strategies.system1_strategy import System1Strategy
from strategies.system2_strategy import System2Strategy
from strategies.system3_strategy import System3Strategy
from strategies.system4_strategy import System4Strategy
from strategies.system5_strategy import System5Strategy
from strategies.system6_strategy import System6Strategy
from strategies.system7_strategy import System7Strategy

# „ÉØ„Éº„Ç´„ÉºÂÅ¥„ÅßË¶≥Ê∏¨„Åó„Åü cand_cnt(=TRDlist) „Çí‰øùÂ≠ò„Åó„ÄÅ„É°„Ç§„É≥„Çπ„É¨„ÉÉ„Éâ„ÅßÂèÇÁÖß„Åô„Çã„Åü„ÇÅ„ÅÆ„Çπ„Éä„ÉÉ„Éó„Ç∑„Éß„ÉÉ„Éà
_CAND_COUNT_SNAPSHOT: dict[str, int] = {}

_LOG_CALLBACK = None
_LOG_START_TS = None  # CLI Áî®„ÅÆÁµåÈÅéÊôÇÈñìÊ∏¨ÂÆöÈñãÂßãÊôÇÂàª

# „É≠„Ç∞„Éï„Ç°„Ç§„É´Ë®≠ÂÆöÔºà„Éá„Éï„Ç©„É´„Éà„ÅØÂõ∫ÂÆö„Éï„Ç°„Ç§„É´Ôºâ„ÄÇÂøÖË¶Å„Å´Âøú„Åò„Å¶Êó•‰ªò‰ªò„Åç„Å∏ÂàáÊõø„ÄÇ
_LOG_FILE_PATH: Path | None = None
_LOG_FILE_MODE: str = "single"  # single | dated


@dataclass(slots=True)
class BaseCachePool:
    """base „Ç≠„É£„ÉÉ„Ç∑„É•„ÅÆÂÖ±ÊúâËæûÊõ∏„Çí„Çπ„É¨„ÉÉ„Éâ„Çª„Éº„Éï„Å´ÁÆ°ÁêÜ„Åô„ÇãË£úÂä©„ÇØ„É©„Çπ„ÄÇ"""

    cache_manager: CacheManager
    shared: dict[str, pd.DataFrame] | None = None
    hits: int = 0
    loads: int = 0
    failures: int = 0
    _lock: Lock = field(default_factory=Lock, init=False, repr=False)

    def __post_init__(self) -> None:
        if self.shared is None:
            self.shared = {}

    def get(
        self,
        symbol: str,
        *,
        rebuild_if_missing: bool = True,
        min_last_date: pd.Timestamp | None = None,
        allowed_recent_dates: set[pd.Timestamp] | None = None,
    ) -> tuple[pd.DataFrame | None, bool]:
        """base ÁπßÔΩ≠ÁπùÔΩ£Áπù„Éª„ÅôÁπùÔΩ•ÁπßË≤ûÂèôË†ïÂä±Ôº†Á∏≤‚à¨ÔΩæÊ®äÂ∂åÁ∏∫ÔΩ´Ëè´ÊôÑÊàüÁ∏∫Âê∂ÔΩãÁ∏≤„Éª"""

        allowed_set = set(allowed_recent_dates or ())
        if min_last_date is not None:
            try:
                min_norm: pd.Timestamp | None = pd.Timestamp(min_last_date).normalize()
            except Exception:
                min_norm = None
        else:
            min_norm = None

        def _detect_last(frame: pd.DataFrame | None) -> pd.Timestamp | None:
            if frame is None or getattr(frame, "empty", True):
                return None
            try:
                if isinstance(frame.index, pd.DatetimeIndex) and len(frame.index):
                    return pd.Timestamp(frame.index[-1]).normalize()
            except Exception:
                pass
            try:
                idx = pd.to_datetime(frame.index, errors="coerce")
                idx = idx[~pd.isna(idx)]
                if len(idx):
                    return pd.Timestamp(idx[-1]).normalize()
            except Exception:
                pass
            try:
                series = frame.get("Date") if frame is not None else None
                if series is not None:
                    series = pd.to_datetime(series, errors="coerce").dropna()
                    if not series.empty:
                        return pd.Timestamp(series.iloc[-1]).normalize()
            except Exception:
                pass
            return None

        with self._lock:
            if self.shared is not None and symbol in self.shared:
                value = self.shared[symbol]
                last_date = _detect_last(value)
                stale = False
                if allowed_set and (last_date is None or last_date not in allowed_set):
                    stale = True
                if not stale and min_norm is not None:
                    if last_date is None or last_date < min_norm:
                        stale = True
                if not stale:
                    self.hits += 1
                    return value, True
                try:
                    if self.shared is not None:
                        self.shared.pop(symbol, None)
                except Exception:
                    pass

        df = load_base_cache(
            symbol,
            rebuild_if_missing=rebuild_if_missing,
            cache_manager=self.cache_manager,
            min_last_date=min_last_date,
            allowed_recent_dates=allowed_set or None,
        )

        with self._lock:
            if self.shared is not None:
                self.shared[symbol] = df
            self.loads += 1
            if df is None or getattr(df, "empty", True):
                self.failures += 1

        return df, False

    def sync_to(self, target: dict[str, pd.DataFrame] | None) -> None:
        """Êó¢Â≠ò„ÅÆÂ§ñÈÉ®ËæûÊõ∏„Å∏ÂÖ±Êúâ„Ç≠„É£„ÉÉ„Ç∑„É•„ÇíÂèçÊò†„Åô„Çã„ÄÇ"""

        if target is None or self.shared is None or target is self.shared:
            return
        with self._lock:
            try:
                target.update(self.shared)
            except Exception:
                pass

    def snapshot_stats(self) -> dict[str, int]:
        with self._lock:
            size = len(self.shared or {})
            return {
                "hits": self.hits,
                "loads": self.loads,
                "failures": self.failures,
                "size": size,
            }


@dataclass(slots=True)
class TodayRunContext:
    """‰øùÊåÅÂÖ±ÊúâÁä∂ÊÖã„Å®„Ç≥„Éº„É´„Éê„ÉÉ„ÇØ„ÇíÈõÜÁ¥Ñ„Åó„ÅüÂΩìÊó•„Ç∑„Ç∞„Éä„É´ÂÆüË°åÁî®„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„ÄÇ"""

    settings: Any
    cache_manager: CacheManager
    signals_dir: Path
    cache_dir: Path
    slots_long: int | None = None
    slots_short: int | None = None
    capital_long: float | None = None
    capital_short: float | None = None
    save_csv: bool = False
    csv_name_mode: str | None = None
    notify: bool = True
    log_callback: Callable[[str], None] | None = None
    progress_callback: Callable[[int, int, str], None] | None = None
    per_system_progress: Callable[[str, str], None] | None = None
    symbol_data: dict[str, pd.DataFrame] | None = None
    parallel: bool = False
    run_start_time: datetime = field(default_factory=datetime.now)
    start_equity: float = 0.0
    run_id: str = ""
    today: pd.Timestamp | None = None
    symbol_universe: list[str] = field(default_factory=list)
    basic_data: dict[str, pd.DataFrame] = field(default_factory=dict)
    base_cache: dict[str, pd.DataFrame] = field(default_factory=dict)
    system_filters: dict[str, list[str]] = field(default_factory=dict)
    per_system_frames: dict[str, pd.DataFrame] = field(default_factory=dict)
    final_signals: pd.DataFrame | None = None


def _get_account_equity() -> float:
    """Return current account equity via Alpaca API.

    Â§±Êïó„Åó„ÅüÂ†¥Âêà„ÅØ 0.0 „ÇíËøî„ÅôÔºà„ÉÜ„Çπ„ÉàÁí∞Â¢É„Å™„Å© API Êú™Ë®≠ÂÆöÊôÇ„ÅÆÂÆâÂÖ®ÂØæÁ≠ñÔºâ„ÄÇ
    """
    try:
        client = ba.get_client(paper=True)
        acct = client.get_account()
        return float(getattr(acct, "equity", 0.0) or 0.0)
    except Exception:
        return 0.0


def _configure_today_logger(*, mode: str = "single", run_id: str | None = None) -> None:
    """today_signals Áî®„ÅÆ„É≠„Ç¨„Éº„Éï„Ç°„Ç§„É´„ÇíÊßãÊàê„Åô„Çã„ÄÇ

    mode:
      - "single": Âõ∫ÂÆö„Éï„Ç°„Ç§„É´ `today_signals.log`
      - "dated":  Êó•‰ªò‰ªò„Åç `today_signals_YYYYMMDD_HHMM.log`ÔºàJSTÔºâ
    run_id: ‰∫àÁ¥ÑÔºàÁèæÁä∂Êú™‰ΩøÁî®Ôºâ„ÄÇÂ∞ÜÊù•„ÄÅ„Éï„Ç°„Ç§„É´Âêç„Å´Âê´„ÇÅ„Åü„ÅÑÂ†¥Âêà„Å´Âà©Áî®„ÄÇ
    """
    global _LOG_FILE_PATH, _LOG_FILE_MODE
    _LOG_FILE_MODE = mode or "single"
    try:
        settings = get_settings(create_dirs=True)
        log_dir = Path(settings.LOGS_DIR)
    except Exception:
        log_dir = Path("logs")
    try:
        log_dir.mkdir(parents=True, exist_ok=True)
    except Exception:
        pass

    if _LOG_FILE_MODE == "dated":
        try:
            jst_now = datetime.now(ZoneInfo("Asia/Tokyo"))
        except Exception:
            jst_now = datetime.now()
        stamp = jst_now.strftime("%Y%m%d_%H%M")
        filename = f"today_signals_{stamp}.log"
    else:
        filename = "today_signals.log"

    _LOG_FILE_PATH = log_dir / filename
    # „Éè„É≥„Éâ„É©„ÇíÊúÄÊñ∞„Éë„Çπ„Å´Âêà„Çè„Åõ„Å¶Âºµ„ÇäÊõø„Åà„Çã
    try:
        logger = logging.getLogger("today_signals")
        for h in list(logger.handlers):
            try:
                if isinstance(h, logging.FileHandler) and getattr(h, "baseFilename", None):
                    if Path(h.baseFilename) != _LOG_FILE_PATH:
                        logger.removeHandler(h)
                        try:
                            h.close()
                        except Exception:
                            pass
            except Exception:
                # „Éè„É≥„Éâ„É©ÊÉÖÂ†±ÂèñÂæó„Å´Â§±Êïó„Åó„ÅüÂ†¥Âêà„ÅØÁÑ°Ë¶ñ
                pass
        # ‰ª•Èôç„ÄÅ_get_today_logger() „ÅåÈÅ©Âàá„Å™„Éè„É≥„Éâ„É©„ÇíËøΩÂä†„Åô„Çã
    except Exception:
        pass


def _get_today_logger() -> logging.Logger:
    """today_signals Áî®„ÅÆ„Éï„Ç°„Ç§„É´„É≠„Ç¨„Éº„ÇíÂèñÂæó„ÄÇ

    „Éá„Éï„Ç©„É´„Éà„ÅØ `logs/today_signals.log`„ÄÇ
    `_configure_today_logger(mode="dated")` ÈÅ©Áî®ÊôÇ„ÅØÊó•‰ªò‰ªò„Åç„Éï„Ç°„Ç§„É´„Å´Âá∫Âäõ„ÄÇ
    UI ÊúâÁÑ°„Å´Èñ¢‰øÇ„Å™„Åè„ÄÅÂÆåÂÖ®„Å™ÂÆüË°å„É≠„Ç∞„ÇíÂ∏∏„Å´„Éï„Ç°„Ç§„É´„Å∏ÊÆã„Åô„ÄÇ
    """
    logger = logging.getLogger("today_signals")
    logger.setLevel(logging.INFO)
    # „É´„Éº„Éà„É≠„Ç¨„Éº„Å∏„ÅÆ‰ºùÊí≠„ÇíÊ≠¢„ÇÅ„Å¶ÈáçË§áÂá∫Âäõ„ÇíÈò≤Ê≠¢
    try:
        logger.propagate = False
    except Exception:
        pass
    # „É´„Éº„Éà„É≠„Ç¨„Éº„Å∏„ÅÆ‰ºùÊí≠„ÇíÊ≠¢„ÇÅ„ÄÅ„Ç≥„É≥„ÇΩ„Éº„É´‰∫åÈáçÂá∫Âäõ„ÇíÈò≤Ê≠¢
    try:
        logger.propagate = False
    except Exception:
        pass
    # ÁõÆÊ®ô„Éï„Ç°„Ç§„É´„Éë„Çπ„ÇíÊ±∫ÂÆö
    try:
        # Áí∞Â¢ÉÂ§âÊï∞„Åß„ÇÇÊó•‰ªòÂà•„É≠„Ç∞ÊåáÂÆö„ÇíË®±ÂèØÔºàUI ÂÆüË°å„Å™„Å© main() „ÇíÁµå„Å™„ÅÑÂ†¥ÂêàÔºâ
        if globals().get("_LOG_FILE_PATH") is None:
            try:
                import os as _os

                _mode_env = (_os.environ.get("TODAY_SIGNALS_LOG_MODE") or "").strip().lower()
                if _mode_env == "dated":
                    try:
                        _jst_now = datetime.now(ZoneInfo("Asia/Tokyo"))
                    except Exception:
                        _jst_now = datetime.now()
                    _stamp = _jst_now.strftime("%Y%m%d_%H%M")
                    try:
                        settings = get_settings(create_dirs=True)
                        _log_dir = Path(settings.LOGS_DIR)
                    except Exception:
                        _log_dir = Path("logs")
                    try:
                        _log_dir.mkdir(parents=True, exist_ok=True)
                    except Exception:
                        pass
                    globals()["_LOG_FILE_PATH"] = _log_dir / f"today_signals_{_stamp}.log"
            except Exception:
                pass

        if globals().get("_LOG_FILE_PATH") is not None:
            log_path = globals().get("_LOG_FILE_PATH")  # type: ignore[assignment]
        else:
            try:
                settings = get_settings(create_dirs=True)
                log_dir = Path(settings.LOGS_DIR)
            except Exception:
                log_dir = Path("logs")
            try:
                log_dir.mkdir(parents=True, exist_ok=True)
            except Exception:
                pass
            log_path = log_dir / "today_signals.log"
    except Exception:
        log_path = Path("logs") / "today_signals.log"

    # Êó¢Â≠ò„ÅÆÂêå‰∏Ä„Éï„Ç°„Ç§„É´„Éè„É≥„Éâ„É©„Åå„ÅÇ„Çã„ÅãÁ¢∫Ë™ç
    has_handler = False
    for h in list(logger.handlers):
        try:
            if isinstance(h, logging.FileHandler):
                base = getattr(h, "baseFilename", None)
                if base:
                    if Path(base).resolve() == Path(str(log_path)).resolve():
                        has_handler = True
                        break
        except Exception:
            continue
    if not has_handler:
        try:
            fh = logging.FileHandler(str(log_path), encoding="utf-8")
            fmt = logging.Formatter("%(asctime)s [%(levelname)s] %(message)s", "%Y-%m-%d %H:%M:%S")
            fh.setFormatter(fmt)
            logger.addHandler(fh)
        except Exception:
            pass
    return logger


def _emit_ui_log(message: str) -> None:
    """UI ÂÅ¥„ÅÆ„É≠„Ç∞„Ç≥„Éº„É´„Éê„ÉÉ„ÇØ„ÅåÁôªÈå≤„Åï„Çå„Å¶„ÅÑ„Çå„Å∞„ÄÅ„Åù„ÅÆ„Åæ„ÅæÊñáÂ≠óÂàó„ÇíÈÄÅ‰ø°„Åô„Çã„ÄÇ"""
    try:
        cb = globals().get("_LOG_CALLBACK")
        if cb and callable(cb):
            cb(str(message))
    except Exception:
        # UI „Ç≥„Éº„É´„Éê„ÉÉ„ÇØÊú™Ë®≠ÂÆö„ÇÑ‰æãÂ§ñ„ÅØÈªô„Å£„Å¶ÁÑ°Ë¶ñÔºàCLI ÂÆüË°åÊôÇ„ÇíËÄÉÊÖÆÔºâ
        pass


def _log(msg: str, ui: bool = True):
    """CLI Âá∫Âäõ„Å´„ÅØ [HH:MM:SS | mÂàÜsÁßí] „Çí‰ªò‰∏é„ÄÇÂøÖË¶Å„Å´Âøú„Åò„Å¶ UI „Ç≥„Éº„É´„Éê„ÉÉ„ÇØ„ÇíÊäëÂà∂„ÄÇ"""
    import time as _t

    # ÂàùÂõûÂëº„Å≥Âá∫„Åó„ÅßÈñãÂßãÊôÇÂàª„ÇíË®≠ÂÆö
    try:
        global _LOG_START_TS
        if _LOG_START_TS is None:
            _LOG_START_TS = _t.time()
    except Exception:
        _LOG_START_TS = None

    # „Éó„É¨„Éï„Ç£„ÉÉ„ÇØ„Çπ„Çí‰ΩúÊàêÔºàÁèæÂú®ÊôÇÂàª + ÂàÜÁßíÁµåÈÅéÔºâ
    try:
        now = _t.strftime("%H:%M:%S")
        elapsed = 0 if _LOG_START_TS is None else max(0, _t.time() - _LOG_START_TS)
        m, s = divmod(int(elapsed), 60)
        prefix = f"[{now} | {m}ÂàÜ{s}Áßí] "
    except Exception:
        prefix = ""

    # „Ç≠„Éº„ÉØ„Éº„Éâ„Å´„Çà„ÇãÈô§Â§ñÂà§ÂÆöÔºàÂÖ®‰ΩìÔºâ
    try:
        if any(k in str(msg) for k in _GLOBAL_SKIP_KEYWORDS):
            return
        ui_allowed = ui and not any(k in str(msg) for k in _UI_ONLY_SKIP_KEYWORDS)
    except Exception:
        ui_allowed = ui

    # CLI „Å∏„ÅØÊï¥ÂΩ¢„Åó„Å¶Âá∫Âäõ
    out = f"{prefix}{msg}"
    try:
        print(out, flush=True)
    except UnicodeEncodeError:
        try:
            import sys as _sys

            encoding = getattr(_sys.stdout, "encoding", "") or "utf-8"
            safe = out.encode(encoding, errors="replace").decode(encoding, errors="replace")
            print(safe, flush=True)
        except Exception:
            try:
                safe = out.encode("ascii", errors="replace").decode("ascii", errors="replace")
                print(safe, flush=True)
            except Exception:
                pass
    except Exception:
        pass

    # UI ÂÅ¥„ÅÆ„Ç≥„Éº„É´„Éê„ÉÉ„ÇØ„Å´„ÅØ„Éï„Ç£„É´„ÇøÊ∏à„Åø„ÅßÈÄöÁü•ÔºàUI „Åß„ÅÆÈáçË§á„Éó„É¨„Éï„Ç£„ÉÉ„ÇØ„ÇπÂõûÈÅøÔºâ
    if ui_allowed:
        _emit_ui_log(str(msg))

    # Â∏∏„Å´„Éï„Ç°„Ç§„É´„Å∏„ÇÇINFO„ÅßÂá∫ÂäõÔºàUI/CLI „ÅÆÂà•„Å™„ÅèÂÆåÂÖ®„Å™„É≠„Ç∞„Çí‰øùÂ≠òÔºâ
    try:
        _get_today_logger().info(str(msg))
    except Exception:
        pass


def _asc_by_score_key(score_key: str | None) -> bool:
    return bool(score_key and score_key.upper() in {"RSI4"})


# „É≠„Ç∞Âá∫Âäõ„Åã„ÇâÈô§Â§ñ„Åô„Çã„Ç≠„Éº„ÉØ„Éº„Éâ
# „É≠„Ç∞ÂÖ®‰Ωì„Åã„ÇâÈô§Â§ñ„Åô„Çã„Ç≠„Éº„ÉØ„Éº„ÉâÔºàCLI/UI ÂÖ±ÈÄöÔºâ
# „Ç§„É≥„Ç∏„Ç±„Éº„Çø„ÉºË®àÁÆóËá™‰Ωì„ÅØ CLI „Å´Âá∫„Åó„Åü„ÅÑ„ÅÆ„ÅßÈô§Â§ñ„Åó„Å™„ÅÑ„ÄÇ
_GLOBAL_SKIP_KEYWORDS = (
    "„Éê„ÉÉ„ÉÅÊôÇÈñì",
    "batch time",
    # ÈäòÊüÑ„ÅÆÈï∑„ÅÑ„ÉÄ„É≥„Éó„ÅØ CLI „Åß„ÇÇÈùûË°®Á§∫„Å´„Åô„Çã
    "ÈäòÊüÑ:",
)
# UI Ë°®Á§∫„Åã„Çâ„ÅÆ„ÅøÈô§Â§ñ„Åô„Çã„Ç≠„Éº„ÉØ„Éº„Éâ
_UI_ONLY_SKIP_KEYWORDS = (
    "ÈÄ≤Êçó",
    "ÂÄôË£úÊäΩÂá∫",
    "ÂÄôË£úÊó•Êï∞",
)


def _filter_logs(lines: list[str], ui: bool = False) -> list[str]:
    """„Ç≠„Éº„ÉØ„Éº„Éâ„Å´Âü∫„Å•„ÅÑ„Å¶„É≠„Ç∞Ë°å„ÇíÈô§Â§ñ„Åô„Çã„ÄÇ

    Args:
        lines: ÂØæË±°„É≠„Ç∞Ë°å„ÅÆ„É™„Çπ„Éà„ÄÇ
        ui: True „ÅÆÂ†¥Âêà„ÅØ UI ÈôêÂÆö„ÅÆÈô§Â§ñ„Ç≠„Éº„ÉØ„Éº„Éâ„ÇÇÈÅ©Áî®„ÄÇ
    """

    skip_keywords = _GLOBAL_SKIP_KEYWORDS + (_UI_ONLY_SKIP_KEYWORDS if ui else ())
    return [ln for ln in lines if not any(k in ln for k in skip_keywords)]


def _prev_counts_path(signals_dir: Path) -> Path:
    try:
        return signals_dir / "previous_per_system_counts.json"
    except Exception:
        return Path("signals/previous_per_system_counts.json")


def _load_prev_counts(signals_dir: Path) -> dict[str, int]:
    fp = _prev_counts_path(signals_dir)
    if not fp.exists():
        return {}
    try:
        data = json.loads(fp.read_text(encoding="utf-8"))
        counts = data.get("counts", {}) if isinstance(data, dict) else {}
        out: dict[str, int] = {}
        for i in range(1, 8):
            key = f"system{i}"
            try:
                out[key] = int(counts.get(key, 0))
            except Exception:
                out[key] = 0
        return out
    except Exception:
        return {}


def _save_prev_counts(signals_dir: Path, per_system_map: dict[str, pd.DataFrame]) -> None:
    try:
        counts = {
            k: (0 if (v is None or v.empty) else int(len(v))) for k, v in per_system_map.items()
        }
        data = {"timestamp": datetime.utcnow().isoformat() + "Z", "counts": counts}
        fp = _prev_counts_path(signals_dir)
        try:
            fp.parent.mkdir(parents=True, exist_ok=True)
        except Exception:
            pass
        fp.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")
    except Exception:
        pass


def _normalize_ohlcv(df: pd.DataFrame) -> pd.DataFrame:
    """ÂàóÂêç„ÇíÂ§ßÊñáÂ≠óOHLCV„Å´Áµ±‰∏Ä"""
    col_map = {
        "open": "Open",
        "high": "High",
        "low": "Low",
        "close": "Close",
        "volume": "Volume",
        "adj_close": "AdjClose",
        "adjusted_close": "AdjClose",
    }
    try:
        return df.rename(columns={k: v for k, v in col_map.items() if k in df.columns})
    except Exception:
        return df


def _pick_series(df: pd.DataFrame, names: list[str]):
    try:
        for nm in names:
            if nm in df.columns:
                s = df[nm]
                if isinstance(s, pd.DataFrame):
                    try:
                        s = s.iloc[:, 0]
                    except Exception:
                        continue
                try:
                    s = pd.to_numeric(s, errors="coerce")
                except Exception:
                    pass
                return s
    except Exception:
        pass
    return None


def _last_scalar(series):
    try:
        if series is None:
            return None
        s2 = series.dropna()
        if s2.empty:
            return None
        return float(s2.iloc[-1])
    except Exception:
        return None


def filter_system1(symbols, data):
    result = []
    for sym in symbols:
        df = data.get(sym)
        if df is None or df.empty:
            continue
        close_s = _pick_series(df, ["close", "Close", "Adj Close", "adj_close"])
        last_close = _last_scalar(close_s)
        if last_close is None or last_close < 5:
            continue
        vol_s = _pick_series(df, ["volume", "Volume", "Vol", "vol"])
        if vol_s is None or close_s is None:
            continue
        try:
            dollar_vol = (close_s * vol_s).dropna()
        except Exception:
            continue
        if dollar_vol.tail(20).mean() < 5e7:
            continue
        result.append(sym)
    return result


def filter_system2(symbols, data):
    result = []
    for sym in symbols:
        df = data.get(sym)
        if df is None or df.empty:
            continue
        close_s = _pick_series(df, ["close", "Close", "Adj Close", "adj_close"])
        last_close = _last_scalar(close_s)
        if last_close is None or last_close < 5:
            continue
        vol_s = _pick_series(df, ["volume", "Volume", "Vol", "vol"])
        if vol_s is None or close_s is None:
            continue
        try:
            dollar_vol = (close_s * vol_s).dropna()
        except Exception:
            continue
        if dollar_vol.tail(20).mean() < 2.5e7:
            continue
        high_s = _pick_series(df, ["high", "High"]) if df is not None else None
        low_s = _pick_series(df, ["low", "Low"]) if df is not None else None
        if high_s is not None and low_s is not None and close_s is not None:
            try:
                tr = (high_s - low_s).dropna().tail(10)
                atr = tr.mean()
            except Exception:
                atr = None
            if atr is not None and atr < (last_close * 0.03):
                continue
        result.append(sym)
    return result


def filter_system3(symbols, data):
    result = []
    for sym in symbols:
        df = data.get(sym)
        if df is None or df.empty:
            continue
        low = df.get("Low", df.get("low"))
        if low is None or float(low.iloc[-1]) < 1:
            continue
        av50 = df.get("AvgVolume50")
        if av50 is None or pd.isna(av50.iloc[-1]) or float(av50.iloc[-1]) < 1_000_000:
            continue
        atr_ratio = df.get("ATR_Ratio")
        if atr_ratio is None or pd.isna(atr_ratio.iloc[-1]) or float(atr_ratio.iloc[-1]) < 0.05:
            continue
        result.append(sym)
    return result


def filter_system4(symbols, data):
    result = []
    for sym in symbols:
        df = data.get(sym)
        if df is None or df.empty:
            continue
        dv50 = df.get("DollarVolume50")
        hv50 = df.get("HV50")
        try:
            if dv50 is None or pd.isna(dv50.iloc[-1]) or float(dv50.iloc[-1]) <= 100_000_000:
                continue
            if hv50 is None or pd.isna(hv50.iloc[-1]):
                continue
            hv = float(hv50.iloc[-1])
            if hv < 10 or hv > 40:
                continue
        except Exception:
            continue
        result.append(sym)
    return result


def filter_system5(symbols, data):
    result = []
    for sym in symbols:
        df = data.get(sym)
        if df is None or df.empty:
            continue
        av50 = df.get("AvgVolume50")
        dv50 = df.get("DollarVolume50")
        atr_pct = df.get("ATR_Pct")
        try:
            if av50 is None or pd.isna(av50.iloc[-1]) or float(av50.iloc[-1]) <= 500_000:
                continue
            if dv50 is None or pd.isna(dv50.iloc[-1]) or float(dv50.iloc[-1]) <= 2_500_000:
                continue
            if (
                atr_pct is None
                or pd.isna(atr_pct.iloc[-1])
                or float(atr_pct.iloc[-1]) <= DEFAULT_ATR_PCT_THRESHOLD
            ):
                continue
        except Exception:
            continue
        result.append(sym)
    return result


def filter_system6(symbols, data):
    result = []
    for sym in symbols:
        df = data.get(sym)
        if df is None or df.empty:
            continue
        low = df.get("Low", df.get("low"))
        if low is None or float(low.iloc[-1]) < 5:
            continue
        dv50 = df.get("DollarVolume50")
        if dv50 is None or pd.isna(dv50.iloc[-1]) or float(dv50.iloc[-1]) <= 10_000_000:
            continue
        result.append(sym)
    return result


def _extract_last_cache_date(df: pd.DataFrame) -> pd.Timestamp | None:
    if df is None or getattr(df, "empty", True):
        return None
    for col in ("date", "Date"):
        if col in df.columns:
            try:
                values = pd.to_datetime(df[col], errors="coerce")
                values = values.dropna()
                if not values.empty:
                    return pd.Timestamp(values.iloc[-1]).normalize()
            except Exception:
                continue
    try:
        idx = pd.to_datetime(df.index, errors="coerce")
        mask = ~pd.isna(idx)
        if mask.any():
            return pd.Timestamp(idx[mask][-1]).normalize()
    except Exception:
        pass
    return None


def _recent_trading_days(today: pd.Timestamp | None, max_back: int) -> list[pd.Timestamp]:
    if today is None:
        return []
    out: list[pd.Timestamp] = []
    seen: set[pd.Timestamp] = set()
    current = pd.Timestamp(today).normalize()
    steps = max(0, int(max_back))
    for _ in range(steps + 1):
        if current in seen:
            break
        out.append(current)
        seen.add(current)
        prev_candidate = get_latest_nyse_trading_day(current - pd.Timedelta(days=1))
        prev_candidate = pd.Timestamp(prev_candidate).normalize()
        if prev_candidate == current:
            break
        current = prev_candidate
    return out


def _build_rolling_from_base(
    symbol: str,
    base_df: pd.DataFrame,
    target_len: int,
    cache_manager: CacheManager | None = None,
) -> pd.DataFrame | None:
    """Convert base cache dataframe to rolling window and optionally persist it."""

    if base_df is None or getattr(base_df, "empty", True):
        return None
    try:
        work = base_df.copy()
    except Exception:
        work = base_df
    if work.index.name is not None:
        work = work.reset_index()
    if "Date" in work.columns:
        work["date"] = pd.to_datetime(work["Date"], errors="coerce")
    elif "date" in work.columns:
        work["date"] = pd.to_datetime(work["date"], errors="coerce")
    else:
        return None
    work = work.dropna(subset=["date"]).sort_values("date")
    col_map = {
        "Open": "open",
        "High": "high",
        "Low": "low",
        "Close": "close",
        "AdjClose": "adjusted_close",
        "Adj Close": "adjusted_close",
        "Volume": "volume",
    }
    try:
        for src, dst in list(col_map.items()):
            if src in work.columns:
                work = work.rename(columns={src: dst})
    except Exception:
        pass
    sliced = work.tail(int(target_len)).reset_index(drop=True)
    if sliced.empty:
        return None
    if cache_manager is not None:
        try:
            cache_manager.write_atomic(sliced, symbol, "rolling")
        except Exception:
            pass
    return sliced


def _load_basic_data(
    symbols: list[str],
    cache_manager: CacheManager,
    settings: Any,
    symbol_data: dict[str, pd.DataFrame] | None,
    *,
    today: pd.Timestamp | None = None,
    freshness_tolerance: int | None = None,
    base_cache: dict[str, pd.DataFrame] | None = None,
    base_cache_pool: BaseCachePool | None = None,
) -> dict[str, pd.DataFrame]:
    from time import perf_counter

    data: dict[str, pd.DataFrame] = {}
    total_syms = len(symbols)
    start_ts = perf_counter()
    chunk = 500

    if freshness_tolerance is None:
        try:
            freshness_tolerance = int(settings.cache.rolling.max_staleness_days)
        except Exception:
            freshness_tolerance = 2
    freshness_tolerance = max(0, int(freshness_tolerance))

    try:
        target_len = int(
            settings.cache.rolling.base_lookback_days + settings.cache.rolling.buffer_days
        )
    except Exception:
        target_len = 0

    base_pool = base_cache_pool or BaseCachePool(cache_manager, base_cache)
    stats_lock = Lock()
    stats: dict[str, int] = {}

    def _record_stat(key: str) -> None:
        with stats_lock:
            stats[key] = stats.get(key, 0) + 1

    def _get_base_cache(symbol: str) -> tuple[pd.DataFrame | None, bool]:
        base_df, cached_hit = base_pool.get(
            symbol,
            rebuild_if_missing=True,
            min_last_date=min_recent_allowed,
            allowed_recent_dates=recent_allowed or None,
        )
        _record_stat("base_cache_hit" if cached_hit else "base_cache_miss")
        if base_df is None or getattr(base_df, "empty", True):
            _record_stat("base_missing")
        return base_df, cached_hit

    recent_allowed: set[pd.Timestamp] = set()
    if today is not None and freshness_tolerance >= 0:
        try:
            recent_allowed = {
                pd.Timestamp(d).normalize()
                for d in _recent_trading_days(pd.Timestamp(today), freshness_tolerance)
            }
        except Exception:
            recent_allowed = set()

    min_recent_allowed: pd.Timestamp | None = None
    if recent_allowed:
        try:
            min_recent_allowed = min(recent_allowed)
        except Exception:
            min_recent_allowed = None

    gap_probe_days = max(freshness_tolerance + 5, 10)

    def _estimate_gap_days(
        today_dt: pd.Timestamp | None, last_dt: pd.Timestamp | None
    ) -> int | None:
        if today_dt is None or last_dt is None:
            return None
        try:
            recent = _recent_trading_days(pd.Timestamp(today_dt), gap_probe_days)
        except Exception:
            recent = []
        for offset, dt in enumerate(recent):
            if dt == last_dt:
                return offset
        try:
            return max(0, int((pd.Timestamp(today_dt) - pd.Timestamp(last_dt)).days))
        except Exception:
            return None

    def _pick_symbol_data(sym: str) -> pd.DataFrame | None:
        try:
            if not symbol_data or sym not in symbol_data:
                return None
            df = symbol_data.get(sym)
            if df is None or getattr(df, "empty", True):
                return None
            x = df.copy()
            if x.index.name is not None:
                x = x.reset_index()
            if "date" in x.columns:
                x["date"] = pd.to_datetime(x["date"], errors="coerce")
            elif "Date" in x.columns:
                x["date"] = pd.to_datetime(x["Date"], errors="coerce")
            else:
                return None
            col_map = {
                "Open": "open",
                "High": "high",
                "Low": "low",
                "Close": "close",
                "Adj Close": "adjusted_close",
                "AdjClose": "adjusted_close",
                "Volume": "volume",
            }
            for k, v in list(col_map.items()):
                if k in x.columns:
                    x = x.rename(columns={k: v})
            required = {"date", "close"}
            if not required.issubset(set(x.columns)):
                return None
            x = x.dropna(subset=["date"]).sort_values("date")
            return x
        except Exception:
            return None

    def _normalize_loaded(df: pd.DataFrame | None) -> pd.DataFrame | None:
        if df is None or getattr(df, "empty", True):
            return None
        try:
            if "Date" not in df.columns:
                work = df.copy()
                if "date" in work.columns:
                    work["Date"] = pd.to_datetime(work["date"], errors="coerce")
                else:
                    work["Date"] = pd.to_datetime(work.index, errors="coerce")
                df = work
            df["Date"] = pd.to_datetime(df["Date"], errors="coerce").dt.normalize()
        except Exception:
            pass
        normalized = _normalize_ohlcv(df)
        try:
            fill_cols = [c for c in ("Open", "High", "Low", "Close", "Volume") if c in normalized.columns]
            if fill_cols:
                normalized = normalized.copy()
                try:
                    filled = normalized[fill_cols].apply(pd.to_numeric, errors="coerce")
                except Exception:
                    filled = normalized[fill_cols]
                normalized.loc[:, fill_cols] = filled.ffill().bfill()
        except Exception:
            pass
        try:
            if "Date" in normalized.columns:
                normalized = normalized.dropna(subset=["Date"])
        except Exception:
            pass
        return normalized

    env_parallel = (os.environ.get("BASIC_DATA_PARALLEL", "") or "").strip().lower()
    try:
        env_parallel_threshold = int(
            os.environ.get("BASIC_DATA_PARALLEL_THRESHOLD", "200")
        )
    except Exception:
        env_parallel_threshold = 200
    if env_parallel in ("1", "true", "yes"):
        use_parallel = total_syms > 1
    elif env_parallel in ("0", "false", "no"):
        use_parallel = False
    else:
        use_parallel = total_syms >= max(0, env_parallel_threshold)

    max_workers: int | None = None
    if use_parallel and total_syms > 0:
        try:
            env_workers = (os.environ.get("BASIC_DATA_MAX_WORKERS", "") or "").strip()
            if env_workers:
                max_workers = int(env_workers)
        except Exception:
            max_workers = None
        if max_workers is None:
            try:
                cfg_workers = getattr(settings.cache.rolling, "load_max_workers", None)
                if cfg_workers:
                    max_workers = int(cfg_workers)
            except Exception:
                pass
        if max_workers is None:
            cpu_count = os.cpu_count() or 4
            max_workers = max(4, cpu_count * 2)
        max_workers = max(1, min(int(max_workers), total_syms))
        try:
            _log(f"üßµ Âü∫Á§é„Éá„Éº„Çø„É≠„Éº„Éâ‰∏¶ÂàóÂåñ: workers={max_workers}")
        except Exception:
            pass

    def _load_one(sym: str) -> tuple[str, pd.DataFrame | None]:
        try:
            source: str | None = None
            df = _pick_symbol_data(sym)
            rebuild_reason: str | None = None
            last_seen_date: pd.Timestamp | None = None
            gap_days: int | None = None
            if df is None or getattr(df, "empty", True):
                df = cache_manager.read(sym, "rolling")
            else:
                source = "prefetched"
            if df is None or getattr(df, "empty", True):
                source = None
            if df is None or getattr(df, "empty", True) or (
                hasattr(df, "__len__") and len(df) < target_len
            ):
                if df is not None and not getattr(df, "empty", True):
                    rebuild_reason = rebuild_reason or "length"
                needs_rebuild = True
            else:
                needs_rebuild = False
            if df is not None and not getattr(df, "empty", True) and source is None:
                source = "rolling"
            if df is not None and not getattr(df, "empty", True):
                last_seen_date = _extract_last_cache_date(df)
                if last_seen_date is None:
                    rebuild_reason = rebuild_reason or "missing_date"
                    needs_rebuild = True
                else:
                    last_seen_date = pd.Timestamp(last_seen_date).normalize()
                    if today is not None and recent_allowed and last_seen_date not in recent_allowed:
                        rebuild_reason = "stale"
                        gap_days = _estimate_gap_days(pd.Timestamp(today), last_seen_date)
                        needs_rebuild = True
            if needs_rebuild:
                if rebuild_reason == "stale":
                    gap_label = (
                        f"Á¥Ñ{gap_days}Âñ∂Ê•≠Êó•" if gap_days is not None else "‰∏çÊòé"
                    )
                    last_label = (
                        str(last_seen_date.date()) if last_seen_date is not None else "‰∏çÊòé"
                    )
                    _log(
                        f"‚ôªÔ∏è rollingÂÜçÊßãÁØâ: {sym} ÊúÄÁµÇÊó•={last_label} | „ÇÆ„É£„ÉÉ„Éó={gap_label}"
                    )
                base_df, cached_hit = _get_base_cache(sym)
                if base_df is None or getattr(base_df, "empty", True):
                    if rebuild_reason:
                        reason_label = "ÈÆÆÂ∫¶‰∏çË∂≥" if rebuild_reason == "stale" else "Êó•‰ªòÊ¨†Êêç"
                        _log(
                            f"‚ö†Ô∏è rollingÂÜçÊßãÁØâÂ§±Êïó: {sym} {reason_label}„ÄÇbase„Ç≠„É£„ÉÉ„Ç∑„É•„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì",
                            ui=False,
                        )
                    return sym, None
                sliced = _build_rolling_from_base(sym, base_df, target_len, cache_manager)
                if sliced is None or getattr(sliced, "empty", True):
                    if rebuild_reason:
                        reason_label = "ÈÆÆÂ∫¶‰∏çË∂≥" if rebuild_reason == "stale" else "Êó•‰ªòÊ¨†Êêç"
                        _log(
                            f"‚ö†Ô∏è rollingÂÜçÊßãÁØâÂ§±Êïó: {sym} {reason_label}„ÄÇbase„Éá„Éº„Çø„ÅåÁ©∫„Åß„Åô",
                            ui=False,
                        )
                    _record_stat("base_missing")
                    return sym, None
                if rebuild_reason == "stale":
                    new_last = _extract_last_cache_date(sliced)
                    try:
                        new_label = (
                            str(pd.Timestamp(new_last).date())
                            if new_last is not None
                            else "‰∏çÊòé"
                        )
                    except Exception:
                        new_label = "‰∏çÊòé"
                    _log(
                        f"‚úÖ rollingÊõ¥Êñ∞ÂÆå‰∫Ü: {sym} ‚Üí ÊúÄÁµÇÊó•={new_label}",
                        ui=False,
                    )
                df = sliced
                source = "rebuilt"
            normalized = _normalize_loaded(df)
            if normalized is not None and not getattr(normalized, "empty", True):
                _record_stat(source or "rolling")
                return sym, normalized
            _record_stat("failed")
            return sym, None
        except Exception:
            _record_stat("failed")
            return sym, None

    def _report_progress(done: int) -> None:
        if done <= 0 or chunk <= 0:
            return
        if done % chunk != 0:
            return
        try:
            elapsed = max(0.001, perf_counter() - start_ts)
            rate = done / elapsed
            remain = max(0, total_syms - done)
            eta_sec = int(remain / rate) if rate > 0 else 0
            m, s = divmod(eta_sec, 60)
            msg = f"üì¶ Âü∫Á§é„Éá„Éº„Çø„É≠„Éº„ÉâÈÄ≤Êçó: {done}/{total_syms} | ETA {m}ÂàÜ{s}Áßí"
            _log(msg, ui=False)
            _emit_ui_log(msg)
        except Exception:
            _log(f"üì¶ Âü∫Á§é„Éá„Éº„Çø„É≠„Éº„ÉâÈÄ≤Êçó: {done}/{total_syms}", ui=False)
            _emit_ui_log(f"üì¶ Âü∫Á§é„Éá„Éº„Çø„É≠„Éº„ÉâÈÄ≤Êçó: {done}/{total_syms}")

    processed = 0
    if use_parallel and max_workers and total_syms > 1:
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {executor.submit(_load_one, sym): sym for sym in symbols}
            for fut in as_completed(futures):
                try:
                    sym, df = fut.result()
                except Exception:
                    sym, df = futures[fut], None
                if df is not None and not getattr(df, "empty", True):
                    data[sym] = df
                processed += 1
                _report_progress(processed)
    else:
        for sym in symbols:
            sym, df = _load_one(sym)
            if df is not None and not getattr(df, "empty", True):
                data[sym] = df
            processed += 1
            _report_progress(processed)

    try:
        total_elapsed = max(0.0, perf_counter() - start_ts)
        total_int = int(total_elapsed)
        m, s = divmod(total_int, 60)
        done_msg = (
            f"üì¶ Âü∫Á§é„Éá„Éº„Çø„É≠„Éº„ÉâÂÆå‰∫Ü: {len(data)}/{total_syms} | ÊâÄË¶Å {m}ÂàÜ{s}Áßí"
            + (" | ‰∏¶Âàó=ON" if use_parallel and max_workers else " | ‰∏¶Âàó=OFF")
        )
        _log(done_msg)
        _emit_ui_log(done_msg)
    except Exception:
        _log(f"üì¶ Âü∫Á§é„Éá„Éº„Çø„É≠„Éº„ÉâÂÆå‰∫Ü: {len(data)}/{total_syms}")
        _emit_ui_log(f"üì¶ Âü∫Á§é„Éá„Éº„Çø„É≠„Éº„ÉâÂÆå‰∫Ü: {len(data)}/{total_syms}")

    try:
        summary_map = {
            "prefetched": "‰∫ãÂâç‰æõÁµ¶",
            "rolling": "rollingÂÜçÂà©Áî®",
            "rebuilt": "baseÂÜçÊßãÁØâ",
            "base_cache_hit": "baseËæûÊõ∏Hit",
            "base_cache_miss": "baseËæûÊõ∏Miss",
            "base_missing": "baseÊ¨†Êêç",
            "failed": "Â§±Êïó",
        }
        summary_parts = [
            f"{label}={stats.get(key, 0)}"
            for key, label in summary_map.items()
            if stats.get(key)
        ]
        if summary_parts:
            _log("üìä Âü∫Á§é„Éá„Éº„Çø„É≠„Éº„ÉâÂÜÖË®≥: " + " / ".join(summary_parts), ui=False)
        pool_stats = base_pool.snapshot_stats()
        if pool_stats["hits"] or pool_stats["loads"] or pool_stats["size"]:
            _log(
                "üì¶ base„Ç≠„É£„ÉÉ„Ç∑„É•ËæûÊõ∏: "
                + f"‰øùÊåÅ={pool_stats['size']} | hit={pool_stats['hits']} | load={pool_stats['loads']} | Ê¨†Êêç={pool_stats['failures']}",
                ui=False,
            )
    except Exception:
        pass

    base_pool.sync_to(base_cache)
    return data


def _load_indicator_data(
    symbols: list[str],
    cache_manager: CacheManager,
    settings: Any,
    symbol_data: dict[str, pd.DataFrame] | None,
) -> dict[str, pd.DataFrame]:
    import time as _t

    data: dict[str, pd.DataFrame] = {}
    total_syms = len(symbols)
    start_ts = _t.time()
    chunk = 500
    for idx, sym in enumerate(symbols, start=1):
        try:
            df = None
            try:
                if symbol_data and sym in symbol_data:
                    df = symbol_data.get(sym)
                    if df is not None and not df.empty:
                        x = df.copy()
                        if x.index.name is not None:
                            x = x.reset_index()
                        if "date" in x.columns:
                            x["date"] = pd.to_datetime(x["date"], errors="coerce")
                        elif "Date" in x.columns:
                            x["date"] = pd.to_datetime(x["Date"], errors="coerce")
                        col_map = {
                            "Open": "open",
                            "High": "high",
                            "Low": "low",
                            "Close": "close",
                            "Adj Close": "adjusted_close",
                            "AdjClose": "adjusted_close",
                            "Volume": "volume",
                        }
                        for k, v in list(col_map.items()):
                            if k in x.columns:
                                x = x.rename(columns={k: v})
                        required = {"date", "close"}
                        if required.issubset(set(x.columns)):
                            x = x.dropna(subset=["date"]).sort_values("date")
                            df = x
                        else:
                            df = None
                    else:
                        df = None
            except Exception:
                df = None
            if df is None or df.empty:
                df = cache_manager.read(sym, "rolling")
            target_len = int(
                settings.cache.rolling.base_lookback_days + settings.cache.rolling.buffer_days
            )
            if df is None or df.empty or (hasattr(df, "__len__") and len(df) < target_len):
                base_df = load_base_cache(
                    sym,
                    rebuild_if_missing=True,
                    cache_manager=cache_manager,
                )
                if base_df is None or base_df.empty:
                    continue
                x = base_df.copy()
                if x.index.name is not None:
                    x = x.reset_index()
                if "Date" in x.columns:
                    x["date"] = pd.to_datetime(x["Date"], errors="coerce")
                elif "date" in x.columns:
                    x["date"] = pd.to_datetime(x["date"], errors="coerce")
                else:
                    continue
                x = x.dropna(subset=["date"]).sort_values("date")
                col_map = {
                    "Open": "open",
                    "High": "high",
                    "Low": "low",
                    "Close": "close",
                    "AdjClose": "adjusted_close",
                    "Volume": "volume",
                }
                for k, v in list(col_map.items()):
                    if k in x.columns:
                        x = x.rename(columns={k: v})
                n = int(
                    settings.cache.rolling.base_lookback_days + settings.cache.rolling.buffer_days
                )
                sliced = x.tail(n).reset_index(drop=True)
                cache_manager.write_atomic(sliced, sym, "rolling")
                df = sliced
            if df is not None and not df.empty:
                try:
                    if "Date" not in df.columns:
                        if "date" in df.columns:
                            df = df.copy()
                            df["Date"] = pd.to_datetime(df["date"], errors="coerce")
                        else:
                            df = df.copy()
                            df["Date"] = pd.to_datetime(df.index, errors="coerce")
                    df["Date"] = pd.to_datetime(df["Date"], errors="coerce").dt.normalize()
                except Exception:
                    pass
                df = _normalize_ohlcv(df)
                data[sym] = df
        except Exception:
            continue
        if total_syms > 0 and idx % chunk == 0:
            try:
                elapsed = max(0.001, _t.time() - start_ts)
                rate = idx / elapsed
                remain = max(0, total_syms - idx)
                eta_sec = int(remain / rate) if rate > 0 else 0
                m, s = divmod(eta_sec, 60)
                msg = f"üßÆ ÊåáÊ®ô„Éá„Éº„Çø„É≠„Éº„ÉâÈÄ≤Êçó: {idx}/{total_syms} | ETA {m}ÂàÜ{s}Áßí"
                _log(msg, ui=False)
                _emit_ui_log(msg)
            except Exception:
                _log(f"üßÆ ÊåáÊ®ô„Éá„Éº„Çø„É≠„Éº„ÉâÈÄ≤Êçó: {idx}/{total_syms}", ui=False)
                _emit_ui_log(f"üßÆ ÊåáÊ®ô„Éá„Éº„Çø„É≠„Éº„ÉâÈÄ≤Êçó: {idx}/{total_syms}")
    try:
        total_elapsed = int(max(0, _t.time() - start_ts))
        m, s = divmod(total_elapsed, 60)
        done_msg = f"üßÆ ÊåáÊ®ô„Éá„Éº„Çø„É≠„Éº„ÉâÂÆå‰∫Ü: {len(data)}/{total_syms} | ÊâÄË¶Å {m}ÂàÜ{s}Áßí"
        _log(done_msg)
        _emit_ui_log(done_msg)
    except Exception:
        _log(f"üßÆ ÊåáÊ®ô„Éá„Éº„Çø„É≠„Éº„ÉâÂÆå‰∫Ü: {len(data)}/{total_syms}")
        _emit_ui_log(f"üßÆ ÊåáÊ®ô„Éá„Éº„Çø„É≠„Éº„ÉâÂÆå‰∫Ü: {len(data)}/{total_syms}")
    return data


def _subset_data(basic_data: dict[str, pd.DataFrame], keys: list[str]) -> dict[str, pd.DataFrame]:
    out = {}
    for s in keys or []:
        v = basic_data.get(s)
        if v is not None and not getattr(v, "empty", True):
            out[s] = v
    return out


@dataclass(frozen=True)
class _StrategyAllocationMeta:
    calc_fn: Callable[..., Any] | None
    risk_pct: float
    max_pct: float
    max_positions: int


def _fetch_positions_and_symbol_map() -> tuple[list[Any], dict[str, str]]:
    """Fetch Alpaca positions and cached symbol-to-system mapping once."""

    try:
        client = ba.get_client(paper=True)
        positions = list(client.get_all_positions())
    except Exception:
        positions = []

    try:
        mapping_path = Path("data/symbol_system_map.json")
        if mapping_path.exists():
            symbol_system_map = json.loads(mapping_path.read_text(encoding="utf-8"))
        else:
            symbol_system_map = {}
    except Exception:
        symbol_system_map = {}

    return positions, symbol_system_map


def _load_active_positions_by_system(
    positions: Sequence[object] | None = None,
    symbol_system_map: Mapping[str, str] | None = None,
) -> dict[str, int]:
    """Return current active position counts grouped by system name.

    Alpaca „ÅÆ‰øùÊúâ„Éù„Ç∏„Ç∑„Éß„É≥„Å® `data/symbol_system_map.json` „ÇíÁ™Å„ÅçÂêà„Çè„Åõ„ÄÅ
    system1„Äú7 „ÅÆ„Å©„ÅÆÊà¶Áï•„Åß‰øùÊúâ„Åó„Å¶„ÅÑ„Çã„Åã„ÇíÊé®ÂÆö„Åô„Çã„ÄÇ
    Â§±Êïó„Åó„ÅüÂ†¥Âêà„ÅØÁ©∫ dict „ÇíËøî„Åô„ÄÇ
    """

    counts: dict[str, int] = {}

    if positions is None or symbol_system_map is None:
        positions, symbol_system_map = _fetch_positions_and_symbol_map()

    if positions is None:
        positions = []
    if symbol_system_map is None:
        symbol_system_map = {}

    for pos in positions:
        try:
            sym = str(getattr(pos, "symbol", "")).upper()
        except Exception:
            continue
        if not sym:
            continue
        try:
            qty = abs(float(getattr(pos, "qty", 0)) or 0.0)
        except Exception:
            qty = 0.0
        if qty <= 0:
            continue
        side = str(getattr(pos, "side", "")).lower()
        mapped = symbol_system_map.get(sym)
        if mapped is None and sym.lower() in symbol_system_map:
            mapped = symbol_system_map.get(sym.lower())
        system_name = str(mapped or "").lower()
        if not system_name:
            if sym == "SPY" and side == "short":
                system_name = "system7"
            else:
                continue
        counts[system_name] = counts.get(system_name, 0) + 1
    return counts


def _amount_pick(
    per_system: dict[str, pd.DataFrame],
    strategies: dict[str, object],
    total_budget: float,
    weights: dict[str, float],
    side: str,
    active_positions: dict[str, int] | None = None,
) -> pd.DataFrame:
    """Ë≥áÈáëÈÖçÂàÜ„Å´Âü∫„Å•„ÅÑ„Å¶ÂÄôË£ú„ÇíÊé°Áî®„ÄÇ
    shares „Å® position_value „Çí‰ªò‰∏é„Åó„Å¶Ëøî„Åô„ÄÇ
    """
    chosen: list[dict[str, Any]] = []
    chosen_symbols: set[str] = set()

    active_positions = active_positions or {}

    # „Ç∑„Çπ„ÉÜ„É†„Åî„Å®„ÅÆÂâ≤ÂΩì‰∫àÁÆó
    budgets = {
        name: float(total_budget) * float(weights.get(name, 0.0)) for name in weights
    }
    remaining = budgets.copy()

    # „Ç∑„Çπ„ÉÜ„É†Âêç„ÅÆÈ†ÜÂ∫è„ÇíÂõ∫ÂÆöÔºàsystem1..system7Ôºâ
    sys_order = [f"system{i}" for i in range(1, 8)]
    ordered_names = [n for n in sys_order if n in weights]

    strategy_meta: dict[str, _StrategyAllocationMeta] = {}
    candidates_by_system: dict[str, list[dict[str, Any]]] = {}
    candidate_index: dict[str, int] = {}

    for name in ordered_names:
        stg = strategies.get(name)
        config = getattr(stg, "config", {}) if stg is not None else {}
        calc_fn = getattr(stg, "calculate_position_size", None)
        if not callable(calc_fn):
            calc_fn = None
        try:
            risk_pct = float(config.get("risk_pct", 0.02))
        except Exception:
            risk_pct = 0.02
        try:
            max_pct = float(config.get("max_pct", 0.10))
        except Exception:
            max_pct = 0.10
        try:
            max_positions = int(config.get("max_positions", 10))
        except Exception:
            max_positions = 10

        strategy_meta[name] = _StrategyAllocationMeta(
            calc_fn=calc_fn,
            risk_pct=risk_pct,
            max_pct=max_pct,
            max_positions=max_positions,
        )

        df = per_system.get(name, pd.DataFrame())
        if df is None or getattr(df, "empty", True):
            candidates_by_system[name] = []
        else:
            candidates_by_system[name] = df.to_dict("records")
        candidate_index[name] = 0

    max_pos_by_system = {
        name: max(0, strategy_meta[name].max_positions - int(active_positions.get(name, 0)))
        for name in ordered_names
    }
    count_by_system: dict[str, int] = {k: 0 for k in ordered_names}

    def _normalize_shares(value: Any) -> int:
        try:
            return int(float(value))
        except Exception:
            return 0

    # „Ç∑„Çπ„ÉÜ„É†„Åî„Å®„Å´„Çπ„Ç≥„Ç¢È†Ü„ÅßÊé°Áî®„ÄÇË§áÊï∞Âë®Âõû„Åó„Å¶1‰ª∂„Åö„Å§Êãæ„ÅÜÔºàÂÅè„Çä„ÇíËªΩÊ∏õÔºâ
    still = True
    while still:
        still = False
        for name in ordered_names:
            records = candidates_by_system.get(name, [])
            if (
                not records
                or remaining.get(name, 0.0) <= 0.0
                or count_by_system.get(name, 0) >= max_pos_by_system.get(name, 0)
                or candidate_index.get(name, 0) >= len(records)
            ):
                continue

            meta = strategy_meta[name]
            idx = candidate_index[name]

            while idx < len(records):
                row = records[idx]
                idx += 1
                candidate_index[name] = idx

                sym = str(row.get("symbol", "")).upper()
                if not sym or sym in chosen_symbols:
                    continue

                entry_raw = row.get("entry_price")
                stop_raw = row.get("stop_price")
                try:
                    entry = float(entry_raw)
                except Exception:
                    entry = None
                try:
                    stop = float(stop_raw)
                except Exception:
                    stop = None
                if entry is None or stop is None or entry <= 0:
                    continue

                desired_shares = 0
                if meta.calc_fn is not None:
                    try:
                        ds = meta.calc_fn(
                            budgets[name],
                            entry,
                            stop,
                            risk_pct=meta.risk_pct,
                            max_pct=meta.max_pct,
                        )
                        desired_shares = _normalize_shares(ds)
                    except Exception:
                        desired_shares = 0

                if desired_shares <= 0:
                    continue

                max_by_cash = int(remaining[name] // abs(entry)) if entry else 0
                shares = min(desired_shares, max_by_cash)
                if shares <= 0:
                    continue

                position_value = shares * abs(entry)
                if position_value <= 0:
                    continue

                rec = dict(row)
                rec["shares"] = int(shares)
                rec["position_value"] = float(round(position_value, 2))
                # Êé°Áî®Áõ¥Ââç„ÅÆÊÆã‰Ωô„Çí system_budget „Å´Ë°®Á§∫ÔºàË¶ã„ÅüÁõÆ„ÅåÊ∏õ„Å£„Å¶„ÅÑ„ÅèÔºâ
                rec["system_budget"] = float(round(remaining[name], 2))
                rec["remaining_after"] = float(round(remaining[name] - position_value, 2))
                chosen.append(rec)
                chosen_symbols.add(sym)
                remaining[name] -= position_value
                count_by_system[name] = count_by_system.get(name, 0) + 1
                still = True
                break

    if not chosen:
        return pd.DataFrame()
    out = pd.DataFrame(chosen)
    out["side"] = side
    return out


def _submit_orders(
    final_df: pd.DataFrame,
    *,
    paper: bool = True,
    order_type: str = "market",
    tif: str = "GTC",
    retries: int = 2,
    delay: float = 0.5,
) -> pd.DataFrame:
    """final_df „Çí„ÇÇ„Å®„Å´ Alpaca „Å∏Ê≥®ÊñáÈÄÅ‰ø°Ôºàshares ÂøÖÈ†àÔºâ„ÄÇ
    Ëøî„ÇäÂÄ§: ÂÆüË°åÁµêÊûú„ÅÆ DataFrameÔºàorder_id/status/error „ÇíÂê´„ÇÄÔºâ
    """
    if final_df is None or final_df.empty:
        _log("(submit) final_df is empty; skip")
        return pd.DataFrame()
    if "shares" not in final_df.columns:
        _log("(submit) shares Âàó„Åå„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ" "Ë≥áÈáëÈÖçÂàÜ„É¢„Éº„Éâ„ÅßÂÆüË°å„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ")
        return pd.DataFrame()
    try:
        client = ba.get_client(paper=paper)
    except Exception as e:
        _log(f"(submit) AlpacaÊé•Á∂ö„Ç®„É©„Éº: {e}")
        return pd.DataFrame()

    results = []
    for _, r in final_df.iterrows():
        sym = str(r.get("symbol"))
        qty = int(r.get("shares") or 0)
        side = "buy" if str(r.get("side")).lower() == "long" else "sell"
        system = str(r.get("system"))
        entry_date = r.get("entry_date")
        if not sym or qty <= 0:
            continue
        # safely parse limit price
        limit_price = None
        if order_type == "limit":
            try:
                val = r.get("entry_price")
                if val is not None and val != "":
                    limit_price = float(val)
            except Exception:
                limit_price = None
        # estimate price for notification purposes
        price_val = None
        try:
            val = r.get("entry_price")
            if val is not None and val != "":
                price_val = float(val)
        except Exception:
            price_val = None
        if limit_price is not None:
            price_val = limit_price
        try:
            order = ba.submit_order_with_retry(
                client,
                sym,
                qty,
                side=side,
                order_type=order_type,
                limit_price=limit_price,
                time_in_force=tif,
                retries=max(0, int(retries)),
                backoff_seconds=max(0.0, float(delay)),
                rate_limit_seconds=max(0.0, float(delay)),
                log_callback=_log,
            )
            results.append(
                {
                    "symbol": sym,
                    "side": side,
                    "qty": qty,
                    "price": price_val,
                    "system": system,
                    "entry_date": entry_date,
                    # Streamlit/Arrow ‰∫íÊèõ„ÅÆ„Åü„ÇÅ UUID „ÇíÊñáÂ≠óÂàóÂåñ
                    "order_id": (
                        str(getattr(order, "id", ""))
                        if getattr(order, "id", None) is not None
                        else ""
                    ),
                    "status": getattr(order, "status", None),
                }
            )
        except Exception as e:
            results.append(
                {
                    "symbol": sym,
                    "side": side,
                    "qty": qty,
                    "price": price_val,
                    "system": system,
                    "entry_date": entry_date,
                    "error": str(e),
                }
            )
    if results:
        out = pd.DataFrame(results)
        # Âøµ„ÅÆ„Åü„ÇÅ order_id Âàó„ÅåÂ≠òÂú®„Åô„Çå„Å∞ÊñáÂ≠óÂàóÂåñÔºà‰ªñÁµåË∑Ø„Åß UUID Âûã„ÅåÊ∑∑„Åò„Çã„ÅÆ„ÇíÈò≤„ÅêÔºâ
        try:
            if "order_id" in out.columns:
                out["order_id"] = out["order_id"].apply(
                    lambda x: str(x) if x not in (None, "") else ""
                )
        except Exception:
            pass
        _log("\n=== Alpaca submission results ===")
        _log(out.to_string(index=False))
        # record entry dates for future day-based rules
        entry_map = load_entry_dates()
        for _, row in out.iterrows():
            sym = str(row.get("symbol"))
            side_val = str(row.get("side", "")).lower()
            if side_val == "buy" and row.get("entry_date"):
                entry_map[sym] = str(row["entry_date"])
            elif side_val == "sell":
                entry_map.pop(sym, None)
        save_entry_dates(entry_map)
        notifier = create_notifier(platform="auto", fallback=True)
        notifier.send_trade_report("integrated", results)
        return out
    return pd.DataFrame()


def _apply_filters(
    df: pd.DataFrame,
    *,
    only_long: bool = False,
    only_short: bool = False,
    top_per_system: int = 0,
) -> pd.DataFrame:
    if df is None or df.empty:
        return df
    out = df.copy()
    if "side" in out.columns:
        if only_long and not only_short:
            out = out[out["side"].str.lower() == "long"]
        if only_short and not only_long:
            out = out[out["side"].str.lower() == "short"]
    if top_per_system and top_per_system > 0 and "system" in out.columns:
        by = ["system"] + (["side"] if "side" in out.columns else [])
        out = out.groupby(by, as_index=False, group_keys=False).head(
            int(top_per_system)
        )  # noqa: E501
    return out


def _initialize_run_context(
    *,
    slots_long: int | None = None,
    slots_short: int | None = None,
    capital_long: float | None = None,
    capital_short: float | None = None,
    save_csv: bool = False,
    csv_name_mode: str | None = None,
    notify: bool = True,
    log_callback: Callable[[str], None] | None = None,
    progress_callback: Callable[[int, int, str], None] | None = None,
    per_system_progress: Callable[[str, str], None] | None = None,
    symbol_data: dict[str, pd.DataFrame] | None = None,
    parallel: bool = False,
) -> TodayRunContext:
    """ÂΩìÊó•„Ç∑„Ç∞„Éä„É´ÂÆüË°åÂâç„Å´ÂÖ±ÊúâË®≠ÂÆö„ÉªÁä∂ÊÖã„Çí„Åæ„Å®„ÇÅ„Åü„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„ÇíÁîüÊàê„Åô„Çã„ÄÇ"""

    settings = get_settings(create_dirs=True)
    cache_manager = CacheManager(settings)
    signals_dir = Path(settings.outputs.signals_dir)
    signals_dir.mkdir(parents=True, exist_ok=True)

    ctx = TodayRunContext(
        settings=settings,
        cache_manager=cache_manager,
        signals_dir=signals_dir,
        cache_dir=cache_manager.rolling_dir,
        slots_long=slots_long,
        slots_short=slots_short,
        capital_long=capital_long,
        capital_short=capital_short,
        save_csv=save_csv,
        csv_name_mode=csv_name_mode,
        notify=notify,
        log_callback=log_callback,
        progress_callback=progress_callback,
        per_system_progress=per_system_progress,
        symbol_data=symbol_data,
        parallel=parallel,
    )
    ctx.run_start_time = datetime.now()
    ctx.start_equity = _get_account_equity()
    try:
        import uuid as _uuid

        ctx.run_id = str(_uuid.uuid4())[:8]
    except Exception:
        ctx.run_id = "--------"
    return ctx


def _prepare_symbol_universe(ctx: TodayRunContext, initial_symbols: list[str] | None) -> list[str]:
    """Determine today's symbol universe and emit initial run banners."""

    cache_dir = ctx.cache_dir
    log_callback = ctx.log_callback
    progress_callback = ctx.progress_callback
    run_id = ctx.run_id

    if initial_symbols and len(initial_symbols) > 0:
        symbols = [s.upper() for s in initial_symbols]
    else:
        from common.universe import build_universe_from_cache, load_universe_file

        universe = load_universe_file()
        if not universe:
            universe = build_universe_from_cache(limit=None)
        symbols = [s.upper() for s in universe]
        if not symbols:
            try:
                files = list(cache_dir.glob("*.*"))
                primaries = [p.stem for p in files if p.stem.upper() == "SPY"]
                others = sorted({p.stem for p in files if len(p.stem) <= 5})[:200]
                symbols = list(dict.fromkeys(primaries + others))
            except Exception:
                symbols = []

    if "SPY" not in symbols:
        symbols.append("SPY")
    ctx.symbol_universe = list(symbols)

    # Run start banner (CLI only)
    try:
        print("#" * 68, flush=True)
    except Exception:
        pass
    _log("# üöÄüöÄüöÄ  Êú¨Êó•„ÅÆ„Ç∑„Ç∞„Éä„É´ ÂÆüË°åÈñãÂßã (Engine)  üöÄüöÄüöÄ", ui=False)
    try:
        import time as _time

        now_str = _time.strftime("%Y-%m-%d %H:%M:%S")
    except Exception:
        now_str = ""
    try:
        universe_total = sum(1 for s in symbols if str(s).upper() != "SPY")
    except Exception:
        universe_total = len(symbols)
    _log(
        f"# ‚è±Ô∏è {now_str} | ÈäòÊüÑÊï∞Ôºö{universe_total}„ÄÄ| RUN-ID: {run_id}",
        ui=False,
    )
    try:
        print("#" * 68 + "\n", flush=True)
    except Exception:
        pass

    _log(
        f"üéØ ÂØæË±°„Ç∑„É≥„Éú„É´Êï∞: {len(symbols)}"
        f" | „Çµ„É≥„Éó„É´: {', '.join(symbols[:10])}"
        f"{'...' if len(symbols) > 10 else ''}"
    )

    if log_callback:
        try:
            log_callback("üß≠ „Ç∑„É≥„Éú„É´Ê±∫ÂÆöÂÆå‰∫Ü„ÄÇÂü∫Á§é„Éá„Éº„Çø„ÅÆ„É≠„Éº„Éâ„Å∏‚Ä¶")
        except Exception:
            pass
    if progress_callback:
        try:
            progress_callback(1, 8, "ÂØæË±°Ë™≠„ÅøËæº„Åø:start")
        except Exception:
            pass

    return symbols


def _load_universe_basic_data(ctx: TodayRunContext, symbols: list[str]) -> dict[str, pd.DataFrame]:
    """Load rolling cache data for the prepared universe and ensure coverage."""

    cache_manager = ctx.cache_manager
    settings = ctx.settings
    progress_callback = ctx.progress_callback
    symbol_data = ctx.symbol_data

    base_pool = BaseCachePool(cache_manager, ctx.base_cache)

    basic_data = _load_basic_data(
        symbols,
        cache_manager,
        settings,
        symbol_data,
        today=ctx.today,
        base_cache=ctx.base_cache,
        base_cache_pool=base_pool,
    )
    ctx.basic_data = basic_data

    if progress_callback:
        try:
            progress_callback(2, 8, "load_basic")
        except Exception:
            pass

    try:
        cov_have = len(basic_data)
        cov_total = len(symbols)
        cov_missing = max(0, cov_total - cov_have)
        _log(
            "üßÆ „Éá„Éº„Çø„Ç´„Éê„É¨„ÉÉ„Ç∏: "
            + f"rollingÂèñÂæóÊ∏à„Åø {cov_have}/{cov_total} | missing={cov_missing}"
        )
        if cov_missing > 0:
            missing_syms = [s for s in symbols if s not in basic_data]
            _log(f"üõ† Ê¨†Êêç„Éá„Éº„ÇøË£úÂÆå‰∏≠: {len(missing_syms)}ÈäòÊüÑ", ui=False)
            from time import perf_counter as _perf

            repair_start = _perf()
            fixed = 0
            try:
                target_len = int(
                    settings.cache.rolling.base_lookback_days
                    + settings.cache.rolling.buffer_days
                )
            except Exception:
                target_len = 0
            for sym in missing_syms:
                try:
                    base_df, _ = base_pool.get(
                        sym, rebuild_if_missing=True
                    )
                    if base_df is None or getattr(base_df, "empty", True):
                        continue
                    sliced = _build_rolling_from_base(sym, base_df, target_len, cache_manager)
                    if sliced is None or getattr(sliced, "empty", True):
                        continue
                    try:
                        if "Date" not in sliced.columns:
                            work = sliced.copy()
                            work["Date"] = pd.to_datetime(work.get("date"), errors="coerce")
                        else:
                            work = sliced
                        work["Date"] = pd.to_datetime(work["Date"], errors="coerce").dt.normalize()
                    except Exception:
                        work = sliced
                    basic_data[sym] = _normalize_ohlcv(work)
                    fixed += 1
                except Exception:
                    continue
            if fixed:
                elapsed = int(max(0, _perf() - repair_start))
                m, s = divmod(elapsed, 60)
                _log(
                    f"üõ† Ê¨†Êêç„Éá„Éº„Çø„Çí {fixed} ÈäòÊüÑ„ÅßË£úÂÆå | ÊâÄË¶Å {m}ÂàÜ{s}Áßí",
                    ui=False,
                )
    except Exception:
        pass

    if base_pool.shared is not None:
        ctx.base_cache = base_pool.shared

    return basic_data


def _precompute_shared_indicators_phase(
    ctx: TodayRunContext, basic_data: dict[str, pd.DataFrame]
) -> dict[str, pd.DataFrame]:
    """Optionally pre-compute shared indicators for the loaded dataset."""

    if not basic_data:
        return basic_data

    try:
        import os as _os

        from common.indicators_precompute import (
            PRECOMPUTED_INDICATORS,
            precompute_shared_indicators,
        )

        try:
            thr_syms = int(_os.environ.get("PRECOMPUTE_SYMBOLS_THRESHOLD", "300"))
        except Exception:
            thr_syms = 300
        if len(basic_data) < max(0, thr_syms):
            _log(
                f"üßÆ ÂÖ±ÊúâÊåáÊ®ô„ÅÆÂâçË®àÁÆó: „Çπ„Ç≠„ÉÉ„ÉóÔºàÂØæË±°ÈäòÊüÑ {len(basic_data)} ‰ª∂ < ÈñæÂÄ§ {thr_syms}Ôºâ"
            )
            return basic_data

        try:
            _log(
                "üßÆ ÂÖ±ÊúâÊåáÊ®ô„ÅÆÂâçË®àÁÆó„ÇíÈñãÂßã: "
                + ", ".join(list(PRECOMPUTED_INDICATORS)[:8])
                + (" ‚Ä¶" if len(PRECOMPUTED_INDICATORS) > 8 else "")
            )
        except Exception:
            _log("üßÆ ÂÖ±ÊúâÊåáÊ®ô„ÅÆÂâçË®àÁÆó„ÇíÈñãÂßã (ATR/SMA/ADX „Åª„Åã)")

        force_parallel = _os.environ.get("PRECOMPUTE_PARALLEL", "").lower()
        try:
            thr_parallel = int(_os.environ.get("PRECOMPUTE_PARALLEL_THRESHOLD", "200"))
        except Exception:
            thr_parallel = 200
        if force_parallel in ("1", "true", "yes"):
            use_parallel = True
        elif force_parallel in ("0", "false", "no"):
            use_parallel = False
        else:
            use_parallel = len(basic_data) >= max(0, thr_parallel)

        try:
            st = get_settings(create_dirs=False)
            pre_workers = int(getattr(st, "THREADS_DEFAULT", 12))
        except Exception:
            pre_workers = 12
        if use_parallel:
            max_workers = max(1, min(int(pre_workers), len(basic_data)))
            try:
                _log(f"üßµ ÂâçË®àÁÆó ‰∏¶Âàó„ÉØ„Éº„Ç´„Éº: {max_workers}")
            except Exception:
                pass
        else:
            max_workers = None
        from time import perf_counter as _perf

        pre_start = _perf()
        basic_data = precompute_shared_indicators(
            basic_data,
            log=_log,
            parallel=use_parallel,
            max_workers=max_workers,
        )
        ctx.basic_data = basic_data
        elapsed = int(max(0, _perf() - pre_start))
        m, s = divmod(elapsed, 60)
        mode_label = "ON" if use_parallel else "OFF"
        _log(f"üßÆ ÂÖ±ÊúâÊåáÊ®ô„ÅÆÂâçË®àÁÆó„ÅåÂÆå‰∫Ü | ÊâÄË¶Å {m}ÂàÜ{s}Áßí | ‰∏¶Âàó={mode_label}")
    except Exception as e:
        _log(f"‚ö†Ô∏è ÂÖ±ÊúâÊåáÊ®ô„ÅÆÂâçË®àÁÆó„Å´Â§±Êïó: {e}")
    return basic_data


def _ensure_cli_logger_configured() -> None:
    """CLI ???????????????????"""
    try:
        if globals().get("_LOG_FILE_PATH") is None:
            import os as _os

            _mode_env = (_os.environ.get("TODAY_SIGNALS_LOG_MODE") or "").strip().lower()
            _configure_today_logger(mode=("single" if _mode_env == "single" else "dated"))
    except Exception:
        pass


def _silence_streamlit_cli_warnings() -> None:
    """CLI ???? Streamlit ? bare mode ????????"""
    try:
        import logging as _lg
        import os as _os

        if _os.environ.get("STREAMLIT_SERVER_ENABLED"):
            return

        class _SilenceBareModeWarnings(_lg.Filter):
            def filter(self, record: _lg.LogRecord) -> bool:  # type: ignore[override]
                msg = str(record.getMessage())
                if "missing ScriptRunContext" in msg:
                    return False
                if "Session state does not function" in msg:
                    return False
                return True

        _names = [
            "streamlit",
            "streamlit.runtime",
            "streamlit.runtime.scriptrunner_utils.script_run_context",
            "streamlit.runtime.state.session_state_proxy",
        ]
        for _name in _names:
            _logger = _lg.getLogger(_name)
            _logger.addFilter(_SilenceBareModeWarnings())
            try:
                _logger.setLevel(_lg.ERROR)
            except Exception:
                pass
    except Exception:
        pass


def _safe_progress_call(
    callback: Callable[[int, int, str], None] | None,
    current: int,
    total: int,
    label: str,
) -> None:
    """?????????????????"""
    if not callback:
        return
    try:
        callback(current, total, label)
    except Exception:
        pass


def _log_previous_counts_summary(signals_dir: Path) -> None:
    """?????????????????"""
    try:
        prev = _load_prev_counts(signals_dir)
        if prev:
            for i in range(1, 8):
                key = f"system{i}"
                v = int(prev.get(key, 0))
                icon = "?" if v > 0 else "?"
                suffix = " ??" if v == 0 else ""
                _log(f"?? {icon} (????) {key}: {v} ?{suffix}")
    except Exception:
        pass


def _apply_system_filters_and_update_ctx(
    ctx: TodayRunContext,
    symbols: list[str],
    basic_data: dict[str, pd.DataFrame],
) -> dict[str, list[str]]:
    """????????????????????????"""
    system1_syms = filter_system1(symbols, basic_data)
    system2_syms = filter_system2(symbols, basic_data)
    system3_syms = filter_system3(symbols, basic_data)
    system4_syms = filter_system4(symbols, basic_data)
    system5_syms = filter_system5(symbols, basic_data)
    system6_syms = filter_system6(symbols, basic_data)
    filters = {
        "system1": system1_syms,
        "system2": system2_syms,
        "system3": system3_syms,
        "system4": system4_syms,
        "system5": system5_syms,
        "system6": system6_syms,
    }
    ctx.system_filters = filters
    try:
        cb2 = globals().get("_PER_SYSTEM_STAGE")
    except Exception:
        cb2 = None
    if cb2 and callable(cb2):
        try:
            cb2("system1", 25, len(system1_syms), None, None, None)
            cb2("system2", 25, len(system2_syms), None, None, None)
            cb2("system3", 25, len(system3_syms), None, None, None)
            cb2("system4", 25, len(system4_syms), None, None, None)
            cb2("system5", 25, len(system5_syms), None, None, None)
            cb2("system6", 25, len(system6_syms), None, None, None)
            cb2(
                "system7",
                25,
                1 if "SPY" in (basic_data or {}) else 0,
                None,
                None,
                None,
            )
        except Exception:
            pass
    return filters


def _log_system1_filter_stats(symbols: list[str], basic_data: dict[str, pd.DataFrame]) -> None:
    """System1 ???????????????????"""
    try:
        s1_total = len(symbols)
        s1_price = 0
        s1_dv = 0
        for _sym in symbols:
            _df = basic_data.get(_sym)
            if _df is None or getattr(_df, "empty", True):
                continue
            try:
                last_close = float(_df.get("close", _df.get("Close")).iloc[-1])  # type: ignore[index]
                if last_close >= 5:
                    s1_price += 1
                else:
                    continue
                _c = _df["close"] if "close" in _df.columns else _df["Close"]
                _v = _df["volume"] if "volume" in _df.columns else _df["Volume"]
                dv20 = float((_c * _v).tail(20).mean())
                if dv20 >= 5e7:
                    s1_dv += 1
            except Exception:
                continue
        _log("?? system1???????: " + f"??={s1_total}, ??>=5: {s1_price}, DV20>=50M: {s1_dv}")
    except Exception:
        pass


def _log_system2_filter_stats(symbols: list[str], basic_data: dict[str, pd.DataFrame]) -> None:
    """System2 ???????????????????"""
    try:
        s2_total = len(symbols)
        c_price = 0
        c_dv = 0
        c_atr = 0
        for _sym in symbols:
            _df = basic_data.get(_sym)
            if _df is None or getattr(_df, "empty", True):
                continue
            try:
                last_close = float(_df["close"].iloc[-1])
                if last_close >= 5:
                    c_price += 1
                else:
                    continue
                dv = float((_df["close"] * _df["volume"]).tail(20).mean())
                if dv >= 2.5e7:
                    c_dv += 1
                else:
                    continue
                if "high" in _df.columns and "low" in _df.columns:
                    _tr = (_df["high"] - _df["low"]).tail(10)
                    _atr = float(_tr.mean())
                    if _atr >= last_close * 0.03:
                        c_atr += 1
            except Exception:
                continue
        _log(
            "?? system2???????: "
            + f"??={s2_total}, ??>=5: {c_price}, DV20>=25M: {c_dv}, ATR>=3%: {c_atr}"
        )
    except Exception:
        pass


def _log_system3_filter_stats(symbols: list[str], basic_data: dict[str, pd.DataFrame]) -> None:
    """System3 ???????????????????"""
    try:
        s3_total = len(symbols)
        s3_low = 0
        s3_av = 0
        s3_atr = 0
        for _sym in symbols:
            _df = basic_data.get(_sym)
            if _df is None or getattr(_df, "empty", True):
                continue
            try:
                _low_ser = _df.get("Low", _df.get("low"))
                if _low_ser is None:
                    continue
                if float(_low_ser.iloc[-1]) >= 1:
                    s3_low += 1
                else:
                    continue
                _av50 = _df.get("AvgVolume50")
                if (
                    _av50 is not None
                    and not pd.isna(_av50.iloc[-1])
                    and float(_av50.iloc[-1]) >= 1_000_000
                ):
                    s3_av += 1
                else:
                    continue
                _atr_ratio = _df.get("ATR_Ratio")
                if (
                    _atr_ratio is not None
                    and not pd.isna(_atr_ratio.iloc[-1])
                    and float(_atr_ratio.iloc[-1]) >= 0.05
                ):
                    s3_atr += 1
            except Exception:
                continue
        _log(
            "?? system3???????: "
            + f"??={s3_total}, Low>=1: {s3_low}, AvgVol50>=1M: {s3_av}, ATR_Ratio>=5%: {s3_atr}"
        )
    except Exception:
        pass


def _log_system4_filter_stats(symbols: list[str], basic_data: dict[str, pd.DataFrame]) -> None:
    """System4 ???????????????????"""
    try:
        s4_total = len(symbols)
        s4_dv = 0
        s4_hv = 0
        for _sym in symbols:
            _df = basic_data.get(_sym)
            if _df is None or getattr(_df, "empty", True):
                continue
            try:
                _dv50 = _df.get("DollarVolume50")
                _hv50 = _df.get("HV50")
                if (
                    _dv50 is not None
                    and not pd.isna(_dv50.iloc[-1])
                    and float(_dv50.iloc[-1]) > 100_000_000
                ):
                    s4_dv += 1
                else:
                    continue
                if _hv50 is not None and not pd.isna(_hv50.iloc[-1]):
                    hv = float(_hv50.iloc[-1])
                    if 10 <= hv <= 40:
                        s4_hv += 1
            except Exception:
                continue
        _log("?? system4???????: " + f"??={s4_total}, DV50>=100M: {s4_dv}, HV50 10?40: {s4_hv}")
    except Exception:
        pass


def _log_system5_filter_stats(symbols: list[str], basic_data: dict[str, pd.DataFrame]) -> None:
    """System5 ???????????????????"""
    try:
        threshold_label = format_atr_pct_threshold_label()
        s5_total = len(symbols)
        s5_av = 0
        s5_dv = 0
        s5_atr = 0
        for _sym in symbols:
            _df = basic_data.get(_sym)
            if _df is None or getattr(_df, "empty", True):
                continue
            try:
                _av50 = _df.get("AvgVolume50")
                if (
                    _av50 is not None
                    and not pd.isna(_av50.iloc[-1])
                    and float(_av50.iloc[-1]) > 500_000
                ):
                    s5_av += 1
                else:
                    continue
                _dv50 = _df.get("DollarVolume50")
                if (
                    _dv50 is not None
                    and not pd.isna(_dv50.iloc[-1])
                    and float(_dv50.iloc[-1]) > 2_500_000
                ):
                    s5_dv += 1
                else:
                    continue
                _atrp = _df.get("ATR_Pct")
                if (
                    _atrp is not None
                    and not pd.isna(_atrp.iloc[-1])
                    and float(_atrp.iloc[-1]) > DEFAULT_ATR_PCT_THRESHOLD
                ):
                    s5_atr += 1
            except Exception:
                continue
        _log(
            "?? system5???????: "
            + f"??={s5_total}, AvgVol50>500k: {s5_av}, DV50>2.5M: {s5_dv}, {threshold_label}: {s5_atr}"
        )
    except Exception:
        pass


def _log_system6_filter_stats(symbols: list[str], basic_data: dict[str, pd.DataFrame]) -> None:
    """System6 ???????????????????"""
    try:
        s6_total = len(symbols)
        s6_low = 0
        s6_dv = 0
        for _sym in symbols:
            _df = basic_data.get(_sym)
            if _df is None or getattr(_df, "empty", True):
                continue
            try:
                _low_ser = _df.get("Low", _df.get("low"))
                if _low_ser is None:
                    continue
                if float(_low_ser.iloc[-1]) >= 5:
                    s6_low += 1
                else:
                    continue
                _dv50 = _df.get("DollarVolume50")
                if (
                    _dv50 is not None
                    and not pd.isna(_dv50.iloc[-1])
                    and float(_dv50.iloc[-1]) > 10_000_000
                ):
                    s6_dv += 1
            except Exception:
                continue
        _log("?? system6???????: " + f"??={s6_total}, Low>=5: {s6_low}, DV50>10M: {s6_dv}")
    except Exception:
        pass


def _log_system7_filter_stats(basic_data: dict[str, pd.DataFrame]) -> None:
    """System7 (SPY) ?????????????????"""
    try:
        spyp = (
            1 if ("SPY" in basic_data and not getattr(basic_data.get("SPY"), "empty", True)) else 0
        )
        _log("?? system7???????: SPY?? | SPY??=" + str(spyp))
    except Exception:
        pass


def _log_system_filter_stats(
    symbols: list[str],
    basic_data: dict[str, pd.DataFrame],
    filters: dict[str, list[str]],
) -> None:
    """????????????????????????"""
    _log("?? ?????????? (system1?system6)?")
    _log_system1_filter_stats(symbols, basic_data)
    _log_system2_filter_stats(symbols, basic_data)
    _log_system3_filter_stats(symbols, basic_data)
    _log_system4_filter_stats(symbols, basic_data)
    _log_system5_filter_stats(symbols, basic_data)
    _log_system6_filter_stats(symbols, basic_data)
    _log_system7_filter_stats(basic_data)
    system1_syms = filters.get("system1", [])
    system2_syms = filters.get("system2", [])
    system3_syms = filters.get("system3", [])
    system4_syms = filters.get("system4", [])
    system5_syms = filters.get("system5", [])
    system6_syms = filters.get("system6", [])
    _log(
        "?? ???????: "
        + f"system1={len(system1_syms)}?, "
        + f"system2={len(system2_syms)}?, "
        + f"system3={len(system3_syms)}?, "
        + f"system4={len(system4_syms)}?, "
        + f"system5={len(system5_syms)}?, "
        + f"system6={len(system6_syms)}?"
    )
def _ensure_rolling_cache_fresh(
    symbol: str,
    rolling_df: 'pd.DataFrame',
    today: 'pd.Timestamp',
    cache_manager: 'CacheManager',
    base_rows: int = 320,
    max_lag_days: int = 2,
) -> 'pd.DataFrame':
    """
    rolling_df„ÅÆÊúÄÁµÇÊó•‰ªò„Ååtoday„Åã„Çâmax_lag_days‰ª•‰∏ä„Ç∫„É¨„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÄÅ
    base„Åã„Çârolling„ÇíÂÜçÁîüÊàê„Åó„ÄÅrolling„Å∏Êõ∏„ÅçÊàª„Åô„ÄÇ
    """
    if rolling_df is None or getattr(rolling_df, 'empty', True):
        # Ê¨†ÊêçÊôÇ„ÅØbase„Åã„ÇâÂÜçÁîüÊàê
        base_df = cache_manager.read(symbol, layer="base", rows=base_rows)
        if base_df is not None and not getattr(base_df, 'empty', True):
            rolling_new = base_df.tail(base_rows).copy()
            cache_manager.write_atomic(symbol, rolling_new, layer="rolling")
            return rolling_new
        return rolling_df
    last_date = None
    try:
        last_date = rolling_df.index[-1]
        if isinstance(last_date, str):
            import pandas as pd
            last_date = pd.to_datetime(last_date)
    except Exception:
        return rolling_df
    lag_days = (today - last_date).days
    if lag_days > max_lag_days:
        # ÈÆÆÂ∫¶‰∏çË∂≥: base„Åã„ÇârollingÂÜçÁîüÊàê
        base_df = cache_manager.read(symbol, layer="base", rows=base_rows)
        if base_df is not None and not getattr(base_df, 'empty', True):
            rolling_new = base_df.tail(base_rows).copy()
            cache_manager.write_atomic(symbol, rolling_new, layer="rolling")
            return rolling_new
    return rolling_df


def _prepare_system2_data(
    basic_data: dict[str, pd.DataFrame],
    system_symbols: list[str],
) -> tuple[dict[str, pd.DataFrame], int, int, int]:
    """System2 ???????????????????"""
    _log("?? ?????????????? (system2)?")
    raw_data = _subset_data(basic_data, system_symbols)
    _log(f"?? ???????: system2={len(raw_data)}??")
    s2_filter = int(len(system_symbols))
    s2_rsi = 0
    s2_up2 = 0
    try:
        for _sym in system_symbols or []:
            _df = raw_data.get(_sym)
            if _df is None or getattr(_df, "empty", True):
                continue
            try:
                last = _df.iloc[-1]
            except Exception:
                continue
            try:
                if float(last.get("RSI3", 0)) > 90:
                    s2_rsi += 1
            except Exception:
                pass
            try:
                if bool(last.get("TwoDayUp", False)):
                    s2_up2 += 1
            except Exception:
                pass
        _log(
            "?? system2????????: "
            + f"??????={s2_filter}, RSI3>90: {s2_rsi}, "
            + f"TwoDayUp: {s2_up2}"
        )
        try:
            cb2 = globals().get("_PER_SYSTEM_STAGE")
        except Exception:
            cb2 = None
        if cb2 and callable(cb2):
            try:
                cb2("system2", 50, int(s2_filter), int(max(s2_rsi, s2_up2)), None, None)
            except Exception:
                pass
    except Exception:
        pass
    return raw_data, s2_filter, s2_rsi, s2_up2


def _prepare_system3_data(
    basic_data: dict[str, pd.DataFrame],
    system_symbols: list[str],
) -> tuple[dict[str, pd.DataFrame], int, int, int]:
    """System3 ???????????????????"""
    _log("?? ?????????????? (system3)?")
    raw_data = _subset_data(basic_data, system_symbols)
    _log(f"?? ???????: system3={len(raw_data)}??")
    s3_filter = int(len(system_symbols))
    s3_close = 0
    s3_drop = 0
    try:
        for _sym in system_symbols or []:
            _df = raw_data.get(_sym)
            if _df is None or getattr(_df, "empty", True):
                continue
            try:
                last = _df.iloc[-1]
            except Exception:
                continue
            try:
                if float(last.get("Close", 0)) > float(last.get("SMA150", float("inf"))):
                    s3_close += 1
            except Exception:
                pass
            try:
                if float(last.get("Drop3D", 0)) >= 0.125:
                    s3_drop += 1
            except Exception:
                pass
        _log(
            "?? system3????????: "
            + f"??????={s3_filter}, Close>SMA150: {s3_close}, "
            + f"3????>=12.5%: {s3_drop}"
        )
        try:
            cb2 = globals().get("_PER_SYSTEM_STAGE")
        except Exception:
            cb2 = None
        if cb2 and callable(cb2):
            try:
                cb2("system3", 50, int(s3_filter), int(max(s3_close, s3_drop)), None, None)
            except Exception:
                pass
    except Exception:
        pass
    return raw_data, s3_filter, s3_close, s3_drop


def _prepare_system4_data(
    basic_data: dict[str, pd.DataFrame],
    system_symbols: list[str],
) -> tuple[dict[str, pd.DataFrame], int, int]:
    """System4 ???????????????????"""
    _log("?? ?????????????? (system4)?")
    raw_data = _subset_data(basic_data, system_symbols)
    _log(f"?? ???????: system4={len(raw_data)}??")
    s4_filter = int(len(system_symbols))
    s4_close = 0
    try:
        for _sym in system_symbols or []:
            _df = raw_data.get(_sym)
            if _df is None or getattr(_df, "empty", True):
                continue
            try:
                last = _df.iloc[-1]
            except Exception:
                continue
            try:
                if float(last.get("Close", 0)) > float(last.get("SMA200", float("inf"))):
                    s4_close += 1
            except Exception:
                pass
        _log(f"?? system4????????: ??????={s4_filter}, Close>SMA200: {s4_close}")
        try:
            cb2 = globals().get("_PER_SYSTEM_STAGE")
        except Exception:
            cb2 = None
        if cb2 and callable(cb2):
            try:
                cb2("system4", 50, int(s4_filter), int(s4_close), None, None)
            except Exception:
                pass
    except Exception:
        pass
    return raw_data, s4_filter, s4_close


def _prepare_system5_data(
    basic_data: dict[str, pd.DataFrame],
    system_symbols: list[str],
) -> tuple[dict[str, pd.DataFrame], int, int, int, int]:
    """System5 ???????????????????"""
    _log("?? ?????????????? (system5)?")
    raw_data = _subset_data(basic_data, system_symbols)
    _log(f"?? ???????: system5={len(raw_data)}??")
    s5_filter = int(len(system_symbols))
    s5_close = 0
    s5_adx = 0
    s5_rsi = 0
    try:
        for _sym in system_symbols or []:
            _df = raw_data.get(_sym)
            if _df is None or getattr(_df, "empty", True):
                continue
            try:
                last = _df.iloc[-1]
            except Exception:
                continue
            try:
                if float(last.get("Close", 0)) > float(last.get("SMA100", 0)) + float(
                    last.get("ATR10", 0)
                ):
                    s5_close += 1
            except Exception:
                pass
            try:
                if float(last.get("ADX7", 0)) > 55:
                    s5_adx += 1
            except Exception:
                pass
            try:
                if float(last.get("RSI3", 100)) < 50:
                    s5_rsi += 1
            except Exception:
                pass
        _log(
            "?? system5????????: "
            + f"??????={s5_filter}, Close>SMA100+ATR10: {s5_close}, "
            + f"ADX7>55: {s5_adx}, RSI3<50: {s5_rsi}"
        )
        try:
            cb2 = globals().get("_PER_SYSTEM_STAGE")
        except Exception:
            cb2 = None
        if cb2 and callable(cb2):
            try:
                cb2("system5", 50, int(s5_filter), int(s5_close), None, None)
            except Exception:
                pass
    except Exception:
        pass
    return raw_data, s5_filter, s5_close, s5_adx, s5_rsi


def _prepare_system6_data(
    basic_data: dict[str, pd.DataFrame],
    system_symbols: list[str],
) -> tuple[dict[str, pd.DataFrame], int, int, int]:
    """System6 ???????????????????"""
    _log("?? ?????????????? (system6)?")
    raw_data = _subset_data(basic_data, system_symbols)
    _log(f"?? ???????: system6={len(raw_data)}??")
    s6_filter = int(len(system_symbols))
    s6_ret = 0
    s6_up2 = 0
    try:
        for _sym in system_symbols or []:
            _df = raw_data.get(_sym)
            if _df is None or getattr(_df, "empty", True):
                continue
            try:
                last = _df.iloc[-1]
            except Exception:
                continue
            try:
                if float(last.get("Return6D", 0)) > 0.20:
                    s6_ret += 1
            except Exception:
                pass
            try:
                if bool(last.get("UpTwoDays", False)):
                    s6_up2 += 1
            except Exception:
                pass
        _log(
            "?? system6????????: "
            + f"??????={s6_filter}, Return6D>20%: {s6_ret}, "
            + f"UpTwoDays: {s6_up2}"
        )
        try:
            cb2 = globals().get("_PER_SYSTEM_STAGE")
        except Exception:
            cb2 = None
        if cb2 and callable(cb2):
            try:
                cb2("system6", 50, int(s6_filter), int(max(s6_ret, s6_up2)), None, None)
            except Exception:
                pass
    except Exception:
        pass
    return raw_data, s6_filter, s6_ret, s6_up2


def _resolve_spy_dataframe(basic_data: dict[str, pd.DataFrame]) -> pd.DataFrame | None:
    """SPY ??????????????????"""
    if "SPY" in basic_data:
        try:
            return get_spy_with_indicators(basic_data["SPY"])
        except Exception:
            return None
    _log(
        "?? SPY ?????????????? (base/full_backup/rolling ?????????)"
        + " SPY.csv ? data_cache/base ???? data_cache/full_backup ?????????"
    )
    return None


@no_type_check
def compute_today_signals(
    symbols: list[str] | None,
    *,
    slots_long: int | None = None,
    slots_short: int | None = None,
    capital_long: float | None = None,
    capital_short: float | None = None,
    save_csv: bool = False,
    csv_name_mode: str | None = None,
    notify: bool = True,
    log_callback: Callable[[str], None] | None = None,
    progress_callback: Callable[[int, int, str], None] | None = None,
    # ËøΩÂä†: ‰∏¶ÂàóÂÆüË°åÊôÇ„Å™„Å©„Å´ system „Åî„Å®„ÅÆÈñãÂßã/ÂÆå‰∫Ü„ÇíÈÄöÁü•„Åô„ÇãËªΩÈáè„Ç≥„Éº„É´„Éê„ÉÉ„ÇØ
    # phase „ÅØ "start" | "done" „ÇíÊÉ≥ÂÆö
    per_system_progress: Callable[[str, str], None] | None = None,
    symbol_data: dict[str, pd.DataFrame] | None = None,
    parallel: bool = False,
) -> tuple[pd.DataFrame, dict[str, pd.DataFrame]]:
    """ÂΩìÊó•„Ç∑„Ç∞„Éä„É´ÊäΩÂá∫ÔºãÈÖçÂàÜ„ÅÆÊú¨‰Ωì„ÄÇ

    Args:
        symbols: ÂØæË±°„Ç∑„É≥„Éú„É´„É™„Çπ„Éà„ÄÇ
        parallel: True „ÅÆÂ†¥Âêà„ÅØ„Ç∑„Çπ„ÉÜ„É†„Åî„Å®„ÅÆ„Ç∑„Ç∞„Éä„É´ÊäΩÂá∫„Çí‰∏¶Ë°åÂÆüË°å„Åô„Çã„ÄÇ

    Êàª„ÇäÂÄ§: (final_df, per_system_df_dict)
    """
    ctx = _initialize_run_context(
        slots_long=slots_long,
        slots_short=slots_short,
        capital_long=capital_long,
        capital_short=capital_short,
        save_csv=save_csv,
        csv_name_mode=csv_name_mode,
        notify=notify,
        log_callback=log_callback,
        progress_callback=progress_callback,
        per_system_progress=per_system_progress,
        symbol_data=symbol_data,
        parallel=parallel,
    )

    # CLI ÁµåÁî±„ÅßÊú™Ë®≠ÂÆö„ÅÆÂ†¥ÂêàÔºàUI Á≠âÔºâ„ÄÅÊó¢ÂÆö„ÅßÊó•‰ªòÂà•„É≠„Ç∞„Å´ÂàáÊõø
    try:
        if globals().get("_LOG_FILE_PATH") is None:
            import os as _os

            _mode_env = (_os.environ.get("TODAY_SIGNALS_LOG_MODE") or "").strip().lower()
            _configure_today_logger(mode=("single" if _mode_env == "single" else "dated"))
    except Exception:
        pass

    _run_id = ctx.run_id
    settings = ctx.settings
    # install log callback for helpers
    globals()["_LOG_CALLBACK"] = ctx.log_callback
    signals_dir = ctx.signals_dir

    run_start_time = ctx.run_start_time
    start_equity = ctx.start_equity
    slots_long = ctx.slots_long
    slots_short = ctx.slots_short
    capital_long = ctx.capital_long
    capital_short = ctx.capital_short
    save_csv = ctx.save_csv
    csv_name_mode = ctx.csv_name_mode
    notify = ctx.notify
    log_callback = ctx.log_callback
    progress_callback = ctx.progress_callback
    per_system_progress = ctx.per_system_progress
    parallel = ctx.parallel

    # CLIÂÆüË°åÊôÇ„ÅÆStreamlitË≠¶Âëä„ÇíÊäëÂà∂ÔºàUI„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„ÅåÁÑ°„ÅÑÂ†¥Âêà„ÅÆ„ÅøÔºâ
    try:
        import logging as _lg
        import os as _os

        if not _os.environ.get("STREAMLIT_SERVER_ENABLED"):

            class _SilenceBareModeWarnings(_lg.Filter):
                def filter(self, record: _lg.LogRecord) -> bool:  # type: ignore[override]
                    msg = str(record.getMessage())
                    if "missing ScriptRunContext" in msg:
                        return False
                    if "Session state does not function" in msg:
                        return False
                    return True

            _names = [
                "streamlit",
                "streamlit.runtime",
                "streamlit.runtime.scriptrunner_utils.script_run_context",
                "streamlit.runtime.state.session_state_proxy",
            ]
            for _name in _names:
                _logger = _lg.getLogger(_name)
                _logger.addFilter(_SilenceBareModeWarnings())
                try:
                    _logger.setLevel(_lg.ERROR)
                except Exception:
                    pass
    except Exception:
        pass

    # ÊúÄÊñ∞Âñ∂Ê•≠Êó•ÔºàNYSEÔºâ
    today = get_latest_nyse_trading_day().normalize()
    ctx.today = today
    _log(f"üìÖ ÊúÄÊñ∞Âñ∂Ê•≠Êó•ÔºàNYSEÔºâ: {today.date()}")
    _log("‚ÑπÔ∏è Ê≥®: EODHD„ÅØÂΩìÊó•ÁµÇÂÄ§„ÅåÊú™ÂèçÊò†„ÅÆ„Åü„ÇÅ„ÄÅÁõ¥ËøëÂñ∂Ê•≠Êó•„Éô„Éº„Çπ„ÅßË®àÁÆó„Åó„Åæ„Åô„ÄÇ")
    # ÈñãÂßãÁõ¥Âæå„Å´ÂâçÂõûÁµêÊûú„Çí„Åæ„Å®„ÇÅ„Å¶Ë°®Á§∫
    try:
        prev = _load_prev_counts(signals_dir)
        if prev:
            for i in range(1, 8):
                key = f"system{i}"
                v = int(prev.get(key, 0))
                icon = "‚úÖ" if v > 0 else "‚ùå"
                _log(f"üßæ {icon} (ÂâçÂõûÁµêÊûú) {key}: {v} ‰ª∂{' üö´' if v == 0 else ''}")
    except Exception:
        pass
    if progress_callback:
        try:
            progress_callback(0, 8, "init")
        except Exception:
            pass

    symbols = _prepare_symbol_universe(ctx, symbols)
    basic_data = _load_universe_basic_data(ctx, symbols)

    basic_data = _precompute_shared_indicators_phase(ctx, basic_data)
    _log("üß™ ‰∫ãÂâç„Éï„Ç£„É´„Çø„ÉºÂÆüË°å‰∏≠ (system1„Äúsystem6)‚Ä¶")
    system1_syms = filter_system1(symbols, basic_data)
    system2_syms = filter_system2(symbols, basic_data)
    system3_syms = filter_system3(symbols, basic_data)
    system4_syms = filter_system4(symbols, basic_data)
    system5_syms = filter_system5(symbols, basic_data)
    system6_syms = filter_system6(symbols, basic_data)
    ctx.system_filters = {
        "system1": system1_syms,
        "system2": system2_syms,
        "system3": system3_syms,
        "system4": system4_syms,
        "system5": system5_syms,
        "system6": system6_syms,
    }
    # ÂêÑ„Ç∑„Çπ„ÉÜ„É†„ÅÆ„Éï„Ç£„É´„Çø„ÉºÈÄöÈÅé‰ª∂Êï∞„ÇíUI„Å∏ÈÄöÁü•
    try:
        cb2 = globals().get("_PER_SYSTEM_STAGE")
    except Exception:
        cb2 = None
    if cb2 and callable(cb2):
        try:
            cb2("system1", 25, len(system1_syms), None, None, None)
            cb2("system2", 25, len(system2_syms), None, None, None)
            cb2("system3", 25, len(system3_syms), None, None, None)
            cb2("system4", 25, len(system4_syms), None, None, None)
            cb2("system5", 25, len(system5_syms), None, None, None)
            cb2("system6", 25, len(system6_syms), None, None, None)
            cb2(
                "system7",
                25,
                1 if "SPY" in (basic_data or {}) else 0,
                None,
                None,
                None,
            )
        except Exception:
            pass
    # System2 „Éï„Ç£„É´„Çø„ÉºÂÜÖË®≥„ÅÆÂèØË¶ñÂåñÔºà‰æ°Ê†º„ÉªÂ£≤Ë≤∑‰ª£Èáë„ÉªATR „ÅÆÊÆµÈöéÈÄöÈÅéÊï∞Ôºâ
    try:
        s2_total = len(symbols)
        c_price = 0
        c_dv = 0
        c_atr = 0
        for _sym in symbols:
            _df = basic_data.get(_sym)
            if _df is None or _df.empty:
                continue
            try:
                # ‰æ°Ê†º„Éï„Ç£„É´„Çø„Éº
                last_close = float(_df["close"].iloc[-1])
                if last_close >= 5:
                    c_price += 1
                else:
                    continue
                # Â£≤Ë≤∑‰ª£Èáë„Éï„Ç£„É´„Çø„ÉºÔºà20Êó•Âπ≥Âùá„ÉªÂé≥ÂØÜÔºâ
                dv = float((_df["close"] * _df["volume"]).tail(20).mean())
                if dv >= 2.5e7:
                    c_dv += 1
                else:
                    continue
                # ATR ÊØîÁéá„Éï„Ç£„É´„Çø„ÉºÔºà10Êó•Ôºâ
                if "high" in _df.columns and "low" in _df.columns:
                    _tr = (_df["high"] - _df["low"]).tail(10)
                    _atr = float(_tr.mean())
                    if _atr >= last_close * 0.03:
                        c_atr += 1
            except Exception:
                continue
        _log(
            "üß™ system2ÂÜÖË®≥: "
            + f"ÂÖÉ={s2_total}, ‰æ°Ê†º>=5: {c_price}, DV20>=25M: {c_dv}, ATR>=3%: {c_atr}"
        )
    except Exception:
        pass
    # System1 „Éï„Ç£„É´„Çø„ÉºÂÜÖË®≥Ôºà‰æ°Ê†º„ÉªÂ£≤Ë≤∑‰ª£ÈáëÔºâ
    try:
        s1_total = len(symbols)
        s1_price = 0
        s1_dv = 0
        for _sym in symbols:
            _df = basic_data.get(_sym)
            if _df is None or _df.empty:
                continue
            try:
                last_close = float(_df.get("close", _df.get("Close")).iloc[-1])  # type: ignore[index]
                if last_close >= 5:
                    s1_price += 1
                else:
                    continue
                # ÂÆâÂÖ®„Å´„Ç´„É©„É†„ÇíÂèñÂæó„Åó„Å¶ DV20 „ÇíË®àÁÆó
                _c = _df["close"] if "close" in _df.columns else _df["Close"]
                _v = _df["volume"] if "volume" in _df.columns else _df["Volume"]
                dv20 = float((_c * _v).tail(20).mean())
                if dv20 >= 5e7:
                    s1_dv += 1
            except Exception:
                continue
        _log("üß™ system1ÂÜÖË®≥: " + f"ÂÖÉ={s1_total}, ‰æ°Ê†º>=5: {s1_price}, DV20>=50M: {s1_dv}")
    except Exception:
        pass
    # System3 „Éï„Ç£„É´„Çø„ÉºÂÜÖË®≥ÔºàLow>=1 ‚Üí AvgVol50>=1M ‚Üí ATR_Ratio>=5%Ôºâ
    try:
        s3_total = len(symbols)
        s3_low = 0
        s3_av = 0
        s3_atr = 0
        for _sym in symbols:
            _df = basic_data.get(_sym)
            if _df is None or _df.empty:
                continue
            try:
                _low_ser = _df.get("Low", _df.get("low"))
                if _low_ser is None:
                    continue
                if float(_low_ser.iloc[-1]) >= 1:
                    s3_low += 1
                else:
                    continue
                _av50 = _df.get("AvgVolume50")
                if (
                    _av50 is not None
                    and not pd.isna(_av50.iloc[-1])
                    and float(_av50.iloc[-1]) >= 1_000_000
                ):
                    s3_av += 1
                else:
                    continue
                _atr_ratio = _df.get("ATR_Ratio")
                if (
                    _atr_ratio is not None
                    and not pd.isna(_atr_ratio.iloc[-1])
                    and float(_atr_ratio.iloc[-1]) >= 0.05
                ):
                    s3_atr += 1
            except Exception:
                continue
        _log(
            "üß™ system3ÂÜÖË®≥: "
            + f"ÂÖÉ={s3_total}, Low>=1: {s3_low}, AvgVol50>=1M: {s3_av}, ATR_Ratio>=5%: {s3_atr}"
        )
    except Exception:
        pass
    # System4 „Éï„Ç£„É´„Çø„ÉºÂÜÖË®≥ÔºàDV50>=100M ‚Üí HV50 10„Äú40Ôºâ
    try:
        s4_total = len(symbols)
        s4_dv = 0
        s4_hv = 0
        for _sym in symbols:
            _df = basic_data.get(_sym)
            if _df is None or _df.empty:
                continue
            try:
                _dv50 = _df.get("DollarVolume50")
                _hv50 = _df.get("HV50")
                if (
                    _dv50 is not None
                    and not pd.isna(_dv50.iloc[-1])
                    and float(_dv50.iloc[-1]) > 100_000_000
                ):
                    s4_dv += 1
                else:
                    continue
                if _hv50 is not None and not pd.isna(_hv50.iloc[-1]):
                    hv = float(_hv50.iloc[-1])
                    if 10 <= hv <= 40:
                        s4_hv += 1
            except Exception:
                continue
        _log("üß™ system4ÂÜÖË®≥: " + f"ÂÖÉ={s4_total}, DV50>=100M: {s4_dv}, HV50 10„Äú40: {s4_hv}")
    except Exception:
        pass
    # System5 „Éï„Ç£„É´„Çø„ÉºÂÜÖË®≥ÔºàAvgVol50>500k ‚Üí DV50>2.5M ‚Üí ATR_Pct>ÈñæÂÄ§Ôºâ
    try:
        threshold_label = format_atr_pct_threshold_label()
        s5_total = len(symbols)
        s5_av = 0
        s5_dv = 0
        s5_atr = 0
        for _sym in symbols:
            _df = basic_data.get(_sym)
            if _df is None or _df.empty:
                continue
            try:
                _av50 = _df.get("AvgVolume50")
                if (
                    _av50 is not None
                    and not pd.isna(_av50.iloc[-1])
                    and float(_av50.iloc[-1]) > 500_000
                ):
                    s5_av += 1
                else:
                    continue
                _dv50 = _df.get("DollarVolume50")
                if (
                    _dv50 is not None
                    and not pd.isna(_dv50.iloc[-1])
                    and float(_dv50.iloc[-1]) > 2_500_000
                ):
                    s5_dv += 1
                else:
                    continue
                _atrp = _df.get("ATR_Pct")
                if (
                    _atrp is not None
                    and not pd.isna(_atrp.iloc[-1])
                    and float(_atrp.iloc[-1]) > DEFAULT_ATR_PCT_THRESHOLD
                ):
                    s5_atr += 1
            except Exception:
                continue
        _log(
            "üß™ system5ÂÜÖË®≥: "
            + f"ÂÖÉ={s5_total}, AvgVol50>500k: {s5_av}, DV50>2.5M: {s5_dv}, {threshold_label}: {s5_atr}"
        )
    except Exception:
        pass
    # System6 „Éï„Ç£„É´„Çø„ÉºÂÜÖË®≥ÔºàLow>=5 ‚Üí DV50>10MÔºâ
    try:
        s6_total = len(symbols)
        s6_low = 0
        s6_dv = 0
        for _sym in symbols:
            _df = basic_data.get(_sym)
            if _df is None or _df.empty:
                continue
            try:
                _low_ser = _df.get("Low", _df.get("low"))
                if _low_ser is None:
                    continue
                if float(_low_ser.iloc[-1]) >= 5:
                    s6_low += 1
                else:
                    continue
                _dv50 = _df.get("DollarVolume50")
                if (
                    _dv50 is not None
                    and not pd.isna(_dv50.iloc[-1])
                    and float(_dv50.iloc[-1]) > 10_000_000
                ):
                    s6_dv += 1
            except Exception:
                continue
        _log("üß™ system6ÂÜÖË®≥: " + f"ÂÖÉ={s6_total}, Low>=5: {s6_low}, DV50>10M: {s6_dv}")
    except Exception:
        pass
    # System7 „ÅØ SPY Âõ∫ÂÆöÔºàÂèÇËÄÉÊÉÖÂ†±„ÅÆ„ÅøÔºâ
    try:
        spyp = (
            1 if ("SPY" in basic_data and not getattr(basic_data.get("SPY"), "empty", True)) else 0
        )
        _log("üß™ system7ÂÜÖË®≥: SPYÂõ∫ÂÆö | SPYÂ≠òÂú®=" + str(spyp))
    except Exception:
        pass
    _log(
        "üß™ „Éï„Ç£„É´„Çø„ÉºÁµêÊûú: "
        + f"system1={len(system1_syms)}‰ª∂, "
        + f"system2={len(system2_syms)}‰ª∂, "
        + f"system3={len(system3_syms)}‰ª∂, "
        + f"system4={len(system4_syms)}‰ª∂, "
        + f"system5={len(system5_syms)}‰ª∂, "
        + f"system6={len(system6_syms)}‰ª∂"
    )
    if progress_callback:
        try:
            progress_callback(3, 8, "filter")
        except Exception:
            pass

    # ÂêÑ„Ç∑„Çπ„ÉÜ„É†Áî®„ÅÆÁîü„Éá„Éº„ÇøËæûÊõ∏„Çí‰∫ãÂâç„Éï„Ç£„É´„Çø„ÉºÂæå„ÅÆÈäòÊüÑ„ÅßÊßãÁØâ
    _log("üßÆ ÊåáÊ®ôË®àÁÆóÁî®„Éá„Éº„Çø„É≠„Éº„Éâ‰∏≠ (system1)‚Ä¶")
    raw_data_system1 = _subset_data(basic_data, system1_syms)
    _log(f"üßÆ ÊåáÊ®ô„Éá„Éº„Çø: system1={len(raw_data_system1)}ÈäòÊüÑ")
    # System1 „Çª„ÉÉ„Éà„Ç¢„ÉÉ„ÉóÂÜÖË®≥ÔºàÊúÄÊñ∞Êó•„ÅÆ setup Âà§ÂÆöÊï∞Ôºâ„Çí CLI „Å´Âá∫Âäõ
    try:
        # „Éï„Ç£„É´„ÇøÈÄöÈÅé„ÅØ‰∫ãÂâç„Éï„Ç£„É´„Çø„ÉºÁµêÊûúÔºàsystem1_symsÔºâÁî±Êù•„ÅßÁ¢∫ÂÆö
        s1_filter = int(len(system1_syms))
        # Áõ¥ËøëÊó•„ÅÆ SMA25>SMA50 „ÇíÈõÜË®àÔºà‰∫ãÂâçË®àÁÆóÊ∏à„ÅøÂàó„ÇíÂèÇÁÖßÔºâ
        s1_setup = 0
        # Â∏ÇÂ†¥Êù°‰ª∂ÔºàSPY„ÅÆClose>SMA100Ôºâ„ÇíÂÖà„Å´Âà§ÂÆö
        _spy_ok = None
        try:
            if "SPY" in (basic_data or {}):
                _spy_df = get_spy_with_indicators(basic_data["SPY"])
                if _spy_df is not None and not getattr(_spy_df, "empty", True):
                    _last = _spy_df.iloc[-1]
                    _spy_ok = int(float(_last.get("Close", 0)) > float(_last.get("SMA100", 0)))
        except Exception:
            _spy_ok = None
        for _sym, _df in (raw_data_system1 or {}).items():
            if _df is None or getattr(_df, "empty", True):
                continue
            try:
                last = _df.iloc[-1]
            except Exception:
                continue
            try:
                sma_pass = float(last.get("SMA25", float("nan"))) > float(
                    last.get("SMA50", float("nan"))
                )
            except Exception:
                sma_pass = False
            if sma_pass:
                s1_setup += 1
        # Âá∫ÂäõÈ†Ü: „Éï„Ç£„É´„ÇøÈÄöÈÅé ‚Üí SPY>SMA100 ‚Üí SMA25>SMA50
        if _spy_ok is None:
            _log(
                f"üß© system1„Çª„ÉÉ„Éà„Ç¢„ÉÉ„ÉóÂÜÖË®≥: „Éï„Ç£„É´„ÇøÈÄöÈÅé={s1_filter}, SPY>SMA100: -, "
                f"SMA25>SMA50: {s1_setup}"
            )
        else:
            _log(
                f"üß© system1„Çª„ÉÉ„Éà„Ç¢„ÉÉ„ÉóÂÜÖË®≥: „Éï„Ç£„É´„ÇøÈÄöÈÅé={s1_filter}, SPY>SMA100: {_spy_ok}, "
                f"SMA25>SMA50: {s1_setup}"
            )
        # UI „ÅÆ STUpass „Å∏ÂèçÊò†Ôºà50%ÊôÇÁÇπÔºâ
        try:
            cb2 = globals().get("_PER_SYSTEM_STAGE")
            if cb2 and callable(cb2):
                # SPY „Ç≤„Éº„ÉàÔºàClose>SMA100Ôºâ„ÅåÂÅΩ„Å™„Çâ STUpass „ÅØ 0 Êâ±„ÅÑ
                s1_setup_eff = int(s1_setup)
                try:
                    if isinstance(_spy_ok, int) and _spy_ok == 0:
                        s1_setup_eff = 0
                except Exception:
                    pass
                cb2("system1", 50, int(s1_filter), int(s1_setup_eff), None, None)
        except Exception:
            pass
        # ÂèÇËÄÉ: System1 „ÅÆ SPY gate Áä∂ÊÖã„Çí UI „Å´Ë£úË∂≥Ë°®Á§∫
        try:
            cb_note = globals().get("_PER_SYSTEM_NOTE")
            if cb_note and callable(cb_note):
                try:
                    if _spy_ok is None:
                        cb_note("system1", "SPY>SMA100: -")
                    else:
                        cb_note(
                            "system1",
                            "SPY>SMA100: OK" if int(_spy_ok) == 1 else "SPY>SMA100: NG",
                        )
                except Exception:
                    pass
        except Exception:
            pass
    except Exception:
        pass
    _log("üßÆ ÊåáÊ®ôË®àÁÆóÁî®„Éá„Éº„Çø„É≠„Éº„Éâ‰∏≠ (system2)‚Ä¶")
    raw_data_system2 = _subset_data(basic_data, system2_syms)
    _log(f"üßÆ ÊåáÊ®ô„Éá„Éº„Çø: system2={len(raw_data_system2)}ÈäòÊüÑ")
    # System2 „Çª„ÉÉ„Éà„Ç¢„ÉÉ„ÉóÂÜÖË®≥: „Éï„Ç£„É´„ÇøÈÄöÈÅé, RSI3>90, TwoDayUp
    try:
        s2_filter = int(len(system2_syms))
        s2_rsi = 0
        s2_up2 = 0
        for _sym in system2_syms or []:
            _df = raw_data_system2.get(_sym)
            if _df is None or getattr(_df, "empty", True):
                continue
            try:
                last = _df.iloc[-1]
            except Exception:
                continue
            try:
                if float(last.get("RSI3", 0)) > 90:
                    s2_rsi += 1
            except Exception:
                pass
            try:
                if bool(last.get("TwoDayUp", False)):
                    s2_up2 += 1
            except Exception:
                pass
        _log(
            "üß© system2„Çª„ÉÉ„Éà„Ç¢„ÉÉ„ÉóÂÜÖË®≥: "
            + f"„Éï„Ç£„É´„ÇøÈÄöÈÅé={s2_filter}, RSI3>90: {s2_rsi}, "
            + f"TwoDayUp: {s2_up2}"
        )
        try:
            cb2 = globals().get("_PER_SYSTEM_STAGE")
            if cb2 and callable(cb2):
                cb2("system2", 50, int(s2_filter), int(max(s2_rsi, s2_up2)), None, None)
        except Exception:
            pass
    except Exception:
        pass
    _log("üßÆ ÊåáÊ®ôË®àÁÆóÁî®„Éá„Éº„Çø„É≠„Éº„Éâ‰∏≠ (system3)‚Ä¶")
    raw_data_system3 = _subset_data(basic_data, system3_syms)
    _log(f"üßÆ ÊåáÊ®ô„Éá„Éº„Çø: system3={len(raw_data_system3)}ÈäòÊüÑ")
    # System3 „Çª„ÉÉ„Éà„Ç¢„ÉÉ„ÉóÂÜÖË®≥: „Éï„Ç£„É´„ÇøÈÄöÈÅé, Close>SMA150, 3Êó•‰∏ãËêΩÁéá>=12.5%
    try:
        s3_filter = int(len(system3_syms))
        s3_close = 0
        s3_drop = 0
        for _sym in system3_syms or []:
            _df = raw_data_system3.get(_sym)
            if _df is None or getattr(_df, "empty", True):
                continue
            try:
                last = _df.iloc[-1]
            except Exception:
                continue
            try:
                if float(last.get("Close", 0)) > float(last.get("SMA150", float("inf"))):
                    s3_close += 1
            except Exception:
                pass
            try:
                if float(last.get("Drop3D", 0)) >= 0.125:
                    s3_drop += 1
            except Exception:
                pass
        _log(
            "üß© system3„Çª„ÉÉ„Éà„Ç¢„ÉÉ„ÉóÂÜÖË®≥: "
            + f"„Éï„Ç£„É´„ÇøÈÄöÈÅé={s3_filter}, Close>SMA150: {s3_close}, "
            + f"3Êó•‰∏ãËêΩÁéá>=12.5%: {s3_drop}"
        )
        try:
            cb2 = globals().get("_PER_SYSTEM_STAGE")
            if cb2 and callable(cb2):
                cb2("system3", 50, int(s3_filter), int(max(s3_close, s3_drop)), None, None)
        except Exception:
            pass
    except Exception:
        pass
    _log("üßÆ ÊåáÊ®ôË®àÁÆóÁî®„Éá„Éº„Çø„É≠„Éº„Éâ‰∏≠ (system4)‚Ä¶")
    raw_data_system4 = _subset_data(basic_data, system4_syms)
    _log(f"üßÆ ÊåáÊ®ô„Éá„Éº„Çø: system4={len(raw_data_system4)}ÈäòÊüÑ")
    # System4 „Çª„ÉÉ„Éà„Ç¢„ÉÉ„ÉóÂÜÖË®≥: „Éï„Ç£„É´„ÇøÈÄöÈÅé, Close>SMA200
    try:
        s4_filter = int(len(system4_syms))
        s4_close = 0
        for _sym in system4_syms or []:
            _df = raw_data_system4.get(_sym)
            if _df is None or getattr(_df, "empty", True):
                continue
            try:
                last = _df.iloc[-1]
            except Exception:
                continue
            try:
                if float(last.get("Close", 0)) > float(last.get("SMA200", float("inf"))):
                    s4_close += 1
            except Exception:
                pass
        _log(f"üß© system4„Çª„ÉÉ„Éà„Ç¢„ÉÉ„ÉóÂÜÖË®≥: „Éï„Ç£„É´„ÇøÈÄöÈÅé={s4_filter}, Close>SMA200: {s4_close}")
        try:
            cb2 = globals().get("_PER_SYSTEM_STAGE")
            if cb2 and callable(cb2):
                cb2("system4", 50, int(s4_filter), int(s4_close), None, None)
        except Exception:
            pass
    except Exception:
        pass
    _log("üßÆ ÊåáÊ®ôË®àÁÆóÁî®„Éá„Éº„Çø„É≠„Éº„Éâ‰∏≠ (system5)‚Ä¶")
    raw_data_system5 = _subset_data(basic_data, system5_syms)
    _log(f"üßÆ ÊåáÊ®ô„Éá„Éº„Çø: system5={len(raw_data_system5)}ÈäòÊüÑ")
    # System5 „Çª„ÉÉ„Éà„Ç¢„ÉÉ„ÉóÂÜÖË®≥: „Éï„Ç£„É´„ÇøÈÄöÈÅé, Close>SMA100+ATR10, ADX7>55, RSI3<50
    try:
        s5_filter = int(len(system5_syms))
        s5_close = 0
        s5_adx = 0
        s5_rsi = 0
        for _sym in system5_syms or []:
            _df = raw_data_system5.get(_sym)
            if _df is None or getattr(_df, "empty", True):
                continue
            try:
                last = _df.iloc[-1]
            except Exception:
                continue
            try:
                if float(last.get("Close", 0)) > float(last.get("SMA100", 0)) + float(
                    last.get("ATR10", 0)
                ):
                    s5_close += 1
            except Exception:
                pass
            try:
                if float(last.get("ADX7", 0)) > 55:
                    s5_adx += 1
            except Exception:
                pass
            try:
                if float(last.get("RSI3", 100)) < 50:
                    s5_rsi += 1
            except Exception:
                pass
        _log(
            "üß© system5„Çª„ÉÉ„Éà„Ç¢„ÉÉ„ÉóÂÜÖË®≥: "
            + f"„Éï„Ç£„É´„ÇøÈÄöÈÅé={s5_filter}, Close>SMA100+ATR10: {s5_close}, "
            + f"ADX7>55: {s5_adx}, RSI3<50: {s5_rsi}"
        )
        try:
            cb2 = globals().get("_PER_SYSTEM_STAGE")
            if cb2 and callable(cb2):
                cb2("system5", 50, int(s5_filter), int(s5_close), None, None)
        except Exception:
            pass
    except Exception:
        pass
    _log("üßÆ ÊåáÊ®ôË®àÁÆóÁî®„Éá„Éº„Çø„É≠„Éº„Éâ‰∏≠ (system6)‚Ä¶")
    raw_data_system6 = _subset_data(basic_data, system6_syms)
    _log(f"üßÆ ÊåáÊ®ô„Éá„Éº„Çø: system6={len(raw_data_system6)}ÈäòÊüÑ")
    # System6 „Çª„ÉÉ„Éà„Ç¢„ÉÉ„ÉóÂÜÖË®≥: „Éï„Ç£„É´„ÇøÈÄöÈÅé, Return6D>20%, UpTwoDays
    try:
        s6_filter = int(len(system6_syms))
        s6_ret = 0
        s6_up2 = 0
        for _sym in system6_syms or []:
            _df = raw_data_system6.get(_sym)
            if _df is None or getattr(_df, "empty", True):
                continue
            try:
                last = _df.iloc[-1]
            except Exception:
                continue
            try:
                if float(last.get("Return6D", 0)) > 0.20:
                    s6_ret += 1
            except Exception:
                pass
            try:
                if bool(last.get("UpTwoDays", False)):
                    s6_up2 += 1
            except Exception:
                pass
        _log(
            "üß© system6„Çª„ÉÉ„Éà„Ç¢„ÉÉ„ÉóÂÜÖË®≥: "
            + f"„Éï„Ç£„É´„ÇøÈÄöÈÅé={s6_filter}, Return6D>20%: {s6_ret}, "
            + f"UpTwoDays: {s6_up2}"
        )
        try:
            cb2 = globals().get("_PER_SYSTEM_STAGE")
            if cb2 and callable(cb2):
                cb2("system6", 50, int(s6_filter), int(max(s6_ret, s6_up2)), None, None)
        except Exception:
            pass
    except Exception:
        pass
    if progress_callback:
        try:
            progress_callback(4, 8, "load_indicators")
        except Exception:
            pass
    # ...raw_data_system...
    if "SPY" in basic_data:
        spy_df = get_spy_with_indicators(basic_data["SPY"])
    else:
        spy_df = None
        _log(
            "‚ö†Ô∏è SPY „Åå„Ç≠„É£„ÉÉ„Ç∑„É•„Å´Ë¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì (base/full_backup/rolling „ÇíÁ¢∫Ë™ç)„ÄÇ"
            "SPY.csv „Çí data_cache/base „ÇÇ„Åó„Åè„ÅØ data_cache/full_backup „Å´ÈÖçÁΩÆ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"
        )

    # „Çπ„Éà„É©„ÉÜ„Ç∏ÂàùÊúüÂåñ
    strategy_objs = [
        System1Strategy(),
        System2Strategy(),
        System3Strategy(),
        System4Strategy(),
        System5Strategy(),
        System6Strategy(),
        System7Strategy(),
    ]
    strategies = {getattr(s, "SYSTEM_NAME", "").lower(): s for s in strategy_objs}
    # „Ç®„É≥„Ç∏„É≥Â±§„ÅØUI‰æùÂ≠ò„ÇíÊéíÈô§ÔºàUIË°®Á§∫„ÅØlog/progress„Ç≥„Éº„É´„Éê„ÉÉ„ÇØÂÅ¥„Å´‰ªª„Åõ„ÇãÔºâ

    def _run_strategy(name: str, stg) -> tuple[str, pd.DataFrame, str, list[str]]:
        logs: list[str] = []

        def _local_log(message: str) -> None:
            logs.append(str(message))
            # UI „Ç≥„Éº„É´„Éê„ÉÉ„ÇØ„Åå„ÅÇ„Çå„Å∞„Éï„Ç£„É´„ÇøÊ∏à„Åø„ÅßÈÄÅ‰ø°„ÄÅÁÑ°„Åë„Çå„Å∞ CLI „Å´Âá∫Âäõ
            try:
                cb = globals().get("_LOG_CALLBACK")
            except Exception:
                cb = None
            if cb and callable(cb):
                _emit_ui_log(f"[{name}] {message}")
            else:
                try:
                    print(f"[{name}] {message}", flush=True)
                except Exception:
                    pass

        if name == "system1":
            base = raw_data_system1
        elif name == "system2":
            base = raw_data_system2
        elif name == "system3":
            base = raw_data_system3
        elif name == "system4":
            base = raw_data_system4
        elif name == "system5":
            base = raw_data_system5
        elif name == "system6":
            base = raw_data_system6
        elif name == "system7":
            base = {"SPY": basic_data.get("SPY")} if "basic_data" in locals() else {}
        else:
            base = basic_data if "basic_data" in locals() else {}
        if name == "system4" and spy_df is None:
            _local_log(
                "‚ö†Ô∏è System4 „ÅØ SPY ÊåáÊ®ô„ÅåÂøÖË¶Å„Åß„Åô„Åå "
                + "SPY „Éá„Éº„Çø„Åå„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ"
                + "„Çπ„Ç≠„ÉÉ„Éó„Åó„Åæ„Åô„ÄÇ"
            )
            return name, pd.DataFrame(), f"‚ùå {name}: 0 ‰ª∂ üö´", logs
        _local_log(f"üîé {name}: „Ç∑„Ç∞„Éä„É´ÊäΩÂá∫„ÇíÈñãÂßã")
        try:
            # ÊÆµÈöéÈÄ≤Êçó: 0/25/50/75/100 „Çí UI ÂÅ¥„Å´Ê©ãÊ∏°„Åó
            def _stage(
                v: int,
                f: int | None = None,
                s: int | None = None,
                c: int | None = None,
                fin: int | None = None,
            ) -> None:
                try:
                    cb2 = globals().get("_PER_SYSTEM_STAGE")
                except Exception:
                    cb2 = None
                if cb2 and callable(cb2):
                    try:
                        cb2(name, max(0, min(100, int(v))), f, s, c, fin)
                    except Exception:
                        pass
                # TRDlist‰ª∂Êï∞„Çπ„Éä„ÉÉ„Éó„Ç∑„Éß„ÉÉ„Éà„ÇíÊõ¥Êñ∞ÔºàÂæåÊÆµ„ÅÆ„É°„Ç§„É≥„Çπ„É¨„ÉÉ„ÉâÈÄöÁü•„Åß‰ΩøÁî®Ôºâ
                try:
                    if c is not None:
                        _CAND_COUNT_SNAPSHOT[name] = int(c)
                except Exception:
                    pass

            import os as _os

            # „Éó„É≠„Çª„Çπ„Éó„Éº„É´Âà©Áî®ÂèØÂê¶ÔºàÁí∞Â¢ÉÂ§âÊï∞„Åß‰∏äÊõ∏„ÅçÂèØÔºâ
            env_pp = _os.environ.get("USE_PROCESS_POOL", "").lower()
            if env_pp in ("0", "false", "no"):
                use_process_pool = False
            elif env_pp in ("1", "true", "yes"):
                use_process_pool = True
            else:
                prefer_pool = getattr(stg, "PREFER_PROCESS_POOL", False)
                use_process_pool = bool(prefer_pool)
                if use_process_pool:
                    _local_log("‚öôÔ∏è „Éó„É≠„Çª„Çπ„Éó„Éº„É´„ÇíÂÑ™ÂÖàË®≠ÂÆö„ÅßÊúâÂäπÂåñ")
            # „ÉØ„Éº„Ç´„ÉºÊï∞„ÅØÁí∞Â¢ÉÂ§âÊï∞„Åå„ÅÇ„Çå„Å∞ÂÑ™ÂÖà„ÄÅÁÑ°„Åë„Çå„Å∞Ë®≠ÂÆö(THREADS_DEFAULT)„Å´ÈÄ£Âãï
            try:
                _env_workers = _os.environ.get("PROCESS_POOL_WORKERS", "").strip()
                if _env_workers:
                    max_workers = int(_env_workers) or None
                else:
                    try:
                        _st = get_settings(create_dirs=False)
                        max_workers = int(getattr(_st, "THREADS_DEFAULT", 8)) or None
                    except Exception:
                        max_workers = None
            except Exception:
                max_workers = None
            # „É´„ÉÉ„ÇØ„Éê„ÉÉ„ÇØ„ÅØ„ÄéÂøÖË¶ÅÊåáÊ®ô„ÅÆÊúÄÂ§ßÁ™ìÔºãŒ±„Äè„ÇíÂãïÁöÑÊé®ÂÆö
            try:
                settings2 = get_settings(create_dirs=True)
                lb_default = int(
                    settings2.cache.rolling.base_lookback_days + settings2.cache.rolling.buffer_days
                )
            except Exception:
                settings2 = None
                lb_default = 300
            # YAML„ÅÆstrategies„Çª„ÇØ„Ç∑„Éß„É≥Á≠â„Åã„Çâ„Éí„É≥„Éà„ÇíÂèñÂæóÔºà„Å™„Åë„Çå„Å∞„Éí„É•„Éº„É™„Çπ„ÉÜ„Ç£„ÉÉ„ÇØÔºâ
            # „É´„ÉÉ„ÇØ„Éê„ÉÉ„ÇØ„ÅÆ„Éû„Éº„Ç∏„É≥/ÊúÄÂ∞èÊó•Êï∞„ÅØÁí∞Â¢ÉÂ§âÊï∞„Åß‰∏äÊõ∏„ÅçÂèØËÉΩ
            try:
                margin = float(_os.environ.get("LOOKBACK_MARGIN", "0.15"))
            except Exception:
                margin = 0.15
            need_map: dict[str, int] = {
                "system1": int(220 * (1 + margin)),
                "system2": int(120 * (1 + margin)),
                # SMA150 „ÇíÂÆâÂÆö„Å´Ë®àÁÆó„Åô„Çã„Åü„ÇÅ 170 Êó•Á®ãÂ∫¶„ÇíË¶ÅÊ±Ç
                "system3": int(170 * (1 + margin)),
                # SMA200 Á≥ª„ÅÆ„Åü„ÇÅ 220 Êó•Á®ãÂ∫¶„ÇíË¶ÅÊ±Ç
                "system4": int(220 * (1 + margin)),
                "system5": int(140 * (1 + margin)),
                "system6": int(80 * (1 + margin)),
                "system7": int(80 * (1 + margin)),
            }
            # Êà¶Áï•ÂÅ¥„Åå get_total_days „ÇíÂÆüË£Ö„Åó„Å¶„ÅÑ„Çå„Å∞ÂÑ™ÂÖà
            custom_need = None
            try:
                fn = getattr(stg, "get_total_days", None)
                if callable(fn):
                    _val = fn(base)
                    if isinstance(_val, int | float):
                        custom_need = int(_val)
                    elif isinstance(_val, str):
                        try:
                            custom_need = int(float(_val))
                        except Exception:
                            custom_need = None
                    else:
                        custom_need = None
            except Exception:
                custom_need = None
            try:
                min_floor = int(_os.environ.get("LOOKBACK_MIN_DAYS", "80"))
            except Exception:
                min_floor = 80
            min_required = custom_need or need_map.get(name, lb_default)
            lookback_days = min(lb_default, max(min_floor, int(min_required)))
            _t0 = __import__("time").time()
            # „Éó„É≠„Çª„Çπ„Éó„Éº„É´‰ΩøÁî®ÊôÇ„ÅØ stage_progress „ÇíÊ∏°„Åï„Å™„ÅÑÔºàpickle/__main__ÂïèÈ°å„ÇíÂõûÈÅøÔºâ
            _stage_cb = None if use_process_pool else _stage
            _log_cb = None if use_process_pool else _local_log
            df = stg.get_today_signals(
                base,
                market_df=spy_df,
                today=today,
                progress_callback=None,
                log_callback=_log_cb,
                stage_progress=_stage_cb,
                use_process_pool=use_process_pool,
                max_workers=max_workers,
                lookback_days=lookback_days,
            )
            _elapsed = int(max(0, __import__("time").time() - _t0))
            _m, _s = divmod(_elapsed, 60)
            _local_log(f"‚è±Ô∏è {name}: ÁµåÈÅé {_m}ÂàÜ{_s}Áßí")
        except Exception as e:  # noqa: BLE001
            _local_log(f"‚ö†Ô∏è {name}: „Ç∑„Ç∞„Éä„É´ÊäΩÂá∫„Å´Â§±Êïó„Åó„Åæ„Åó„Åü: {e}")
            # „Éó„É≠„Çª„Çπ„Éó„Éº„É´Áï∞Â∏∏ÊôÇ„ÅØ„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÔºàÈùû„Éó„Éº„É´Ôºâ„Åß‰∏ÄÂ∫¶„Å†„ÅëÂÜçË©¶Ë°å
            try:
                msg = str(e).lower()
            except Exception:
                msg = ""
            needs_fallback = any(
                k in msg
                for k in [
                    "process pool",
                    "a child process terminated",
                    "terminated abruptly",
                    "forkserver",
                    "__main__",
                ]
            )
            if needs_fallback:
                _local_log("üõü „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÂÜçË©¶Ë°å: „Éó„É≠„Çª„Çπ„Éó„Éº„É´ÁÑ°ÂäπÂåñ„ÅßÂÆüË°å„Åó„Åæ„Åô")
                try:
                    _t0b = __import__("time").time()
                    df = stg.get_today_signals(
                        base,
                        market_df=spy_df,
                        today=today,
                        progress_callback=None,
                        log_callback=_local_log,
                        stage_progress=None,
                        use_process_pool=False,
                        max_workers=None,
                        lookback_days=lookback_days,
                    )
                    _elapsed_b = int(max(0, __import__("time").time() - _t0b))
                    _m2, _s2 = divmod(_elapsed_b, 60)
                    _local_log(f"‚è±Ô∏è {name} (fallback): ÁµåÈÅé {_m2}ÂàÜ{_s2}Áßí")
                except Exception as e2:  # noqa: BLE001
                    _local_log(f"‚ùå {name}: „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ„ÇÇÂ§±Êïó: {e2}")
                    df = pd.DataFrame()
            else:
                df = pd.DataFrame()
        if not df.empty:
            if "score_key" in df.columns and len(df):
                first_key = df["score_key"].iloc[0]
            else:
                first_key = None
            asc = _asc_by_score_key(first_key)
            df = df.sort_values("score", ascending=asc, na_position="last")
            df = df.reset_index(drop=True)
        if df is not None and not df.empty:
            msg = f"üìä {name}: {len(df)} ‰ª∂"
        else:
            msg = f"‚ùå {name}: 0 ‰ª∂ üö´"
        _local_log(msg)
        return name, df, msg, logs

    # ÊäΩÂá∫ÈñãÂßãÂâç„Å´„Çª„ÉÉ„Éà„Ç¢„ÉÉ„ÉóÈÄöÈÅé„ÅÆ„Åæ„Å®„ÇÅ„ÇíÂá∫Âäõ
    try:
        setup_summary = []
        for name, val in (
            ("system1", locals().get("s1_setup")),
            (
                "system2",
                max(locals().get("s2_rsi", 0), locals().get("s2_up2", 0)),
            ),
            (
                "system3",
                max(locals().get("s3_close", 0), locals().get("s3_drop", 0)),
            ),
            ("system4", locals().get("s4_close")),
            ("system5", locals().get("s5_close")),
            (
                "system6",
                max(locals().get("s6_ret", 0), locals().get("s6_up2", 0)),
            ),
            ("system7", 1 if ("SPY" in (basic_data or {})) else 0),
        ):
            try:
                if val is not None:
                    setup_summary.append(f"{name}={int(val)}")
            except Exception:
                continue
        if setup_summary:
            _log("üß© „Çª„ÉÉ„Éà„Ç¢„ÉÉ„ÉóÈÄöÈÅé„Åæ„Å®„ÇÅ: " + ", ".join(setup_summary))
    except Exception:
        pass

    _log("üöÄ ÂêÑ„Ç∑„Çπ„ÉÜ„É†„ÅÆÂΩìÊó•„Ç∑„Ç∞„Éä„É´ÊäΩÂá∫„ÇíÈñãÂßã")
    per_system: dict[str, pd.DataFrame] = {}
    total = len(strategies)
    # ‰∫ãÂâç„Å´ÂÖ®„Ç∑„Çπ„ÉÜ„É†„Å∏„Çπ„ÉÜ„Éº„Ç∏0%ÔºàfilterÈñãÂßãÔºâ„ÇíÂêåÊôÇÈÄöÁü•ÔºàUIÂêåÊúüË°®Á§∫Áî®Ôºâ
    try:
        cb2 = globals().get("_PER_SYSTEM_STAGE")
    except Exception:
        cb2 = None
    if cb2 and callable(cb2):
        # 0% „Çπ„ÉÜ„Éº„Ç∏„ÅÆ„ÄåÂØæË±°‚Üí„Äç„ÅØ„É¶„Éã„Éê„Éº„ÇπÁ∑èÊï∞„Éô„Éº„ÇπÔºàSPY„ÅØÈô§Â§ñÔºâ
        try:
            universe_total = sum(1 for s in (symbols or []) if str(s).upper() != "SPY")
        except Exception:
            universe_total = len(symbols) if symbols is not None else 0
            try:
                has_spy = 1 if "SPY" in (symbols or []) else 0
                universe_total = max(0, int(universe_total) - has_spy)
            except Exception:
                pass
        for name in strategies.keys():
            try:
                cb2(name, 0, int(universe_total), None, None, None)
            except Exception:
                pass
    if parallel:
        if progress_callback:
            try:
                progress_callback(5, 8, "run_strategies")
            except Exception:
                pass
        with ThreadPoolExecutor() as executor:
            futures: dict[Future, str] = {}
            for name, stg in strategies.items():
                # system„Åî„Å®„ÅÆÈñãÂßã„ÇíÈÄöÁü•
                if per_system_progress:
                    try:
                        per_system_progress(name, "start")
                    except Exception:
                        pass
                # CLIÂ∞ÇÁî®: ÂêÑ„Ç∑„Çπ„ÉÜ„É†ÈñãÂßã„ÇíÂç≥ÊôÇË°®Á§∫ÔºàUI„Å´„ÅØÂá∫„Åï„Å™„ÅÑÔºâ
                try:
                    _log(f"‚ñ∂ {name} ÈñãÂßã", ui=False)
                except Exception:
                    pass
                fut = executor.submit(_run_strategy, name, stg)
                futures[fut] = name
            for _idx, fut in enumerate(as_completed(futures), start=1):
                name, df, msg, logs = fut.result()
                per_system[name] = df
                # Âç≥ÊôÇ: TRDlistÔºàÂÄôË£ú‰ª∂Êï∞Ôºâ„Çí75%ÊÆµÈöé„Å®„Åó„Å¶ÈÄöÁü•Ôºà‰∏äÈôê„ÅØmax_positionsÔºâ
                try:
                    cb2 = globals().get("_PER_SYSTEM_STAGE")
                except Exception:
                    cb2 = None
                if cb2 and callable(cb2):
                    try:
                        try:
                            _mx = int(get_settings(create_dirs=False).risk.max_positions)
                        except Exception:
                            _mx = 10
                        _cand_cnt = (
                            0 if (df is None or getattr(df, "empty", True)) else int(len(df))
                        )
                        if _mx > 0:
                            _cand_cnt = min(int(_cand_cnt), int(_mx))
                        cb2(name, 75, None, None, int(_cand_cnt), None)
                    except Exception:
                        pass
                # UI „ÅåÁÑ°„ÅÑÂ†¥Âêà„ÅØ CLI Âêë„Åë„Å´Á∞°Áï•„É≠„Ç∞„ÇíÈõÜÁ¥ÑÂá∫Âäõ„ÄÇUI „Åå„ÅÇ„ÇãÂ†¥Âêà„ÅØÂÆå‰∫ÜÂæå„Å´ÂÜçÈÄÅ„ÄÇ
                # ÔºàUI „Å´„ÅØ„ÉØ„Éº„Ç´„ÉºÂÆüË°å‰∏≠„Å´ÈÄêÊ¨°ÈÄÅ‰ø°Ê∏à„Åø„ÅÆ„Åü„ÇÅ„ÄÅ„Åì„Åì„Åß„ÅÆÂÜçÈÄÅ„ÅØË°å„Çè„Å™„ÅÑÔºâ
                # CLIÂ∞ÇÁî®: „ÉØ„Éº„Ç´„ÉºÂèéÈõÜ„É≠„Ç∞„ÇíÂ∏∏„Å´Âá∫ÂäõÔºàUI„Å´„ÅØÈÄÅ„Çâ„Å™„ÅÑÔºâ
                for line in _filter_logs(logs, ui=False):
                    _log(f"[{name}] {line}", ui=False)
                # UI „Ç≥„Éº„É´„Éê„ÉÉ„ÇØ„Åå„ÅÇ„ÇãÂ†¥Âêà„ÅØ‰Ωï„ÇÇ„Åó„Å™„ÅÑÔºàÈáçË§áÈò≤Ê≠¢Ôºâ
                # ÂÆå‰∫ÜÈÄöÁü•
                if per_system_progress:
                    try:
                        per_system_progress(name, "done")
                    except Exception:
                        pass
                # CLIÂ∞ÇÁî®: ÂÆå‰∫Ü„ÇíÁ∞°ÊΩîË°®Á§∫Ôºà‰ª∂Êï∞‰ªò„Åç„ÄÇÂ§±ÊïóÊôÇ„ÅØ‰ª∂Êï∞‰∏çÊòé„Åß„ÇÇÁ∂öË°åÔºâ
                try:
                    _cnt = 0 if (df is None or getattr(df, "empty", True)) else int(len(df))
                except Exception:
                    _cnt = -1
                try:
                    _log(f"‚úÖ {name} ÂÆå‰∫Ü: {('?' if _cnt < 0 else _cnt)}‰ª∂", ui=False)
                except Exception:
                    pass
                # ÂâçÂõûÁµêÊûú„ÅØÈñãÂßãÊôÇ„Å´„Åæ„Å®„ÇÅ„Å¶Âá∫Âäõ„Åô„Çã„Åü„ÇÅ„ÄÅ„Åì„Åì„Åß„ÅØÂá∫„Åï„Å™„ÅÑ
                if progress_callback:
                    try:
                        progress_callback(5 + min(_idx, 1), 8, name)
                    except Exception:
                        pass
        if progress_callback:
            try:
                progress_callback(6, 8, "strategies_done")
            except Exception:
                pass
    else:
        for _idx, (name, stg) in enumerate(strategies.items(), start=1):
            if progress_callback:
                try:
                    progress_callback(5, 8, name)
                except Exception:
                    pass
            # È†ÜÊ¨°ÂÆüË°åÊôÇ„ÇÇÈñãÂßã„ÇíÈÄöÁü•
            if per_system_progress:
                try:
                    per_system_progress(name, "start")
                except Exception:
                    pass
            # CLIÂ∞ÇÁî®: ÂêÑ„Ç∑„Çπ„ÉÜ„É†ÈñãÂßã„ÇíÂç≥ÊôÇË°®Á§∫ÔºàUI„Å´„ÅØÂá∫„Åï„Å™„ÅÑÔºâ
            try:
                _log(f"‚ñ∂ {name} ÈñãÂßã", ui=False)
            except Exception:
                pass
            name, df, msg, logs = _run_strategy(name, stg)
            per_system[name] = df
            # CLIÂ∞ÇÁî®: „ÉØ„Éº„Ç´„ÉºÂèéÈõÜ„É≠„Ç∞„ÇíÂ∏∏„Å´Âá∫ÂäõÔºàUI„Å´„ÅØÈÄÅ„Çâ„Å™„ÅÑÔºâ
            for line in _filter_logs(logs, ui=False):
                _log(f"[{name}] {line}", ui=False)
            # Âç≥ÊôÇ: TRDlistÔºàÂÄôË£ú‰ª∂Êï∞Ôºâ„Çí75%ÊÆµÈöé„Å®„Åó„Å¶ÈÄöÁü•Ôºà‰∏äÈôê„ÅØmax_positionsÔºâ
            try:
                cb2 = globals().get("_PER_SYSTEM_STAGE")
            except Exception:
                cb2 = None
            if cb2 and callable(cb2):
                try:
                    try:
                        _mx = int(get_settings(create_dirs=False).risk.max_positions)
                    except Exception:
                        _mx = 10
                    _cand_cnt = 0 if (df is None or getattr(df, "empty", True)) else int(len(df))
                    if _mx > 0:
                        _cand_cnt = min(int(_cand_cnt), int(_mx))
                    cb2(name, 75, None, None, int(_cand_cnt), None)
                except Exception:
                    pass
            if per_system_progress:
                try:
                    per_system_progress(name, "done")
                except Exception:
                    pass
            # CLIÂ∞ÇÁî®: ÂÆå‰∫Ü„ÇíÁ∞°ÊΩîË°®Á§∫Ôºà‰ª∂Êï∞‰ªò„ÅçÔºâ
            try:
                _cnt = 0 if (df is None or getattr(df, "empty", True)) else int(len(df))
            except Exception:
                _cnt = -1
            try:
                _log(f"‚úÖ {name} ÂÆå‰∫Ü: {('?' if _cnt < 0 else _cnt)}‰ª∂", ui=False)
            except Exception:
                pass
            # Âç≥ÊôÇ„ÅÆ75%ÂÜçÈÄöÁü•„ÅØË°å„Çè„Å™„ÅÑÔºà„É°„Ç§„É≥„Çπ„É¨„ÉÉ„ÉâÂÅ¥„Åß‰∏ÄÊã¨ÈÄöÁü•Ôºâ
            # ÂâçÂõûÁµêÊûú„ÅØÈñãÂßãÊôÇ„Å´„Åæ„Å®„ÇÅ„Å¶Âá∫Âäõ„Åô„Çã„Åü„ÇÅ„ÄÅ„Åì„Åì„Åß„ÅØÂá∫„Åï„Å™„ÅÑ
        if progress_callback:
            try:
                progress_callback(6, 8, "strategies_done")
            except Exception:
                pass

    # „Ç∑„Çπ„ÉÜ„É†Âà•„ÅÆÈ†ÜÂ∫è„ÇíÊòéÁ§∫Ôºà1..7Ôºâ„Å´Âõ∫ÂÆö
    order_1_7 = [f"system{i}" for i in range(1, 8)]
    per_system = {k: per_system.get(k, pd.DataFrame()) for k in order_1_7 if k in per_system}
    ctx.per_system_frames = dict(per_system)

    # ËøΩÂä†: Alpaca„ÅÆ„Ç∑„Éß„Éº„ÉàÂèØÂê¶„Åß system2/6 ÂÄôË£ú„Çí‰∫ãÂâç„Éï„Ç£„É´„ÇøÔºàÂèñÂæóÂ§±ÊïóÊôÇ„ÅØ„Çπ„Ç≠„ÉÉ„ÉóÔºâ
    try:
        # ÂØæË±°„Ç∑„Çπ„ÉÜ„É†„Å®ÂÄôË£úÈäòÊüÑ
        short_systems = ["system2", "system6"]
        symbols_to_check: list[str] = []
        for nm in short_systems:
            dfc = per_system.get(nm, pd.DataFrame())
            if dfc is not None and not getattr(dfc, "empty", True) and "symbol" in dfc.columns:
                symbols_to_check.extend([str(s).upper() for s in dfc["symbol"].tolist()])
        symbols_to_check = sorted(list({s for s in symbols_to_check if s and s != "SPY"}))
        if symbols_to_check:
            try:
                client_short = ba.get_client(paper=True)
                shortable_map = ba.get_shortable_map(client_short, symbols_to_check)
            except Exception:
                shortable_map = {}
            for nm in short_systems:
                dfc = per_system.get(nm, pd.DataFrame())
                if dfc is None or getattr(dfc, "empty", True) or "symbol" not in dfc.columns:
                    continue
                if not shortable_map:
                    # ÂèñÂæó„Åß„Åç„Å™„Åë„Çå„Å∞„Éï„Ç£„É´„Çø„Åõ„ÅöÁ∂ôÁ∂ö
                    continue
                mask = (
                    dfc["symbol"]
                    .astype(str)
                    .str.upper()
                    .map(lambda s: bool(shortable_map.get(s, False)))
                )
                filtered = dfc[mask].reset_index(drop=True)
                dropped = int(len(dfc) - len(filtered))
                per_system[nm] = filtered
                if dropped > 0:
                    _log(
                        f"üö´ {nm}: „Ç∑„Éß„Éº„Éà‰∏çÂèØ„ÅßÈô§Â§ñ: {dropped} ‰ª∂ (‰æã: "
                        + ", ".join(dfc.loc[~mask, "symbol"].astype(str).head(5))
                        + (" „Åª„Åã" + str(dropped - 5) + "‰ª∂" if dropped > 5 else "")
                        + ")"
                    )
                    # ‰øùÂ≠ò: Èô§Â§ñÈäòÊüÑ„É™„Çπ„ÉàÔºà„Éá„Éê„ÉÉ„Ç∞/Áõ£ÊüªÁî®Ôºâ
                    try:
                        from config.settings import get_settings as _gs

                        _stg = _gs(create_dirs=True)
                        _dir = Path(getattr(_stg.outputs, "results_csv_dir", "results_csv"))
                    except Exception:
                        _dir = Path("results_csv")
                    try:
                        _dir.mkdir(parents=True, exist_ok=True)
                    except Exception:
                        pass
                    try:
                        _excluded = (
                            dfc.loc[~mask, ["symbol"]].astype(str).copy()
                            if ("symbol" in dfc.columns)
                            else pd.DataFrame(columns=["symbol"])
                        )
                        _excluded["reason"] = "not_shortable"
                        _fp = _dir / f"shortability_excluded_{nm}.csv"
                        _excluded.to_csv(_fp, index=False, encoding="utf-8")
                        _log(f"üìù {nm}: „Ç∑„Éß„Éº„Éà‰∏çÂèØ„ÅÆÈô§Â§ñÈäòÊüÑCSV„Çí‰øùÂ≠ò: {_fp}")
                    except Exception:
                        pass
    except Exception:
        pass

    metrics_summary_context = None

    # ‰∏¶ÂàóÂÆüË°åÊôÇ„ÅØ„ÉØ„Éº„Ç´„Éº„Çπ„É¨„ÉÉ„Éâ„Åã„Çâ„ÅÆ UI Êõ¥Êñ∞„ÅåÊäëÂà∂„Åï„Çå„Çã„Åü„ÇÅ„ÄÅ
    # „É°„Ç§„É≥„Çπ„É¨„ÉÉ„Éâ„ÅßÂÄôË£ú‰ª∂Êï∞ÔºàTRDlistÔºâ„Çí75%ÊÆµÈöé„Å®„Åó„Å¶ÈÄöÁü•„Åô„Çã
    try:
        cb2 = globals().get("_PER_SYSTEM_STAGE")
    except Exception:
        cb2 = None
    if cb2 and callable(cb2):
        try:
            # UI„ÅÆTRDlistË°®Á§∫„ÅØÊúÄÂ§ß„Éù„Ç∏„Ç∑„Éß„É≥Êï∞„ÇíË∂Ö„Åà„Å™„ÅÑ„Çà„ÅÜ‰∏∏„ÇÅ„Çã
            try:
                _mx = int(get_settings(create_dirs=False).risk.max_positions)
            except Exception:
                _mx = 10
            for _name in order_1_7:
                # „ÉØ„Éº„Ç´„Éº„Åã„Çâ„ÅÆ„Çπ„Éä„ÉÉ„Éó„Ç∑„Éß„ÉÉ„Éà„Åå„ÅÇ„Çå„Å∞ÂÑ™ÂÖàÔºàÂûã„ÇÜ„Çâ„ÅéÁ≠â„ÇíË∂Ö„Åà„Å¶‰ø°È†º„Åß„Åç„ÇãÂÄ§Ôºâ
                _cand_cnt = None
                try:
                    _cand_cnt = int(_CAND_COUNT_SNAPSHOT.get(_name))
                except Exception:
                    _cand_cnt = None
                if _cand_cnt is None:
                    _df_sys = per_system.get(_name, pd.DataFrame())
                    _cand_cnt = int(
                        0 if _df_sys is None or getattr(_df_sys, "empty", True) else len(_df_sys)
                    )
                if _mx > 0:
                    _cand_cnt = min(int(_cand_cnt), int(_mx))
                cb2(_name, 75, None, None, int(_cand_cnt), None)
        except Exception:
            pass

    # „É°„Éà„É™„ÇØ„Çπ‰øùÂ≠òÂâç„Å´„ÄÅÂΩìÊó•„ÅÆ„Éà„É¨„Éº„ÉâÂÄôË£úTop10„ÇíÁ∞°ÊòìÂá∫ÂäõÔºà„Éá„Éê„ÉÉ„Ç∞/ÂèØË¶ñÂåñÁî®Ôºâ
    try:
        # ËøΩÂä†: ÂÄôË£úÊó•„Ç≠„Éº„ÅÆË®∫Êñ≠Ôºàtoday/prevÊó•Ê≠£Ë¶èÂåñ„ÅÆÁ¢∫Ë™çÔºâ
        try:
            from common.today_signals import get_latest_nyse_trading_day as _gln  # type: ignore
        except Exception:
            _gln = None
        all_rows: list[pd.DataFrame] = []
        for _sys_name, df in per_system.items():
            if df is None or df.empty:
                continue
            x = df.copy()
            if "score" in x.columns:
                try:
                    asc = False
                    if "score_key" in x.columns and len(x):
                        asc = _asc_by_score_key(str(x.iloc[0].get("score_key")))
                    x["_sort_val"] = x["score"].astype(float)
                    if not asc:
                        x["_sort_val"] = -x["_sort_val"]
                except Exception:
                    x["_sort_val"] = 0.0
            else:
                x["_sort_val"] = 0.0
            all_rows.append(x)
        if all_rows:
            merged = pd.concat(all_rows, ignore_index=True)
            merged = merged.sort_values("_sort_val", kind="stable", na_position="last")
            top10 = merged.head(10).drop(columns=["_sort_val"], errors="ignore")
            _log("üìù ‰∫ãÂâç„Éà„É¨„Éº„Éâ„É™„Çπ„Éà(Top10, „É°„Éà„É™„ÇØ„Çπ‰øùÂ≠òÂâç)")
            cols = [
                c
                for c in [
                    "symbol",
                    "system",
                    "side",
                    "entry_date",
                    "entry_price",
                    "stop_price",
                    "score_key",
                    "score",
                ]
                if c in top10.columns
            ]
            if not top10.empty:
                _log(top10[cols].to_string(index=False))
            else:
                _log("(ÂÄôË£ú„Å™„Åó)")
        # ËøΩÂä†: „Ç∑„Çπ„ÉÜ„É†Âà•„ÅÆTop10„ÇíÂÄãÂà•„Å´Âá∫ÂäõÔºàsystem2„Äúsystem6Ôºâ
        try:
            for _sys_name in [f"system{i}" for i in range(2, 7)]:
                _df = per_system.get(_sys_name, pd.DataFrame())
                _log(f"üìù ‰∫ãÂâç„Éà„É¨„Éº„Éâ„É™„Çπ„Éà({_sys_name} Top10, „É°„Éà„É™„ÇØ„Çπ‰øùÂ≠òÂâç)")
                if _df is None or getattr(_df, "empty", True):
                    _log("(ÂÄôË£ú„Å™„Åó)")
                    continue
                x = _df.copy()
                if "score" in x.columns:
                    try:
                        asc = False
                        if "score_key" in x.columns and len(x):
                            asc = _asc_by_score_key(str(x.iloc[0].get("score_key")))
                        x["_sort_val"] = x["score"].astype(float)
                        if not asc:
                            x["_sort_val"] = -x["_sort_val"]
                    except Exception:
                        x["_sort_val"] = 0.0
                else:
                    x["_sort_val"] = 0.0
                x = x.sort_values("_sort_val", kind="stable", na_position="last")
                top10_s = x.head(10).drop(columns=["_sort_val"], errors="ignore")
                cols_s = [
                    c
                    for c in [
                        "symbol",
                        "system",
                        "side",
                        "entry_date",
                        "entry_price",
                        "stop_price",
                        "score_key",
                        "score",
                    ]
                    if c in top10_s.columns
                ]
                if not top10_s.empty:
                    _log(top10_s[cols_s].to_string(index=False))
                else:
                    _log("(ÂÄôË£ú„Å™„Åó)")
            # ËøΩÂä†: ÂêÑsystem„Åß entry_date „ÅÆ„É¶„Éã„Éº„ÇØÊó•‰ªò„ÇíÂá∫ÂäõÔºàÊúÄÂ§ß3‰ª∂Ôºâ
            try:
                if "entry_date" in _df.columns and not _df.empty:
                    uniq = sorted(
                        {
                            pd.to_datetime(v).date()
                            for v in _df["entry_date"].tolist()
                            if v is not None
                        }
                    )
                    sample_dates = ", ".join([str(d) for d in uniq[:3]])
                    _log(
                        f"üóìÔ∏è {_sys_name} entryÊó•„É¶„Éã„Éº„ÇØ: {sample_dates}"
                        + (" ..." if len(uniq) > 3 else "")
                    )
            except Exception:
                pass
        except Exception:
            pass
    except Exception:
        pass

    positions_cache: list[Any] | None = None
    symbol_system_map_cache: dict[str, str] | None = None

    # --- Êó•Ê¨°„É°„Éà„É™„ÇØ„ÇπÔºà‰∫ãÂâç„Éï„Ç£„É´„ÇøÈÄöÈÅéÊï∞„ÉªÂÄôË£úÊï∞Ôºâ„ÅÆ‰øùÂ≠ò ---
    try:
        metrics_rows = []
        # ‰∫ãÂâç„Éï„Ç£„É´„ÇøÈÄöÈÅéÊï∞ÔºàÂ≠òÂú®„Åó„Å™„ÅÑ„Ç∑„Çπ„ÉÜ„É†„ÅØ0Êâ±„ÅÑÔºâ
        prefilter_map = {
            "system1": len(locals().get("system1_syms", []) or []),
            "system2": len(locals().get("system2_syms", []) or []),
            "system3": len(locals().get("system3_syms", []) or []),
            "system4": len(locals().get("system4_syms", []) or []),
            "system5": len(locals().get("system5_syms", []) or []),
            "system6": len(locals().get("system6_syms", []) or []),
            "system7": 1 if ("SPY" in (locals().get("basic_data", {}) or {})) else 0,
        }
        # ÂÄôË£úÊï∞Ôºàper_system„ÅÆË°åÊï∞Ôºâ
        for sys_name in order_1_7:
            df_sys = per_system.get(sys_name, pd.DataFrame())
            candidates = int(0 if df_sys is None or getattr(df_sys, "empty", True) else len(df_sys))
            pre_count = int(prefilter_map.get(sys_name, 0))
            metrics_rows.append(
                {
                    "date": locals().get("today"),
                    "system": sys_name,
                    "prefilter_pass": pre_count,
                    "candidates": candidates,
                }
            )
        if metrics_rows:
            metrics_df = pd.DataFrame(metrics_rows)
            try:
                settings_out = get_settings(create_dirs=True)
                out_dir = Path(settings_out.outputs.results_csv_dir)
            except Exception:
                out_dir = Path("results_csv")
            try:
                out_dir.mkdir(parents=True, exist_ok=True)
            except Exception:
                pass
            out_fp = out_dir / "daily_metrics.csv"
            try:
                if out_fp.exists():
                    metrics_df.to_csv(out_fp, mode="a", header=False, index=False, encoding="utf-8")
                else:
                    metrics_df.to_csv(out_fp, index=False, encoding="utf-8")
                _log(f"üìà „É°„Éà„É™„ÇØ„Çπ‰øùÂ≠ò: {out_fp} „Å´ {len(metrics_rows)} Ë°å„ÇíËøΩË®ò")
            except Exception as e:
                _log(f"‚ö†Ô∏è „É°„Éà„É™„ÇØ„Çπ‰øùÂ≠ò„Å´Â§±Êïó: {e}")
            # ÈÄöÁü•: ÊúÄÁµÇ„Çπ„ÉÜ„Éº„Ç∏ÂΩ¢ÂºèÔºàTgt/FILpass/STUpass/TRDlist/Entry/ExitÔºâ„ÅßÈÄÅ‰ø°
            try:
                # 0%„ÅÆTgt„ÅØ„É¶„Éã„Éê„Éº„ÇπÁ∑èÊï∞ÔºàSPYÈô§„ÅèÔºâ
                try:
                    tgt_base = sum(1 for s in (symbols or []) if str(s).upper() != "SPY")
                except Exception:
                    tgt_base = len(symbols) if symbols is not None else 0
                    try:
                        if "SPY" in (symbols or []):
                            tgt_base = max(0, int(tgt_base) - 1)
                    except Exception:
                        pass

                # Exit ‰ª∂Êï∞„ÇíÁ∞°ÊòìÊé®ÂÆöÔºàAlpaca „ÅÆ‰øùÊúâ„Éù„Ç∏„Ç∑„Éß„É≥„Å®ÂêÑ Strategy „ÅÆ compute_exit „ÇíÂà©Áî®Ôºâ
                if positions_cache is None or symbol_system_map_cache is None:
                    positions_cache, symbol_system_map_cache = _fetch_positions_and_symbol_map()

                def _estimate_exit_counts_today(
                    positions0: Sequence[object],
                    symbol_system_map0: Mapping[str, str],
                ) -> dict[str, int]:
                    counts: dict[str, int] = {}
                    try:
                        # ‰æ°Ê†º„É≠„Éº„ÉâÈñ¢Êï∞„ÅØÂÖ±ÈÄö„É≠„Éº„ÉÄ„Éº„ÇíÂà©Áî®
                        from common.data_loader import load_price as _load_price  # lazy import

                        # SPY „Åã„ÇâÊú¨Êó•„ÅÆÂü∫Ê∫ñÊó•ÔºàÊúÄÊñ∞Âñ∂Ê•≠Êó•Ôºâ„ÇíÊé®ÂÆö
                        latest_trading_day = None
                        try:
                            spy_df0 = _load_price("SPY", cache_profile="rolling")
                            if spy_df0 is not None and not spy_df0.empty:
                                latest_trading_day = pd.to_datetime(spy_df0.index[-1]).normalize()
                        except Exception:
                            latest_trading_day = None

                        # „Ç®„É≥„Éà„É™„ÉºÊó•„ÅÆ„É≠„Éº„Ç´„É´Ë®òÈå≤„Å® system Êé®ÂÆö„Éû„ÉÉ„Éó
                        entry_map0 = load_entry_dates()
                        symbol_map_local = symbol_system_map0 or {}

                        for pos in positions0 or []:
                            try:
                                sym = str(getattr(pos, "symbol", "")).upper()
                                if not sym:
                                    continue
                                qty = int(abs(float(getattr(pos, "qty", 0)) or 0))
                                if qty <= 0:
                                    continue
                                pos_side = str(getattr(pos, "side", "")).lower()
                                mapped = symbol_map_local.get(sym)
                                if mapped is None and sym.lower() in symbol_map_local:
                                    mapped = symbol_map_local.get(sym.lower())
                                system0 = str(mapped or "").lower()
                                if not system0:
                                    if sym == "SPY" and pos_side == "short":
                                        system0 = "system7"
                                    else:
                                        continue
                                if system0 == "system7":
                                    continue
                                entry_date_str0 = entry_map0.get(sym)
                                if not entry_date_str0:
                                    continue
                                # ‰æ°Ê†º„Éá„Éº„ÇøË™≠ËæºÔºàfullÔºâ
                                dfp = _load_price(sym, cache_profile="full")
                                if dfp is None or dfp.empty:
                                    continue
                                try:
                                    dfp2 = dfp.copy(deep=False)
                                    if "Date" in dfp2.columns:
                                        dfp2.index = pd.Index(
                                            pd.to_datetime(dfp2["Date"]).dt.normalize()
                                        )
                                    else:
                                        dfp2.index = pd.Index(
                                            pd.to_datetime(dfp2.index).normalize()
                                        )
                                except Exception:
                                    continue
                                if latest_trading_day is None and len(dfp2.index) > 0:
                                    latest_trading_day = pd.to_datetime(dfp2.index[-1]).normalize()
                                # „Ç®„É≥„Éà„É™„ÉºÊó•„ÅÆ„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ
                                try:
                                    idx = dfp2.index
                                    ent_dt = pd.to_datetime(entry_date_str0).normalize()
                                    if ent_dt in idx:
                                        ent_arr = idx.get_indexer([ent_dt])
                                    else:
                                        ent_arr = idx.get_indexer([ent_dt], method="bfill")
                                    entry_idx0 = (
                                        int(ent_arr[0]) if len(ent_arr) and ent_arr[0] >= 0 else -1
                                    )
                                    if entry_idx0 < 0:
                                        continue
                                except Exception:
                                    continue

                                # StrategyÊØé„ÅÆ entry/stop „ÇíËøë‰ººÔºàUI„Å®ÂêåÁ≠â„ÅÆÁ∞°ÊòìÁâàÔºâ
                                entry_price0 = None
                                stop_price0 = None
                                try:
                                    prev_close0 = float(
                                        dfp2.iloc[int(max(0, entry_idx0 - 1))]["Close"]
                                    )
                                    if system0 == "system1":
                                        stg0 = System1Strategy()
                                        entry_price0 = float(dfp2.iloc[int(entry_idx0)]["Open"])
                                        atr20 = float(
                                            dfp2.iloc[int(max(0, entry_idx0 - 1))]["ATR20"]
                                        )
                                        stop_mult0 = float(
                                            stg0.config.get("stop_atr_multiple", 5.0)
                                        )
                                        stop_price0 = entry_price0 - stop_mult0 * atr20
                                    elif system0 == "system2":
                                        stg0 = System2Strategy()
                                        entry_price0 = float(dfp2.iloc[int(entry_idx0)]["Open"])
                                        atr = float(dfp2.iloc[int(max(0, entry_idx0 - 1))]["ATR10"])
                                        stop_mult0 = float(
                                            stg0.config.get("stop_atr_multiple", 3.0)
                                        )
                                        stop_price0 = entry_price0 + stop_mult0 * atr
                                    elif system0 == "system6":
                                        stg0 = System6Strategy()
                                        ratio0 = float(
                                            stg0.config.get("entry_price_ratio_vs_prev_close", 1.05)
                                        )
                                        entry_price0 = round(prev_close0 * ratio0, 2)
                                        atr = float(dfp2.iloc[int(max(0, entry_idx0 - 1))]["ATR10"])
                                        stop_mult0 = float(
                                            stg0.config.get("stop_atr_multiple", 3.0)
                                        )
                                        stop_price0 = entry_price0 + stop_mult0 * atr
                                    elif system0 == "system3":
                                        stg0 = System3Strategy()
                                        ratio0 = float(
                                            stg0.config.get("entry_price_ratio_vs_prev_close", 0.93)
                                        )
                                        entry_price0 = round(prev_close0 * ratio0, 2)
                                        atr = float(dfp2.iloc[int(max(0, entry_idx0 - 1))]["ATR10"])
                                        stop_mult0 = float(
                                            stg0.config.get("stop_atr_multiple", 2.5)
                                        )
                                        stop_price0 = entry_price0 - stop_mult0 * atr
                                    elif system0 == "system4":
                                        stg0 = System4Strategy()
                                        entry_price0 = float(dfp2.iloc[int(entry_idx0)]["Open"])
                                        atr40 = float(
                                            dfp2.iloc[int(max(0, entry_idx0 - 1))]["ATR40"]
                                        )
                                        stop_mult0 = float(
                                            stg0.config.get("stop_atr_multiple", 1.5)
                                        )
                                        stop_price0 = entry_price0 - stop_mult0 * atr40
                                    elif system0 == "system5":
                                        stg0 = System5Strategy()
                                        ratio0 = float(
                                            stg0.config.get("entry_price_ratio_vs_prev_close", 0.97)
                                        )
                                        entry_price0 = round(prev_close0 * ratio0, 2)
                                        atr = float(dfp2.iloc[int(max(0, entry_idx0 - 1))]["ATR10"])
                                        stop_mult0 = float(
                                            stg0.config.get("stop_atr_multiple", 3.0)
                                        )
                                        stop_price0 = entry_price0 - stop_mult0 * atr
                                        try:
                                            stg0._last_entry_atr = atr  # type: ignore[attr-defined]
                                        except Exception:
                                            pass
                                    else:
                                        continue
                                except Exception:
                                    continue
                                if entry_price0 is None or stop_price0 is None:
                                    continue
                                try:
                                    exit_price0, exit_date0 = stg0.compute_exit(
                                        dfp2,
                                        int(entry_idx0),
                                        float(entry_price0),
                                        float(stop_price0),
                                    )
                                except Exception:
                                    continue
                                today_norm0 = pd.to_datetime(dfp2.index[-1]).normalize()
                                if latest_trading_day is not None:
                                    today_norm0 = latest_trading_day
                                is_today_exit0 = (
                                    pd.to_datetime(exit_date0).normalize() == today_norm0
                                )
                                if is_today_exit0:
                                    if system0 == "system5":
                                        # System5 „ÅØÁøåÊó•ÂØÑ„ÇäÊ±∫Ê∏à„ÅÆ„Åü„ÇÅ„Ç´„Ç¶„É≥„ÉàÂØæË±°Â§ñ
                                        pass
                                    else:
                                        counts[system0] = counts.get(system0, 0) + 1
                            except Exception:
                                continue
                    except Exception:
                        return {}
                    return counts

                exit_counts_map = _estimate_exit_counts_today(
                    positions_cache or [], symbol_system_map_cache or {}
                ) or {}
                # UI „Å∏„ÇÇ Exit ‰ª∂Êï∞„ÇíÈÄÅ„ÇãÔºàÊó©Êúü„Å´ÂèØË¶ñÂåñÔºâ
                try:
                    cb_exit = globals().get("_PER_SYSTEM_EXIT")
                except Exception:
                    cb_exit = None
                if cb_exit and callable(cb_exit):
                    try:
                        for _nm, _cnt in (exit_counts_map or {}).items():
                            try:
                                cb_exit(_nm, int(_cnt))
                            except Exception:
                                pass
                    except Exception:
                        pass
                # Êó¢„Å´ÈõÜË®àÊ∏à„Åø„ÅÆÂÄ§„ÇíÂÜçÊßãÊàê
                setup_map = {
                    # System1 „ÅØ SPY „Ç≤„Éº„ÉàÔºàClose>SMA100Ôºâ„ÅåÂÅΩ„Å™„Çâ 0 Êâ±„ÅÑ
                    "system1": int(
                        (
                            locals().get("s1_setup")
                            if (
                                (locals().get("_spy_ok") is None)
                                or (int(locals().get("_spy_ok", 0)) == 1)
                            )
                            else 0
                        )
                        or 0
                    ),
                    "system2": int(max(locals().get("s2_rsi", 0), locals().get("s2_up2", 0))),
                    "system3": int(max(locals().get("s3_close", 0), locals().get("s3_drop", 0))),
                    "system4": int(locals().get("s4_close") or 0),
                    "system5": int(locals().get("s5_close") or 0),
                    "system6": int(max(locals().get("s6_ret", 0), locals().get("s6_up2", 0))),
                    "system7": 1 if ("SPY" in (locals().get("basic_data", {}) or {})) else 0,
                }
                metrics_summary_context = {
                    "prefilter_map": dict(prefilter_map),
                    "exit_counts_map": dict(exit_counts_map),
                    "setup_map": dict(setup_map),
                    "tgt_base": int(tgt_base),
                }
            except Exception:
                pass
        # Á∞°Êòì„É≠„Ç∞
        try:
            summary = ", ".join(
                [
                    (
                        f"{r['system']}: ÂØæË±°‚Üí{r['prefilter_pass']}, "
                        f"tradeÂÄôË£úÊï∞‚Üí{r['candidates']}"
                    )
                    for r in metrics_rows
                ]
            )
            if summary:
                _log(f"üìä „É°„Éà„É™„ÇØ„ÇπÊ¶ÇË¶Å: {summary}")
        except Exception:
            pass
    except Exception:
        _log("‚ö†Ô∏è „É°„Éà„É™„ÇØ„ÇπÈõÜË®à„Åß‰æãÂ§ñ„ÅåÁô∫Áîü„Åó„Åæ„Åó„ÅüÔºàÂá¶ÁêÜÁ∂öË°åÔºâ")

    if positions_cache is None or symbol_system_map_cache is None:
        positions_cache, symbol_system_map_cache = _fetch_positions_and_symbol_map()

    # 1) Êû†ÈÖçÂàÜÔºà„Çπ„É≠„ÉÉ„ÉàÔºâ„É¢„Éº„Éâ or 2) ÈáëÈ°çÈÖçÂàÜ„É¢„Éº„Éâ
    def _normalize_alloc(d: dict[str, float], default_map: dict[str, float]) -> dict[str, float]:
        try:
            filtered = {k: float(v) for k, v in d.items() if float(v) > 0}
            s = sum(filtered.values())
            if s <= 0:
                filtered = default_map
                s = sum(filtered.values())
            return {k: v / s for k, v in filtered.items()}
        except Exception:
            s = sum(default_map.values())
            return {k: v / s for k, v in default_map.items()}

    defaults_long = {"system1": 0.25, "system3": 0.25, "system4": 0.25, "system5": 0.25}
    defaults_short = {"system2": 0.40, "system6": 0.40, "system7": 0.20}
    try:
        settings_alloc_long = getattr(settings.ui, "long_allocations", {}) or {}
        settings_alloc_short = getattr(settings.ui, "short_allocations", {}) or {}
    except Exception:
        settings_alloc_long, settings_alloc_short = {}, {}
    long_alloc = _normalize_alloc(settings_alloc_long, defaults_long)
    short_alloc = _normalize_alloc(settings_alloc_short, defaults_short)

    active_positions_map = _load_active_positions_by_system(
        positions_cache, symbol_system_map_cache
    )
    max_positions_per_system: dict[str, int] = {}
    for name, stg in strategies.items():
        try:
            limit_val = int(
                getattr(stg, "config", {}).get("max_positions", settings.risk.max_positions)
            )
        except Exception:
            limit_val = int(settings.risk.max_positions)
        max_positions_per_system[name] = max(0, limit_val)
    available_slots_map: dict[str, int] = {}
    for name, limit_val in max_positions_per_system.items():
        taken = int(active_positions_map.get(name, 0))
        available_slots_map[name] = max(0, int(limit_val) - taken)

    try:
        if active_positions_map:
            summary = ", ".join(
                f"{k}={int(v)}" for k, v in sorted(active_positions_map.items()) if int(v) > 0
            )
            if summary:
                _log("üì¶ ÁèæÂú®‰øùÊúâ„Éù„Ç∏„Ç∑„Éß„É≥Êï∞: " + summary)
    except Exception:
        pass
    try:
        lines = []
        for name in sorted(max_positions_per_system.keys()):
            limit_val = int(max_positions_per_system.get(name, 0))
            remain = int(available_slots_map.get(name, limit_val))
            if remain < limit_val:
                lines.append(f"{name}={remain}/{limit_val}")
        if lines:
            _log("ü™ß Âà©Áî®ÂèØËÉΩ„Çπ„É≠„ÉÉ„Éà (ÊÆã/‰∏äÈôê): " + ", ".join(lines))
    except Exception:
        pass

    _log("üß∑ ÂÄôË£ú„ÅÆÈÖçÂàÜÔºà„Çπ„É≠„ÉÉ„ÉàÊñπÂºè or ÈáëÈ°çÈÖçÂàÜÔºâ„ÇíÂÆüË°å")
    if capital_long is None and capital_short is None:
        # Êóß„Çπ„É≠„ÉÉ„ÉàÊñπÂºèÔºàÂæåÊñπ‰∫íÊèõÔºâ
        max_pos = int(settings.risk.max_positions)
        slots_long = slots_long if slots_long is not None else max_pos
        slots_short = slots_short if slots_short is not None else max_pos

        def _distribute_slots(
            weights: dict[str, float], total_slots: int, counts: dict[str, int]
        ) -> dict[str, int]:
            base = {k: int(total_slots * weights.get(k, 0.0)) for k in weights}
            for k in list(base.keys()):
                if counts.get(k, 0) <= 0:
                    base[k] = 0
                elif base[k] == 0:
                    base[k] = 1
            used = sum(base.values())
            remain = max(0, total_slots - used)
            if remain > 0:
                order = sorted(
                    weights.keys(),
                    key=lambda k: (counts.get(k, 0), weights.get(k, 0.0)),
                    reverse=True,
                )
                idx = 0
                while remain > 0 and order:
                    k = order[idx % len(order)]
                    if counts.get(k, 0) > base.get(k, 0):
                        base[k] += 1
                        remain -= 1
                    idx += 1
                    if idx > 10000:
                        break
            for k in list(base.keys()):
                base[k] = min(base[k], counts.get(k, 0))
            return base

        long_counts_raw: dict[str, int] = {}
        long_counts_available: dict[str, int] = {}
        for k in long_alloc:
            df = per_system.get(k, pd.DataFrame())
            cand_cnt = 0 if df is None or getattr(df, "empty", True) else int(len(df))
            long_counts_raw[k] = cand_cnt
            long_counts_available[k] = min(cand_cnt, int(available_slots_map.get(k, 0)))

        short_counts_raw: dict[str, int] = {}
        short_counts_available: dict[str, int] = {}
        for k in short_alloc:
            df = per_system.get(k, pd.DataFrame())
            cand_cnt = 0 if df is None or getattr(df, "empty", True) else int(len(df))
            short_counts_raw[k] = cand_cnt
            short_counts_available[k] = min(cand_cnt, int(available_slots_map.get(k, 0)))

        def _fmt_alloc(name: str, avail_map: dict[str, int], cand_map: dict[str, int]) -> str:
            avail = int(avail_map.get(name, 0))
            cand = int(cand_map.get(name, 0))
            return f"{name}={avail}" if avail == cand else f"{name}={avail}/{cand}"

        _log(
            "üßÆ Êû†ÈÖçÂàÜÔºàÂà©Áî®ÂèØËÉΩ„Çπ„É≠„ÉÉ„Éà/ÂÄôË£úÊï∞Ôºâ: "
            + ", ".join([_fmt_alloc(k, long_counts_available, long_counts_raw) for k in long_alloc])
            + " | "
            + ", ".join(
                [_fmt_alloc(k, short_counts_available, short_counts_raw) for k in short_alloc]
            )
        )
        long_slots = _distribute_slots(long_alloc, slots_long, long_counts_available)
        short_slots = _distribute_slots(short_alloc, slots_short, short_counts_available)

        chosen_frames: list[pd.DataFrame] = []
        for name, slot in {**long_slots, **short_slots}.items():
            df = per_system.get(name, pd.DataFrame())
            if df is None or df.empty:
                continue
            free_slots = int(available_slots_map.get(name, 0))
            use_slot = min(int(slot), free_slots)
            if use_slot <= 0:
                continue
            take = df.head(use_slot).copy()
            take["alloc_weight"] = (
                long_alloc.get(name) or short_alloc.get(name) or 0.0
            )  # noqa: E501
            chosen_frames.append(take)
        final_df = (
            pd.concat(chosen_frames, ignore_index=True)
            if chosen_frames
            else pd.DataFrame()  # noqa: E501
        )
    else:
        # ÈáëÈ°çÈÖçÂàÜ„É¢„Éº„Éâ
        _settings = get_settings(create_dirs=False)
        _default_cap = float(getattr(_settings.ui, "default_capital", 100000))
        _ratio = float(getattr(_settings.ui, "default_long_ratio", 0.5))

        _cl = None if capital_long is None or float(capital_long) <= 0 else float(capital_long)
        _cs = None if capital_short is None or float(capital_short) <= 0 else float(capital_short)

        if _cl is None and _cs is None:
            total = _default_cap
            capital_long = total * _ratio
            capital_short = total * (1.0 - _ratio)
        elif _cl is None and _cs is not None:
            total = _cs
            capital_long = total * _ratio
            capital_short = total * (1.0 - _ratio)
        elif _cs is None and _cl is not None:
            total = _cl
            capital_long = total * _ratio
            capital_short = total * (1.0 - _ratio)
        else:
            # mypy/pyrightÂØæÂøúÔºà„Åì„ÅÆÂàÜÂ≤ê„Åß„ÅØ None „Å´„Å™„Çâ„Å™„ÅÑÔºâ
            from typing import cast as _cast

            capital_long = float(_cast(float, capital_long))
            capital_short = float(_cast(float, capital_short))

        strategies_map = {k: v for k, v in strategies.items()}
        _log(f"üí∞ ÈáëÈ°çÈÖçÂàÜ: long=${capital_long}, short=${capital_short}")
        # ÂèÇËÄÉ: „Ç∑„Çπ„ÉÜ„É†Âà•„ÅÆ‰∫àÁÆóÂÜÖË®≥„ÇíÂá∫Âäõ
        try:
            long_budgets = {
                k: float(capital_long) * float(long_alloc.get(k, 0.0)) for k in long_alloc
            }
            short_budgets = {
                k: float(capital_short) * float(short_alloc.get(k, 0.0)) for k in short_alloc
            }
            _log(
                "üìä long‰∫àÁÆóÂÜÖË®≥: " + ", ".join([f"{k}=${v:,.0f}" for k, v in long_budgets.items()])
            )
            _log(
                "üìä short‰∫àÁÆóÂÜÖË®≥: "
                + ", ".join([f"{k}=${v:,.0f}" for k, v in short_budgets.items()])
            )
        except Exception:
            pass
        long_df = _amount_pick(
            {k: per_system.get(k, pd.DataFrame()) for k in long_alloc},
            strategies_map,
            float(capital_long),
            long_alloc,
            side="long",
            active_positions=active_positions_map,
        )
        short_df = _amount_pick(
            {k: per_system.get(k, pd.DataFrame()) for k in short_alloc},
            strategies_map,
            float(capital_short),
            short_alloc,
            side="short",
            active_positions=active_positions_map,
        )
        parts = [df for df in [long_df, short_df] if df is not None and not df.empty]  # noqa: E501
        final_df = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame()  # noqa: E501

        # ÂêÑ„Ç∑„Çπ„ÉÜ„É†„ÅÆÊúÄÂ§ß„Éù„Ç∏„Ç∑„Éß„É≥‰∏äÈôê=10 „ÇíÂé≥Ê†ºÂåñ
        if not final_df.empty and "system" in final_df.columns:
            final_df = (
                final_df.sort_values(["system", "score"], ascending=[True, True])
                .groupby("system", as_index=False, group_keys=False)
                .head(int(get_settings(create_dirs=False).risk.max_positions))
                .reset_index(drop=True)
            )

    if not final_df.empty:
        # ‰∏¶„Å≥„ÅØ side ‚Üí systemÁï™Âè∑ ‚Üí ÂêÑsystem„ÅÆ„Çπ„Ç≥„Ç¢ÊñπÂêëÔºàRSIÁ≥ª„ÅÆ„ÅøÊòáÈ†Ü„ÄÅ„Åù„Çå‰ª•Â§ñ„ÅØÈôçÈ†ÜÔºâ
        tmp = final_df.copy()
        if "system" in tmp.columns:
            try:
                tmp["_system_no"] = (
                    tmp["system"].astype(str).str.extract(r"(\d+)").fillna(0).astype(int)
                )
            except Exception:
                tmp["_system_no"] = 0
        # ‰∏ÄÊó¶ side, system Áï™Âè∑„ÅßÂÆâÂÆö„ÇΩ„Éº„Éà
        tmp = tmp.sort_values(
            [c for c in ["side", "_system_no"] if c in tmp.columns], kind="stable"
        )
        # system „Åî„Å®„Å´ score „ÇíÊñπÂêëÊåáÂÆö„Åß‰∏¶„ÅπÊõø„Åà
        try:
            parts2: list[pd.DataFrame] = []
            for sys_name, g in tmp.groupby("system", sort=False):
                if "score" in g.columns:
                    asc = False
                    try:
                        # system4ÔºàRSIÁ≥ªÔºâ„ÅØ„Çπ„Ç≥„Ç¢Â∞è„Åï„ÅÑ„Åª„Å©ËâØ„ÅÑ
                        if isinstance(sys_name, str) and sys_name.lower() == "system4":
                            asc = True
                    except Exception:
                        asc = False
                    g = g.sort_values("score", ascending=asc, na_position="last", kind="stable")
                parts2.append(g)
            tmp = pd.concat(parts2, ignore_index=True)
        except Exception:
            pass
        tmp = tmp.drop(columns=["_system_no"], errors="ignore")
        final_df = tmp.reset_index(drop=True)
        # ÂÖàÈ†≠„Å´ÈÄ£Áï™Ôºà1Âßã„Åæ„ÇäÔºâ„Çí‰ªò‰∏é
        try:
            final_df.insert(0, "no", range(1, len(final_df) + 1))
        except Exception:
            pass
        # systemÂà•„ÅÆ‰ª∂Êï∞/ÈáëÈ°ç„Çµ„Éû„É™„ÇíÂá∫Âäõ
        try:
            if "position_value" in final_df.columns:
                grp = (
                    final_df.groupby("system")["position_value"].agg(["count", "sum"]).reset_index()
                )
                counts_map = {
                    str(r["system"]).strip().lower(): int(r["count"])
                    for _, r in grp.iterrows()
                    if str(r["system"]).strip()
                }
                values_map = {
                    str(r["system"]).strip().lower(): float(r["sum"])
                    for _, r in grp.iterrows()
                    if str(r["system"]).strip()
                }
                summary_lines = format_group_counts_and_values(counts_map, values_map)
                if summary_lines:
                    _log("üßæ Long/Short„Çµ„Éû„É™: " + ", ".join(summary_lines))
            else:
                grp = final_df.groupby("system").size().to_dict()
                counts_map = {
                    str(key).strip().lower(): int(value)
                    for key, value in grp.items()
                    if str(key).strip()
                }
                summary_lines = format_group_counts(counts_map)
                if summary_lines:
                    _log("üßæ Long/Short„Çµ„Éû„É™: " + ", ".join(summary_lines))
            # system „Åî„Å®„ÅÆÊúÄÁµÇ„Ç®„É≥„Éà„É™„ÉºÊï∞„ÇíÂá∫Âäõ
            try:
                if isinstance(grp, dict):
                    for k, v in grp.items():
                        _log(f"‚úÖ {k}: {int(v)} ‰ª∂")
                else:
                    for _, r in grp.iterrows():
                        _log(f"‚úÖ {r['system']}: {int(r['count'])} ‰ª∂")
            except Exception:
                pass
            # ËøΩÂä†: „Ç®„É≥„Éà„É™„ÉºÈäòÊüÑ„ÅÆ system „Åî„Å®„ÅÆ„Åæ„Å®„ÇÅ
            try:
                lines = []
                for sys_name, g in final_df.groupby("system"):
                    syms = ", ".join(list(g["symbol"].astype(str))[:20])
                    lines.append(f"{sys_name}: {syms}")
                if lines:
                    _log("üßæ „Ç®„É≥„Éà„É™„ÉºÂÜÖË®≥:\n" + "\n".join(lines))
            except Exception:
                pass
        except Exception:
            pass
        _log(f"üìä ÊúÄÁµÇÂÄôË£ú‰ª∂Êï∞: {len(final_df)}")
    else:
        _log("üì≠ ÊúÄÁµÇÂÄôË£ú„ÅØ0‰ª∂„Åß„Åó„Åü")
    if progress_callback:
        try:
            progress_callback(7, 8, "finalize")
        except Exception:
            pass

    # ÊúÄÁµÇÊé°Áî®‰ª∂Êï∞ÔºàEntryÔºâ„Çí100%ÊÆµÈöé„Å®„Åó„Å¶ÈÄöÁü•ÔºàUI „Ç´„Ç¶„É≥„ÇøÊï¥ÂêàÔºâ
    try:
        cb2 = globals().get("_PER_SYSTEM_STAGE")
    except Exception:
        cb2 = None
    if cb2 and callable(cb2):
        try:
            # per-system ÂÄôË£úÔºàTRDlistÔºâ„ÅØ‰∏ä„ÅßÈÄöÁü•Ê∏à„Åø„ÄÇ„Åì„Åì„Åß„ÅØÊúÄÁµÇÊé°Áî®Êï∞„ÇíÊ∏°„Åô„ÄÇ
            final_counts: dict[str, int] = {}
            try:
                if (
                    final_df is not None
                    and not getattr(final_df, "empty", True)
                    and "system" in final_df.columns
                ):
                    final_counts = (
                        final_df.groupby("system").size().to_dict()  # type: ignore[assignment]
                    )
            except Exception:
                final_counts = {}
            for _name in order_1_7:
                _df_sys = per_system.get(_name, pd.DataFrame())
                _cand_cnt = int(
                    0 if _df_sys is None or getattr(_df_sys, "empty", True) else len(_df_sys)
                )
                _final_cnt = int(final_counts.get(_name, 0))
                cb2(_name, 100, None, None, _cand_cnt, _final_cnt)
        except Exception:
            pass

    if metrics_summary_context:
        try:
            prefilter_map = dict(metrics_summary_context.get("prefilter_map", {}))
            exit_counts_map_ctx = metrics_summary_context.get("exit_counts_map", {})
            exit_counts_map = (
                {k: v for k, v in exit_counts_map_ctx.items()}
                if isinstance(exit_counts_map_ctx, dict)
                else {}
            )
            setup_map = dict(metrics_summary_context.get("setup_map", {}))
            tgt_base = int(metrics_summary_context.get("tgt_base", 0))
            final_counts = {}
            try:
                if (
                    final_df is not None
                    and not getattr(final_df, "empty", True)
                    and "system" in final_df.columns
                ):
                    final_counts = (
                        final_df.groupby("system").size().to_dict()  # type: ignore[assignment]
                    )
            except Exception:
                final_counts = {}
            lines = []
            for sys_name in order_1_7:
                tgt = tgt_base if sys_name != "system7" else 1
                fil = int(prefilter_map.get(sys_name, 0))
                stu = int(setup_map.get(sys_name, 0))
                try:
                    _df_trd = per_system.get(sys_name, pd.DataFrame())
                    trd = int(
                        0 if _df_trd is None or getattr(_df_trd, "empty", True) else len(_df_trd)
                    )
                except Exception:
                    trd = 0
                ent = int(final_counts.get(sys_name, 0))
                exv = exit_counts_map.get(sys_name)
                ex_txt = "-" if exv is None else str(int(exv))
                value = (
                    f"Tgt {tgt} / FIL {fil} / STU {stu} / "
                    f"TRD {trd} / Entry {ent} / Exit {ex_txt}"
                )
                lines.append({"name": sys_name, "value": value})
            title = "üìà Êú¨Êó•„ÅÆÊúÄÁµÇ„É°„Éà„É™„ÇØ„ÇπÔºàsystemÂà•Ôºâ"
            _td = locals().get("today")
            try:
                _td_str = str(getattr(_td, "date", lambda: None)() or _td)
            except Exception:
                _td_str = ""
            run_end_time = datetime.now()
            end_equity = _get_account_equity()
            start_equity_val = float(start_equity or 0.0)
            end_equity_val = float(end_equity or 0.0)
            profit_amt = max(end_equity_val - start_equity_val, 0.0)
            loss_amt = max(start_equity_val - end_equity_val, 0.0)
            try:
                total_entries = int(sum(int(v) for v in final_counts.values()))
            except Exception:
                total_entries = 0
            try:
                total_exits = int(sum(int(v) for v in exit_counts_map.values() if v is not None))
            except Exception:
                total_exits = 0
            start_time_str = run_start_time.strftime("%H:%M:%S")
            end_time_str = run_end_time.strftime("%H:%M:%S")
            duration_seconds = max(0, int((run_end_time - run_start_time).total_seconds()))
            hours, remainder = divmod(duration_seconds, 3600)
            minutes, seconds = divmod(remainder, 60)
            duration_str = f"{hours:02d}:{minutes:02d}:{seconds:02d}"
            summary_pairs = [
                ("ÊåáÂÆöÈäòÊüÑÁ∑èÊï∞", f"{int(tgt_base):,}"),
                (
                    "ÈñãÂßãÊôÇÈñì/ÂÆå‰∫ÜÊôÇÈñì",
                    f"{start_time_str} / {end_time_str} (ÊâÄË¶Å: {duration_str})",
                ),
                (
                    "ÈñãÂßãÊôÇË≥áÁî£/ÂÆå‰∫ÜÊôÇË≥áÁî£",
                    f"${start_equity_val:,.2f} / ${end_equity_val:,.2f}",
                ),
                (
                    "„Ç®„É≥„Éà„É™„ÉºÈäòÊüÑÊï∞/„Ç®„Ç∞„Ç∏„ÉÉ„ÉàÈäòÊüÑÊï∞",
                    f"{total_entries} / {total_exits}",
                ),
                (
                    "Âà©ÁõäÈ°ç/ÊêçÂ§±È°ç",
                    f"${profit_amt:,.2f} / ${loss_amt:,.2f}",
                ),
            ]
            summary_fields = [
                {"name": key, "value": value, "inline": True} for key, value in summary_pairs
            ]
            msg = "ÂØæË±°Êó•: " + str(_td_str)
            msg += "\n" + "\n".join(f"{k}: {v}" for k, v in summary_pairs)
            notifier = create_notifier(platform="auto", fallback=True)
            notifier.send(title, msg, fields=summary_fields + lines)
        except Exception:
            pass

    # ÈÄöÁü•„ÅØ progress_callback „ÅÆÊúâÁÑ°„Å´Èñ¢‰øÇ„Å™„ÅèÂÆüË°å„Åô„Çã
    if notify:
        try:
            from tools.notify_signals import send_signal_notification

            send_signal_notification(final_df)
        except Exception:
            _log("‚ö†Ô∏è ÈÄöÁü•„Å´Â§±Êïó„Åó„Åæ„Åó„Åü„ÄÇ")

    # CSV ‰øùÂ≠òÔºà‰ªªÊÑèÔºâ
    if save_csv and not final_df.empty:
        # „Éï„Ç°„Ç§„É´Âêç„É¢„Éº„Éâ: date(YYYY-MM-DD) | datetime(YYYY-MM-DD_HHMM) | runid(YYYY-MM-DD_RUNID)
        mode = (csv_name_mode or "date").lower()
        date_str = today.strftime("%Y-%m-%d")
        suffix = date_str
        if mode == "datetime":
            try:
                jst_now = datetime.now(ZoneInfo("Asia/Tokyo"))
            except Exception:
                jst_now = datetime.now()
            suffix = f"{date_str}_{jst_now.strftime('%H%M')}"
        elif mode == "runid":
            try:
                # _run_id „ÅØÊú¨Èñ¢Êï∞ÂÖàÈ†≠„ÅßÊé°Áï™Ê∏à„Åø
                suffix = f"{date_str}_{_run_id}"
            except Exception:
                suffix = date_str

        out_all = signals_dir / f"signals_final_{suffix}.csv"
        final_df.to_csv(out_all, index=False)
        # „Ç∑„Çπ„ÉÜ„É†Âà•
        for name, df in per_system.items():
            if df is None or df.empty:
                continue
            out = signals_dir / f"signals_{name}_{suffix}.csv"
            df.to_csv(out, index=False)
        _log(f"üíæ ‰øùÂ≠ò: {signals_dir} „Å´CSV„ÇíÊõ∏„ÅçÂá∫„Åó„Åæ„Åó„Åü")
    if progress_callback:
        try:
            progress_callback(8, 8, "done")
        except Exception:
            pass

    # ÁµÇ‰∫Ü„É≠„Ç∞ÔºàUI/CLI ÂèåÊñπ„ÅßË®òÈå≤„Åï„Çå„ÇãÔºâ
    try:
        cnt = 0 if final_df is None else len(final_df)
        _log(f"‚úÖ „Ç∑„Ç∞„Éä„É´Ê§úÂá∫Âá¶ÁêÜ ÁµÇ‰∫Ü | ÊúÄÁµÇÂÄôË£ú {cnt} ‰ª∂")
    except Exception:
        pass

    # === CLI „Éê„Éä„ÉºÔºàÁµÇ‰∫Ü„ÅÆÊòéÁ¢∫ÂåñÔºâ===
    try:
        import time as _time

        _end_txt = _time.strftime("%Y-%m-%d %H:%M:%S")
    except Exception:
        _end_txt = ""
    try:
        print("#" * 68, flush=True)
    except Exception:
        pass
    _log("# üèÅüèÅüèÅ  Êú¨Êó•„ÅÆ„Ç∑„Ç∞„Éä„É´ ÂÆüË°åÁµÇ‰∫Ü (Engine)  üèÅüèÅüèÅ", ui=False)
    _log(f"# ‚è±Ô∏è {_end_txt} | RUN-ID: {_run_id}", ui=False)
    try:
        print("#" * 68 + "\n", flush=True)
    except Exception:
        pass

    # clear callback
    try:
        globals().pop("_LOG_CALLBACK", None)
    except Exception:
        pass

    ctx.final_signals = final_df
    return final_df, per_system


def build_cli_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="ÂÖ®„Ç∑„Çπ„ÉÜ„É†ÂΩìÊó•„Ç∑„Ç∞„Éä„É´ÊäΩÂá∫„ÉªÈõÜÁ¥Ñ")
    parser.add_argument(
        "--symbols",
        nargs="*",
        help="ÂØæË±°„Ç∑„É≥„Éú„É´„ÄÇÊú™ÊåáÂÆö„Å™„ÇâË®≠ÂÆö„ÅÆauto_tickers„Çí‰ΩøÁî®",
    )
    parser.add_argument(
        "--slots-long",
        type=int,
        default=None,
        help="Ë≤∑„ÅÑ„Çµ„Ç§„Éâ„ÅÆÊúÄÂ§ßÊé°Áî®Êï∞Ôºà„Çπ„É≠„ÉÉ„ÉàÊñπÂºèÔºâ",
    )
    parser.add_argument(
        "--slots-short",
        type=int,
        default=None,
        help="Â£≤„Çä„Çµ„Ç§„Éâ„ÅÆÊúÄÂ§ßÊé°Áî®Êï∞Ôºà„Çπ„É≠„ÉÉ„ÉàÊñπÂºèÔºâ",
    )
    parser.add_argument(
        "--capital-long",
        type=float,
        default=None,
        help=("Ë≤∑„ÅÑ„Çµ„Ç§„Éâ‰∫àÁÆóÔºà„Éâ„É´Ôºâ„ÄÇ" "ÊåáÂÆöÊôÇ„ÅØÈáëÈ°çÈÖçÂàÜ„É¢„Éº„Éâ"),
    )
    parser.add_argument(
        "--capital-short",
        type=float,
        default=None,
        help=("Â£≤„Çä„Çµ„Ç§„Éâ‰∫àÁÆóÔºà„Éâ„É´Ôºâ„ÄÇ" "ÊåáÂÆöÊôÇ„ÅØÈáëÈ°çÈÖçÂàÜ„É¢„Éº„Éâ"),
    )
    parser.add_argument(
        "--save-csv",
        action="store_true",
        help="signals„Éá„Ç£„É¨„ÇØ„Éà„É™„Å´CSV„Çí‰øùÂ≠ò„Åô„Çã",
    )
    parser.add_argument(
        "--parallel",
        action="store_true",
        help="„Ç∑„Çπ„ÉÜ„É†„Åî„Å®„ÅÆÂΩìÊó•„Ç∑„Ç∞„Éä„É´ÊäΩÂá∫„Çí‰∏¶ÂàóÂÆüË°å„Åô„Çã",
    )
    # Alpaca Ëá™ÂãïÁô∫Ê≥®„Ç™„Éó„Ç∑„Éß„É≥
    parser.add_argument(
        "--alpaca-submit",
        action="store_true",
        help="Alpaca „Å´Ëá™ÂãïÁô∫Ê≥®Ôºàshares ÂøÖÈ†àÔºâ",
    )
    parser.add_argument(
        "--order-type",
        choices=["market", "limit"],
        default="market",
        help="Ê≥®ÊñáÁ®ÆÂà•",
    )
    parser.add_argument(
        "--tif",
        choices=["GTC", "DAY"],
        default="GTC",
        help="Time In Force",
    )
    parser.add_argument(
        "--live",
        action="store_true",
        help="„É©„Ç§„ÉñÂè£Â∫ß„ÅßÁô∫Ê≥®Ôºà„Éá„Éï„Ç©„É´„Éà„ÅØPaperÔºâ",
    )
    parser.add_argument(
        "--log-file-mode",
        choices=["single", "dated"],
        default=None,
        help="„É≠„Ç∞‰øùÂ≠òÂΩ¢Âºè: single=Âõ∫ÂÆö today_signals.log / dated=Êó•‰ªòÂà•„Éï„Ç°„Ç§„É´",
    )
    parser.add_argument(
        "--csv-name-mode",
        choices=["date", "datetime", "runid"],
        default=None,
        help=(
            "CSV„Éï„Ç°„Ç§„É´Âêç„ÅÆÂΩ¢Âºè: date=YYYY-MM-DD / "
            "datetime=YYYY-MM-DD_HHMM / runid=YYYY-MM-DD_RUNID"
        ),
    )
    # Ë®àÁîª -> ÂÆüË°å„Éñ„É™„ÉÉ„Ç∏ÔºàÂÆâÂÖ®„ÅÆ„Åü„ÇÅÊó¢ÂÆö„ÅØ„Éâ„É©„Ç§„É©„É≥Ôºâ
    parser.add_argument(
        "--run-planned-exits",
        choices=["off", "open", "close", "auto"],
        default=None,
        help=(
            "Êâã‰ªïËàû„ÅÑË®àÁîª„ÅÆËá™ÂãïÂÆüË°å: off=ÁÑ°Âäπ / open=ÂØÑ„Çä(OPG) / "
            "close=Âºï„Åë(CLS) / auto=ÊôÇÈñìÂ∏Ø„ÅßËá™ÂãïÂà§ÂÆö"
        ),
    )
    parser.add_argument(
        "--planned-exits-dry-run",
        action="store_true",
        help="Êâã‰ªïËàû„ÅÑË®àÁîª„ÅÆËá™ÂãïÂÆüË°å„Çí„Éâ„É©„Ç§„É©„É≥„Å´„Åô„ÇãÔºàÊó¢ÂÆö„ÅØÂÆüÁô∫Ê≥®Ôºâ",
    )
    return parser


def parse_cli_args() -> argparse.Namespace:
    parser = build_cli_parser()
    return parser.parse_args()


def configure_logging_for_cli(args: argparse.Namespace) -> None:
    env_mode = os.environ.get("TODAY_SIGNALS_LOG_MODE", "").strip().lower()
    mode = args.log_file_mode or (env_mode if env_mode in {"single", "dated"} else None) or "dated"
    _configure_today_logger(mode=mode)
    try:
        sel_path = globals().get("_LOG_FILE_PATH")
        _log(f"üìù „É≠„Ç∞‰øùÂ≠òÂÖà: {sel_path}", ui=False)
    except Exception:
        pass


def run_signal_pipeline(args: argparse.Namespace) -> tuple[pd.DataFrame, dict[str, pd.DataFrame]]:
    return compute_today_signals(
        args.symbols,
        slots_long=args.slots_long,
        slots_short=args.slots_short,
        capital_long=args.capital_long,
        capital_short=args.capital_short,
        save_csv=args.save_csv,
        csv_name_mode=args.csv_name_mode,
        parallel=args.parallel,
    )


def log_final_candidates(final_df: pd.DataFrame) -> list[Signal]:
    if final_df.empty:
        _log("üì≠ Êú¨Êó•„ÅÆÊúÄÁµÇÂÄôË£ú„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ")
        return []

    _log("\n=== ÊúÄÁµÇÂÄôË£úÔºàÊé®Â•®Ôºâ ===")
    cols = [
        "symbol",
        "system",
        "side",
        "signal_type",
        "entry_date",
        "entry_price",
        "stop_price",
        "shares",
        "position_value",
        "score_key",
        "score",
    ]
    show = [c for c in cols if c in final_df.columns]
    _log(final_df[show].to_string(index=False))
    signals_for_merge = [
        Signal(
            system_id=int(str(r.get("system")).replace("system", "") or 0),
            symbol=str(r.get("symbol")),
            side="BUY" if str(r.get("side")).lower() == "long" else "SELL",
            strength=float(r.get("score", 0.0)),
            meta={},
        )
        for _, r in final_df.iterrows()
    ]
    return signals_for_merge


def merge_signals_for_cli(signals_for_merge: list[Signal]) -> None:
    if not signals_for_merge:
        return
    merge_signals([signals_for_merge], portfolio_state={}, market_state={})


def maybe_submit_orders(final_df: pd.DataFrame, args: argparse.Namespace) -> None:
    if final_df.empty or not args.alpaca_submit:
        return
    submit_orders_df(
        final_df,
        paper=(not args.live),
        order_type=args.order_type,
        system_order_type=None,
        tif=args.tif,
        retries=2,
        delay=0.5,
        log_callback=_log,
        notify=True,
    )


def maybe_run_planned_exits(args: argparse.Namespace) -> None:
    try:
        from schedulers.next_day_exits import submit_planned_exits as _run_planned
    except Exception:
        _run_planned = None
    env_run = os.environ.get("RUN_PLANNED_EXITS", "").lower()
    run_mode = (
        args.run_planned_exits
        or (env_run if env_run in {"off", "open", "close", "auto"} else None)
        or "off"
    )
    dry_run = True if args.planned_exits_dry_run else False
    if _run_planned and run_mode != "off":
        sel = run_mode
        if run_mode == "auto":
            now = datetime.now(ZoneInfo("America/New_York"))
            hhmm = now.strftime("%H%M")
            sel = (
                "open"
                if ("0930" <= hhmm <= "0945")
                else ("close" if ("1550" <= hhmm <= "1600") else "off")
            )
        if sel in {"open", "close"}:
            _log(f"‚è±Ô∏è Êâã‰ªïËàû„ÅÑË®àÁîª„ÅÆËá™ÂãïÂÆüË°å: {sel} (dry_run={dry_run})")
            try:
                df_exec = _run_planned(sel, dry_run=dry_run)
                if df_exec is not None and not df_exec.empty:
                    _log(df_exec.to_string(index=False), ui=False)
                else:
                    _log("ÂØæË±°„ÅÆÊâã‰ªïËàû„ÅÑË®àÁîª„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ", ui=False)
            except Exception as e:
                _log(f"‚ö†Ô∏è Êâã‰ªïËàû„ÅÑË®àÁîª„ÅÆËá™ÂãïÂÆüË°å„Å´Â§±Êïó: {e}")


def main():
    args = parse_cli_args()
    configure_logging_for_cli(args)
    final_df, _per_system = run_signal_pipeline(args)
    signals_for_merge = log_final_candidates(final_df)
    merge_signals_for_cli(signals_for_merge)
    maybe_submit_orders(final_df, args)
    maybe_run_planned_exits(args)


if __name__ == "__main__":
    main()
