"""Extract rolling window data with indicators from full backup cache.

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ ``data_cache/full_backup`` ã«ä¿å­˜ã•ã‚ŒãŸãƒ•ãƒ«å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’
èª­ã¿è¾¼ã¿ã€ãƒ­ãƒ¼ãƒªãƒ³ã‚°ç”¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ ``data_cache/rolling`` ã‚’ 330 æ—¥åˆ†
ï¼ˆè¨­å®šå€¤ã«åŸºã¥ãï¼‰ã¸å†æ§‹ç¯‰ã—ã¾ã™ã€‚å‡ºåŠ›æ™‚ã«ã¯å„æˆ¦ç•¥ã§åˆ©ç”¨ã™ã‚‹ä¸»è¦
ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼ï¼ˆATR/SMA/RSI/ADX ãªã©ï¼‰ã‚’äº‹å‰è¨ˆç®—ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚

ç›´æŽ¥ CLI ã‹ã‚‰å®Ÿè¡Œã§ãã‚‹ã»ã‹ã€``extract_rolling_from_full`` é–¢æ•°ã‚’é€šã˜ã¦
ãƒ†ã‚¹ãƒˆã‚„ä»–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‹ã‚‰å†åˆ©ç”¨ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚
"""

from __future__ import annotations

import argparse
from collections.abc import Callable, Iterable
from dataclasses import dataclass, field
import logging
from pathlib import Path
import sys
from typing import Any


ROOT_DIR = Path(__file__).resolve().parents[1]
if str(ROOT_DIR) not in sys.path:
    sys.path.insert(0, str(ROOT_DIR))

import pandas as pd  # noqa: E402  ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè§£æ±ºå¾Œã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

from common.cache_manager import CacheManager  # noqa: E402
from common.symbol_universe import build_symbol_universe_from_settings  # noqa: E402
from common.symbols_manifest import (  # noqa: E402
    MANIFEST_FILENAME,
    load_symbol_manifest,
)
from common.utils import safe_filename  # noqa: E402
from config.settings import get_settings  # noqa: E402
from indicators_common import add_indicators  # noqa: E402

LOGGER = logging.getLogger(__name__)

SUPPORTED_SUFFIXES = {".csv", ".parquet", ".feather"}


@dataclass
class ExtractionStats:
    """é›†è¨ˆçµæžœã‚’ä¿æŒã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ã€‚"""

    total_symbols: int = 0
    processed_symbols: int = 0
    updated_symbols: int = 0
    skipped_no_data: int = 0
    errors: dict[str, str] = field(default_factory=dict)

    def to_dict(self) -> dict[str, Any]:
        return {
            "total_symbols": self.total_symbols,
            "processed_symbols": self.processed_symbols,
            "updated_symbols": self.updated_symbols,
            "skipped_no_data": self.skipped_no_data,
            "errors": dict(self.errors),
        }


def _log_message(message: str, log: Callable[[str], None] | None) -> None:
    if log:
        try:
            log(message)
        except Exception:  # pragma: no cover - ãƒ­ã‚°ãŒå¤±æ•—ã—ã¦ã‚‚ç¶šè¡Œ
            pass
    LOGGER.info(message)


def _normalize_positive_int(value: Any | None) -> int | None:
    if value is None:
        return None
    try:
        parsed = int(value)
    except (TypeError, ValueError):
        return None
    return parsed if parsed > 0 else None


def _discover_symbols(full_dir: Path) -> list[str]:
    """Detect available symbols from the full backup directory."""

    symbols: set[str] = set()
    for path in full_dir.glob("*"):
        if not path.is_file():
            continue
        if path.suffix.lower() not in SUPPORTED_SUFFIXES:
            continue
        if path.name.startswith("_"):
            continue
        stem = path.stem.strip()
        if stem:
            symbols.add(stem)
    return sorted(symbols)


def _prepare_rolling_frame(df: pd.DataFrame, target_days: int) -> pd.DataFrame | None:
    """Normalize full-history dataframe and compute indicators for rolling cache."""

    if df is None or getattr(df, "empty", True):
        return None

    try:
        work = df.copy()
    except Exception:  # pragma: no cover - defensive fallback
        work = pd.DataFrame(df)

    if "date" not in work.columns:
        if "Date" in work.columns:
            work = work.rename(columns={"Date": "date"})
        else:
            try:
                idx_series = pd.to_datetime(work.index, errors="coerce")
            except Exception:
                idx_series = None
            if idx_series is None or idx_series.isna().all():
                return None
            work = work.reset_index(drop=True)
            work["date"] = idx_series

    work["date"] = pd.to_datetime(work["date"], errors="coerce")
    work = work.dropna(subset=["date"])  # ä¸æ­£æ—¥ä»˜ã‚’é™¤å¤–
    if work.empty:
        return None
    work = (
        work.sort_values("date")
        .drop_duplicates("date", keep="last")
        .reset_index(drop=True)
    )

    calc = work.copy()
    calc["Date"] = pd.to_datetime(calc["date"], errors="coerce").dt.normalize()

    # Upper-case OHLCV columns for indicator calculation
    col_pairs = (
        ("open", "Open"),
        ("high", "High"),
        ("low", "Low"),
        ("close", "Close"),
        ("volume", "Volume"),
    )
    for src, dst in col_pairs:
        if src in calc.columns and dst not in calc.columns:
            calc[dst] = calc[src]

    if "AdjClose" not in calc.columns:
        for cand in ("adjusted_close", "adj_close", "adjclose"):
            if cand in calc.columns:
                calc["AdjClose"] = calc[cand]
                break

    required = {"Open", "High", "Low", "Close"}
    if required - set(calc.columns):
        missing = ",".join(sorted(required - set(calc.columns)))
        raise ValueError(f"missing_price_columns:{missing}")

    enriched = add_indicators(calc)

    enriched["date"] = pd.to_datetime(
        enriched.get("date", enriched.get("Date")), errors="coerce"
    )
    enriched = enriched.drop(columns=["Date"], errors="ignore")
    enriched = enriched.dropna(subset=["date"]).sort_values("date")
    if target_days > 0:
        enriched = enriched.tail(int(target_days))
    enriched = enriched.reset_index(drop=True)

    cols = ["date"] + [c for c in enriched.columns if c != "date"]
    return enriched.loc[:, cols]


def _resolve_symbol_universe(
    cache_manager: CacheManager,
    symbols: Iterable[str] | None,
    log: Callable[[str], None] | None,
) -> list[str]:
    if symbols is not None:
        return [s for s in (sym.strip() for sym in symbols) if s]

    manifest_symbols = load_symbol_manifest(cache_manager.full_dir)
    if manifest_symbols:
        _log_message(
            (
                "â„¹ï¸ cache_daily_data ãƒžãƒ‹ãƒ•ã‚§ã‚¹ãƒˆ({file}) ã‹ã‚‰ "
                "{count} éŠ˜æŸ„ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ"
            ).format(file=MANIFEST_FILENAME, count=len(manifest_symbols)),
            log,
        )

        available = _discover_symbols(cache_manager.full_dir)
        available_set = {sym.upper() for sym in available}
        filtered = [sym for sym in manifest_symbols if sym.upper() in available_set]

        if filtered:
            missing = len(manifest_symbols) - len(filtered)
            if missing:
                _log_message(
                    (
                        "â„¹ï¸ full_backup ã«æœªå­˜åœ¨ã® {missing} éŠ˜æŸ„ã‚’é™¤å¤–ã— "
                        "{count} éŠ˜æŸ„ã‚’å‡¦ç†å¯¾è±¡ã¨ã—ã¾ã™"
                    ).format(missing=missing, count=len(filtered)),
                    log,
                )
            return filtered

        if available:
            _log_message(
                (
                    "âš ï¸ ãƒžãƒ‹ãƒ•ã‚§ã‚¹ãƒˆéŠ˜æŸ„ãŒ full_backup ã«å­˜åœ¨ã—ãªã„ãŸã‚ "
                    "full_backup ã‚’èµ°æŸ»ã—ãŸ {count} éŠ˜æŸ„ã‚’åˆ©ç”¨ã—ã¾ã™"
                ).format(count=len(available)),
                log,
            )
            return available

        _log_message("âš ï¸ full_backup ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰å‡¦ç†å¯¾è±¡ã‚’æ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ", log)
        return []

    # cache_daily_data ã¨åŒä¸€ãƒ­ã‚¸ãƒƒã‚¯ã§éŠ˜æŸ„é›†åˆã‚’æ§‹ç¯‰
    try:
        settings = getattr(cache_manager, "settings", None)
        fetched = build_symbol_universe_from_settings(settings, logger=LOGGER)
    except Exception as exc:  # pragma: no cover - ãƒ­ã‚°ã®ã¿
        _log_message(f"âš ï¸ NASDAQ/EODHD ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹å–å¾—ã«å¤±æ•—: {exc}", log)
        fetched = []

    if fetched:
        safe_symbols = list(dict.fromkeys(safe_filename(sym) for sym in fetched))
        available = _discover_symbols(cache_manager.full_dir)
        if available:
            available_set = {sym.upper() for sym in available}
            filtered = [
                sym for sym in safe_symbols if sym.upper() in available_set
            ]
            missing = len(safe_symbols) - len(filtered)
            if missing:
                _log_message(
                    (
                        "â„¹ï¸ NASDAQ/EODHD ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ {total} ä»¶ã®ã†ã¡ "
                        "{missing} ä»¶ãŒ full_backup ã«å­˜åœ¨ã—ãªã„ãŸã‚é™¤å¤–ã—ã¾ã™"
                    ).format(total=len(safe_symbols), missing=missing),
                    log,
                )
            if filtered:
                return filtered
        _log_message(
            f"â„¹ï¸ NASDAQ/EODHD ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ {len(safe_symbols)} éŠ˜æŸ„ã‚’å‡¦ç†å¯¾è±¡ã¨ã—ã¾ã™",
            log,
        )
        return safe_symbols

    discovered = _discover_symbols(cache_manager.full_dir)
    _log_message(
        (
            "â„¹ï¸ ãƒžãƒ‹ãƒ•ã‚§ã‚¹ãƒˆæœªæ¤œå‡ºã®ãŸã‚ full_backup ã‚’èµ°æŸ»ã—ã¦ "
            "{count} éŠ˜æŸ„ã‚’æ¤œå‡ºã—ã¾ã—ãŸ"
        ).format(count=len(discovered)),
        log,
    )
    return discovered


def extract_rolling_from_full(
    cache_manager: CacheManager,
    *,
    symbols: Iterable[str] | None = None,
    target_days: int | None = None,
    max_symbols: int | None = None,
    log: Callable[[str], None] | None = None,
) -> ExtractionStats:
    """Extract rolling window slices from full backup cache and persist them.

    ``max_symbols`` can be used to cap the number of symbols processed.  When
    not provided explicitly the method falls back to
    ``cache_manager.rolling_cfg.max_symbols`` if it is configured with a
    positive integer value.
    """

    if target_days is None:
        try:
            target_days = int(
                cache_manager.rolling_cfg.base_lookback_days
                + cache_manager.rolling_cfg.buffer_days
            )
        except Exception:
            target_days = 330
    target_days = max(1, int(target_days))

    symbol_list = _resolve_symbol_universe(cache_manager, symbols, log)

    stats = ExtractionStats(total_symbols=len(symbol_list))

    if not symbol_list:
        _log_message("å¯¾è±¡ã‚·ãƒ³ãƒœãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", log)
        return stats

    _log_message(
        f"ðŸ” rolling å†æ§‹ç¯‰ã‚’é–‹å§‹: {len(symbol_list)} éŠ˜æŸ„ | æœŸé–“={target_days}å–¶æ¥­æ—¥", log
    )

    for idx, symbol in enumerate(symbol_list, start=1):
        stats.processed_symbols += 1
        try:
            full_df = cache_manager.read(symbol, "full")
        except Exception as exc:
            message = f"{type(exc).__name__}: {exc}"
            stats.errors[symbol] = message
            _log_message(f"âš ï¸ {symbol}: full èª­ã¿è¾¼ã¿ã«å¤±æ•— ({message})", log)
            continue

        if full_df is None or getattr(full_df, "empty", True):
            stats.skipped_no_data += 1
            _log_message(f"â­ï¸ {symbol}: full ãƒ‡ãƒ¼ã‚¿ç„¡ã—ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—", log)
            continue

        try:
            enriched = _prepare_rolling_frame(full_df, target_days)
        except Exception as exc:  # pragma: no cover - logging only
            message = f"{type(exc).__name__}: {exc}"
            stats.errors[symbol] = message
            _log_message(f"âš ï¸ {symbol}: ã‚¤ãƒ³ã‚¸è¨ˆç®—ã«å¤±æ•— ({message})", log)
            continue

        if enriched is None or getattr(enriched, "empty", True):
            stats.skipped_no_data += 1
            _log_message(f"â­ï¸ {symbol}: æœ‰åŠ¹ãªãƒ­ãƒ¼ãƒªãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ç„¡ã—", log)
            continue

        try:
            cache_manager.write_atomic(enriched, symbol, "rolling")
        except Exception as exc:  # pragma: no cover - logging only
            message = f"{type(exc).__name__}: {exc}"
            stats.errors[symbol] = message
            _log_message(f"âš ï¸ {symbol}: rolling æ›¸ãè¾¼ã¿ã«å¤±æ•— ({message})", log)
            continue

        stats.updated_symbols += 1
        if idx % 100 == 0 or idx == len(symbol_list):
            _log_message(
                f"âœ… é€²æ—: {idx}/{len(symbol_list)} éŠ˜æŸ„å‡¦ç†å®Œäº†", log
            )

    _log_message(
        "âœ… rolling å†æ§‹ç¯‰å®Œäº†: "
        + f"å¯¾è±¡={stats.total_symbols} | æ›´æ–°={stats.updated_symbols} | "
        + f"æ¬ æ={stats.skipped_no_data} | ã‚¨ãƒ©ãƒ¼={len(stats.errors)}",
        log,
    )
    return stats


def _build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description="full_backup ã‹ã‚‰ rolling ã‚’å†æ§‹ç¯‰ã—ä¸»è¦ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼ã‚’ä»˜ä¸Ž",
    )
    parser.add_argument(
        "--symbols",
        nargs="+",
        help="å‡¦ç†å¯¾è±¡ã‚·ãƒ³ãƒœãƒ«ï¼ˆæœªæŒ‡å®šæ™‚ã¯ cache_daily_data ãƒžãƒ‹ãƒ•ã‚§ã‚¹ãƒˆ/å…¨éŠ˜æŸ„ï¼‰",
    )
    parser.add_argument(
        "--target-days",
        type=int,
        help="ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã«ä¿æŒã™ã‚‹å–¶æ¥­æ—¥æ•°ï¼ˆæ—¢å®š: è¨­å®šå€¤ base+bufferï¼‰",
    )
    parser.add_argument(
        "--max-symbols",
        type=int,
        help="å‡¦ç†ä¸Šé™éŠ˜æŸ„æ•°ï¼ˆ0 ä»¥ä¸‹ã§ç„¡åˆ¶é™ã€‚æ—¢å®š: è¨­å®šå€¤ rolling.max_symbolsï¼‰",
    )
    return parser


def main(argv: list[str] | None = None) -> int:
    logging.basicConfig(level=logging.INFO)
    parser = _build_parser()
    args = parser.parse_args(argv)

    settings = get_settings(create_dirs=True)
    cache_manager = CacheManager(settings)

    def _console_log(msg: str) -> None:
        print(msg, flush=True)

    stats = extract_rolling_from_full(
        cache_manager,
        symbols=args.symbols,
        target_days=args.target_days,
        max_symbols=args.max_symbols,
        log=_console_log,
    )

    if stats.errors:
        return 1
    return 0


if __name__ == "__main__":  # pragma: no cover - CLI entry point
    raise SystemExit(main())
