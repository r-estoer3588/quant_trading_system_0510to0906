"""Extract rolling window data with indicators from full backup cache.

„Åì„ÅÆ„Çπ„ÇØ„É™„Éó„Éà„ÅØ ``data_cache/full_backup`` „Å´‰øùÂ≠ò„Åï„Çå„Åü„Éï„É´Â±•Ê≠¥„Éá„Éº„Çø„Çí
Ë™≠„ÅøËæº„Åø„ÄÅ„É≠„Éº„É™„É≥„Ç∞Áî®„Ç≠„É£„ÉÉ„Ç∑„É• ``data_cache/rolling`` „Çí 330 Êó•ÂàÜ
ÔºàË®≠ÂÆöÂÄ§„Å´Âü∫„Å•„ÅèÔºâ„Å∏ÂÜçÊßãÁØâ„Åó„Åæ„Åô„ÄÇÂá∫ÂäõÊôÇ„Å´„ÅØÂêÑÊà¶Áï•„ÅßÂà©Áî®„Åô„Çã‰∏ªË¶Å
„Ç§„É≥„Ç∏„Ç±„Éº„Çø„ÉºÔºàATR/SMA/RSI/ADX „Å™„Å©Ôºâ„Çí‰∫ãÂâçË®àÁÆó„Åó„Å¶‰øùÂ≠ò„Åó„Åæ„Åô„ÄÇ

Áõ¥Êé• CLI „Åã„ÇâÂÆüË°å„Åß„Åç„Çã„Åª„Åã„ÄÅ``extract_rolling_from_full`` Èñ¢Êï∞„ÇíÈÄö„Åò„Å¶
„ÉÜ„Çπ„Éà„ÇÑ‰ªñ„Çπ„ÇØ„É™„Éó„Éà„Åã„ÇâÂÜçÂà©Áî®„Åô„Çã„Åì„Å®„ÇÇÂèØËÉΩ„Åß„Åô„ÄÇ
"""

from __future__ import annotations

import argparse
import concurrent.futures
import json
import logging
import os
import sys
import time
from collections.abc import Callable, Iterable
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any

ROOT_DIR = Path(__file__).resolve().parents[1]
if str(ROOT_DIR) not in sys.path:
    sys.path.insert(0, str(ROOT_DIR))

import pandas as pd  # noqa: E402  „Éá„Ç£„É¨„ÇØ„Éà„É™Ëß£Ê±∫Âæå„Å´„Ç§„É≥„Éù„Éº„Éà

from common.cache_manager import CacheManager  # noqa: E402
from common.indicators_common import add_indicators  # noqa: E402
from common.symbol_universe import build_symbol_universe_from_settings  # noqa: E402
from common.symbols_manifest import (  # noqa: E402
    MANIFEST_FILENAME,
    load_symbol_manifest,
)
from common.utils import safe_filename  # noqa: E402
from config.settings import get_settings  # noqa: E402

# json already imported at top

LOGGER = logging.getLogger(__name__)

SUPPORTED_SUFFIXES = {".csv", ".parquet", ".feather"}


@dataclass
class ExtractionStats:
    """ÈõÜË®àÁµêÊûú„Çí‰øùÊåÅ„Åô„Çã„Éá„Éº„Çø„ÇØ„É©„Çπ„ÄÇ"""

    total_symbols: int = 0
    processed_symbols: int = 0
    updated_symbols: int = 0
    skipped_no_data: int = 0
    errors: dict[str, str] = field(default_factory=dict)

    def to_dict(self) -> dict[str, Any]:
        return {
            "total_symbols": self.total_symbols,
            "processed_symbols": self.processed_symbols,
            "updated_symbols": self.updated_symbols,
            "skipped_no_data": self.skipped_no_data,
            "errors": dict(self.errors),
        }


def _log_message(message: str, log: Callable[[str], None] | None) -> None:
    # If an external logging callable is provided (e.g. console printer),
    # use it and avoid emitting the same message via the module logger to
    # prevent duplicate lines in logs. If no external logger is provided,
    # fall back to the module logger.
    if log:
        try:
            log(message)
        except Exception:  # pragma: no cover - „É≠„Ç∞„ÅåÂ§±Êïó„Åó„Å¶„ÇÇÁ∂öË°å
            pass
        return
    LOGGER.info(message)


def _normalize_positive_int(value: Any | None) -> int | None:
    if value is None:
        return None
    try:
        parsed = int(value)
    except (TypeError, ValueError):
        return None
    return parsed if parsed > 0 else None


def _discover_symbols(full_dir: Path) -> list[str]:
    """Detect available symbols from the full backup directory."""

    symbols: set[str] = set()
    for path in full_dir.glob("*"):
        if not path.is_file():
            continue
        if path.suffix.lower() not in SUPPORTED_SUFFIXES:
            continue
        if path.name.startswith("_"):
            continue
        stem = path.stem.strip()
        if stem:
            symbols.add(stem)
    return sorted(symbols)


def _round_numeric_columns(df: pd.DataFrame, decimals: int | None) -> pd.DataFrame:
    """Êï∞ÂÄ§Âàó„Çí ``decimals`` Ê°Å„Å´‰∏∏„ÇÅ„Åü DataFrame „ÇíËøî„Åô„ÄÇ"""

    if decimals is None:
        return df
    try:
        dec = int(decimals)
    except (TypeError, ValueError):
        return df
    numeric = df.select_dtypes(include="number")
    if numeric.empty:
        return df
    rounded = df.copy()
    try:
        rounded[numeric.columns] = numeric.round(dec)
    except Exception:
        return df
    return rounded


def _prepare_rolling_frame(df: pd.DataFrame, target_days: int) -> pd.DataFrame | None:
    """Normalize full-history dataframe and compute indicators for rolling cache."""

    if df is None or getattr(df, "empty", True):
        return None

    try:
        work = df.copy()
    except Exception:  # pragma: no cover - defensive fallback
        work = pd.DataFrame(df)

    if "date" not in work.columns:
        if "Date" in work.columns:
            work = work.rename(columns={"Date": "date"})
        else:
            try:
                idx_series = pd.to_datetime(work.index, errors="coerce")
            except Exception:
                idx_series = None
            if idx_series is None or idx_series.isna().all():
                return None
            work = work.reset_index(drop=True)
            work["date"] = idx_series

    work["date"] = pd.to_datetime(work["date"], errors="coerce")
    work = work.dropna(subset=["date"])  # ‰∏çÊ≠£Êó•‰ªò„ÇíÈô§Â§ñ
    if work.empty:
        return None
    work = (
        work.sort_values("date")
        .drop_duplicates("date", keep="last")
        .reset_index(drop=True)
    )

    calc = work.copy()

    # Ensure we have Date column for indicator calculations, avoiding duplication
    if "Date" not in calc.columns:
        if "date" in calc.columns:
            calc["Date"] = pd.to_datetime(calc["date"], errors="coerce").dt.normalize()
            # Remove lowercase date to avoid duplication
            calc = calc.drop(columns=["date"])
        else:
            # This shouldn't happen as we normalized date earlier
            calc["Date"] = pd.to_datetime(calc.index, errors="coerce").normalize()

    # Only convert columns if PascalCase versions don't already exist,
    # and proactively drop lowercase duplicates if TitleCase already
    # exists. This handles both new data (from cache_daily_data.py with
    # PascalCase) and legacy data (with lowercase columns), minimizing
    # later duplicate cleanup.
    col_pairs = (
        ("open", "Open"),
        ("high", "High"),
        ("low", "Low"),
        ("close", "Close"),
        ("volume", "Volume"),
    )
    for src, dst in col_pairs:
        if src in calc.columns and dst not in calc.columns:
            calc[dst] = calc[src]
            calc = calc.drop(columns=[src])
        # If both exist (e.g., legacy artifacts), drop the lowercase
        # source to avoid duplicate groups
        elif src in calc.columns and dst in calc.columns:
            calc = calc.drop(columns=[src])

    # Handle AdjClose conversion and de-duplication comprehensively
    adj_synonyms = ("adjusted_close", "adj_close", "adjclose")
    if "AdjClose" in calc.columns:
        # If TitleCase exists, drop any lowercase/underscore synonyms to
        # avoid duplicate groups
        drop_src = [c for c in adj_synonyms if c in calc.columns]
        if drop_src:
            calc = calc.drop(columns=drop_src)
    else:
        # Create AdjClose from the first available synonym and drop all
        # synonyms afterwards
        for cand in adj_synonyms:
            if cand in calc.columns:
                calc["AdjClose"] = calc[cand]
                break
        # Drop any remaining synonyms (including the source) to ensure a
        # single canonical column
        drop_src = [c for c in adj_synonyms if c in calc.columns]
        if drop_src:
            calc = calc.drop(columns=drop_src)

    required = {"Open", "High", "Low", "Close"}
    if required - set(calc.columns):
        missing = ",".join(sorted(required - set(calc.columns)))
        raise ValueError(f"missing_price_columns:{missing}")

    # ÊåáÊ®ôË®àÁÆó„Å´ÂøÖË¶Å„Å™ÈÅéÂéª„Éá„Éº„Çø„ÇíÁ¢∫‰øù„Åô„Çã„Åü„ÇÅ„ÅÆ lookback margin
    try:
        settings = get_settings(create_dirs=True)
        lookback_margin = int(getattr(settings.cache, "indicator_lookback_margin", 200))
    except Exception:
        lookback_margin = 200

    # add_indicators „Å´Ê∏°„ÅôÂâç„Å´„ÄÅtarget_days „Å´Âä†„Åà„Å¶‰ΩôÂàÜ„Å™ÈÅéÂéª„ÇíÂê´„ÇÅ„Çã
    # „Åì„Çå„Å´„Çà„Çä ROC200 Á≠â„ÅÆÈï∑ÊúüÊåáÊ®ô„Åå tail ÈÉ®ÂàÜ„ÅßÈÅ©Âàá„Å´Ë®àÁÆó„Åï„Çå„Çã
    if target_days > 0 and lookback_margin > 0:
        prefetch_days = int(target_days) + int(lookback_margin)
        calc_for_ind = calc.copy().tail(prefetch_days)
    else:
        calc_for_ind = calc

    enriched = add_indicators(calc_for_ind)

    # Clean duplicate columns (can be skipped for performance if data is already clean)
    enriched = _clean_duplicate_columns(enriched, skip_cleanup=False)

    # normalize date column
    date_col = enriched.get("date", enriched.get("Date"))
    if date_col is not None:
        enriched["date"] = pd.to_datetime(date_col, errors="coerce")
    enriched = enriched.drop(columns=["Date"], errors="ignore")
    enriched = enriched.dropna(subset=["date"]).sort_values("date")
    if target_days > 0:
        enriched = enriched.tail(int(target_days))
    enriched = enriched.reset_index(drop=True)

    cols = ["date"] + [c for c in enriched.columns if c != "date"]
    return enriched.loc[:, cols]


def _clean_duplicate_columns(
    df: pd.DataFrame, skip_cleanup: bool = False
) -> pd.DataFrame:
    """Remove duplicate columns, keeping PascalCase/uppercase versions."""
    if df is None or df.empty:
        return df

    # Skip cleanup if requested (for performance when data is already clean)
    if skip_cleanup:
        return df

    columns = df.columns.tolist()
    duplicates_to_remove = []

    # Build case-insensitive mapping to find duplicates
    col_mapping = {}
    for col in columns:
        key = col.lower()
        if key not in col_mapping:
            col_mapping[key] = []
        col_mapping[key].append(col)

    # For each group of similar columns, keep the best one
    for _key, similar_cols in col_mapping.items():
        if len(similar_cols) <= 1:
            continue

        # Priority order: PascalCase > ALL_CAPS > lowercase
        priority_scores = []
        for col in similar_cols:
            if col.isupper():  # ATR10, SMA25, etc.
                score = 3
            elif col[0].isupper():  # Open, Close, DollarVolume20, etc.
                score = 2
            elif "_" in col:  # adjusted_close, return_3d, etc.
                score = 1
            else:  # lowercase: atr10, sma25, etc.
                score = 0
            priority_scores.append((score, col))

        # Sort by priority (highest first) and keep the best one
        priority_scores.sort(reverse=True)
        # best_col = priority_scores[0][1]  # Unused variable removed

        # Mark others for removal
        for _, col in priority_scores[1:]:
            duplicates_to_remove.append(col)

    # Remove duplicate columns (should not occur with fixed data processing)
    if duplicates_to_remove:
        # Only show error message if duplicates still occur (indicates a problem)
        removed_cols = ", ".join(duplicates_to_remove)
        print(
            f"‚ö†Ô∏è ‰∫àÊúü„Åó„Å™„ÅÑÈáçË§áÂàó„ÇíÊ§úÂá∫„ÉªÂâäÈô§: {len(duplicates_to_remove)}Âàó ({removed_cols})"
        )
        df = df.drop(columns=duplicates_to_remove)

    return df


def _process_symbol_worker(args: tuple) -> tuple[str, bool, str | None]:
    """Worker function run in a separate process.

    Returns (symbol, success_flag, message). message is None on success,
    or 'no_data' / error message on failure.
    """
    symbol, target_days, round_decimals, nan_warnings = args
    try:
        settings = get_settings(create_dirs=True)
        cm = CacheManager(settings)
        try:
            full_df = cm.read(symbol, "full")
        except Exception as exc:
            return (symbol, False, f"read_error:{exc}")
        if full_df is None or getattr(full_df, "empty", True):
            return (symbol, False, "no_data")
        enriched = _prepare_rolling_frame(full_df, target_days)
        if enriched is None or getattr(enriched, "empty", True):
            return (symbol, False, "no_data")
        if round_decimals is not None:
            try:
                enriched = _round_numeric_columns(enriched, round_decimals)
            except Exception:
                pass
        try:
            # Write both CSV and Feather formats
            _write_dual_format(cm, enriched, symbol)
        except Exception as exc:
            return (symbol, False, f"write_error:{exc}")
        return (symbol, True, None)
    except Exception as exc:
        return (symbol, False, f"{type(exc).__name__}:{exc}")


def _write_dual_format(cm: CacheManager, df: pd.DataFrame, symbol: str) -> None:
    """Write both CSV and Feather formats for better performance."""
    import shutil

    # Get rolling directory
    rolling_dir = cm.rolling_dir
    rolling_dir.mkdir(parents=True, exist_ok=True)

    # Apply rounding if configured
    round_dec = getattr(getattr(cm, "rolling_cfg", None), "round_decimals", None)
    from common.dataframe_utils import round_dataframe

    df_to_write = round_dataframe(df, round_dec)

    # Write CSV (for compatibility)
    csv_path = rolling_dir / f"{symbol}.csv"
    csv_tmp = rolling_dir / f"{symbol}.csv.tmp"
    try:
        # Use standard pandas CSV writing with explicit format settings
        df_to_write.to_csv(csv_tmp, index=True, float_format="%.6f")
        shutil.move(csv_tmp, csv_path)
    finally:
        if csv_tmp.exists():
            csv_tmp.unlink(missing_ok=True)

    # Write Feather (for performance)
    feather_path = rolling_dir / f"{symbol}.feather"
    feather_tmp = rolling_dir / f"{symbol}.feather.tmp"
    try:
        df_to_write.reset_index(drop=True).to_feather(feather_tmp)
        shutil.move(feather_tmp, feather_path)
    finally:
        if feather_tmp.exists():
            feather_tmp.unlink(missing_ok=True)


def _resolve_symbol_universe(
    cache_manager: CacheManager,
    symbols: Iterable[str] | None,
    log: Callable[[str], None] | None,
) -> list[str]:
    if symbols is not None:
        return [s for s in (sym.strip() for sym in symbols) if s]

    manifest_symbols = load_symbol_manifest(cache_manager.full_dir)
    if manifest_symbols:
        try:
            msg = (
                f"‚ÑπÔ∏è cache_daily_data „Éû„Éã„Éï„Çß„Çπ„Éà({MANIFEST_FILENAME}) „Åã„Çâ "
                f"{len(manifest_symbols)} ÈäòÊüÑ„ÇíË™≠„ÅøËæº„Åø„Åæ„Åó„Åü"
            )
            _log_message(msg, log)
        except Exception:
            _log_message(
                f"‚ÑπÔ∏è cache_daily_data „Éû„Éã„Éï„Çß„Çπ„Éà({MANIFEST_FILENAME}) „ÇíË™≠„ÅøËæº„Åø„Åæ„Åó„Åü",
                log,
            )

        available = _discover_symbols(cache_manager.full_dir)
        available_set = {sym.upper() for sym in available}
        filtered = [sym for sym in manifest_symbols if sym.upper() in available_set]

        if filtered:
            missing = len(manifest_symbols) - len(filtered)
            if missing:
                _log_message(
                    (
                        f"‚ÑπÔ∏è full_backup „Å´Êú™Â≠òÂú®„ÅÆ {missing} ÈäòÊüÑ„ÇíÈô§Â§ñ„Åó {len(filtered)} ÈäòÊüÑ„ÇíÂá¶ÁêÜÂØæË±°„Å®„Åó„Åæ„Åô"
                    ),
                    log,
                )
            return filtered

        if available:
            _log_message(
                (
                    "‚ö†Ô∏è „Éû„Éã„Éï„Çß„Çπ„ÉàÈäòÊüÑ„Åå full_backup „Å´Â≠òÂú®„Åó„Å™„ÅÑ„Åü„ÇÅ "
                    f"full_backup „ÇíËµ∞Êüª„Åó„Åü {len(available)} ÈäòÊüÑ„ÇíÂà©Áî®„Åó„Åæ„Åô"
                ),
                log,
            )
            return available

        _log_message(
            "‚ö†Ô∏è full_backup „Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÂá¶ÁêÜÂØæË±°„ÇíÊ§úÂá∫„Åß„Åç„Åæ„Åõ„Çì„Åß„Åó„Åü", log
        )
        return []

    # cache_daily_data „Å®Âêå‰∏Ä„É≠„Ç∏„ÉÉ„ÇØ„ÅßÈäòÊüÑÈõÜÂêà„ÇíÊßãÁØâ
    try:
        settings = getattr(cache_manager, "settings", None)
        fetched = build_symbol_universe_from_settings(settings, logger=LOGGER)
    except Exception as exc:  # pragma: no cover - „É≠„Ç∞„ÅÆ„Åø
        _log_message(f"‚ö†Ô∏è NASDAQ/EODHD „É¶„Éã„Éê„Éº„ÇπÂèñÂæó„Å´Â§±Êïó: {exc}", log)
        fetched = []

    if fetched:
        safe_symbols = list(dict.fromkeys(safe_filename(sym) for sym in fetched))
        available = _discover_symbols(cache_manager.full_dir)
        if available:
            available_set = {sym.upper() for sym in available}
            filtered = [sym for sym in safe_symbols if sym.upper() in available_set]
            missing = len(safe_symbols) - len(filtered)
            if missing:
                _log_message(
                    (
                        f"‚ÑπÔ∏è NASDAQ/EODHD „É¶„Éã„Éê„Éº„Çπ {len(safe_symbols)} ‰ª∂„ÅÆ„ÅÜ„Å° "
                        f"{missing} ‰ª∂„Åå full_backup „Å´Â≠òÂú®„Åó„Å™„ÅÑ„Åü„ÇÅÈô§Â§ñ„Åó„Åæ„Åô"
                    ),
                    log,
                )
            if filtered:
                return filtered
            # ÂèñÂæó„Åó„Åü„É¶„Éã„Éê„Éº„Çπ„Å® full_backup „Å´ÈáçË§á„ÅåÁÑ°„ÅÑÂ†¥Âêà„ÅØ
            # full_backup „ÇíËµ∞Êüª„Åó„ÅüÈäòÊüÑ„ÇíÂà©Áî®„Åô„ÇãÔºà„ÉÜ„Çπ„Éà‰∫íÊèõÊÄß„ÅÆ„Åü„ÇÅÔºâ
            _log_message(
                (
                    "‚ö†Ô∏è NASDAQ/EODHD „É¶„Éã„Éê„Éº„Çπ„Åå full_backup „Å´Â≠òÂú®„Åó„Å™„ÅÑ„Åü„ÇÅ "
                    f"full_backup „ÇíËµ∞Êüª„Åó„Åü {len(available)} ÈäòÊüÑ„ÇíÂà©Áî®„Åó„Åæ„Åô"
                ),
                log,
            )
            return available

        _log_message(
            f"‚ÑπÔ∏è NASDAQ/EODHD „É¶„Éã„Éê„Éº„Çπ {len(safe_symbols)} ÈäòÊüÑ„ÇíÂá¶ÁêÜÂØæË±°„Å®„Åó„Åæ„Åô",
            log,
        )
        return safe_symbols

    discovered = _discover_symbols(cache_manager.full_dir)
    _log_message(
        (
            f"‚ÑπÔ∏è „Éû„Éã„Éï„Çß„Çπ„ÉàÊú™Ê§úÂá∫„ÅÆ„Åü„ÇÅ full_backup „ÇíËµ∞Êüª„Åó„Å¶ {len(discovered)} ÈäòÊüÑ„ÇíÊ§úÂá∫„Åó„Åæ„Åó„Åü"
        ),
        log,
    )
    return discovered


def extract_rolling_from_full(
    cache_manager: CacheManager,
    *,
    symbols: Iterable[str] | None = None,
    target_days: int | None = None,
    max_symbols: int | None = None,
    log: Callable[[str], None] | None = None,
    nan_warnings: bool = False,
    workers: int | None = None,
    adaptive: bool = True,
) -> ExtractionStats:
    """Extract rolling window slices from full backup cache and persist them.

    ``max_symbols`` can be used to cap the number of symbols processed.  When
    not provided explicitly the method falls back to
    ``cache_manager.rolling_cfg.max_symbols`` if it is configured with a
    positive integer value.
    """

    # Record start time
    start_time = time.time()
    start_dt = datetime.fromtimestamp(start_time).strftime("%Y-%m-%d %H:%M:%S")

    if target_days is None:
        try:
            target_days = int(
                cache_manager.rolling_cfg.base_lookback_days
                + cache_manager.rolling_cfg.buffer_days
            )
        except Exception:
            target_days = 330
    target_days = max(1, int(target_days))

    symbol_list = _resolve_symbol_universe(cache_manager, symbols, log)

    stats = ExtractionStats(total_symbols=len(symbol_list))

    if not symbol_list:
        _log_message("ÂØæË±°„Ç∑„É≥„Éú„É´„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ", log)
        return stats

    _log_message(f"üïê ÈñãÂßãÊôÇÂàª: {start_dt}", log)
    _log_message(
        f"üîÅ rolling ÂÜçÊßãÁØâ„ÇíÈñãÂßã: {len(symbol_list)} ÈäòÊüÑ | ÊúüÈñì={target_days}Âñ∂Ê•≠Êó•",
        log,
    )

    try:
        # tests may provide a SimpleNamespace without nested attributes;
        # fall back safely
        round_decimals = getattr(
            getattr(cache_manager, "rolling_cfg", None), "round_decimals", None
        )
        if round_decimals is None:
            settings_obj = getattr(cache_manager, "settings", None)
            cache_obj = getattr(settings_obj, "cache", None)
            round_decimals = getattr(cache_obj, "round_decimals", None)
    except Exception:
        round_decimals = None

    # Determine initial worker count preference
    cfg_workers = getattr(getattr(cache_manager, "rolling_cfg", None), "workers", None)
    # If explicit workers passed to function, it takes precedence
    if workers is None:
        workers = cfg_workers

    # Serial fallback if workers not specified
    if workers is None:
        # keep original sequential behavior
        for idx, symbol in enumerate(symbol_list, start=1):
            stats.processed_symbols += 1
            try:
                full_df = cache_manager.read(symbol, "full")
            except Exception as exc:
                message = f"{type(exc).__name__}: {exc}"
                stats.errors[symbol] = message
                _log_message(f"‚ö†Ô∏è {symbol}: full Ë™≠„ÅøËæº„Åø„Å´Â§±Êïó ({message})", log)
                continue

            if full_df is None or getattr(full_df, "empty", True):
                stats.skipped_no_data += 1
                _log_message(f"‚è≠Ô∏è {symbol}: full „Éá„Éº„ÇøÁÑ°„Åó„ÅÆ„Åü„ÇÅ„Çπ„Ç≠„ÉÉ„Éó", log)
                continue

            try:
                enriched = _prepare_rolling_frame(full_df, target_days)
            except Exception as exc:  # pragma: no cover - logging only
                message = f"{type(exc).__name__}: {exc}"
                stats.errors[symbol] = message
                _log_message(f"‚ö†Ô∏è {symbol}: „Ç§„É≥„Ç∏Ë®àÁÆó„Å´Â§±Êïó ({message})", log)
                continue

            if enriched is None or getattr(enriched, "empty", True):
                stats.skipped_no_data += 1
                _log_message(f"‚è≠Ô∏è {symbol}: ÊúâÂäπ„Å™„É≠„Éº„É™„É≥„Ç∞„Éá„Éº„ÇøÁÑ°„Åó", log)
                continue

            try:
                enriched = _round_numeric_columns(enriched, round_decimals)
                cache_manager.write_atomic(enriched, symbol, "rolling")
            except Exception as exc:  # pragma: no cover - logging only
                message = f"{type(exc).__name__}: {exc}"
                stats.errors[symbol] = message
                _log_message(f"‚ö†Ô∏è {symbol}: rolling Êõ∏„ÅçËæº„Åø„Å´Â§±Êïó ({message})", log)
                continue

            stats.updated_symbols += 1
            if idx % 100 == 0 or idx == len(symbol_list):
                _log_message(f"‚úÖ ÈÄ≤Êçó: {idx}/{len(symbol_list)} ÈäòÊüÑÂá¶ÁêÜÂÆå‰∫Ü", log)
    else:
        # Parallel execution with adaptive concurrency control
        try:
            workers = int(workers)
        except Exception:
            workers = 0

        # establish sensible bounds
        cpu = os.cpu_count() or 1
        max_possible = max(1, min(32, int(cpu * 2), len(symbol_list)))
        if workers and workers > 0:
            initial_workers = int(workers)
        else:
            settings_obj = getattr(cache_manager, "settings", None)
            cache_obj = getattr(settings_obj, "cache", None)
            rolling_obj = getattr(cache_obj, "rolling", None)
            try:
                initial_workers = int(getattr(rolling_obj, "workers", 4) or 4)
            except Exception:
                initial_workers = 4
        current_workers = max(1, min(initial_workers, max_possible))

        _log_message(
            (
                f"‚ÑπÔ∏è ‰∏¶ÂàóÂá¶ÁêÜ: ÂàùÊúü„ÉØ„Éº„Ç´„Éº={current_workers} "
                f"ÊúÄÂ§ß„ÉØ„Éº„Ç´„Éº={max_possible} ÈÅ©ÂøúÂûã={'ÊúâÂäπ' if adaptive else 'ÁÑ°Âäπ'}"
            ),
            log,
        )

        args_list = [
            (symbol, target_days, round_decimals, nan_warnings)
            for symbol in symbol_list
        ]

        # prepare progress output file
        try:
            settings_obj = getattr(cache_manager, "settings", None)
            cache_obj = getattr(settings_obj, "cache", None)
            rolling_obj = getattr(cache_obj, "rolling", None)
            report_seconds = int(
                getattr(rolling_obj, "adaptive_report_seconds", 10) or 10
            )
        except Exception:
            report_seconds = 10

        logs_dir_candidate = (
            getattr(cache_manager.settings.outputs, "logs_dir", None)
            or getattr(cache_manager.settings, "LOGS_DIR", None)
            or "logs"
        )
        logs_dir_path = Path(str(logs_dir_candidate))
        try:
            logs_dir_path.mkdir(parents=True, exist_ok=True)
        except Exception:
            pass
        progress_path = logs_dir_path / "rolling_progress.json"

        # create executor with upper bound; we will control active submissions
        with concurrent.futures.ProcessPoolExecutor(max_workers=max_possible) as exe:
            next_idx = 0
            active: dict[concurrent.futures.Future, tuple[str, float]] = {}

            # adaptive measurement
            window_durations: list[float] = []
            window_count = 8
            prev_throughput = None

            while stats.processed_symbols < len(symbol_list):
                # submit tasks until reaching current_workers
                while len(active) < current_workers and next_idx < len(args_list):
                    args = args_list[next_idx]
                    fut = exe.submit(_process_symbol_worker, args)
                    active[fut] = (args[0], time.time())
                    next_idx += 1

                if not active:
                    break

                done, _ = concurrent.futures.wait(
                    active.keys(), return_when=concurrent.futures.FIRST_COMPLETED
                )
                for fut in done:
                    symbol, start_ts = active.pop(fut)
                    stats.processed_symbols += 1
                    end_ts = time.time()
                    duration = max(0.0001, end_ts - start_ts)
                    window_durations.append(duration)
                    # keep window size bounded
                    if len(window_durations) > window_count:
                        window_durations.pop(0)

                    try:
                        sym, ok, message = fut.result()
                    except Exception as exc:
                        stats.errors[symbol] = str(exc)
                        _log_message(f"‚ö†Ô∏è {symbol}: worker ‰æãÂ§ñ ({exc})", log)
                        continue

                    if not ok:
                        if message == "no_data":
                            stats.skipped_no_data += 1
                            _log_message(
                                f"‚è≠Ô∏è {symbol}: full „Éá„Éº„ÇøÁÑ°„Åó„ÅÆ„Åü„ÇÅ„Çπ„Ç≠„ÉÉ„Éó", log
                            )
                        else:
                            stats.errors[symbol] = message or "error"
                            _log_message(f"‚ö†Ô∏è {symbol}: Âá¶ÁêÜÂ§±Êïó ({message})", log)
                    else:
                        stats.updated_symbols += 1

                # write progress JSON periodically
                try:
                    now_ts = int(time.time())
                    if (
                        not progress_path.exists()
                        or now_ts - int(progress_path.stat().st_mtime) >= report_seconds
                    ):
                        prog = {
                            "total": stats.total_symbols,
                            "processed": stats.processed_symbols,
                            "updated": stats.updated_symbols,
                            "skipped": stats.skipped_no_data,
                            "errors": len(stats.errors),
                            "current_workers": current_workers,
                            "recent_window_seconds": [
                                round(d, 3) for d in window_durations
                            ],
                            "timestamp": now_ts,
                        }
                        try:
                            with open(progress_path, "w", encoding="utf-8") as pf:
                                json.dump(prog, pf, ensure_ascii=False)
                        except Exception:
                            pass
                except Exception:
                    pass

                # report progress periodically
                if (
                    stats.processed_symbols % 100 == 0
                    or stats.processed_symbols == len(symbol_list)
                ):
                    _log_message(
                        f"‚úÖ ÈÄ≤Êçó: {stats.processed_symbols}/{len(symbol_list)} ÈäòÊüÑÂá¶ÁêÜÂÆå‰∫Ü",
                        log,
                    )

                # adaptive adjustment: evaluate throughput over window
                if adaptive and len(window_durations) >= max(4, window_count // 2):
                    window_time = sum(window_durations)
                    if window_time <= 0:
                        continue
                    throughput = len(window_durations) / window_time
                    # try small adjustments: increase or decrease by 1
                    if prev_throughput is None:
                        prev_throughput = throughput
                    else:
                        # if throughput improved notably, try increasing workers
                        if (
                            throughput > prev_throughput * 1.02
                            and current_workers < max_possible
                        ):
                            current_workers += 1
                            _log_message(
                                f"‚ÑπÔ∏è „ÉØ„Éº„Ç´„ÉºÊï∞„ÇíÂ¢ó„ÇÑ„Åó„Åæ„Åô -> {current_workers}", log
                            )
                            prev_throughput = throughput
                        # if throughput degraded notably, decrease workers
                        elif (
                            throughput < prev_throughput * 0.98 and current_workers > 1
                        ):
                            current_workers = max(1, current_workers - 1)
                            _log_message(
                                f"‚ÑπÔ∏è „ÉØ„Éº„Ç´„ÉºÊï∞„ÇíÊ∏õ„Çâ„Åó„Åæ„Åô -> {current_workers}", log
                            )
                            prev_throughput = throughput
                        else:
                            # small/no change, keep current
                            prev_throughput = throughput

    # Calculate completion time and duration
    end_time = time.time()
    end_dt = datetime.fromtimestamp(end_time).strftime("%Y-%m-%d %H:%M:%S")
    duration_seconds = end_time - start_time

    # Format duration as H:MM:SS
    hours = int(duration_seconds // 3600)
    minutes = int((duration_seconds % 3600) // 60)
    seconds = int(duration_seconds % 60)
    duration_str = f"{hours}:{minutes:02d}:{seconds:02d}"

    _log_message(f"üïê ÁµÇ‰∫ÜÊôÇÂàª: {end_dt}", log)
    _log_message(f"‚è∞ ÊâÄË¶ÅÊôÇÈñì: {duration_str}", log)
    _log_message(
        "‚úÖ rolling ÂÜçÊßãÁØâÂÆå‰∫Ü: "
        + f"ÂØæË±°={stats.total_symbols} | Êõ¥Êñ∞={stats.updated_symbols} | "
        + f"Ê¨†Êêç={stats.skipped_no_data} | „Ç®„É©„Éº={len(stats.errors)}",
        log,
    )
    return stats


def _build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description="full_backup „Åã„Çâ rolling „ÇíÂÜçÊßãÁØâ„Åó‰∏ªË¶Å„Ç§„É≥„Ç∏„Ç±„Éº„Çø„Éº„Çí‰ªò‰∏é",
    )
    parser.add_argument(
        "--symbols",
        nargs="+",
        help="Âá¶ÁêÜÂØæË±°„Ç∑„É≥„Éú„É´ÔºàÊú™ÊåáÂÆöÊôÇ„ÅØ cache_daily_data „Éû„Éã„Éï„Çß„Çπ„Éà/ÂÖ®ÈäòÊüÑÔºâ",
    )
    parser.add_argument(
        "--target-days",
        type=int,
        help="„É≠„Éº„É™„É≥„Ç∞„Å´‰øùÊåÅ„Åô„ÇãÂñ∂Ê•≠Êó•Êï∞ÔºàÊó¢ÂÆö: Ë®≠ÂÆöÂÄ§ base+bufferÔºâ",
    )
    parser.add_argument(
        "--max-symbols",
        type=int,
        help="Âá¶ÁêÜ‰∏äÈôêÈäòÊüÑÊï∞Ôºà0 ‰ª•‰∏ã„ÅßÁÑ°Âà∂Èôê„ÄÇÊó¢ÂÆö: Ë®≠ÂÆöÂÄ§ rolling.max_symbolsÔºâ",
    )
    parser.add_argument(
        "--nan-warnings",
        action="store_true",
        help="ÊåáÊ®ô NaN Ë≠¶Âëä„ÇíÊúâÂäπÂåñÔºàÊó¢ÂÆö: ÁÑ°Âäπ„ÄÅ„É≠„Ç∞ÊäëÊ≠¢Ôºâ",
    )
    parser.add_argument(
        "--workers",
        type=int,
        help="‰∏¶Âàó„ÉØ„Éº„Ç´„ÉºÊï∞„ÅÆ‰∏äÈôêÔºàÊú™ÊåáÂÆö„ÅßË®≠ÂÆöÂÄ§„Åæ„Åü„ÅØ„Éá„Éï„Ç©„É´„Éà„Çí‰ΩøÁî®Ôºâ",
    )
    parser.add_argument(
        "--no-adaptive",
        action="store_true",
        help="ÈÅ©ÂøúÁöÑ„ÉØ„Éº„Ç´„ÉºË™øÊï¥„ÇíÁÑ°ÂäπÂåñÔºàÊó¢ÂÆö: ÊúâÂäπÔºâ",
    )
    return parser


def main(argv: list[str] | None = None) -> int:
    logging.basicConfig(level=logging.INFO)
    parser = _build_parser()
    args = parser.parse_args(argv)

    settings = get_settings(create_dirs=True)
    cache_manager = CacheManager(settings)

    def _console_log(msg: str) -> None:
        LOGGER.info(msg)

    stats = extract_rolling_from_full(
        cache_manager,
        symbols=args.symbols,
        target_days=args.target_days,
        max_symbols=args.max_symbols,
        log=_console_log,
        nan_warnings=bool(getattr(args, "nan_warnings", False)),
        workers=getattr(args, "workers", None),
        adaptive=(not bool(getattr(args, "no_adaptive", False))),
    )

    if stats.errors:
        return 1
    return 0


if __name__ == "__main__":  # pragma: no cover - CLI entry point
    raise SystemExit(main())
