"""Extract rolling window data with indicators from full backup cache.

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ ``data_cache/full_backup`` ã«ä¿å­˜ã•ã‚ŒãŸãƒ•ãƒ«å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’
èª­ã¿è¾¼ã¿ã€ãƒ­ãƒ¼ãƒªãƒ³ã‚°ç”¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ ``data_cache/rolling`` ã‚’ 330 æ—¥åˆ†
ï¼ˆè¨­å®šå€¤ã«åŸºã¥ãï¼‰ã¸å†æ§‹ç¯‰ã—ã¾ã™ã€‚å‡ºåŠ›æ™‚ã«ã¯å„æˆ¦ç•¥ã§åˆ©ç”¨ã™ã‚‹ä¸»è¦
ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼ï¼ˆATR/SMA/RSI/ADX ãªã©ï¼‰ã‚’äº‹å‰è¨ˆç®—ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚

ç›´æŽ¥ CLI ã‹ã‚‰å®Ÿè¡Œã§ãã‚‹ã»ã‹ã€``extract_rolling_from_full`` é–¢æ•°ã‚’é€šã˜ã¦
ãƒ†ã‚¹ãƒˆã‚„ä»–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‹ã‚‰å†åˆ©ç”¨ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚
"""

from __future__ import annotations

import argparse
from collections.abc import Callable, Iterable
from dataclasses import dataclass, field
import logging
import os
from pathlib import Path
import sys
from typing import Any


ROOT_DIR = Path(__file__).resolve().parents[1]
if str(ROOT_DIR) not in sys.path:
    sys.path.insert(0, str(ROOT_DIR))

import pandas as pd  # noqa: E402  ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè§£æ±ºå¾Œã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import requests  # noqa: E402

from common.cache_manager import CacheManager  # noqa: E402
from common.symbols_manifest import (  # noqa: E402
    MANIFEST_FILENAME,
    load_symbol_manifest,
)
from config.settings import get_settings  # noqa: E402
from indicators_common import add_indicators  # noqa: E402

LOGGER = logging.getLogger(__name__)

SUPPORTED_SUFFIXES = {".csv", ".parquet", ".feather"}


@dataclass
class ExtractionStats:
    """é›†è¨ˆçµæžœã‚’ä¿æŒã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ã€‚"""

    total_symbols: int = 0
    processed_symbols: int = 0
    updated_symbols: int = 0
    skipped_no_data: int = 0
    errors: dict[str, str] = field(default_factory=dict)
    reference_symbol_count: int | None = None

    def to_dict(self) -> dict[str, Any]:
        return {
            "total_symbols": self.total_symbols,
            "processed_symbols": self.processed_symbols,
            "updated_symbols": self.updated_symbols,
            "skipped_no_data": self.skipped_no_data,
            "errors": dict(self.errors),
            "reference_symbol_count": self.reference_symbol_count,
        }


def _log_message(message: str, log: Callable[[str], None] | None) -> None:
    if log:
        try:
            log(message)
        except Exception:  # pragma: no cover - ãƒ­ã‚°ãŒå¤±æ•—ã—ã¦ã‚‚ç¶šè¡Œ
            pass
    LOGGER.info(message)


def _normalize_positive_int(value: Any | None) -> int | None:
    if value is None:
        return None
    try:
        parsed = int(value)
    except (TypeError, ValueError):
        return None
    return parsed if parsed > 0 else None


def _discover_symbols(full_dir: Path) -> list[str]:
    """Detect available symbols from the full backup directory."""

    symbols: set[str] = set()
    for path in full_dir.glob("*"):
        if not path.is_file():
            continue
        if path.suffix.lower() not in SUPPORTED_SUFFIXES:
            continue
        if path.name.startswith("_"):
            continue
        stem = path.stem.strip()
        if stem:
            symbols.add(stem)
    return sorted(symbols)


def _prepare_rolling_frame(df: pd.DataFrame, target_days: int) -> pd.DataFrame | None:
    """Normalize full-history dataframe and compute indicators for rolling cache."""

    if df is None or getattr(df, "empty", True):
        return None

    try:
        work = df.copy()
    except Exception:  # pragma: no cover - defensive fallback
        work = pd.DataFrame(df)

    if "date" not in work.columns:
        if "Date" in work.columns:
            work = work.rename(columns={"Date": "date"})
        else:
            try:
                idx_series = pd.to_datetime(work.index, errors="coerce")
            except Exception:
                idx_series = None
            if idx_series is None or idx_series.isna().all():
                return None
            work = work.reset_index(drop=True)
            work["date"] = idx_series

    work["date"] = pd.to_datetime(work["date"], errors="coerce")
    work = work.dropna(subset=["date"])  # ä¸æ­£æ—¥ä»˜ã‚’é™¤å¤–
    if work.empty:
        return None
    work = (
        work.sort_values("date")
        .drop_duplicates("date", keep="last")
        .reset_index(drop=True)
    )

    calc = work.copy()
    calc["Date"] = pd.to_datetime(calc["date"], errors="coerce").dt.normalize()

    # Upper-case OHLCV columns for indicator calculation
    col_pairs = (
        ("open", "Open"),
        ("high", "High"),
        ("low", "Low"),
        ("close", "Close"),
        ("volume", "Volume"),
    )
    for src, dst in col_pairs:
        if src in calc.columns and dst not in calc.columns:
            calc[dst] = calc[src]

    if "AdjClose" not in calc.columns:
        for cand in ("adjusted_close", "adj_close", "adjclose"):
            if cand in calc.columns:
                calc["AdjClose"] = calc[cand]
                break

    required = {"Open", "High", "Low", "Close"}
    if required - set(calc.columns):
        missing = ",".join(sorted(required - set(calc.columns)))
        raise ValueError(f"missing_price_columns:{missing}")

    enriched = add_indicators(calc)

    enriched["date"] = pd.to_datetime(
        enriched.get("date", enriched.get("Date")), errors="coerce"
    )
    enriched = enriched.drop(columns=["Date"], errors="ignore")
    enriched = enriched.dropna(subset=["date"]).sort_values("date")
    if target_days > 0:
        enriched = enriched.tail(int(target_days))
    enriched = enriched.reset_index(drop=True)

    cols = ["date"] + [c for c in enriched.columns if c != "date"]
    return enriched.loc[:, cols]


def _resolve_symbol_universe(
    cache_manager: CacheManager,
    symbols: Iterable[str] | None,
    log: Callable[[str], None] | None,
) -> list[str]:
    if symbols is not None:
        return [s for s in (sym.strip() for sym in symbols) if s]

    manifest_symbols = load_symbol_manifest(cache_manager.full_dir)
    if manifest_symbols:
        _log_message(
            (
                "â„¹ï¸ cache_daily_data ãƒžãƒ‹ãƒ•ã‚§ã‚¹ãƒˆ({file}) ã‹ã‚‰ "
                "{count} éŠ˜æŸ„ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ"
            ).format(file=MANIFEST_FILENAME, count=len(manifest_symbols)),
            log,
        )

        available = _discover_symbols(cache_manager.full_dir)
        available_set = {sym.upper() for sym in available}
        filtered = [sym for sym in manifest_symbols if sym.upper() in available_set]

        if filtered:
            missing = len(manifest_symbols) - len(filtered)
            if missing:
                _log_message(
                    (
                        "â„¹ï¸ full_backup ã«æœªå­˜åœ¨ã® {missing} éŠ˜æŸ„ã‚’é™¤å¤–ã— "
                        "{count} éŠ˜æŸ„ã‚’å‡¦ç†å¯¾è±¡ã¨ã—ã¾ã™"
                    ).format(missing=missing, count=len(filtered)),
                    log,
                )
            return filtered

        if available:
            _log_message(
                (
                    "âš ï¸ ãƒžãƒ‹ãƒ•ã‚§ã‚¹ãƒˆéŠ˜æŸ„ãŒ full_backup ã«å­˜åœ¨ã—ãªã„ãŸã‚ "
                    "full_backup ã‚’èµ°æŸ»ã—ãŸ {count} éŠ˜æŸ„ã‚’åˆ©ç”¨ã—ã¾ã™"
                ).format(count=len(available)),
                log,
            )
            return available

        _log_message("âš ï¸ full_backup ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰å‡¦ç†å¯¾è±¡ã‚’æ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ", log)
        return []

    discovered = _discover_symbols(cache_manager.full_dir)
    _log_message(
        (
            "â„¹ï¸ ãƒžãƒ‹ãƒ•ã‚§ã‚¹ãƒˆæœªæ¤œå‡ºã®ãŸã‚ full_backup ã‚’èµ°æŸ»ã—ã¦ "
            "{count} éŠ˜æŸ„ã‚’æ¤œå‡ºã—ã¾ã—ãŸ"
        ).format(count=len(discovered)),
        log,
    )
    return discovered


def _fetch_eodhd_symbol_count(
    cache_manager: CacheManager,
    log: Callable[[str], None] | None,
    *,
    exchanges: Iterable[str] | None = None,
) -> int | None:
    """Fetch the reference symbol count from EODHD exchange endpoints."""

    if exchanges is None:
        exchanges = ("NYSE", "NASDAQ", "AMEX")

    try:
        settings = cache_manager.settings
    except AttributeError:
        settings = None

    if settings is None:
        return None

    base_url = (
        getattr(settings, "API_EODHD_BASE", None)
        or getattr(getattr(settings, "data", None), "eodhd_base", None)
    )
    if not base_url:
        return None
    base_url = str(base_url).rstrip("/")

    api_key = getattr(settings, "EODHD_API_KEY", None) or ""
    if not api_key:
        data_cfg = getattr(settings, "data", None)
        if data_cfg is not None:
            env_name = getattr(data_cfg, "api_key_env", "EODHD_API_KEY")
            api_key = os.getenv(env_name, "")
    if not api_key:
        api_key = os.getenv("EODHD_API_KEY", "")
    if not api_key:
        return None

    try:
        timeout = int(getattr(settings, "REQUEST_TIMEOUT", 10))
    except Exception:
        timeout = 10

    discovered: set[str] = set()
    successful: list[str] = []
    errors: list[str] = []

    for exchange in exchanges:
        if not exchange:
            continue
        exch = str(exchange).strip().upper()
        if not exch:
            continue
        url = (
            f"{base_url}/api/exchange-symbol-list/{exch}?"
            f"api_token={api_key}&fmt=json"
        )
        try:
            response = requests.get(url, timeout=timeout)
            response.raise_for_status()
        except Exception as exc:  # pragma: no cover - logging only
            errors.append(f"{exch}:{type(exc).__name__}")
            _log_message(
                f"âš ï¸ EODHD {exch}: éŠ˜æŸ„ä¸€è¦§å–å¾—ã«å¤±æ•— ({exc})",
                log,
            )
            continue

        try:
            payload = response.json()
        except ValueError:
            payload = None

        if isinstance(payload, dict):
            for key in ("data", "symbols", "items", "results"):
                if key in payload and isinstance(payload[key], list):
                    payload = payload[key]
                    break

        if not isinstance(payload, list):
            errors.append(f"{exch}:invalid_payload")
            _log_message(
                f"âš ï¸ EODHD {exch}: éŠ˜æŸ„ä¸€è¦§ãŒç„¡åŠ¹ãªå½¢å¼ã§ã—ãŸ",
                log,
            )
            continue

        before = len(discovered)
        for row in payload:
            if not isinstance(row, dict):
                continue
            code = (
                row.get("Code")
                or row.get("code")
                or row.get("Symbol")
                or row.get("symbol")
            )
            if not code:
                continue
            ticker = str(code).strip().upper()
            if ticker:
                discovered.add(ticker)

        added = len(discovered) - before
        if added > 0:
            successful.append(f"{exch}:{added}")
        else:
            errors.append(f"{exch}:empty")
            _log_message(
                f"âš ï¸ EODHD {exch}: éŠ˜æŸ„ãŒæ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ",
                log,
            )

    if discovered:
        summary = ", ".join(successful) if successful else "unknown"
        _log_message(
            (
                "â„¹ï¸ EODHDå‚ç…§éŠ˜æŸ„æ•°ãƒã‚§ãƒƒã‚¯: {count}ä»¶ "
                "(é‡è¤‡é™¤å¤–, å†…è¨³={summary})"
            ).format(count=len(discovered), summary=summary),
            log,
        )
        return len(discovered)

    if errors:
        _log_message(
            "âš ï¸ EODHD éŠ˜æŸ„æ•°ãƒã‚§ãƒƒã‚¯ã«å¤±æ•—ã—ã¾ã—ãŸ: " + ", ".join(errors),
            log,
        )

    return None


def extract_rolling_from_full(
    cache_manager: CacheManager,
    *,
    symbols: Iterable[str] | None = None,
    target_days: int | None = None,
    max_symbols: int | None = None,
    log: Callable[[str], None] | None = None,
) -> ExtractionStats:
    """Extract rolling window slices from full backup cache and persist them.

    ``max_symbols`` can be used to cap the number of symbols processed.  When
    not provided explicitly the method falls back to
    ``cache_manager.rolling_cfg.max_symbols`` if it is configured with a
    positive integer value.
    """

    if target_days is None:
        try:
            target_days = int(
                cache_manager.rolling_cfg.base_lookback_days
                + cache_manager.rolling_cfg.buffer_days
            )
        except Exception:
            target_days = 330
    target_days = max(1, int(target_days))

    symbol_list = _resolve_symbol_universe(cache_manager, symbols, log)

    stats = ExtractionStats(total_symbols=len(symbol_list))

    if symbols is None:
        reference_count = _fetch_eodhd_symbol_count(cache_manager, log)
        if reference_count is not None:
            stats.reference_symbol_count = reference_count
            diff = reference_count - len(symbol_list)
            if diff:
                _log_message(
                    (
                        "âš ï¸ EODHDå‚ç…§éŠ˜æŸ„æ•° {ref}ä»¶ã¨ rolling å¯¾è±¡ {target}ä»¶ãŒ"
                        " ä¸ä¸€è‡´ã§ã™ (å·®åˆ†={diff:+d})"
                    ).format(ref=reference_count, target=len(symbol_list), diff=diff),
                    log,
                )
            else:
                _log_message(
                    f"â„¹ï¸ EODHDå‚ç…§éŠ˜æŸ„æ•°ã¨ rolling å¯¾è±¡ãŒä¸€è‡´ ({len(symbol_list)} éŠ˜æŸ„)",
                    log,
                )

    if not symbol_list:
        _log_message("å¯¾è±¡ã‚·ãƒ³ãƒœãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", log)
        return stats

    _log_message(
        f"ðŸ” rolling å†æ§‹ç¯‰ã‚’é–‹å§‹: {len(symbol_list)} éŠ˜æŸ„ | æœŸé–“={target_days}å–¶æ¥­æ—¥", log
    )

    for idx, symbol in enumerate(symbol_list, start=1):
        stats.processed_symbols += 1
        try:
            full_df = cache_manager.read(symbol, "full")
        except Exception as exc:
            message = f"{type(exc).__name__}: {exc}"
            stats.errors[symbol] = message
            _log_message(f"âš ï¸ {symbol}: full èª­ã¿è¾¼ã¿ã«å¤±æ•— ({message})", log)
            continue

        if full_df is None or getattr(full_df, "empty", True):
            stats.skipped_no_data += 1
            _log_message(f"â­ï¸ {symbol}: full ãƒ‡ãƒ¼ã‚¿ç„¡ã—ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—", log)
            continue

        try:
            enriched = _prepare_rolling_frame(full_df, target_days)
        except Exception as exc:  # pragma: no cover - logging only
            message = f"{type(exc).__name__}: {exc}"
            stats.errors[symbol] = message
            _log_message(f"âš ï¸ {symbol}: ã‚¤ãƒ³ã‚¸è¨ˆç®—ã«å¤±æ•— ({message})", log)
            continue

        if enriched is None or getattr(enriched, "empty", True):
            stats.skipped_no_data += 1
            _log_message(f"â­ï¸ {symbol}: æœ‰åŠ¹ãªãƒ­ãƒ¼ãƒªãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ç„¡ã—", log)
            continue

        try:
            cache_manager.write_atomic(enriched, symbol, "rolling")
        except Exception as exc:  # pragma: no cover - logging only
            message = f"{type(exc).__name__}: {exc}"
            stats.errors[symbol] = message
            _log_message(f"âš ï¸ {symbol}: rolling æ›¸ãè¾¼ã¿ã«å¤±æ•— ({message})", log)
            continue

        stats.updated_symbols += 1
        if idx % 100 == 0 or idx == len(symbol_list):
            _log_message(
                f"âœ… é€²æ—: {idx}/{len(symbol_list)} éŠ˜æŸ„å‡¦ç†å®Œäº†", log
            )

    _log_message(
        "âœ… rolling å†æ§‹ç¯‰å®Œäº†: "
        + f"å¯¾è±¡={stats.total_symbols} | æ›´æ–°={stats.updated_symbols} | "
        + f"æ¬ æ={stats.skipped_no_data} | ã‚¨ãƒ©ãƒ¼={len(stats.errors)}",
        log,
    )
    return stats


def _build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description="full_backup ã‹ã‚‰ rolling ã‚’å†æ§‹ç¯‰ã—ä¸»è¦ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼ã‚’ä»˜ä¸Ž",
    )
    parser.add_argument(
        "--symbols",
        nargs="+",
        help="å‡¦ç†å¯¾è±¡ã‚·ãƒ³ãƒœãƒ«ï¼ˆæœªæŒ‡å®šæ™‚ã¯ cache_daily_data ãƒžãƒ‹ãƒ•ã‚§ã‚¹ãƒˆ/å…¨éŠ˜æŸ„ï¼‰",
    )
    parser.add_argument(
        "--target-days",
        type=int,
        help="ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã«ä¿æŒã™ã‚‹å–¶æ¥­æ—¥æ•°ï¼ˆæ—¢å®š: è¨­å®šå€¤ base+bufferï¼‰",
    )
    parser.add_argument(
        "--max-symbols",
        type=int,
        help="å‡¦ç†ä¸Šé™éŠ˜æŸ„æ•°ï¼ˆ0 ä»¥ä¸‹ã§ç„¡åˆ¶é™ã€‚æ—¢å®š: è¨­å®šå€¤ rolling.max_symbolsï¼‰",
    )
    return parser


def main(argv: list[str] | None = None) -> int:
    logging.basicConfig(level=logging.INFO)
    parser = _build_parser()
    args = parser.parse_args(argv)

    settings = get_settings(create_dirs=True)
    cache_manager = CacheManager(settings)

    def _console_log(msg: str) -> None:
        print(msg, flush=True)

    stats = extract_rolling_from_full(
        cache_manager,
        symbols=args.symbols,
        target_days=args.target_days,
        max_symbols=args.max_symbols,
        log=_console_log,
    )

    if stats.errors:
        return 1
    return 0


if __name__ == "__main__":  # pragma: no cover - CLI entry point
    raise SystemExit(main())
