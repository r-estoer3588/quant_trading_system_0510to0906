from __future__ import annotations

import argparse
from collections.abc import Iterable
from concurrent.futures import ThreadPoolExecutor, as_completed
import csv
import queue
from dataclasses import dataclass
from datetime import datetime, timezone
import logging
import os
from pathlib import Path
import shutil
import sys
import time
import threading
from typing import TYPE_CHECKING, Literal

from dotenv import load_dotenv
import pandas as pd
import requests
from requests.adapters import HTTPAdapter

if TYPE_CHECKING:
    pass


def _migrate_root_csv_to_full() -> None:
    """ãƒ¬ã‚¬ã‚·ãƒ¼ãª CSV ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ ``CacheManager`` ã®æ§‹æˆã¸ç§»è¡Œã™ã‚‹ã€‚

    æ—§ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã¯ ``data_cache/`` ã‚„ ``data_cache_recent/`` ç›´ä¸‹ã«
    ã‚·ãƒ³ãƒœãƒ«ã”ã¨ã® CSV ã‚’é…ç½®ã—ã¦ã„ãŸã€‚ç¾åœ¨ã¯ ``CacheManager`` ã«ã‚ˆã‚Š
    ``data_cache/full_backup/`` ã¨ ``data_cache/base/`` ã«æ•´ç†ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€
    æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°ã“ã®é–¢æ•°ã§ç§»å‹•ã™ã‚‹ã€‚ç§»è¡Œã«å¤±æ•—ã—ã¦ã‚‚ãƒ­ã‚°ã‚’
    å‡ºåŠ›ã™ã‚‹ã®ã¿ã§å‡¦ç†ã‚’ç¶™ç¶šã™ã‚‹ã€‚
    """

    global DATA_CACHE_DIR, BASE_CACHE_DIR

    try:
        full_dir = cm.full_dir
        base_dir = BASE_CACHE_DIR
    except Exception:  # pragma: no cover - ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¤±æ•—æ™‚ã¯ç§»è¡Œä¸è¦
        return

    def _move_csv(src_dir: Path, dest_dir: Path) -> Path:
        if src_dir == dest_dir:
            return dest_dir
        dest_dir.mkdir(parents=True, exist_ok=True)
        for src in src_dir.glob("*.csv"):
            dest = dest_dir / src.name
            if dest.exists():
                continue
            try:
                src.rename(dest)
            except Exception:  # pragma: no cover - Windows ãªã©ã§ rename å¤±æ•—
                try:
                    shutil.move(str(src), str(dest))
                except Exception as e:  # pragma: no cover - logging only
                    logging.warning("ç§»è¡Œå¤±æ•—: %s -> %s (%s)", src, dest, e)
        return dest_dir

    DATA_CACHE_DIR = _move_csv(DATA_CACHE_DIR, full_dir)
    if LEGACY_RECENT_DIR is not None:
        BASE_CACHE_DIR = _move_csv(LEGACY_RECENT_DIR, base_dir)


# è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒªãƒã‚¸ãƒˆãƒª ãƒ«ãƒ¼ãƒˆï¼‰ã‚’ import ãƒ‘ã‚¹ã«è¿½åŠ ã—ã¦ã€
# ç›´ä¸‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« `indicators_common.py` ã‚’è§£æ±ºå¯èƒ½ã«ã™ã‚‹
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from indicators_common import add_indicators  # noqa: E402

from common.cache_manager import CacheManager, compute_base_indicators  # noqa: E402

try:
    from common.cache_manager import round_dataframe  # type: ignore # noqa: E402
except ImportError:  # pragma: no cover - tests may stub cache_manager

    def round_dataframe(df: pd.DataFrame, decimals: int | None) -> pd.DataFrame:
        if decimals is None:
            return df
        try:
            decimals_int = int(decimals)
        except Exception:
            return df
        try:
            return df.copy().round(decimals_int)
        except Exception:
            try:
                return df.round(decimals_int)
            except Exception:
                return df


from common.symbol_universe import build_symbol_universe  # noqa: E402
from common.symbols_manifest import save_symbol_manifest  # noqa: E402

CacheUpdateInterrupted: type[BaseException] | None
try:  # Local import guard for optional bulk updater
    from scripts.update_from_bulk_last_day import (
        run_bulk_update,
        CacheUpdateInterrupted as _CacheUpdateInterrupted,
    )
except Exception:  # pragma: no cover - unavailable in constrained envs
    run_bulk_update = None
    CacheUpdateInterrupted = None
else:
    CacheUpdateInterrupted = _CacheUpdateInterrupted


def _attempt_bulk_refresh(symbols: list[str] | None, progress_interval: int = 500):
    """Try to run the optional bulk updater if available.

    Returns whatever the bulk updater returns, or None if unavailable or on
    error. This mirrors the previous behavior expected by callers.
    """
    if run_bulk_update is None:
        return None
    try:
        # é€²æ—è¡¨ç¤ºç”¨ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
        def progress_callback(processed: int, total: int, updated: int) -> None:
            interval = max(1, int(progress_interval or 500))
            if processed % interval == 0 or processed == total:
                print(
                    f"ğŸ“Š Bulké€²æ—: {processed}/{total} éŠ˜æŸ„å‡¦ç†æ¸ˆã¿ (æ›´æ–°: {updated})",
                    flush=True,
                )

        # run_bulk_update expects a CacheManager instance as first arg
        # and accepts `universe=` for filtering by symbols
        print(
            f"ğŸš€ Bulkæ›´æ–°ã‚’é–‹å§‹ã—ã¾ã™: å¯¾è±¡={len(symbols) if symbols is not None else 'å…¨éŠ˜æŸ„'} "
            f"(é€²æ—è¡¨ç¤ºé–“éš”={progress_interval}ä»¶)",
            flush=True,
        )
        return run_bulk_update(
            cm,
            universe=symbols,
            fetch_universe=False,
            progress_callback=progress_callback,
        )
    except Exception as exc:  # pragma: no cover - defensive fallback
        if CacheUpdateInterrupted is not None and isinstance(exc, CacheUpdateInterrupted):
            raise
        return None


def _report_bulk_interrupt(exc: BaseException, total_symbols: int) -> None:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹ Bulk æ›´æ–°ã®ä¸­æ–­çŠ¶æ³ã‚’æ¨™æº–å‡ºåŠ›ã¸è¨˜éŒ²ã™ã‚‹ã€‚"""

    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    processed = 0
    updated = 0
    total_for_report = max(total_symbols, 0)
    if CacheUpdateInterrupted is not None and isinstance(exc, CacheUpdateInterrupted):
        processed = getattr(exc, "processed", 0)
        updated = getattr(exc, "updated", 0)
        total_for_report = max(total_for_report, processed)

    print("ğŸ›‘ Bulk æ›´æ–°ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚Šä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚", flush=True)
    summary = (
        f"   â†³ {timestamp} æ™‚ç‚¹ | å‡¦ç†æ¸ˆã¿: {processed}/{total_for_report} éŠ˜æŸ„ / "
        f"æ›´æ–°æ¸ˆã¿: {updated} éŠ˜æŸ„"
    )
    print(summary, flush=True)


BASE_SUBDIR_NAME = "base"

CACHE_ROUND_DECIMALS: int | None = None

# -----------------------------
# è¨­å®š/ç’°å¢ƒ
# -----------------------------

# .env ã‹ã‚‰ API ã‚­ãƒ¼ç­‰ã‚’å–ã‚Šè¾¼ã‚€ï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã® .envï¼‰
load_dotenv(dotenv_path=r".env")

try:
    from config.settings import get_settings

    _settings = get_settings(create_dirs=True)
    cm = CacheManager(_settings)
    LOG_DIR = Path(_settings.LOGS_DIR)
    DATA_CACHE_DIR = Path(_settings.DATA_CACHE_DIR)
    LEGACY_RECENT_DIR = Path(_settings.DATA_CACHE_RECENT_DIR)
    BASE_CACHE_DIR = Path(_settings.DATA_CACHE_DIR) / BASE_SUBDIR_NAME
    CACHE_ROUND_DECIMALS = getattr(_settings.cache, "round_decimals", None)
    THREADS_DEFAULT = int(_settings.THREADS_DEFAULT)
    REQUEST_TIMEOUT = int(_settings.REQUEST_TIMEOUT)
    DOWNLOAD_RETRIES = int(_settings.DOWNLOAD_RETRIES)
    API_THROTTLE_SECONDS = float(_settings.API_THROTTLE_SECONDS)
    API_BASE = str(_settings.API_EODHD_BASE).rstrip("/")
    API_KEY = _settings.EODHD_API_KEY or os.getenv("EODHD_API_KEY", "")
    ROUND_DECIMALS = getattr(_settings.cache, "round_decimals", None)
except Exception:
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆsettings ãŒèª­ã‚ãªã„å ´åˆï¼‰
    LOG_DIR = Path(os.path.dirname(__file__)) / "logs"
    DATA_CACHE_DIR = Path(os.path.dirname(__file__)) / ".." / "data_cache"
    LEGACY_RECENT_DIR = Path(os.path.dirname(__file__)) / ".." / "data_cache_recent"
    BASE_CACHE_DIR = Path(os.path.dirname(__file__)) / ".." / "data_cache" / BASE_SUBDIR_NAME
    THREADS_DEFAULT = int(os.getenv("THREADS_DEFAULT", 8))
    REQUEST_TIMEOUT = int(os.getenv("REQUEST_TIMEOUT", 10))
    DOWNLOAD_RETRIES = int(os.getenv("DOWNLOAD_RETRIES", 3))
    API_THROTTLE_SECONDS = float(os.getenv("API_THROTTLE_SECONDS", 1.5))
    API_BASE = os.getenv(
        "API_EODHD_BASE",
        "https://eodhistoricaldata.com",
    ).rstrip("/")
    API_KEY = os.getenv("EODHD_API_KEY", "")
    ROUND_DECIMALS = None

LOG_DIR.mkdir(parents=True, exist_ok=True)
DATA_CACHE_DIR = DATA_CACHE_DIR.resolve()
DATA_CACHE_DIR.mkdir(parents=True, exist_ok=True)
try:
    LEGACY_RECENT_DIR = LEGACY_RECENT_DIR.resolve()
except Exception:
    LEGACY_RECENT_DIR = None
BASE_CACHE_DIR = BASE_CACHE_DIR.resolve()
BASE_CACHE_DIR.mkdir(parents=True, exist_ok=True)
_session_lock = threading.Lock()
_requests_session: requests.Session | None = None


def _get_requests_session() -> requests.Session:
    global _requests_session
    if _requests_session is None:
        with _session_lock:
            if _requests_session is None:
                pool_size = max(4, int(THREADS_DEFAULT) * 2)
                session = requests.Session()
                adapter = HTTPAdapter(
                    pool_connections=pool_size,
                    pool_maxsize=pool_size,
                )
                session.mount("https://", adapter)
                session.mount("http://", adapter)
                _requests_session = session
    return _requests_session


# -----------------------------
# ãƒ­ã‚®ãƒ³ã‚°
# -----------------------------

logging.basicConfig(
    filename=str(LOG_DIR / "cache_log.txt"),
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)

if os.getenv("SKIP_CACHE_MIGRATION") != "1":
    _migrate_root_csv_to_full()

# -----------------------------
# ã‚¹ãƒ­ãƒƒãƒˆãƒªãƒ³ã‚°åˆ¶å¾¡
# -----------------------------


class _ThrottleController:
    """å…±æœ‰ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒã‚¿ã€‚
    ``configure`` ã§è¨­å®šã•ã‚ŒãŸå¾…æ©Ÿæ™‚é–“ã‚’å…ƒã« ``wait`` ãŒæ¬¡ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¾ã§
    ã®ã‚¹ãƒªãƒ¼ãƒ—æ™‚é–“ã‚’æ±ºå®šã™ã‚‹ã€‚429 ãŒè¿”ã£ãŸå ´åˆãªã©ã« ``backoff`` ã‚’å‘¼ã¶ã¨
    ä¸€æ™‚çš„ã«å¾…æ©Ÿæ™‚é–“ã‚’å»¶é•·ã™ã‚‹ã€‚
    """

    def __init__(self, throttle_seconds: float) -> None:
        self._lock = threading.Lock()
        self._delay = max(0.0, float(throttle_seconds))
        self._next_time = 0.0
        self._block_until = 0.0

    def configure(self, throttle_seconds: float, concurrency_scale: int = 1) -> float:
        """ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒã‚¿ã®é–“éš”ã‚’æ›´æ–°ã—ã€å®ŸåŠ¹é…å»¶ã‚’è¿”ã™ã€‚"""
        delay = max(0.0, float(throttle_seconds))
        scale = max(1, int(concurrency_scale))
        if delay > 0 and scale > 1:
            delay /= scale
        with self._lock:
            self._delay = delay
            now = time.monotonic()
            self._next_time = now
            if self._block_until < now:
                self._block_until = now
            return self._delay

    def wait(self) -> None:
        while True:
            with self._lock:
                delay = self._delay
                if delay <= 0:
                    return
                now = time.monotonic()
                block_wait = self._block_until - now
                if block_wait > 0:
                    wait = block_wait
                else:
                    wait = self._next_time - now
                if wait <= 0:
                    self._next_time = now + delay
                    return
            time.sleep(min(delay, wait))

    def backoff(self, seconds: float) -> None:
        """ãƒ¬ãƒ¼ãƒˆåˆ¶é™é•åæ™‚ã«ãƒãƒƒã‚¯ã‚ªãƒ•æ™‚é–“ã‚’è¨­å®šã—ã€ãƒ­ã‚°ã«è¨˜éŒ²ã™ã‚‹ã€‚"""
        if seconds <= 0:
            return
        with self._lock:
            target = time.monotonic() + float(seconds)
            if target > self._block_until:
                self._block_until = target
        logging.warning(f"ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒãƒƒã‚¯ã‚ªãƒ•: {seconds:.1f}ç§’å¾…æ©Ÿ")

    def current_delay(self) -> float:
        with self._lock:
            return self._delay


_throttle_controller = _ThrottleController(API_THROTTLE_SECONDS)


def _configure_api_throttle(
    concurrency_scale: int = 1, throttle_seconds: float | None = None
) -> float:
    """Fetch ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã«å¿œã˜ã¦ API ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’èª¿æ•´ã™ã‚‹ã€‚"""
    throttle = API_THROTTLE_SECONDS if throttle_seconds is None else throttle_seconds
    return _throttle_controller.configure(throttle, concurrency_scale)


def _throttle_api_call() -> None:
    """API å‘¼ã³å‡ºã—å‰ã«å…±æœ‰ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒã‚¿ã¸å¾…æ©Ÿã‚’æŒ‡ç¤ºã™ã‚‹ã€‚"""
    _throttle_controller.wait()


# -----------------------------
# ãƒ–ãƒ©ãƒƒã‚¯ãƒªã‚¹ãƒˆï¼ˆã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³: æœˆå˜ä½ï¼‰
# -----------------------------

FAILED_LIST_PATH = LOG_DIR / "eodhd_failed_symbols.csv"
LEGACY_FAILED_LIST = Path(__file__).resolve().parents[1] / "eodhd_failed_symbols.csv"


@dataclass
class FailedEntry:
    symbol: str
    last_failed_at: datetime  # å¤±æ•—æ—¥
    count: int = 1


def _parse_date(s: str) -> datetime:
    try:
        return datetime.fromisoformat(s)
    except Exception:
        try:
            return datetime.strptime(s, "%Y-%m-%d %H:%M:%S")
        except Exception:
            return datetime.now(timezone.utc)


def _migrate_legacy_failed_if_needed() -> None:
    """ãƒªãƒã‚¸ãƒˆãƒªç›´ä¸‹ã®æ—§ CSVï¼ˆã‚·ãƒ³ãƒœãƒ«ã®ã¿ï¼‰ã‚’ logs/ ã«ç§»è¡Œã™ã‚‹ã€‚
    æ—§å½¢å¼: 1åˆ—ï¼ˆsymbolï¼‰
    æ–°å½¢å¼: 3åˆ—ï¼ˆsymbol,last_failed_at,countï¼‰
    """
    symbols = []
    if LEGACY_FAILED_LIST.exists() and not FAILED_LIST_PATH.exists():
        try:
            with open(LEGACY_FAILED_LIST, encoding="utf-8") as f:
                for line in f:
                    s = line.strip()
                    if s:
                        symbols.append(s.upper())
        except Exception:
            pass

    now = datetime.now(timezone.utc).isoformat()
    FAILED_LIST_PATH.parent.mkdir(parents=True, exist_ok=True)
    try:
        with open(FAILED_LIST_PATH, "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow(["symbol", "last_failed_at", "count"])  # header
            for s in sorted(set(symbols)):
                writer.writerow([s, now, 1])
    except Exception:
        pass


def _load_failed_map() -> dict[str, FailedEntry]:
    """CSV ã‹ã‚‰å¤±æ•—æƒ…å ±ã‚’èª­ã¿è¾¼ã‚€ã€‚"""
    _migrate_legacy_failed_if_needed()
    entries: dict[str, FailedEntry] = {}
    if not FAILED_LIST_PATH.exists():
        return entries

    try:
        df = pd.read_csv(FAILED_LIST_PATH)
        # æ–°å½¢å¼ï¼ˆãƒ˜ãƒƒãƒ€ã‚ã‚Šï¼‰
        if set(df.columns.str.lower()) >= {"symbol", "last_failed_at"}:
            for _, row in df.iterrows():
                sym = str(row["symbol"]).upper().strip()
                if not sym:
                    continue
                last_dt = _parse_date(str(row["last_failed_at"]))
                cnt = int(row.get("count", 1) or 1)
                entries[sym] = FailedEntry(sym, last_dt, cnt)
            return entries
        # æ—§å½¢å¼ï¼ˆ1åˆ—ã®ã¿ï¼‰
        else:
            now = datetime.now(timezone.utc)
            for s in df.iloc[:, 0].astype(str).str.upper():
                s = s.strip()
                if s:
                    entries[s] = FailedEntry(s, now, 1)
            return entries
    except Exception:
        # CSV ãŒå£Šã‚Œã¦ã„ã‚‹ç­‰ã®å ´åˆã¯ç©ºæ‰±ã„
        return {}


def _save_failed_map(entries: dict[str, FailedEntry]) -> None:
    FAILED_LIST_PATH.parent.mkdir(parents=True, exist_ok=True)
    rows = []
    for e in entries.values():
        rows.append([e.symbol, e.last_failed_at.isoformat(), int(e.count)])
    rows.sort(key=lambda r: r[0])
    with open(FAILED_LIST_PATH, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["symbol", "last_failed_at", "count"])  # header
        writer.writerows(rows)


def load_monthly_blacklist() -> set[str]:
    """å½“æœˆã«å¤±æ•—ã—ãŸéŠ˜æŸ„ã‚’é›†åˆã§è¿”ã™ï¼ˆåŒä¸€æœˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰ã€‚"""
    m = _load_failed_map()
    now = datetime.now(timezone.utc)
    skip: set[str] = set()
    for sym, e in m.items():
        if e.last_failed_at.year == now.year and e.last_failed_at.month == now.month:
            skip.add(sym)
    return skip


def update_failed_symbols(failed: Iterable[str]) -> None:
    """å¤±æ•—éŠ˜æŸ„ã‚’æ›´æ–°ï¼ˆå½“æœˆã®å¤±æ•—æ—¥æ™‚ã‚’ä¸Šæ›¸ãã€å›æ•°ã‚’ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ãƒˆï¼‰ã€‚"""
    failed_set = {str(s).upper().strip() for s in failed if str(s).strip()}
    if not failed_set:
        return
    m = _load_failed_map()
    now = datetime.now(timezone.utc)
    for s in failed_set:
        if s in m:
            e = m[s]
            e.last_failed_at = now
            e.count = int(e.count) + 1
        else:
            m[s] = FailedEntry(s, now, 1)
    _save_failed_map(m)


def remove_recovered_symbols(succeeded: Iterable[str]) -> None:
    """æˆåŠŸã—ãŸéŠ˜æŸ„ã¯ãƒ–ãƒ©ãƒƒã‚¯ãƒªã‚¹ãƒˆã‹ã‚‰å‰Šé™¤ã€‚"""
    suc_set = {str(s).upper().strip() for s in succeeded if str(s).strip()}
    if not suc_set:
        return
    m = _load_failed_map()
    changed = False
    for s in list(suc_set):
        if s in m:
            del m[s]
            changed = True
    if changed:
        _save_failed_map(m)


# -----------------------------
# ãƒ‡ãƒ¼ã‚¿å–å¾—
# -----------------------------


def get_all_symbols() -> list[str]:
    try:
        symbols = build_symbol_universe(
            API_BASE,
            API_KEY,
            timeout=REQUEST_TIMEOUT,
            logger=logging.getLogger(__name__),
        )
    except Exception as exc:  # pragma: no cover - ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç•°å¸¸æ™‚ã¯ç©ºé›†åˆ
        logging.error("éŠ˜æŸ„ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ã®å–å¾—ã«å¤±æ•—: %s", exc)
        return []

    logging.info("NASDAQ/EODHD ãƒ•ã‚£ãƒ«ã‚¿å¾Œã®éŠ˜æŸ„æ•°: %s", len(symbols))
    return symbols


def get_with_retry(url: str, retries: int = DOWNLOAD_RETRIES, delay: float = 2.0):
    session = _get_requests_session()
    for i in range(max(1, retries)):
        sleep_for = delay
        try:
            _throttle_api_call()
            r = session.get(url, timeout=REQUEST_TIMEOUT)
            if r.status_code == 200:
                return r
            if r.status_code == 429:
                sleep_for = max(delay, API_THROTTLE_SECONDS) * (i + 1)
                logging.warning("429 Too Many Requests (%s/%s) - %s", i + 1, retries, url)
                _throttle_controller.backoff(sleep_for)
            else:
                logging.warning(f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {r.status_code} - {url}")
        except Exception as e:
            logging.warning(f"è©¦è¡Œ{i + 1}å›ç›®ã®ã‚¨ãƒ©ãƒ¼: {e}")
        if sleep_for > 0:
            time.sleep(sleep_for)
    return None


def get_eodhd_data(symbol: str) -> pd.DataFrame | None:
    # APIå‘¼ã³å‡ºã—ç”¨ã«å°æ–‡å­—å¤‰æ›ï¼ˆå†…éƒ¨ç®¡ç†ã¯å¤§æ–‡å­—ã®ã¾ã¾ï¼‰
    api_symbol = symbol.lower()
    url = f"{API_BASE}/api/eod/{api_symbol}.US?api_token={API_KEY}&period=d&fmt=json"
    r = get_with_retry(url)
    if r is None:
        return None
    try:
        data = r.json()
        if not isinstance(data, list) or len(data) == 0:
            logging.warning(f"{symbol}: ç©ºã¾ãŸã¯ç„¡åŠ¹ãªJSONå¿œç­”")
            return None
        df = pd.DataFrame(data)
        df["date"] = pd.to_datetime(df["date"])
        df = df.rename(
            columns={
                "date": "Date",
                "open": "Open",
                "high": "High",
                "low": "Low",
                "close": "Close",
                "adjusted_close": "AdjClose",
                "volume": "Volume",
            }
        )
        df.set_index("Date", inplace=True)
        df = df.sort_index()
        return df
    except Exception as e:
        logging.error(f"{symbol}: ãƒ‡ãƒ¼ã‚¿æ•´å½¢ä¸­ã®ã‚¨ãƒ©ãƒ¼ - {e}")
        return None


RESERVED_WORDS = {
    "CON",
    "PRN",
    "AUX",
    "NUL",
    "COM1",
    "COM2",
    "COM3",
    "COM4",
    "COM5",
    "COM6",
    "COM7",
    "COM8",
    "COM9",
    "LPT1",
    "LPT2",
    "LPT3",
    "LPT4",
    "LPT5",
    "LPT6",
    "LPT7",
    "LPT8",
    "LPT9",
}


@dataclass(slots=True)
class CacheResult:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥å‡¦ç†ã®çµæœã‚’æ ¼ç´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ã€‚"""

    symbol: str
    message: str
    used_api: bool
    success: bool


@dataclass(slots=True)
class CacheJob:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥å‡¦ç†ã‚¸ãƒ§ãƒ–ã‚’æ ¼ç´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ã€‚"""

    symbol: str
    safe_symbol: str
    filepath: Path
    basepath: Path | None
    df: pd.DataFrame | None
    mode: Literal["skip", "save_full", "rebuild_base", "error"]
    message: str
    used_api: bool
    success: bool

    def to_result(self) -> CacheResult:
        return CacheResult(self.symbol, self.message, self.used_api, self.success)


def safe_filename(symbol: str) -> str:
    # Windows äºˆç´„èªã‚’é¿ã‘ã‚‹ï¼ˆå¤§æ–‡å­—å°æ–‡å­—ç„¡è¦–ï¼‰
    if symbol.upper() in RESERVED_WORDS:
        return symbol + "_RESV"
    return symbol


def _prepare_cache_job(
    symbol: str,
    output_dir: Path,
    base_dir: Path | None = None,
) -> CacheJob:
    """æŒ‡å®šã‚·ãƒ³ãƒœãƒ«ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¸ãƒ§ãƒ–ã‚’æº–å‚™ã™ã‚‹ã€‚

    æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ãƒã‚§ãƒƒã‚¯ã¨APIå–å¾—ã®å¿…è¦æ€§ã‚’åˆ¤æ–­ã—ã€é©åˆ‡ãªãƒ¢ãƒ¼ãƒ‰ã‚’è¨­å®šã™ã‚‹ã€‚
    """
    output_dir = Path(output_dir)
    base_dir = Path(base_dir) if base_dir is not None else None
    safe_symbol = safe_filename(symbol)
    filepath = output_dir / f"{safe_symbol}.csv"
    basepath = base_dir / f"{safe_symbol}.csv" if base_dir else None

    today = datetime.today().date()
    if filepath.exists():
        mod_time = datetime.fromtimestamp(filepath.stat().st_mtime)
        if mod_time.date() == today:
            if basepath and not basepath.exists():
                existing_df = None
                try:
                    existing_df = pd.read_csv(filepath)
                    base_df = compute_base_indicators(existing_df)
                except Exception as exc:  # pragma: no cover - logging only
                    logging.warning(
                        "%s: æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®baseå†æ§‹ç¯‰ã«å¤±æ•— (%s)",
                        symbol,
                        exc,
                    )
                    base_df = None
                if base_df is not None and not base_df.empty:
                    return CacheJob(
                        symbol=symbol,
                        safe_symbol=safe_symbol,
                        filepath=filepath,
                        basepath=basepath,
                        df=existing_df,
                        mode="rebuild_base",
                        message=f"{symbol}: already cached",
                        used_api=False,
                        success=True,
                    )
            return CacheJob(
                symbol=symbol,
                safe_symbol=safe_symbol,
                filepath=filepath,
                basepath=basepath,
                df=None,
                mode="skip",
                message=f"{symbol}: already cached",
                used_api=False,
                success=True,
            )

    df = get_eodhd_data(symbol)
    if df is not None and not df.empty:
        return CacheJob(
            symbol=symbol,
            safe_symbol=safe_symbol,
            filepath=filepath,
            basepath=basepath,
            df=df,
            mode="save_full",
            message=f"{symbol}: saved",
            used_api=True,
            success=True,
        )
    return CacheJob(
        symbol=symbol,
        safe_symbol=safe_symbol,
        filepath=filepath,
        basepath=basepath,
        df=None,
        mode="error",
        message=f"{symbol}: failed to fetch",
        used_api=True,
        success=False,
    )


def _process_cache_job(job: CacheJob) -> CacheResult:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¸ãƒ§ãƒ–ã‚’å‡¦ç†ã—ã€çµæœã‚’è¿”ã™ã€‚

    ã‚¸ãƒ§ãƒ–ã®ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ã¦ã€ã‚¹ã‚­ãƒƒãƒ—ã€baseå†æ§‹ç¯‰ã€ãƒ•ãƒ«ä¿å­˜ã‚’å®Ÿè¡Œã™ã‚‹ã€‚
    """
    if job.mode in {"skip", "error"}:
        return job.to_result()

    if job.mode == "rebuild_base":
        if job.basepath is None or job.df is None:
            return job.to_result()
        try:
            job.basepath.parent.mkdir(parents=True, exist_ok=True)
            base_df = compute_base_indicators(job.df)
        except Exception as exc:  # pragma: no cover - logging only
            logging.warning("%s: baseè¨ˆç®—ã«å¤±æ•— (%s)", job.symbol, exc)
            return job.to_result()
        if base_df is not None and not base_df.empty:
            base_reset = round_dataframe(
                base_df.reset_index(),
                CACHE_ROUND_DECIMALS,
            )
            base_reset.to_csv(job.basepath, index=False)
        return CacheResult(job.symbol, job.message, job.used_api, True)

    # mode == "save_full"
    df = job.df
    if df is None or df.empty:
        msg = f"{job.symbol}: ä¿å­˜å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã—ãŸ"
        return CacheResult(job.symbol, msg, job.used_api, False)

    try:
        job.filepath.parent.mkdir(parents=True, exist_ok=True)
        try:
            full_df = add_indicators(df.copy())
        except Exception:
            full_df = add_indicators(df)
        df_reset = full_df.reset_index().rename(columns=str.lower)
        df_reset = round_dataframe(df_reset, CACHE_ROUND_DECIMALS)
        df_reset.to_csv(job.filepath, index=False)
    except Exception as exc:  # pragma: no cover - logging only
        logging.error("%s: ãƒ‡ãƒ¼ã‚¿ä¿å­˜ä¸­ã®ã‚¨ãƒ©ãƒ¼ (%s)", job.symbol, exc)
        return CacheResult(
            job.symbol,
            f"{job.symbol}: ä¿å­˜æ™‚ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ",
            job.used_api,
            False,
        )

    base_saved = False
    if job.basepath is not None:
        try:
            job.basepath.parent.mkdir(parents=True, exist_ok=True)
            base_df = compute_base_indicators(df)
        except Exception as exc:
            logging.warning("%s: baseè¨ˆç®—ã«å¤±æ•— (%s)", job.symbol, exc)
            base_df = None
        if base_df is not None and not base_df.empty:
            base_reset = round_dataframe(
                base_df.reset_index(),
                CACHE_ROUND_DECIMALS,
            )
            base_reset.to_csv(job.basepath, index=False)
            base_saved = True

    msg = job.message
    if base_saved and "base saved" not in msg:
        msg = f"{msg} (base saved)"
    return CacheResult(job.symbol, msg, job.used_api, True)


def cache_single(
    symbol: str,
    output_dir: Path,
    base_dir: Path | None = None,
    throttle_seconds: float | None = None,
) -> tuple[str, bool, bool]:
    """æŒ‡å®šã‚·ãƒ³ãƒœãƒ«ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã€‚

    æˆ»ã‚Šå€¤: (message, used_api, success)
    """
    _configure_api_throttle(1, throttle_seconds)
    job = _prepare_cache_job(symbol, output_dir, base_dir)
    result = _process_cache_job(job)
    return (result.message, result.used_api, result.success)


def cache_data(
    symbols: list[str],
    output_dir: Path | str = DATA_CACHE_DIR,
    base_dir: Path | None = BASE_CACHE_DIR,
    max_workers: int | None = None,
    fetch_workers: int | None = 1,
    save_workers: int | None = None,
    throttle_seconds: float | None = 0.0667,
    progress_interval: int = 600,
    heartbeat_seconds: int | None = 20,
) -> None:
    """æŒ‡å®šã‚·ãƒ³ãƒœãƒ«ãƒªã‚¹ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¸¦åˆ—ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹ã€‚

    APIå–å¾—ã¨ä¿å­˜/è¨ˆç®—ã‚’åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œã—ã€åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    if base_dir is not None:
        base_dir = Path(base_dir)
        base_dir.mkdir(parents=True, exist_ok=True)

    max_workers = int(max_workers or THREADS_DEFAULT)
    # APIå–å¾—ã¯å¸¸ã«é †æ¬¡å®Ÿè¡Œï¼ˆfetch_workers=1ï¼‰ã«å›ºå®šã—ã¾ã™ã€‚
    # ä¿å­˜/æŒ‡æ¨™è¨ˆç®—ã®ã¿ã‚’ä¸¦åˆ—åŒ–ã—ã¦ I/O ã‚’åŠ¹ç‡åŒ–ã—ã¾ã™ã€‚
    fetch_workers = 1
    if save_workers is None:
        save_workers = max_workers

    fetch_workers = max(1, int(fetch_workers))
    save_workers = max(1, int(save_workers))

    effective_throttle = _configure_api_throttle(fetch_workers, throttle_seconds)
    configured_throttle = 0.0667 if throttle_seconds is None else float(throttle_seconds)
    if effective_throttle > 0:
        print(
            f"â„¹ï¸ APIã‚¹ãƒ­ãƒƒãƒˆãƒªãƒ³ã‚°: è¨­å®šå€¤ {configured_throttle:.3f} ç§’ â†’ "
            f"å®ŸåŠ¹ {effective_throttle:.3f} ç§’/ãƒªã‚¯ã‚¨ã‚¹ãƒˆ (fetch workers={fetch_workers})",
            flush=True,
        )
    else:
        print(
            f"â„¹ï¸ APIã‚¹ãƒ­ãƒƒãƒˆãƒªãƒ³ã‚°ãªã— (fetch workers={fetch_workers})",
            flush=True,
        )

    # å½“æœˆãƒ–ãƒ©ãƒƒã‚¯ãƒªã‚¹ãƒˆã«è©²å½“ã™ã‚‹éŠ˜æŸ„ã‚’ã‚¹ã‚­ãƒƒãƒ—
    monthly_blacklist = load_monthly_blacklist()
    symbols_to_fetch = [s for s in symbols if s.upper() not in monthly_blacklist]
    skipped_due_to_cooldown = len(symbols) - len(symbols_to_fetch)

    failed: list[str] = []
    succeeded: list[str] = []
    results_list: list[tuple[str, str, bool]] = []
    completed_count = 0
    pending_writers = 0

    def handle_result(result: CacheResult) -> None:
        """çµæœã‚’å‡¦ç†ã—ã€çµ±è¨ˆã‚’æ›´æ–°ã™ã‚‹ã€‚"""
        nonlocal completed_count
        index = completed_count
        completed_count += 1
        results_list.append((result.symbol, result.message, result.used_api))
        logging.info(result.message)
        print(f"[{index}] {result.message}")
        # é€²æ—è¡¨ç¤º
        if progress_interval > 0 and completed_count % progress_interval == 0:
            total = len(symbols_to_fetch)
            print(
                f"ğŸ“Š é€²æ—: {completed_count}/{total} éŠ˜æŸ„å®Œäº† "
                f"({completed_count / total * 100:.1f}%)",
                flush=True,
            )
        if not result.success:
            failed.append(result.symbol)
        else:
            succeeded.append(result.symbol)

    def drain_results(block: bool = False) -> None:
        """çµæœã‚­ãƒ¥ãƒ¼ã‹ã‚‰çµæœã‚’å‡¦ç†ã™ã‚‹ã€‚ãƒ¡ãƒ¢ãƒªåˆ¶é™ä»˜ãã€‚"""
        nonlocal pending_writers
        if pending_writers <= 0:
            return
        timeout = 0.1 if block else 0
        while pending_writers > 0:
            try:
                result = results_queue.get(block=block, timeout=timeout)
            except queue.Empty:
                break
            pending_writers -= 1
            handle_result(result)

    # ä¿å­˜ãƒ»ã‚¤ãƒ³ã‚¸è¨ˆç®—ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œã—ã¦ API å–å¾—ã¨ã®é‡ãªã‚Šã‚’ç¢ºä¿ã™ã‚‹
    def writer_task(job: CacheJob) -> CacheResult:
        """ä¿å­˜ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ã€‚ã‚­ãƒ¥ãƒ¼æº€æ¯æ™‚ã¯å¾…æ©Ÿã€‚"""
        try:
            result = _process_cache_job(job)
        except Exception:  # pragma: no cover - logging only
            logging.exception("%s: ä¿å­˜å‡¦ç†ã§äºˆæœŸã›ã¬ä¾‹å¤–", job.symbol)
            result = CacheResult(
                job.symbol,
                f"{job.symbol}: ä¿å­˜å‡¦ç†ã§ä¾‹å¤–ãŒç™ºç”Ÿã—ã¾ã—ãŸ",
                job.used_api,
                False,
            )
        try:
            results_queue.put(result, timeout=10)  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
        except queue.Full:
            logging.error("%s: çµæœã‚­ãƒ¥ãƒ¼ãŒæº€æ¯ã®ãŸã‚å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—", job.symbol)
            # æº€æ¯æ™‚ã¯ç›´æ¥å‡¦ç†
            handle_result(result)
            nonlocal pending_writers
            pending_writers -= 1
        return result

    results_queue: queue.Queue[CacheResult] = queue.Queue(maxsize=1000)  # ãƒ¡ãƒ¢ãƒªåˆ¶é™: æœ€å¤§1000ä»¶

    print(
        f"ğŸš€ ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™: {len(symbols_to_fetch)} éŠ˜æŸ„ "
        f"(fetch_workers={fetch_workers} (sequential), save_workers={save_workers})",
        flush=True,
    )

    # å‹•ä½œæ–¹é‡ã®æ˜ç¤º: APIå–å¾—ã¯å¸¸ã«é †æ¬¡å®Ÿè¡Œ(fetch_workers=1)ã—ã€
    # CSVä¿å­˜ã¨æŒ‡æ¨™è¨ˆç®—ã®ã¿ã‚’ä¸¦åˆ—åŒ–ã—ã¦ I/O ã‚’åŠ¹ç‡åŒ–ã—ã¾ã™ã€‚
    print(
        "â„¹ï¸ å‹•ä½œæ–¹é‡: APIå–å¾—ã¯é †æ¬¡å®Ÿè¡Œ(fetch_workers=1)ã—ã€"
        "CSVä¿å­˜ã¨æŒ‡æ¨™è¨ˆç®—ã¯ä¸¦åˆ—åŒ–(save_workers)ã—ã¦åŠ¹ç‡åŒ–ã—ã¾ã™ã€‚",
        flush=True,
    )

    # ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰: ä¸€å®šç§’ã”ã¨ã«é€²æ—ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚
    stop_event = threading.Event()
    monitor_thread: threading.Thread | None = None
    try:
        hb = int(heartbeat_seconds) if heartbeat_seconds is not None else 0
    except Exception:
        hb = 0

    def _heartbeat_monitor() -> None:
        total = len(symbols_to_fetch)
        while not stop_event.wait(max(1, hb)):
            processed = completed_count
            pending = pending_writers
            pct = (processed / total * 100) if total else 0.0
            # ãƒ­ãƒ¼ã‚«ãƒ«æ™‚åˆ»ã§ãƒŸãƒªç§’ç²¾åº¦ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ä»˜ä¸ã—ã€
            # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«é¢¨ãƒ©ãƒ™ãƒ«ã‚’è§’æ‹¬å¼§ã§è¡¨ç¤ºã—ã¾ã™ã€‚
            # ä¾‹: 2025-09-23T12:34:56.789+09:00 [HEARTBEAT] â± é€²æ—: 12/100 (12.0%)
            now = datetime.now(timezone.utc).astimezone().isoformat(timespec="milliseconds")
            label = "[HEARTBEAT]"
            print(
                f"{now} {label} â± é€²æ—: {processed}/{total} éŠ˜æŸ„å®Œäº† "
                f"({pct:.1f}%) - pending_writers={pending}",
                flush=True,
            )

    if hb and hb > 0:
        monitor_thread = threading.Thread(target=_heartbeat_monitor, daemon=True)
        monitor_thread.start()

    # èµ·å‹•æ™‚ã«ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆè¨­å®šã‚’æ˜ç¤º
    if hb and hb > 0:
        print(f"â„¹ï¸ ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆè¨­å®š: {hb} ç§’ã”ã¨ã«é€²æ—ã‚’å‡ºåŠ›ã—ã¾ã™ï¼ˆ0ã§ç„¡åŠ¹åŒ–ï¼‰", flush=True)
    else:
        print(
            "â„¹ï¸ ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚è¡¨ç¤ºã¯é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ä¾å­˜ã«ãªã‚Šã¾ã™ã€‚",
            flush=True,
        )

    with ThreadPoolExecutor(max_workers=save_workers) as writer_executor:
        with ThreadPoolExecutor(max_workers=fetch_workers) as fetch_executor:
            future_to_symbol = {
                fetch_executor.submit(
                    _prepare_cache_job,
                    symbol,
                    output_dir,
                    base_dir,
                ): symbol
                for symbol in symbols_to_fetch
            }
            for future in as_completed(future_to_symbol):
                symbol = future_to_symbol[future]
                try:
                    job = future.result()
                except Exception:  # pragma: no cover - logging only
                    logging.exception("%s: å–å¾—å‡¦ç†ã§äºˆæœŸã›ã¬ä¾‹å¤–", symbol)
                    handle_result(
                        CacheResult(
                            symbol,
                            f"{symbol}: å–å¾—å‡¦ç†ã§ä¾‹å¤–ãŒç™ºç”Ÿã—ã¾ã—ãŸ",
                            True,
                            False,
                        )
                    )
                    continue
                if job.mode in {"save_full", "rebuild_base"}:
                    pending_writers += 1
                    writer_executor.submit(writer_task, job)
                else:
                    handle_result(job.to_result())
                drain_results()
        writer_executor.shutdown(wait=True)
        while pending_writers > 0:
            drain_results(block=True)
        drain_results()

    # ãƒ–ãƒ©ãƒƒã‚¯ãƒªã‚¹ãƒˆæ›´æ–°/å›å¾©å‰Šé™¤
    if failed:
        update_failed_symbols(failed)
    if succeeded:
        remove_recovered_symbols(succeeded)

    # çµ±è¨ˆã®å‡ºåŠ›
    cached_count = sum(1 for _, _, used_api in results_list if not used_api)
    api_count = sum(1 for _, _, used_api in results_list if used_api)
    print(
        f"âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¸ˆã¿: {cached_count}ä»¶, APIä½¿ç”¨: {api_count}ä»¶, "
        f"å¤±æ•—: {len(failed)}ä»¶, ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³é™¤å¤–: {skipped_due_to_cooldown}ä»¶"
    )
    # ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰ã¸çµ‚äº†ã‚’é€šçŸ¥ã—ã¦å®‰å…¨ã«åœæ­¢ã‚’å¾…ã¤ï¼ˆãƒ‡ãƒ¼ãƒ¢ãƒ³ã§ã‚ã‚‹ãŸã‚å¿…é ˆã§ã¯ãªã„ãŒæ˜ç¤ºï¼‰
    try:
        stop_event.set()
        if monitor_thread is not None:
            monitor_thread.join(timeout=1)
    except Exception:
        pass


def _cli_main() -> None:
    parser = argparse.ArgumentParser(description="EODHD ãƒ‡ã‚¤ãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½œæˆã™ã‚‹")
    parser.add_argument(
        "--chunk-size",
        type=int,
        default=None,
        help="æŒ‡å®šã—ãŸå ´åˆã€éŠ˜æŸ„ãƒªã‚¹ãƒˆã‚’ã“ã®ã‚µã‚¤ã‚ºã§åˆ†å‰²ã—ã¦å¯¾è±¡ãƒãƒ£ãƒ³ã‚¯ã®ã¿ã‚’å–å¾—ã™ã‚‹",
    )
    parser.add_argument(
        "--chunk-index",
        type=int,
        default=1,
        help="chunk-size ã¨ä½µç”¨ã€‚1 å§‹ã¾ã‚Šã§ä½•ç•ªç›®ã®ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†ã™ã‚‹ã‹ã‚’æŒ‡å®šã™ã‚‹",
    )
    parser.add_argument(
        "--max-workers",
        type=int,
        default=None,
        help="ThreadPoolExecutor ã®ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’ä¸Šæ›¸ãã™ã‚‹",
    )
    parser.add_argument(
        "--fetch-workers",
        type=int,
        default=1,
        help="APIå–å¾—ã‚¹ãƒ†ãƒ¼ã‚¸ã®ä¸¦åˆ—åº¦ã‚’æŒ‡å®šã™ã‚‹ (æ—¢å®š: 1ã€é †æ¬¡å®Ÿè¡Œã§ãƒ¬ãƒ¼ãƒˆåˆ¶é™éµå®ˆ)",
    )
    parser.add_argument(
        "--save-workers",
        type=int,
        default=None,
        help="ä¿å­˜/ã‚¤ãƒ³ã‚¸è¨ˆç®—ã‚¹ãƒ†ãƒ¼ã‚¸ã®ä¸¦åˆ—åº¦ã‚’æŒ‡å®šã™ã‚‹ (æ—¢å®š: max_workers)",
    )
    parser.add_argument(
        "--throttle-seconds",
        type=float,
        default=0.0667,
        help="API å‘¼ã³å‡ºã—é–“éš”ã‚’ç§’å˜ä½ã§ä¸Šæ›¸ãã™ã‚‹ (æ—¢å®š: 0.0667ç§’ã€ç´„15req/sec)",
    )
    parser.add_argument(
        "--full",
        action="store_true",
        help="å¼·åˆ¶çš„ã« full ã‹ã‚‰å†å–å¾— (bulk ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ãªã„)",
    )
    parser.add_argument(
        "--bulk-today",
        action="store_true",
        help="æœ¬æ—¥ã® Bulk æ›´æ–°ã‚’æ˜ç¤ºçš„ã«å®Ÿè¡Œã™ã‚‹ï¼ˆå¾“æ¥ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‹•ä½œã«ç›¸å½“ï¼‰",
    )
    parser.add_argument(
        "--skip-bulk",
        action="store_true",
        help="bulk æ›´æ–°ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ API ã‹ã‚‰å–å¾—ã™ã‚‹",
    )
    parser.add_argument(
        "--progress-interval",
        type=int,
        default=300,
        help="é€²æ—è¡¨ç¤ºã®é–“éš”ã‚’ä»¶æ•°ã§æŒ‡å®šã™ã‚‹ (æ—¢å®š: 300ä»¶)",
    )
    parser.add_argument(
        "--heartbeat-seconds",
        type=int,
        default=20,
        help="ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆç›£è¦–ã®é–“éš”ã‚’ç§’å˜ä½ã§æŒ‡å®šã™ã‚‹ (æ—¢å®š: 20ç§’)ã€‚0ã§ç„¡åŠ¹åŒ–",
    )
    # --parallel-fetch ã‚’å»ƒæ­¢: APIå–å¾—ã¯å¸¸ã«é †æ¬¡(fetch_workers=1)
    args = parser.parse_args()

    # å¤‰æ›´: å¼•æ•°ãŒä½•ã‚‚æŒ‡å®šã•ã‚Œãªã‹ã£ãŸå ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ãƒ•ãƒ«å–å¾—ã™ã‚‹ã€‚
    # ãŸã ã— `--bulk-today` ã‚’æ˜ç¤ºçš„ã«æŒ‡å®šã—ãŸå ´åˆã®ã¿ Bulk ã‚’å®Ÿè¡Œã™ã‚‹ã€‚
    if not args.full and not args.skip_bulk and not args.bulk_today:
        args.full = True

    # symbols = get_all_symbols()[:3]  # ç°¡æ˜“ãƒ†ã‚¹ãƒˆç”¨
    symbols = get_all_symbols()
    if not symbols:
        print("âš ï¸ å¯¾è±¡éŠ˜æŸ„ãŒæ¤œå‡ºã§ããªã‹ã£ãŸãŸã‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚", flush=True)
        return

    safe_symbols = [safe_filename(s) for s in symbols]
    try:
        save_symbol_manifest(safe_symbols, DATA_CACHE_DIR)
    except Exception as exc:  # pragma: no cover - logging only
        logging.warning("ã‚·ãƒ³ãƒœãƒ«ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆã®ä¿å­˜ã«å¤±æ•—: %s", exc)

    # å…¨ä½“ä»¶æ•°ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦ãŠãï¼ˆãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã‚„è¡¨ç¤ºã§ä½¿ç”¨ï¼‰
    total_symbols = len(symbols)

    fallback_to_full = bool(args.full)
    if not args.full and not args.skip_bulk:
        stats = None
        try:
            stats = _attempt_bulk_refresh(symbols, progress_interval=args.progress_interval)
        except BaseException as exc:  # noqa: BLE001 - ä¸­æ–­æ¤œçŸ¥ã®ãŸã‚
            if isinstance(exc, KeyboardInterrupt) or (
                CacheUpdateInterrupted is not None and isinstance(exc, CacheUpdateInterrupted)
            ):
                _report_bulk_interrupt(exc, total_symbols)
                return
            raise
        if stats is None:
            print(
                "âš ï¸ Bulk æ›´æ–°ãŒå®Ÿè¡Œã§ããªã‹ã£ãŸãŸã‚ API å†å–å¾—ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚",
                flush=True,
            )
            fallback_to_full = True
        elif not stats.has_payload:
            print(
                "â„¹ï¸ Bulk API ã®å¿œç­”ãŒç©ºã ã£ãŸãŸã‚è¿½åŠ æ›´æ–°ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚",
                flush=True,
            )
            return
        elif stats.filtered_rows == 0:
            print(
                "âš ï¸ Bulk ãƒ‡ãƒ¼ã‚¿ã«å‡¦ç†å¯¾è±¡éŠ˜æŸ„ãŒå­˜åœ¨ã—ãªã‹ã£ãŸãŸã‚ "
                "API å†å–å¾—ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚",
                flush=True,
            )
            fallback_to_full = True
        else:
            print(
                (
                    f"âœ… Bulkæ›´æ–°å®Œäº†: å¯¾è±¡={stats.processed_symbols} éŠ˜æŸ„ / "
                    f"æ›´æ–°={stats.updated_symbols} éŠ˜æŸ„ (ãƒ•ã‚£ãƒ«ã‚¿å¾Œ {stats.filtered_rows} è¡Œ)"
                ),
                flush=True,
            )
            if stats.universe_error:
                msg = stats.universe_error_message or "ç†ç”±ä¸æ˜"
                print(
                    "âš ï¸ éŠ˜æŸ„ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹å–å¾—ã«å•é¡ŒãŒã‚ã£ãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™:",
                    msg,
                    flush=True,
                )
            if stats.updated_symbols == 0:
                print("â„¹ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯æ—¢ã«æœ€æ–°ã®ãŸã‚è¿½åŠ å–å¾—ã¯ä¸è¦ã§ã™ã€‚", flush=True)
            return

    if fallback_to_full or args.full or args.skip_bulk:
        if args.skip_bulk and not args.full:
            print("â„¹ï¸ --skip-bulk æŒ‡å®šã®ãŸã‚ API ã‹ã‚‰ã®å†å–å¾—ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚", flush=True)

        # chunk_sizeé©ç”¨
        if args.chunk_size:
            chunk_size = max(1, args.chunk_size)
            chunk_index = max(1, args.chunk_index)
            start = chunk_size * (chunk_index - 1)
            if start >= total_symbols:
                print(
                    f"âš ï¸ ãƒãƒ£ãƒ³ã‚¯é–‹å§‹ä½ç½® {start + 1} ãŒéŠ˜æŸ„æ•° {total_symbols} ã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚"
                    "å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
                )
                return
            end = min(total_symbols, start + chunk_size)
            symbols = symbols[start:end]
            print(
                f"{total_symbols}éŠ˜æŸ„ä¸­ {start + 1}ã€œ{end} ä»¶ç›® (è¨ˆ {len(symbols)} éŠ˜æŸ„) ã‚’"
                f"å–å¾—ã—ã¾ã™ï¼ˆãƒãƒ£ãƒ³ã‚¯ {chunk_index}ã€ã‚µã‚¤ã‚º {chunk_size}ï¼‰ã€‚"
            )
        else:
            print(f"{len(symbols)}éŠ˜æŸ„ã‚’å–å¾—ã—ã¾ã™ï¼ˆã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³æœˆæ¬¡ãƒ–ãƒ©ãƒƒã‚¯ãƒªã‚¹ãƒˆé©ç”¨å¾Œã«é™¤å¤–ï¼‰")

        cache_data(
            symbols,
            output_dir=DATA_CACHE_DIR,
            base_dir=BASE_CACHE_DIR,
            max_workers=args.max_workers,
            fetch_workers=args.fetch_workers,
            save_workers=args.save_workers,
            throttle_seconds=args.throttle_seconds,
            progress_interval=args.progress_interval,
            heartbeat_seconds=args.heartbeat_seconds,
        )
        print("ãƒ‡ãƒ¼ã‚¿ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå®Œäº†ã—ã¾ã—ãŸã€‚", flush=True)

    # chunk_sizeãƒ–ãƒ­ãƒƒã‚¯ã‚’å‰Šé™¤ï¼ˆä¸Šè¨˜ã«çµ±åˆï¼‰
    # if args.chunk_size: ...


if __name__ == "__main__":
    _cli_main()
