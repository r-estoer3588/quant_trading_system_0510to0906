from __future__ import annotations

import atexit
import json
import logging
import os
import shutil
import threading
from collections import defaultdict
from collections.abc import Iterable
from pathlib import Path
from typing import Any, ClassVar, cast

import numpy as np
import pandas as pd

from common.utils import describe_dtype, safe_filename
from config.settings import Settings, get_settings
from indicators_common import add_indicators

logger = logging.getLogger(__name__)

BASE_SUBDIR = "base"


class _RollingIssueAggregator:
    """
    rolling cacheæœªæ•´å‚™ãƒ­ã‚°ã‚’é›†ç´„ã—ã€å†—é•·å‡ºåŠ›ã‚’åˆ¶å¾¡ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚

    ç’°å¢ƒå¤‰æ•°:
    - COMPACT_TODAY_LOGS=1: é›†ç´„æ©Ÿèƒ½æœ‰åŠ¹åŒ–
    - ROLLING_ISSUES_VERBOSE_HEAD=N: å…ˆé ­Nä»¶ã®ã¿è©³ç´°WARNINGã€ä»¥é™ã¯DEBUG
    """

    _instance = None
    _lock = threading.Lock()

    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        if self._initialized:
            return

        self.compact_mode = os.getenv("COMPACT_TODAY_LOGS", "0") == "1"
        self.verbose_head = int(os.getenv("ROLLING_ISSUES_VERBOSE_HEAD", "20"))
        self.issues = defaultdict(list)  # category -> [symbols]
        self.warning_count = 0
        self.logger = logging.getLogger(__name__)
        self._initialized = True

        if self.compact_mode:
            # ãƒ—ãƒ­ã‚»ã‚¹çµ‚äº†æ™‚ã«ã‚µãƒãƒªãƒ¼ã‚’å‡ºåŠ›
            atexit.register(self._output_summary)

    def report_issue(self, category: str, symbol: str, message: str = "") -> None:
        """
        rolling cache ã®æœªæ•´å‚™å•é¡Œã‚’å ±å‘Šã™ã‚‹ã€‚

        Args:
            category: å•é¡Œã‚«ãƒ†ã‚´ãƒªï¼ˆä¾‹: "missing_rolling", "insufficient_data"ï¼‰
            symbol: å¯¾è±¡ã‚·ãƒ³ãƒœãƒ«
            message: è¿½åŠ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆçœç•¥å¯ï¼‰
        """
        if not self.compact_mode:
            # å¾“æ¥é€šã‚Šã®å€‹åˆ¥WARNING
            full_msg = f"[{category}] {symbol}"
            if message:
                full_msg += f": {message}"
            self.logger.warning(full_msg)
            return

        # é›†ç´„ãƒ¢ãƒ¼ãƒ‰
        self.issues[category].append(symbol)
        self.warning_count += 1

        # å…ˆé ­Nä»¶ã®ã¿è©³ç´°WARNING
        if len(self.issues[category]) <= self.verbose_head:
            full_msg = f"[{category}] {symbol}"
            if message:
                full_msg += f": {message}"
            self.logger.warning(full_msg)
        else:
            # Nä»¶ã‚’è¶…ãˆãŸã‚‰DEBUGãƒ¬ãƒ™ãƒ«
            full_msg = f"[{category}] {symbol}"
            if message:
                full_msg += f": {message}"
            self.logger.debug(full_msg)

    def _output_summary(self) -> None:
        """ãƒ—ãƒ­ã‚»ã‚¹çµ‚äº†æ™‚ã«ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚µãƒãƒªã‚’å‡ºåŠ›ã™ã‚‹ã€‚"""
        if not self.issues:
            return

        self.logger.info("=== Rolling Cache Issues Summary ===")
        total_issues = sum(len(symbols) for symbols in self.issues.values())
        self.logger.info(f"Total issues reported: {total_issues}")

        for category, symbols in self.issues.items():
            unique_symbols = list(set(symbols))  # é‡è¤‡é™¤å»
            count = len(unique_symbols)

            if count <= 10:
                symbol_list = ", ".join(unique_symbols)
                self.logger.info(f"[{category}]: {count} symbols - {symbol_list}")
            else:
                sample = ", ".join(unique_symbols[:5])
                self.logger.info(
                    f"[{category}]: {count} symbols - {sample} ... (+{count-5} more)"
                )


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_rolling_issue_aggregator = _RollingIssueAggregator()


def report_rolling_issue(category: str, symbol: str, message: str = "") -> None:
    """
    rolling cache ã®æœªæ•´å‚™å•é¡Œã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¢ã‚°ãƒªã‚²ãƒ¼ã‚¿ãƒ¼ã«å ±å‘Šã™ã‚‹ã€‚

    Args:
        category: å•é¡Œã‚«ãƒ†ã‚´ãƒªï¼ˆä¾‹: "missing_rolling", "insufficient_data"ï¼‰
        symbol: å¯¾è±¡ã‚·ãƒ³ãƒœãƒ«
        message: è¿½åŠ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆçœç•¥å¯ï¼‰
    """
    _rolling_issue_aggregator.report_issue(category, symbol, message)


def round_dataframe(df: pd.DataFrame, decimals: int | None) -> pd.DataFrame:
    """Return a DataFrame rounded to the requested number of decimals.

    pandas.DataFrame.round ã¯æ•°å€¤åˆ—ã®ã¿ã‚’å¯¾è±¡ã¨ã—ã€æ—¥ä»˜ã‚„æ–‡å­—åˆ—åˆ—ã«ã¯å½±éŸ¿ã—ãªã„ã€‚
    ãŸã ã— ``decimals`` ãŒä¸æ­£å€¤ã®å ´åˆã‚„ä¸¸ã‚å‡¦ç†ãŒä¾‹å¤–ã‚’é€å‡ºã—ãŸå ´åˆã¯ã€
    å…ƒã® DataFrame ã‚’ãã®ã¾ã¾è¿”ã™ã€‚
    """
    if df is None or decimals is None:
        return df

    try:
        decimals_int = int(decimals)
    except (ValueError, TypeError):
        return df

    # Define category-specific rounding by column name (lowercase)
    price_atr_cols = {
        "open",
        "close",
        "high",
        "low",
        "atr10",
        "atr14",
        "atr20",
        "atr40",
        "atr50",
        "adjusted_close",
        "adjclose",
        "adj_close",
    }
    volume_cols = {"volume", "dollarvolume20", "dollarvolume50", "avgvolume50"}
    oscillator_cols = {"rsi3", "rsi4", "rsi14", "adx7"}
    pct_cols = {
        "roc200",
        "return_3d",
        "return_6d",
        "atr_ratio",
        "atr_pct",
        "hv50",
        "return_pct",
    }

    out = df.copy()
    lc_map = {c.lower(): c for c in out.columns}

    def _safe_round(series: pd.Series, ndigits: int) -> pd.Series:
        try:
            return pd.to_numeric(series, errors="coerce").round(ndigits)
        except Exception:
            return series

    # Group and apply rounding
    groups = {
        2: price_atr_cols | oscillator_cols,
        4: pct_cols,
    }
    rounded_cols = set()

    for ndigits, names in groups.items():
        cols_to_round = [lc_map[lname] for lname in names if lname in lc_map]
        for col in cols_to_round:
            out[col] = _safe_round(out[col], ndigits)
        rounded_cols.update(cols_to_round)

    # Volume-like columns
    vol_cols_to_round = [lc_map[lname] for lname in volume_cols if lname in lc_map]
    for col in vol_cols_to_round:
        try:
            s = pd.to_numeric(out[col], errors="coerce").round(0)
            out[col] = s.astype("Int64")
        except Exception:
            out[col] = _safe_round(out[col], 0)
    rounded_cols.update(vol_cols_to_round)

    # Remaining numeric columns
    for col in out.columns:
        if col not in rounded_cols and pd.api.types.is_numeric_dtype(out[col]):
            out[col] = _safe_round(out[col], decimals_int)

    return out


def make_csv_formatters(
    frame: pd.DataFrame, dec_point: str = ".", thous_sep: str | None = None
) -> dict:
    """Create a pandas.to_csv formatters dict honoring decimal point and thousands sep.

    Returns: dict mapping column name -> callable
    """
    lc_map = {c.lower(): c for c in frame.columns}
    fmt: dict = {}

    def _add_thousands_sep(int_str: str, sep: str) -> str:
        neg = int_str.startswith("-")
        prefix = "-" if neg else ""
        num_str = int_str[1:] if neg else int_str
        parts = []
        while num_str:
            parts.append(num_str[-3:])
            num_str = num_str[:-3]
        return prefix + sep.join(reversed(parts))

    def _num_formatter(nd: int):
        def _f(x):
            if pd.isna(x):
                return ""
            try:
                s = f"{float(x):.{nd}f}"
                if thous_sep:
                    int_part, _, frac_part = s.partition(".")
                    int_part = _add_thousands_sep(int_part, thous_sep)
                    s = f"{int_part}.{frac_part}" if frac_part else int_part
                return s.replace(".", dec_point) if dec_point != "." else s
            except (ValueError, TypeError):
                return str(x)

        return _f

    def _int_formatter():
        def _f(x):
            if pd.isna(x):
                return ""
            try:
                s = f"{int(round(float(x))):d}"
                return _add_thousands_sep(s, thous_sep) if thous_sep else s
            except (ValueError, TypeError):
                return str(x)

        return _f

    # Define formatting rules
    rules = {
        _num_formatter(2): [
            "open",
            "close",
            "high",
            "low",
            "atr10",
            "atr14",
            "atr20",
            "atr40",
            "atr50",
            "rsi3",
            "rsi4",
            "rsi14",
            "adx7",
        ],
        _num_formatter(4): [
            "roc200",
            "return_3d",
            "return_6d",
            "atr_ratio",
            "atr_pct",
            "hv50",
        ],
        _int_formatter(): ["volume", "dollarvolume20", "dollarvolume50", "avgvolume50"],
    }

    for formatter, names in rules.items():
        for name in names:
            if name in lc_map:
                fmt[lc_map[name]] = formatter
    return fmt


def _write_dataframe_to_csv(df: pd.DataFrame, path: Path, settings: Settings) -> None:
    """Utility to write a DataFrame to CSV with formatting."""
    # Be defensive: tests may supply a SimpleNamespace for settings lacking
    # nested attributes (e.g. settings.cache.csv). Use getattr fallbacks and
    # sensible defaults to avoid raising AttributeError here.
    try:
        cache_cfg = getattr(settings, "cache", None)
        csv_cfg = getattr(cache_cfg, "csv", None)
        dec_point = getattr(csv_cfg, "decimal_point", ".")
        thous_sep = getattr(csv_cfg, "thousands_sep", None)
        sep = getattr(csv_cfg, "field_sep", ",")

        fmt_map = make_csv_formatters(df, dec_point=dec_point, thous_sep=thous_sep)

        if fmt_map:
            df_out = df.copy()
            for col, func in fmt_map.items():
                if col in df_out.columns:
                    try:
                        df_out[col] = df_out[col].apply(func)
                    except Exception:
                        df_out[col] = df_out[col].astype(str)
            df_out.to_csv(path, index=False, sep=sep)
        else:
            df.to_csv(path, index=False, decimal=dec_point, sep=sep)
    except Exception as e:
        logger.error(f"Failed to write formatted CSV {path.name}: {e}")
        try:
            df.to_csv(path, index=False)
        except Exception as e2:
            logger.error(f"Failed to write CSV fallback {path.name}: {e2}")


# å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯ã§å‚ç…§ã™ã‚‹ä¸»è¦æŒ‡æ¨™åˆ—ï¼ˆèª­ã¿è¾¼ã¿å¾Œã¯å°æ–‡å­—åŒ–ã•ã‚Œã‚‹ï¼‰
MAIN_INDICATOR_COLUMNS = (
    "open",
    "high",
    "low",
    "close",
    "volume",
    "sma25",
    "sma50",
    "sma100",
    "sma150",
    "sma200",
    "ema20",
    "ema50",
    "atr10",
    "atr14",
    "atr20",
    "atr40",
    "atr50",
    "adx7",
    "rsi3",
    "rsi4",
    "rsi14",
    "roc200",
    "hv50",
    "dollarvolume20",
    "dollarvolume50",
    "avgvolume50",
    "return_3d",
    "return_6d",
    "return_pct",
    "drop3d",
    "atr_ratio",
    "atr_pct",
)

# å„æŒ‡æ¨™åˆ—ãŒæœ‰åŠ¹å€¤ã‚’æŒã¤ãŸã‚ã«æœ€ä½é™å¿…è¦ã¨ã™ã‚‹è¦³æ¸¬æ—¥æ•°ã®ç›®å®‰
_INDICATOR_MIN_OBSERVATIONS: dict[str, int] = {
    "sma25": 20,
    "sma50": 50,
    "sma100": 100,
    "sma150": 150,
    "sma200": 200,
    "ema20": 1,
    "ema50": 1,
    "atr10": 11,
    "atr14": 15,
    "atr20": 21,
    "atr40": 41,
    "atr50": 51,
    "adx7": 14,
    "rsi3": 3,
    "rsi4": 4,
    "rsi14": 14,
    "roc200": 201,
    "hv50": 51,
    "dollarvolume20": 20,
    "dollarvolume50": 50,
    "avgvolume50": 50,
    "return_3d": 4,
    "return_6d": 7,
    "return_pct": 2,
    "drop3d": 4,
    "atr_ratio": 11,
    "atr_pct": 11,
}


class CacheManager:
    """
    äºŒå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ï¼ˆfull / rollingï¼‰ã€‚
    - æ—¢å­˜ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ(csv/parquet)ã¯è‡ªå‹•æ¤œå‡ºãƒ»è¸è¥²
    - system5/6ã‚¹ã‚¿ã‚¤ãƒ«ã®ã‚³ãƒ¡ãƒ³ãƒˆãƒ»é€²æ—ãƒ­ã‚°ç²’åº¦ã‚’è¸è¥²
    """

    _GLOBAL_WARNED: ClassVar[set[tuple[str, str, str]]] = set()

    def __init__(self, settings: Settings):
        self.settings = settings
        self.full_dir = Path(settings.cache.full_dir)
        self.rolling_dir = Path(settings.cache.rolling_dir)
        self.rolling_cfg = settings.cache.rolling
        self.file_format = getattr(settings.cache, "file_format", "auto")
        self.rolling_meta_path = self.rolling_dir / self.rolling_cfg.meta_file
        self.full_dir.mkdir(parents=True, exist_ok=True)
        self.rolling_dir.mkdir(parents=True, exist_ok=True)
        self._ui_prefix = "[CacheManager]"
        self._warned = self._GLOBAL_WARNED

    def _warn_once(
        self, ticker: str, profile: str, category: str, message: str
    ) -> None:
        key = (ticker, profile, category)
        if key in self._warned:
            return
        self._warned.add(key)
        logger.warning(message)

    def _recompute_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """Recalculate derived indicator columns when base OHLC data is updated."""
        if df is None or df.empty or "date" not in df.columns:
            return df

        required = {"open", "high", "low", "close"}
        if not required.issubset(set(df.columns)):
            return df

        base = df.copy()
        base["date"] = pd.to_datetime(base["date"], errors="coerce")
        base = base.dropna(subset=["date"]).sort_values("date").reset_index(drop=True)
        if base.empty:
            return df

        for col in ("open", "high", "low", "close", "volume"):
            if col in base.columns:
                base[col] = pd.to_numeric(base[col], errors="coerce")

        case_map = {
            "open": "Open",
            "high": "High",
            "low": "Low",
            "close": "Close",
            "volume": "Volume",
        }
        base_renamed = base.rename(
            columns={k: v for k, v in case_map.items() if k in base.columns}
        )
        base_renamed["Date"] = base_renamed["date"]

        try:
            enriched = add_indicators(base_renamed)
            enriched = enriched.drop(columns=["Date"], errors="ignore")
            # æŒ‡æ¨™åˆ—ã‚’æ¨™æº–åŒ–ï¼ˆå¤§æ–‡å­—çµ±ä¸€ï¼‰ã€ãã®ä»–ã¯å°æ–‡å­—åŒ–
            enriched = standardize_indicator_columns(enriched)
            # åŸºæœ¬åˆ—ï¼ˆdate, open, highç­‰ï¼‰ã®ã¿å°æ–‡å­—ã«å¤‰æ›
            basic_cols = {"open", "high", "low", "close", "volume", "date"}
            enriched.columns = [
                c.lower() if c.lower() in basic_cols else c for c in enriched.columns
            ]
            enriched["date"] = pd.to_datetime(
                enriched.get("date", base["date"]), errors="coerce"
            )

            # Overwrite indicator columns with freshly computed values while
            # preserving original OHLCV and date columns. This ensures appended
            # rows receive correct indicator values and existing indicators are
            # consistent with the latest OHLC history.
            combined = df.copy()
            ohlcv = {"open", "high", "low", "close", "volume"}
            for col, series in enriched.items():
                if col == "date":
                    # ensure date column exists and normalized
                    combined["date"] = series
                    continue
                if col in ohlcv:
                    # keep original OHLCV from df
                    continue
                # replace or create indicator columns from enriched
                combined[col] = series

            # drop any duplicated columns just in case
            return combined.loc[:, ~combined.columns.duplicated(keep="first")]
        except Exception as e:
            logger.error(f"Failed to recompute indicators: {e}")
            return df

    def _detect_path(self, base_dir: Path, ticker: str) -> Path:
        """Detects the cache file path, defaulting based on settings."""
        for ext in [".csv", ".parquet", ".feather"]:
            if (p := base_dir / f"{ticker}{ext}").exists():
                return p

        fmt = (self.file_format or "auto").lower()
        if fmt == "parquet":
            return base_dir / f"{ticker}.parquet"
        if fmt == "feather":
            return base_dir / f"{ticker}.feather"
        return base_dir / f"{ticker}.csv"

    def _read_with_fallback(
        self, path: Path, ticker: str, profile: str
    ) -> pd.DataFrame | None:
        """Reads a file with specific logic for different formats and fallbacks."""
        if not path.exists():
            return None

        try:
            if path.suffix == ".feather":
                return pd.read_feather(path)
            if path.suffix == ".parquet":
                return pd.read_parquet(path)
            if path.suffix == ".csv":
                try:
                    return pd.read_csv(path, parse_dates=["date"])
                except ValueError as e:
                    if "Missing column provided to 'parse_dates': 'date'" in str(e):
                        df = pd.read_csv(path)
                        if "Date" in df.columns:
                            df = df.rename(columns={"Date": "date"})
                            df["date"] = pd.to_datetime(df["date"], errors="coerce")
                        return df
                    raise
            return None
        except Exception as e:
            msg = f"{self._ui_prefix} èª­ã¿è¾¼ã¿å¤±æ•—: {path.name} ({e})"
            self._warn_once(ticker, profile, f"read_error:{path.name}", msg)
            # Try CSV as a last resort if another format failed
            if path.suffix != ".csv":
                csv_path = path.with_suffix(".csv")
                if csv_path.exists():
                    return self._read_with_fallback(csv_path, ticker, profile)
            return None

    def read(self, ticker: str, profile: str) -> pd.DataFrame | None:
        """Reads data from the cache, handling different formats and normalization."""
        base = self.full_dir if profile == "full" else self.rolling_dir
        path = self._detect_path(base, ticker)

        df = self._read_with_fallback(path, ticker, profile)
        if df is None:
            # rolling cacheãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯é›†ç´„ãƒ­ã‚°ã«å ±å‘Š
            if profile == "rolling":
                report_rolling_issue(
                    "missing_rolling", ticker, "rolling cache not found"
                )
            return None

        # Normalize columns
        df.columns = [c.lower() for c in df.columns]
        if df.columns.has_duplicates:
            df = df.loc[:, ~df.columns.duplicated(keep="first")]

        if "date" in df.columns:
            df["date"] = pd.to_datetime(df["date"], errors="coerce")
            df = (
                df.dropna(subset=["date"])
                .sort_values("date")
                .drop_duplicates("date")
                .reset_index(drop=True)
            )

        # æŒ‡æ¨™åˆ—ã‚’å¤§æ–‡å­—ã«æ¨™æº–åŒ–ï¼ˆæ–°æ©Ÿèƒ½ï¼‰
        df = standardize_indicator_columns(df)

        self._perform_health_check(df, ticker, profile)
        return df

    def write_atomic(self, df: pd.DataFrame, ticker: str, profile: str) -> None:
        """Atomically writes a DataFrame to the cache."""
        base = self.full_dir if profile == "full" else self.rolling_dir
        base.mkdir(parents=True, exist_ok=True)
        path = self._detect_path(base, ticker)
        tmp_path = path.with_suffix(path.suffix + ".tmp")

        try:
            # settings may be a SimpleNamespace in tests; use getattr fallbacks
            if profile == "rolling":
                round_dec = getattr(
                    getattr(self, "rolling_cfg", None), "round_decimals", None
                )
            else:
                # Prefer nested settings.cache.round_decimals when available
                round_dec = None
                try:
                    round_dec = getattr(self.settings.cache, "round_decimals", None)
                except Exception:
                    try:
                        round_dec = getattr(self.settings, "round_decimals", None)
                    except Exception:
                        round_dec = None
            df_to_write = round_dataframe(df, round_dec)

            if path.suffix == ".parquet":
                df_to_write.to_parquet(tmp_path, index=False)
            elif path.suffix == ".feather":
                df_to_write.reset_index(drop=True).to_feather(tmp_path)
            else:  # .csv
                _write_dataframe_to_csv(df_to_write, tmp_path, self.settings)

            shutil.move(tmp_path, path)
        finally:
            if os.path.exists(tmp_path):
                try:
                    os.remove(tmp_path)
                except OSError as e:
                    logger.error(f"Failed to remove temporary file {tmp_path}: {e}")

    def _check_nan_rates(self, df: pd.DataFrame, ticker: str, profile: str):
        """Checks for high NaN rates in key indicator columns."""
        try:
            if df is None or df.empty:
                return

            # Build list of indicator columns present in df
            cols = [c for c in df.columns if c.lower() in _INDICATOR_MIN_OBSERVATIONS]
            if not cols:
                return

            # For each column compute a NaN ratio over a recent window but
            # exclude the initial warm-up rows where the indicator cannot exist.
            recent_window = min(len(df), 120)
            warnings: list[tuple[str, float]] = []
            for col in cols:
                try:
                    series = pd.to_numeric(df[col], errors="coerce").reset_index(
                        drop=True
                    )
                except Exception:
                    continue

                lookback = int(_INDICATOR_MIN_OBSERVATIONS.get(col.lower(), 0))

                # If series is shorter than lookback, skip â€” indicator not applicable
                # This handles newly listed stocks where NaN is expected
                if lookback and len(series) <= lookback:
                    continue

                # If the entire series is NaN, warn (indicator missing entirely)
                # But only after confirming we have sufficient data length
                if series.isna().all():
                    warnings.append((col, 1.0))
                    continue

                # Consider only the recent portion where values should be populated
                eval_series = series.tail(recent_window)
                try:
                    nan_ratio = float(eval_series.isna().mean())
                except Exception:
                    nan_ratio = 1.0

                if nan_ratio >= 0.99:
                    warnings.append((col, nan_ratio))

            if warnings:
                # Log a single warning summarizing affected columns
                parts = ", ".join(f"{c}:{r:.2%}" for c, r in warnings)
                msg = f"{self._ui_prefix} âš ï¸ {ticker} {profile} cache: NaNç‡é«˜ ({parts})"
                self._warn_once(ticker, profile, f"nan_rate:{parts}", msg)
        except Exception as e:
            logger.error(f"{self._ui_prefix} NaNç‡ãƒã‚§ãƒƒã‚¯å¤±æ•—: {e}")

    def _check_column_dtypes(self, df: pd.DataFrame, ticker: str, profile: str):
        """Checks for incorrect dtypes in OHLCV columns."""
        for col in ["open", "high", "low", "close", "volume"]:
            if col in df.columns and not pd.api.types.is_numeric_dtype(df[col]):
                dtype_repr = describe_dtype(df[col])
                msg = f"{self._ui_prefix} âš ï¸ {ticker} {profile} cache: {col}å‹ä¸ä¸€è‡´ ({dtype_repr})"
                self._warn_once(ticker, profile, f"dtype:{col}:{dtype_repr}", msg)

    def _check_non_positive_prices(self, df: pd.DataFrame, ticker: str, profile: str):
        """Checks if price columns contain only non-positive values."""
        for col in ["close", "high", "low"]:
            if col in df.columns:
                vals = pd.to_numeric(df[col], errors="coerce")
                if not vals.empty and (vals <= 0).all():
                    msg = (
                        f"{self._ui_prefix} âš ï¸ {ticker} {profile} cache: {col}å…¨ã¦éæ­£å€¤"
                    )
                    self._warn_once(ticker, profile, f"non_positive:{col}", msg)

    def _perform_health_check(
        self, df: pd.DataFrame, ticker: str, profile: str
    ) -> None:
        """Performs a series of health checks on the DataFrame."""
        if df is None or df.empty:
            return
        try:
            # Perform NaN rate checks first
            self._check_nan_rates(df, ticker, profile)
            self._check_column_dtypes(df, ticker, profile)
            self._check_non_positive_prices(df, ticker, profile)
        except Exception as e:
            msg = f"{self._ui_prefix} âš ï¸ {ticker} {profile} cache: å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯å¤±æ•— ({e})"
            self._warn_once(
                ticker, profile, f"healthcheck_error:{type(e).__name__}", msg
            )

    def upsert_both(self, ticker: str, new_rows: pd.DataFrame) -> None:
        """Upserts new rows into both 'full' and 'rolling' caches."""
        for profile in ("full", "rolling"):
            self._upsert_one(ticker, new_rows, profile)

    def _upsert_one(self, ticker: str, new_rows: pd.DataFrame, profile: str) -> None:
        if new_rows is not None and not new_rows.empty and "date" in new_rows.columns:
            new_rows = new_rows.copy()
            new_rows["date"] = pd.to_datetime(new_rows["date"], errors="coerce")
            new_rows = new_rows.dropna(subset=["date"])

        cur = self.read(ticker, profile)
        if cur is None or cur.empty:
            merged = new_rows.copy() if new_rows is not None else pd.DataFrame()
        else:
            merged = (
                pd.concat([cur, new_rows], ignore_index=True)
                if new_rows is not None
                else cur
            )

        if not merged.empty:
            merged = (
                merged.sort_values("date")
                .drop_duplicates("date")
                .reset_index(drop=True)
            )
            if profile == "rolling":
                merged = self._enforce_rolling_window(merged)
            merged = self._recompute_indicators(merged)

        if not merged.empty:
            self.write_atomic(merged, ticker, profile)

    @property
    def _rolling_target_len(self) -> int:
        return int(self.rolling_cfg.base_lookback_days + self.rolling_cfg.buffer_days)

    def _enforce_rolling_window(self, df: pd.DataFrame) -> pd.DataFrame:
        if "date" not in df.columns or df.empty:
            return df
        target_len = self._rolling_target_len
        return (
            df.tail(target_len).reset_index(drop=True) if len(df) > target_len else df
        )

    def prune_rolling_if_needed(self, anchor_ticker: str = "SPY") -> dict:
        """Prunes the rolling cache if enough new data has been added."""
        try:
            meta_text = self.rolling_meta_path.read_text(encoding="utf-8")
            last_meta = json.loads(meta_text)
        except (FileNotFoundError, json.JSONDecodeError):
            last_meta = {"anchor_rows_at_prune": 0}

        anchor_df = self.read(anchor_ticker, "rolling")
        if anchor_df is None or anchor_df.empty:
            logger.info(f"{self._ui_prefix} rollingæœªæ•´å‚™ã®ãŸã‚pruneã‚¹ã‚­ãƒƒãƒ—")
            return {"pruned_files": 0, "dropped_rows_total": 0}

        cur_rows = len(anchor_df)
        prev_rows = int(last_meta.get("anchor_rows_at_prune", 0))
        progressed = cur_rows - prev_rows

        prune_chunk = int(self.rolling_cfg.prune_chunk_days)
        if progressed < prune_chunk:
            msg = f"{self._ui_prefix} é€²æ—{progressed}å–¶æ¥­æ—¥ (<{prune_chunk}) ã®ãŸã‚pruneä¸è¦"
            logger.info(msg)
            return {"pruned_files": 0, "dropped_rows_total": 0}

        msg = f"{self._ui_prefix} â³ pruneé–‹å§‹: anchor={anchor_ticker}, é€²æ—={progressed}å–¶æ¥­æ—¥"
        logger.info(msg)
        pruned_files, dropped_total = 0, 0

        for path in self.rolling_dir.glob("*.*"):
            if path.name.startswith("_"):
                continue

            df = self.read(path.stem, "rolling")
            if df is None or df.empty:
                continue

            can_drop = len(df) - self._rolling_target_len
            drop_n = min(prune_chunk, can_drop)
            if drop_n > 0:
                new_df = df.iloc[drop_n:].reset_index(drop=True)
                self.write_atomic(new_df, path.stem, "rolling")
                pruned_files += 1
                dropped_total += drop_n

        self.rolling_meta_path.write_text(
            json.dumps({"anchor_rows_at_prune": cur_rows}, indent=2), encoding="utf-8"
        )
        msg = f"{self._ui_prefix} âœ… pruneå®Œäº†: files={pruned_files}, dropped_rows={dropped_total}"
        logger.info(msg)
        return {"pruned_files": pruned_files, "dropped_rows_total": dropped_total}

    def analyze_rolling_gaps(self, system_symbols: list[str] | None = None) -> dict:
        """
        rolling cache ã®æ•´å‚™çŠ¶æ³ã‚’åˆ†æã—ã€æœªæ•´å‚™ã‚·ãƒ³ãƒœãƒ«ã®è©³ç´°ãƒ­ã‚°ã‚’é›†ç´„ã™ã‚‹ã€‚

        Args:
            system_symbols: åˆ†æå¯¾è±¡ã‚·ãƒ³ãƒœãƒ«ã®ãƒªã‚¹ãƒˆã€‚Noneã®å ´åˆã¯base cacheã®å…¨ã‚·ãƒ³ãƒœãƒ«
                ã‚’å¯¾è±¡ã¨ã™ã‚‹ã€‚

        Returns:
            åˆ†æçµæœè¾æ›¸:
            - total_symbols: åˆ†æå¯¾è±¡ã‚·ãƒ³ãƒœãƒ«æ•°
            - available_in_rolling: rolling cacheã«å­˜åœ¨ã™ã‚‹ã‚·ãƒ³ãƒœãƒ«æ•°
            - missing_from_rolling: rolling cacheã«å­˜åœ¨ã—ãªã„ã‚·ãƒ³ãƒœãƒ«æ•°
            - missing_symbols: æœªæ•´å‚™ã‚·ãƒ³ãƒœãƒ«ã®ãƒªã‚¹ãƒˆ
            - coverage_percentage: ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡ï¼ˆ%ï¼‰
        """
        logger.info(f"{self._ui_prefix} ğŸ” rolling cacheæ•´å‚™çŠ¶æ³ã®åˆ†æã‚’é–‹å§‹")

        # åˆ†æå¯¾è±¡ã‚·ãƒ³ãƒœãƒ«ã®æ±ºå®š
        if system_symbols is None:
            # base cacheã‹ã‚‰å…¨ã‚·ãƒ³ãƒœãƒ«ã‚’å–å¾—
            base_files = list(self.full_dir.parent.glob(f"{BASE_SUBDIR}/*.*"))
            system_symbols = [p.stem for p in base_files if not p.name.startswith("_")]
            logger.info(
                f"{self._ui_prefix} base cacheã‹ã‚‰{len(system_symbols)}ã‚·ãƒ³ãƒœãƒ«ã‚’æ¤œå‡º"
            )
        else:
            logger.info(
                f"{self._ui_prefix} æŒ‡å®šã•ã‚ŒãŸ{len(system_symbols)}ã‚·ãƒ³ãƒœãƒ«ã‚’åˆ†æå¯¾è±¡ã¨ã—ã¾ã™"
            )

        available_symbols = []
        missing_symbols = []

        # å„ã‚·ãƒ³ãƒœãƒ«ã®rolling cacheå­˜åœ¨ç¢ºèª
        for symbol in system_symbols:
            rolling_data = self.read(symbol, "rolling")
            if rolling_data is not None and not rolling_data.empty:
                available_symbols.append(symbol)
            else:
                missing_symbols.append(symbol)
                # é›†ç´„ãƒ­ã‚°ã«æœªæ•´å‚™ã‚’å ±å‘Š
                report_rolling_issue("missing_from_analysis", symbol)

        total_symbols = len(system_symbols)
        available_count = len(available_symbols)
        missing_count = len(missing_symbols)
        coverage_percentage = (
            (available_count / total_symbols * 100) if total_symbols > 0 else 0
        )

        # çµæœãƒ­ã‚°
        logger.info(f"{self._ui_prefix} ğŸ“Š åˆ†æå®Œäº†:")
        logger.info(f"{self._ui_prefix}   - åˆ†æå¯¾è±¡: {total_symbols}ã‚·ãƒ³ãƒœãƒ«")
        logger.info(
            f"{self._ui_prefix}   - rolling cacheæ•´å‚™æ¸ˆã¿: {available_count}ã‚·ãƒ³ãƒœãƒ«"
        )
        logger.info(
            f"{self._ui_prefix}   - rolling cacheæœªæ•´å‚™: {missing_count}ã‚·ãƒ³ãƒœãƒ«"
        )
        logger.info(f"{self._ui_prefix}   - ã‚«ãƒãƒ¬ãƒƒã‚¸: {coverage_percentage:.1f}%")

        if missing_symbols:
            # é›†ç´„ãƒ­ã‚°ã«ã‚ˆã‚‹æœªæ•´å‚™ã‚·ãƒ³ãƒœãƒ«å ±å‘Šï¼ˆæ—¢å­˜ã®warningã‚’ç½®ãæ›ãˆï¼‰
            for symbol in missing_symbols[:10]:  # å…ˆé ­10ä»¶ã‚’è©³ç´°å ±å‘Š
                report_rolling_issue("missing_analysis_detailed", symbol)

            # å¾“æ¥ã®å½¢å¼ã®ãƒ­ã‚°ã‚‚æ¡ä»¶ä»˜ãã§ç¶­æŒï¼ˆé›†ç´„ç„¡åŠ¹æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            if not _rolling_issue_aggregator.compact_mode:
                logger.warning(
                    f"{self._ui_prefix} ğŸš¨ æœªæ•´å‚™ã‚·ãƒ³ãƒœãƒ«: {missing_symbols[:10]}"
                )
                if len(missing_symbols) > 10:
                    logger.warning(
                        f"{self._ui_prefix}   ... ä»–{len(missing_symbols) - 10}ã‚·ãƒ³ãƒœãƒ«"
                    )

        return {
            "total_symbols": total_symbols,
            "available_in_rolling": available_count,
            "missing_from_rolling": missing_count,
            "missing_symbols": missing_symbols,
            "coverage_percentage": coverage_percentage,
        }

    def get_rolling_health_summary(self) -> dict:
        """
        rolling cache ã®å¥å…¨æ€§ã‚µãƒãƒªãƒ¼ã‚’å–å¾—ã™ã‚‹ã€‚

        Returns:
            å¥å…¨æ€§ã‚µãƒãƒªãƒ¼è¾æ›¸:
            - meta_exists: ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨
            - meta_content: ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹
            - rolling_files_count: rolling cacheãƒ•ã‚¡ã‚¤ãƒ«æ•°
            - target_length: ç›®æ¨™ãƒ‡ãƒ¼ã‚¿é•·
            - anchor_symbol_status: ã‚¢ãƒ³ã‚«ãƒ¼ã‚·ãƒ³ãƒœãƒ«ï¼ˆSPYï¼‰ã®çŠ¶æ…‹
        """
        logger.info(f"{self._ui_prefix} ğŸ©º rolling cacheå¥å…¨æ€§ãƒã‚§ãƒƒã‚¯ã‚’é–‹å§‹")

        # ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
        meta_exists = self.rolling_meta_path.exists()
        meta_content = {}
        if meta_exists:
            try:
                with open(self.rolling_meta_path, encoding="utf-8") as f:
                    meta_content = json.load(f)
            except (json.JSONDecodeError, OSError) as e:
                logger.warning(f"{self._ui_prefix} ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")

        # rolling cacheãƒ•ã‚¡ã‚¤ãƒ«æ•°
        rolling_files = [
            p for p in self.rolling_dir.glob("*.*") if not p.name.startswith("_")
        ]
        rolling_files_count = len(rolling_files)

        # ç›®æ¨™ãƒ‡ãƒ¼ã‚¿é•·
        target_length = self._rolling_target_len

        # SPYã‚¢ãƒ³ã‚«ãƒ¼ã®çŠ¶æ…‹ç¢ºèª
        spy_data = self.read("SPY", "rolling")
        anchor_status = {
            "exists": spy_data is not None and not spy_data.empty,
            "rows": len(spy_data) if spy_data is not None else 0,
            "meets_target": False,
        }
        if spy_data is not None:
            anchor_status["meets_target"] = len(spy_data) >= target_length

        result = {
            "meta_exists": meta_exists,
            "meta_content": meta_content,
            "rolling_files_count": rolling_files_count,
            "target_length": target_length,
            "anchor_symbol_status": anchor_status,
        }

        logger.info(f"{self._ui_prefix} âœ… å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯å®Œäº†:")
        logger.info(
            f"{self._ui_prefix}   - ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«: {'å­˜åœ¨' if meta_exists else 'ä¸åœ¨'}"
        )
        logger.info(f"{self._ui_prefix}   - rolling files: {rolling_files_count}å€‹")
        spy_status = "æ­£å¸¸" if anchor_status["meets_target"] else "è¦ç¢ºèª"
        logger.info(
            f"{self._ui_prefix}   - SPYçŠ¶æ…‹: {spy_status} ({anchor_status['rows']}è¡Œ)"
        )

        return result


def _base_dir() -> Path:
    settings = get_settings(create_dirs=True)
    base = Path(settings.DATA_CACHE_DIR) / BASE_SUBDIR
    base.mkdir(parents=True, exist_ok=True)
    return base


def compute_base_indicators(df: pd.DataFrame) -> pd.DataFrame:
    """OHLCVã®DataFrameã«å…±é€šãƒ™ãƒ¼ã‚¹æŒ‡æ¨™ã‚’ä»˜åŠ ã—ã¦è¿”ã™ã€‚"""
    if df is None or df.empty:
        return df

    x = df.copy()

    # Normalize column names
    rename_map = {c: c.lower() for c in x.columns}
    x = x.rename(columns=rename_map)

    # Ensure 'Date' column and set as index
    if "date" in x.columns:
        x = x.rename(columns={"date": "Date"})
    if "Date" in x.columns:
        x["Date"] = pd.to_datetime(x["Date"], errors="coerce")
        x = x.dropna(subset=["Date"]).sort_values("Date").set_index("Date")

    # Standardize OHLCV column names
    ohlcv_map = {
        "open": "Open",
        "high": "High",
        "low": "Low",
        "adjusted_close": "Close",
        "adj_close": "Close",
        "adjclose": "Close",
        "close": "Close",
        "volume": "Volume",
        "vol": "Volume",
    }
    final_rename = {c: ohlcv_map[c] for c in x.columns if c in ohlcv_map}
    x = x.rename(columns=final_rename)

    required = {"High", "Low", "Close"}
    if not required.issubset(x.columns):
        missing_cols = required - set(x.columns)
        logger.warning(
            f"{__name__}: å¿…é ˆåˆ—æ¬ è½ã®ãŸã‚ã‚¤ãƒ³ã‚¸è¨ˆç®—ã‚’ã‚¹ã‚­ãƒƒãƒ—: missing={missing_cols}"
        )
        return x.reset_index()

    close = pd.to_numeric(x["Close"], errors="coerce")
    high = pd.to_numeric(x["High"], errors="coerce")
    low = pd.to_numeric(x["Low"], errors="coerce")
    vol = None
    if "Volume" in x.columns:
        vol = pd.to_numeric(x["Volume"], errors="coerce")

    # SMA/EMA - å¤§æ–‡å­—çµ±ä¸€
    for n in [25, 50, 100, 150, 200]:
        x[f"SMA{n}"] = close.rolling(n).mean()
    for n in [20, 50]:
        x[f"EMA{n}"] = close.ewm(span=n, adjust=False).mean()

    # ATR - å¤§æ–‡å­—çµ±ä¸€
    tr = pd.concat(
        [high - low, (high - close.shift()).abs(), (low - close.shift()).abs()], axis=1
    ).max(axis=1)
    for n in [10, 14, 20, 40, 50]:
        x[f"ATR{n}"] = tr.rolling(n).mean()

    # RSI (Wilder) - å¤§æ–‡å­—çµ±ä¸€
    def _rsi(s: pd.Series, n: int) -> pd.Series:
        delta = s.diff()
        gain = delta.clip(lower=0).ewm(alpha=1 / n, adjust=False).mean()
        loss = -delta.clip(upper=0).ewm(alpha=1 / n, adjust=False).mean()
        rs = gain / loss.replace(0, np.nan)
        return 100 - (100 / (1 + rs))

    for n in [3, 4, 14]:
        x[f"RSI{n}"] = _rsi(close, n)

    # ROC & HV - å¤§æ–‡å­—çµ±ä¸€
    x["ROC200"] = close.pct_change(200) * 100.0
    log_ret = (close / close.shift(1)).apply(np.log)
    std_dev = log_ret.rolling(50).std()
    x["HV50"] = std_dev * np.sqrt(252) * 100.0

    # DollarVolume - å¤§æ–‡å­—çµ±ä¸€
    if vol is not None:
        x["DollarVolume20"] = (close * vol).rolling(20).mean()
        x["DollarVolume50"] = (close * vol).rolling(50).mean()

    return x.reset_index()


def get_indicator_column_flexible(df: pd.DataFrame, indicator: str) -> pd.Series | None:
    """æŒ‡æ¨™åˆ—ã‚’å¤§æ–‡å­—ãƒ»å°æ–‡å­—ä¸¡å¯¾å¿œã§å–å¾—ã€‚å¤§æ–‡å­—å„ªå…ˆã€å°æ–‡å­—ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‚

    Args:
        df: å¯¾è±¡DataFrame
        indicator: æŒ‡æ¨™åï¼ˆä¾‹: "ATR10"ï¼‰

    Returns:
        è©²å½“ã™ã‚‹åˆ—ã®Seriesã€å­˜åœ¨ã—ãªã„å ´åˆã¯None
    """
    # å¤§æ–‡å­—å„ªå…ˆ
    if indicator in df.columns:
        return df[indicator]

    # å°æ–‡å­—ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    lower_indicator = indicator.lower()
    if lower_indicator in df.columns:
        return df[lower_indicator]

    return None


def standardize_indicator_columns(df: pd.DataFrame) -> pd.DataFrame:
    """æŒ‡æ¨™åˆ—åã‚’å¤§æ–‡å­—ã«æ¨™æº–åŒ–ã€‚æ—¢å­˜ã®å°æ–‡å­—åˆ—ã¯å‰Šé™¤ã€‚

    Args:
        df: å¯¾è±¡DataFrame

    Returns:
        æ¨™æº–åŒ–ã•ã‚ŒãŸDataFrame
    """
    result = df.copy()

    # æ¨™æº–åŒ–ã™ã‚‹æŒ‡æ¨™ã®ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆå°æ–‡å­— -> å¤§æ–‡å­—ï¼‰
    indicator_mapping = {
        "atr10": "ATR10",
        "atr14": "ATR14",
        "atr20": "ATR20",
        "atr40": "ATR40",
        "atr50": "ATR50",
        "sma25": "SMA25",
        "sma50": "SMA50",
        "sma100": "SMA100",
        "sma150": "SMA150",
        "sma200": "SMA200",
        "ema20": "EMA20",
        "ema50": "EMA50",
        "rsi3": "RSI3",
        "rsi4": "RSI4",
        "rsi14": "RSI14",
        "roc200": "ROC200",
        "hv50": "HV50",
        "dollarvolume20": "DollarVolume20",
        "dollarvolume50": "DollarVolume50",
        "avgvolume50": "AvgVolume50",
        "adx7": "ADX7",
    }

    # å°æ–‡å­— -> å¤§æ–‡å­—ã¸ã®å¤‰æ›ã¨é‡è¤‡å‰Šé™¤
    for old_name, new_name in indicator_mapping.items():
        if old_name in result.columns and new_name not in result.columns:
            # å°æ–‡å­—åˆ—ã‚’å¤§æ–‡å­—ã«ãƒªãƒãƒ¼ãƒ 
            result = result.rename(columns={old_name: new_name})
        elif old_name in result.columns and new_name in result.columns:
            # ä¸¡æ–¹å­˜åœ¨ã™ã‚‹å ´åˆã¯å°æ–‡å­—åˆ—ã‚’å‰Šé™¤
            result = result.drop(columns=[old_name])

    return result


def base_cache_path(symbol: str) -> Path:
    return _base_dir() / f"{safe_filename(symbol)}.csv"


def save_base_cache(
    symbol: str, df: pd.DataFrame, settings: Settings | None = None
) -> Path:
    """Saves the base cache DataFrame to a CSV file."""
    path = base_cache_path(symbol)
    df_reset = df.reset_index() if df.index.name is not None else df
    path.parent.mkdir(parents=True, exist_ok=True)

    # Backwards-compatible: if settings is not provided, obtain global settings
    if settings is None:
        settings = get_settings(create_dirs=True)

    round_dec = getattr(getattr(settings, "cache", None), "round_decimals", None)
    df_to_write = round_dataframe(df_reset, round_dec)

    _write_dataframe_to_csv(df_to_write, path, settings)
    return path


_DEFAULT_CACHE_MANAGER: CacheManager | None = None


def _get_default_cache_manager() -> CacheManager:
    """Returns a module-level singleton CacheManager instance."""
    global _DEFAULT_CACHE_MANAGER
    if _DEFAULT_CACHE_MANAGER is None:
        settings = get_settings(create_dirs=False)
        _DEFAULT_CACHE_MANAGER = CacheManager(settings)
    return _DEFAULT_CACHE_MANAGER


def _read_legacy_cache(symbol: str) -> pd.DataFrame | None:
    """Reads from the old cache location for backward compatibility."""
    legacy_path = Path("data_cache") / f"{safe_filename(symbol)}.csv"
    if not legacy_path.exists():
        return None
    try:
        return pd.read_csv(legacy_path)
    except Exception:
        return None


def load_base_cache(
    symbol: str,
    *,
    rebuild_if_missing: bool = True,
    cache_manager: CacheManager | None = None,
    min_last_date: pd.Timestamp | None = None,
    allowed_recent_dates: Iterable[object] | None = None,
    prefer_precomputed_indicators: bool = True,
) -> pd.DataFrame | None:
    """Loads base cache, with optional freshness validation and rebuilding."""
    cm = cache_manager or _get_default_cache_manager()
    path = base_cache_path(symbol)
    df: pd.DataFrame | None = None

    if path.exists():
        try:
            df = pd.read_csv(path, parse_dates=["Date"])
            df = df.dropna(subset=["Date"]).sort_values("Date").set_index("Date")
        except Exception:
            df = None

    if df is not None:
        last_date = df.index[-1] if not df.empty else None
        is_stale = False
        if allowed_recent_dates and last_date:
            allowed_set = {
                pd.Timestamp(cast(Any, d)).normalize()
                for d in allowed_recent_dates
                if d is not None and not pd.isna(d)
            }
            if last_date.normalize() not in allowed_set:
                is_stale = True
        if not is_stale and min_last_date and last_date:
            if last_date.normalize() < pd.Timestamp(min_last_date).normalize():
                is_stale = True

        if not is_stale:
            return df.reset_index()

        if not rebuild_if_missing:
            return df.reset_index()

        msg = f"{__name__}: base cache for {symbol} is stale, rebuilding."
        logger.info(msg)
        df = None  # Force rebuild

    if df is None and rebuild_if_missing:
        raw = (
            cm.read(symbol, "full")
            or cm.read(symbol, "rolling")
            or _read_legacy_cache(symbol)
        )
        if raw is not None and not raw.empty:
            # If caller prefers to reuse precomputed indicator columns and
            # the raw frame appears to contain indicator columns, avoid
            # calling expensive `compute_base_indicators` and save the raw
            # data as the base cache directly.
            try:
                lc_cols = {c.lower() for c in raw.columns}
            except Exception:
                lc_cols = set()

            has_any_indicator = any((col in lc_cols) for col in MAIN_INDICATOR_COLUMNS)
            if prefer_precomputed_indicators and has_any_indicator:
                out = raw.copy()
                # Normalize date column name to 'Date' for base cache consistency
                if "date" in out.columns and "Date" not in out.columns:
                    out = out.rename(columns={"date": "Date"})
                # If index is a DatetimeIndex and there's no Date column, expose it
                if "Date" not in out.columns and isinstance(
                    out.index, pd.DatetimeIndex
                ):
                    try:
                        out = out.reset_index()
                        # ensure the date column is named 'Date'
                        if out.columns[0].lower() == "index":
                            out = out.rename(columns={out.columns[0]: "Date"})
                    except Exception:
                        pass
                try:
                    save_base_cache(symbol, out, cm.settings)
                except Exception:
                    # fallback to computing indicators if saving fails
                    out = compute_base_indicators(raw)
                    save_base_cache(symbol, out, cm.settings)
                return out

            # otherwise compute indicators normally
            out = compute_base_indicators(raw)
            save_base_cache(symbol, out, cm.settings)
            return out

    return df.reset_index() if df is not None else None
