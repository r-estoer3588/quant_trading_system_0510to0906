from __future__ import annotations

import atexit
import json
import logging
import os
import shutil
import threading
from collections import defaultdict
from collections.abc import Iterable
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import Any, ClassVar, cast

import numpy as np
import pandas as pd

from common.utils import describe_dtype, safe_filename
from config.settings import Settings, get_settings
from indicators_common import add_indicators

logger = logging.getLogger(__name__)

BASE_SUBDIR = "base"


class _RollingIssueAggregator:
    """
    rolling cacheæœªæ•´å‚™ãƒ­ã‚°ã‚’é›†ç´„ã—ã€å†—é•·å‡ºåŠ›ã‚’åˆ¶å¾¡ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚

    ç’°å¢ƒå¤‰æ•°:
    - COMPACT_TODAY_LOGS=1: é›†ç´„æ©Ÿèƒ½æœ‰åŠ¹åŒ–
    - ROLLING_ISSUES_VERBOSE_HEAD=N: å…ˆé ­Nä»¶ã®ã¿è©³ç´°WARNINGã€ä»¥é™ã¯DEBUG
    """

    _instance = None
    _lock = threading.Lock()

    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        if self._initialized:
            return

        self.compact_mode = os.getenv("COMPACT_TODAY_LOGS", "0") == "1"
        self.verbose_head = int(os.getenv("ROLLING_ISSUES_VERBOSE_HEAD", "20"))
        self.issues = defaultdict(list)  # category -> [symbols]
        self.warning_count = 0
        self.logger = logging.getLogger(__name__)
        self._initialized = True

        if self.compact_mode:
            # ãƒ—ãƒ­ã‚»ã‚¹çµ‚äº†æ™‚ã«ã‚µãƒãƒªãƒ¼ã‚’å‡ºåŠ›
            atexit.register(self._output_summary)

    def report_issue(self, category: str, symbol: str, message: str = "") -> None:
        """
        rolling cache ã®æœªæ•´å‚™å•é¡Œã‚’å ±å‘Šã™ã‚‹ã€‚

        Args:
            category: å•é¡Œã‚«ãƒ†ã‚´ãƒªï¼ˆä¾‹: "missing_rolling", "insufficient_data"ï¼‰
            symbol: å¯¾è±¡ã‚·ãƒ³ãƒœãƒ«
            message: è¿½åŠ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆçœç•¥å¯ï¼‰
        """
        if not self.compact_mode:
            # å¾“æ¥é€šã‚Šã®å€‹åˆ¥WARNING
            full_msg = f"[{category}] {symbol}"
            if message:
                full_msg += f": {message}"
            self.logger.warning(full_msg)
            return

        # é›†ç´„ãƒ¢ãƒ¼ãƒ‰
        self.issues[category].append(symbol)
        self.warning_count += 1

        # å…ˆé ­Nä»¶ã®ã¿è©³ç´°WARNING
        if len(self.issues[category]) <= self.verbose_head:
            full_msg = f"[{category}] {symbol}"
            if message:
                full_msg += f": {message}"
            self.logger.warning(full_msg)
        else:
            # Nä»¶ã‚’è¶…ãˆãŸã‚‰DEBUGãƒ¬ãƒ™ãƒ«
            full_msg = f"[{category}] {symbol}"
            if message:
                full_msg += f": {message}"
            self.logger.debug(full_msg)

    def _output_summary(self) -> None:
        """ãƒ—ãƒ­ã‚»ã‚¹çµ‚äº†æ™‚ã«ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚µãƒãƒªã‚’å‡ºåŠ›ã™ã‚‹ã€‚"""
        if not self.issues:
            return

        self.logger.info("=== Rolling Cache Issues Summary ===")
        total_issues = sum(len(symbols) for symbols in self.issues.values())
        self.logger.info(f"Total issues reported: {total_issues}")

        for category, symbols in self.issues.items():
            unique_symbols = list(set(symbols))  # é‡è¤‡é™¤å»
            count = len(unique_symbols)

            if count <= 10:
                symbol_list = ", ".join(unique_symbols)
                self.logger.info(f"[{category}]: {count} symbols - {symbol_list}")
            else:
                sample = ", ".join(unique_symbols[:5])
                self.logger.info(
                    f"[{category}]: {count} symbols - {sample} ... (+{count-5} more)"
                )


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_rolling_issue_aggregator = _RollingIssueAggregator()


def report_rolling_issue(category: str, symbol: str, message: str = "") -> None:
    """
    rolling cache ã®æœªæ•´å‚™å•é¡Œã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¢ã‚°ãƒªã‚²ãƒ¼ã‚¿ãƒ¼ã«å ±å‘Šã™ã‚‹ã€‚

    Args:
        category: å•é¡Œã‚«ãƒ†ã‚´ãƒªï¼ˆä¾‹: "missing_rolling", "insufficient_data"ï¼‰
        symbol: å¯¾è±¡ã‚·ãƒ³ãƒœãƒ«
        message: è¿½åŠ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆçœç•¥å¯ï¼‰
    """
    _rolling_issue_aggregator.report_issue(category, symbol, message)


def round_dataframe(df: pd.DataFrame, decimals: int | None) -> pd.DataFrame:
    """Return a DataFrame rounded to the requested number of decimals.

    pandas.DataFrame.round ã¯æ•°å€¤åˆ—ã®ã¿ã‚’å¯¾è±¡ã¨ã—ã€æ—¥ä»˜ã‚„æ–‡å­—åˆ—åˆ—ã«ã¯å½±éŸ¿ã—ãªã„ã€‚
    ãŸã ã— ``decimals`` ãŒä¸æ­£å€¤ã®å ´åˆã‚„ä¸¸ã‚å‡¦ç†ãŒä¾‹å¤–ã‚’é€å‡ºã—ãŸå ´åˆã¯ã€
    å…ƒã® DataFrame ã‚’ãã®ã¾ã¾è¿”ã™ã€‚
    """
    if df is None or decimals is None:
        return df

    try:
        decimals_int = int(decimals)
    except (ValueError, TypeError):
        return df

    # Define category-specific rounding by column name (lowercase)
    price_atr_cols = {
        "open",
        "close",
        "high",
        "low",
        "atr10",
        "atr14",
        "atr20",
        "atr40",
        "atr50",
        "adjusted_close",
        "adjclose",
        "adj_close",
    }
    volume_cols = {"volume", "dollarvolume20", "dollarvolume50", "avgvolume50"}
    oscillator_cols = {"rsi3", "rsi4", "rsi14", "adx7"}
    pct_cols = {
        "roc200",
        "return_3d",
        "return_6d",
        "atr_ratio",
        "atr_pct",
        "hv50",
        "return_pct",
    }

    out = df.copy()
    lc_map = {c.lower(): c for c in out.columns}

    def _safe_round(series: pd.Series, ndigits: int) -> pd.Series:
        try:
            return pd.to_numeric(series, errors="coerce").round(ndigits)
        except Exception:
            return series

    # Group and apply rounding
    groups = {
        2: price_atr_cols | oscillator_cols,
        4: pct_cols,
    }
    rounded_cols = set()

    for ndigits, names in groups.items():
        cols_to_round = [lc_map[lname] for lname in names if lname in lc_map]
        for col in cols_to_round:
            out[col] = _safe_round(out[col], ndigits)
        rounded_cols.update(cols_to_round)

    # Volume-like columns
    vol_cols_to_round = [lc_map[lname] for lname in volume_cols if lname in lc_map]
    for col in vol_cols_to_round:
        try:
            s = pd.to_numeric(out[col], errors="coerce").round(0)
            out[col] = s.astype("Int64")
        except Exception:
            out[col] = _safe_round(out[col], 0)
    rounded_cols.update(vol_cols_to_round)

    # Remaining numeric columns
    for col in out.columns:
        if col not in rounded_cols and pd.api.types.is_numeric_dtype(out[col]):
            out[col] = _safe_round(out[col], decimals_int)

    return out


def make_csv_formatters(
    frame: pd.DataFrame, dec_point: str = ".", thous_sep: str | None = None
) -> dict:
    """Create a pandas.to_csv formatters dict honoring decimal point and thousands sep.

    Returns: dict mapping column name -> callable
    """
    lc_map = {c.lower(): c for c in frame.columns}
    fmt: dict = {}

    def _add_thousands_sep(int_str: str, sep: str) -> str:
        neg = int_str.startswith("-")
        prefix = "-" if neg else ""
        num_str = int_str[1:] if neg else int_str
        parts = []
        while num_str:
            parts.append(num_str[-3:])
            num_str = num_str[:-3]
        return prefix + sep.join(reversed(parts))

    def _num_formatter(nd: int):
        def _f(x):
            if pd.isna(x):
                return ""
            try:
                s = f"{float(x):.{nd}f}"
                if thous_sep:
                    int_part, _, frac_part = s.partition(".")
                    int_part = _add_thousands_sep(int_part, thous_sep)
                    s = f"{int_part}.{frac_part}" if frac_part else int_part
                return s.replace(".", dec_point) if dec_point != "." else s
            except (ValueError, TypeError):
                return str(x)

        return _f

    def _int_formatter():
        def _f(x):
            if pd.isna(x):
                return ""
            try:
                s = f"{int(round(float(x))):d}"
                return _add_thousands_sep(s, thous_sep) if thous_sep else s
            except (ValueError, TypeError):
                return str(x)

        return _f

    # Define formatting rules
    rules = {
        _num_formatter(2): [
            "open",
            "close",
            "high",
            "low",
            "atr10",
            "atr14",
            "atr20",
            "atr40",
            "atr50",
            "rsi3",
            "rsi4",
            "rsi14",
            "adx7",
        ],
        _num_formatter(4): [
            "roc200",
            "return_3d",
            "return_6d",
            "atr_ratio",
            "atr_pct",
            "hv50",
        ],
        _int_formatter(): ["volume", "dollarvolume20", "dollarvolume50", "avgvolume50"],
    }

    for formatter, names in rules.items():
        for name in names:
            if name in lc_map:
                fmt[lc_map[name]] = formatter
    return fmt


def _write_dataframe_to_csv(df: pd.DataFrame, path: Path, settings: Settings) -> None:
    """Utility to write a DataFrame to CSV with formatting."""
    # Be defensive: tests may supply a SimpleNamespace for settings lacking
    # nested attributes (e.g. settings.cache.csv). Use getattr fallbacks and
    # sensible defaults to avoid raising AttributeError here.
    try:
        cache_cfg = getattr(settings, "cache", None)
        csv_cfg = getattr(cache_cfg, "csv", None)
        dec_point = getattr(csv_cfg, "decimal_point", ".")
        thous_sep = getattr(csv_cfg, "thousands_sep", None)
        sep = getattr(csv_cfg, "field_sep", ",")

        fmt_map = make_csv_formatters(df, dec_point=dec_point, thous_sep=thous_sep)

        if fmt_map:
            df_out = df.copy()
            for col, func in fmt_map.items():
                if col in df_out.columns:
                    try:
                        df_out[col] = df_out[col].apply(func)
                    except Exception:
                        df_out[col] = df_out[col].astype(str)
            df_out.to_csv(path, index=False, sep=sep)
        else:
            df.to_csv(path, index=False, decimal=dec_point, sep=sep)
    except Exception as e:
        logger.error(f"Failed to write formatted CSV {path.name}: {e}")
        try:
            df.to_csv(path, index=False)
        except Exception as e2:
            logger.error(f"Failed to write CSV fallback {path.name}: {e2}")


# å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯ã§å‚ç…§ã™ã‚‹ä¸»è¦æŒ‡æ¨™åˆ—ï¼ˆèª­ã¿è¾¼ã¿å¾Œã¯å°æ–‡å­—åŒ–ã•ã‚Œã‚‹ï¼‰
MAIN_INDICATOR_COLUMNS = (
    "open",
    "high",
    "low",
    "close",
    "volume",
    "sma25",
    "sma50",
    "sma100",
    "sma150",
    "sma200",
    "ema20",
    "ema50",
    "atr10",
    "atr14",
    "atr20",
    "atr40",
    "atr50",
    "adx7",
    "rsi3",
    "rsi4",
    "rsi14",
    "roc200",
    "hv50",
    "dollarvolume20",
    "dollarvolume50",
    "avgvolume50",
    "return_3d",
    "return_6d",
    "return_pct",
    "drop3d",
    "atr_ratio",
    "atr_pct",
)

# å„æŒ‡æ¨™åˆ—ãŒæœ‰åŠ¹å€¤ã‚’æŒã¤ãŸã‚ã«æœ€ä½é™å¿…è¦ã¨ã™ã‚‹è¦³æ¸¬æ—¥æ•°ã®ç›®å®‰
_INDICATOR_MIN_OBSERVATIONS: dict[str, int] = {
    "sma25": 20,
    "sma50": 50,
    "sma100": 100,
    "sma150": 150,
    "sma200": 200,
    "ema20": 1,
    "ema50": 1,
    "atr10": 11,
    "atr14": 15,
    "atr20": 21,
    "atr40": 41,
    "atr50": 51,
    "adx7": 14,
    "rsi3": 3,
    "rsi4": 4,
    "rsi14": 14,
    "roc200": 201,
    "hv50": 51,
    "dollarvolume20": 20,
    "dollarvolume50": 50,
    "avgvolume50": 50,
    "return_3d": 4,
    "return_6d": 7,
    "return_pct": 2,
    "drop3d": 4,
    "atr_ratio": 11,
    "atr_pct": 11,
}


class CacheManager:
    """
    äºŒå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ï¼ˆfull / rollingï¼‰ã€‚
    - æ—¢å­˜ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ(csv/parquet)ã¯è‡ªå‹•æ¤œå‡ºãƒ»è¸è¥²
    - system5/6ã‚¹ã‚¿ã‚¤ãƒ«ã®ã‚³ãƒ¡ãƒ³ãƒˆãƒ»é€²æ—ãƒ­ã‚°ç²’åº¦ã‚’è¸è¥²
    """

    _GLOBAL_WARNED: ClassVar[set[tuple[str, str, str]]] = set()

    def __init__(self, settings: Settings):
        self.settings = settings
        self.full_dir = Path(settings.cache.full_dir)
        self.rolling_dir = Path(settings.cache.rolling_dir)
        self.rolling_cfg = settings.cache.rolling
        self.file_format = getattr(settings.cache, "file_format", "auto")
        self.rolling_meta_path = self.rolling_dir / self.rolling_cfg.meta_file
        self.full_dir.mkdir(parents=True, exist_ok=True)
        self.rolling_dir.mkdir(parents=True, exist_ok=True)
        self._ui_prefix = "[CacheManager]"
        self._warned = self._GLOBAL_WARNED

    def _warn_once(
        self, ticker: str, profile: str, category: str, message: str
    ) -> None:
        key = (ticker, profile, category)
        if key in self._warned:
            return
        self._warned.add(key)
        logger.warning(message)

    def _read_base_and_tail(self, ticker: str, tail_rows: int = 330) -> pd.DataFrame | None:
        """baseã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã¿ã€rollingç›¸å½“ã®è¡Œæ•°ã§tailå‡¦ç†ã‚’è¡Œã†"""
        try:
            # baseãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰èª­ã¿è¾¼ã¿
            base_dir = self.full_dir.parent / "base"
            path = self._detect_path(base_dir, ticker)
            
            if not path.exists():
                return None
                
            df = self._read_with_fallback(path, ticker, "base")
            if df is None or df.empty:
                return None
                
            # tailå‡¦ç†ã§rollingç›¸å½“ã®ã‚µã‚¤ã‚ºã«
            return df.tail(tail_rows)
            
        except Exception as e:
            logger.warning(f"Failed to read base and tail for {ticker}: {e}")
            return None

    def _recompute_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """Recalculate derived indicator columns when base OHLC data is updated."""
        if df is None or df.empty or "date" not in df.columns:
            return df

        required = {"open", "high", "low", "close"}
        if not required.issubset(set(df.columns)):
            return df

        base = df.copy()
        base["date"] = pd.to_datetime(base["date"], errors="coerce")
        base = base.dropna(subset=["date"]).sort_values("date").reset_index(drop=True)
        if base.empty:
            return df

        for col in ("open", "high", "low", "close", "volume"):
            if col in base.columns:
                base[col] = pd.to_numeric(base[col], errors="coerce")

        case_map = {
            "open": "Open",
            "high": "High",
            "low": "Low",
            "close": "Close",
            "volume": "Volume",
        }
        base_renamed = base.rename(
            columns={k: v for k, v in case_map.items() if k in base.columns}
        )
        base_renamed["Date"] = base_renamed["date"]

        try:
            enriched = add_indicators(base_renamed)
            enriched = enriched.drop(columns=["Date"], errors="ignore")
            # æŒ‡æ¨™åˆ—ã‚’æ¨™æº–åŒ–ï¼ˆå¤§æ–‡å­—çµ±ä¸€ï¼‰ã€ãã®ä»–ã¯å°æ–‡å­—åŒ–
            enriched = standardize_indicator_columns(enriched)
            # åŸºæœ¬åˆ—ï¼ˆdate, open, highç­‰ï¼‰ã®ã¿å°æ–‡å­—ã«å¤‰æ›
            basic_cols = {"open", "high", "low", "close", "volume", "date"}
            enriched.columns = [
                c.lower() if c.lower() in basic_cols else c for c in enriched.columns
            ]
            enriched["date"] = pd.to_datetime(
                enriched.get("date", base["date"]), errors="coerce"
            )

            # Overwrite indicator columns with freshly computed values while
            # preserving original OHLCV and date columns. This ensures appended
            # rows receive correct indicator values and existing indicators are
            # consistent with the latest OHLC history.
            combined = df.copy()
            ohlcv = {"open", "high", "low", "close", "volume"}
            for col, series in enriched.items():
                if col == "date":
                    # ensure date column exists and normalized
                    combined["date"] = series
                    continue
                if col in ohlcv:
                    # keep original OHLCV from df
                    continue
                # replace or create indicator columns from enriched
                combined[col] = series

            # drop any duplicated columns just in case
            return combined.loc[:, ~combined.columns.duplicated(keep="first")]
        except Exception as e:
            logger.error(f"Failed to recompute indicators: {e}")
            return df

    def _detect_path(self, base_dir: Path, ticker: str) -> Path:
        """Detects the cache file path, prioritizing feather for rolling profile."""
        # For rolling profile, prioritize feather for better performance
        if base_dir == self.rolling_dir:
            search_order = [".feather", ".parquet", ".csv"]
        else:
            search_order = [".csv", ".parquet", ".feather"]

        for ext in search_order:
            if (p := base_dir / f"{ticker}{ext}").exists():
                return p

        # Default format selection based on settings
        fmt = (self.file_format or "auto").lower()
        if fmt == "parquet":
            return base_dir / f"{ticker}.parquet"
        if fmt == "feather":
            return base_dir / f"{ticker}.feather"
        return base_dir / f"{ticker}.csv"

    def _read_with_fallback(
        self, path: Path, ticker: str, profile: str
    ) -> pd.DataFrame | None:
        """Reads a file with specific logic for different formats and fallbacks."""
        if not path.exists():
            return None

        try:
            if path.suffix == ".feather":
                return pd.read_feather(path)
            if path.suffix == ".parquet":
                return pd.read_parquet(path)
            if path.suffix == ".csv":
                try:
                    return pd.read_csv(path, parse_dates=["date"])
                except ValueError as e:
                    if "Missing column provided to 'parse_dates': 'date'" in str(e):
                        df = pd.read_csv(path)
                        if "Date" in df.columns:
                            df = df.rename(columns={"Date": "date"})
                            df["date"] = pd.to_datetime(df["date"], errors="coerce")
                        return df
                    raise
            return None
        except Exception as e:
            msg = f"{self._ui_prefix} èª­ã¿è¾¼ã¿å¤±æ•—: {path.name} ({e})"
            self._warn_once(ticker, profile, f"read_error:{path.name}", msg)
            # Try CSV as a last resort if another format failed
            if path.suffix != ".csv":
                csv_path = path.with_suffix(".csv")
                if csv_path.exists():
                    return self._read_with_fallback(csv_path, ticker, profile)
            return None

    def read(self, ticker: str, profile: str) -> pd.DataFrame | None:
        """Reads data from the cache, handling different formats and normalization."""
        
        # rollingç„¡åŠ¹åŒ–è¨­å®šã®ãƒã‚§ãƒƒã‚¯
        if profile == "rolling" and getattr(self.settings.cache, 'disable_rolling_cache', False):
            # rollingãŒç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã‚‹å ´åˆã€baseã‹ã‚‰èª­ã¿è¾¼ã‚“ã§tailå‡¦ç†
            base_df = self._read_base_and_tail(ticker)
            if base_df is not None:
                return base_df
            # baseãŒç„¡ã„å ´åˆã¯full_backupã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            profile = "full"
        
        base = self.full_dir if profile == "full" else self.rolling_dir
        path = self._detect_path(base, ticker)

        df = self._read_with_fallback(path, ticker, profile)
        if df is None:
            # rolling cacheãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯é›†ç´„ãƒ­ã‚°ã«å ±å‘Š
            if profile == "rolling":
                report_rolling_issue(
                    "missing_rolling", ticker, "rolling cache not found"
                )
            return None

        # Normalize columns
        df.columns = [c.lower() for c in df.columns]
        if df.columns.has_duplicates:
            df = df.loc[:, ~df.columns.duplicated(keep="first")]

        if "date" in df.columns:
            df["date"] = pd.to_datetime(df["date"], errors="coerce")
            df = (
                df.dropna(subset=["date"])
                .sort_values("date")
                .drop_duplicates("date")
                .reset_index(drop=True)
            )

        # Backward compatibility: some historical rolling CSVs used '6d_return'
        # instead of 'return_6d'. We standardize to 'return_6d' on load so that
        # downstream code (which uniformly expects 'return_6d') continues to work.
        # If both exist, prefer the already-standard 'return_6d' and drop the legacy.
        if "6d_return" in df.columns:
            if "return_6d" not in df.columns:
                try:
                    df = df.rename(columns={"6d_return": "return_6d"})
                except Exception:
                    pass
            else:
                # Both columns present: keep the canonical one; drop the legacy.
                try:
                    df = df.drop(columns=["6d_return"], errors="ignore")
                except Exception:
                    pass

        # æŒ‡æ¨™åˆ—ã‚’å¤§æ–‡å­—ã«æ¨™æº–åŒ–ï¼ˆæ–°æ©Ÿèƒ½ï¼‰
        df = standardize_indicator_columns(df)

        self._perform_health_check(df, ticker, profile)
        return df

    def write_atomic(self, df: pd.DataFrame, ticker: str, profile: str) -> None:
        """Atomically writes a DataFrame to the cache."""
        base = self.full_dir if profile == "full" else self.rolling_dir
        base.mkdir(parents=True, exist_ok=True)
        path = self._detect_path(base, ticker)
        tmp_path = path.with_suffix(path.suffix + ".tmp")

        try:
            # settings may be a SimpleNamespace in tests; use getattr fallbacks
            if profile == "rolling":
                round_dec = getattr(
                    getattr(self, "rolling_cfg", None), "round_decimals", None
                )
            else:
                # Prefer nested settings.cache.round_decimals when available
                round_dec = None
                try:
                    round_dec = getattr(self.settings.cache, "round_decimals", None)
                except Exception:
                    try:
                        round_dec = getattr(self.settings, "round_decimals", None)
                    except Exception:
                        round_dec = None
            df_to_write = round_dataframe(df, round_dec)

            if path.suffix == ".parquet":
                df_to_write.to_parquet(tmp_path, index=False)
            elif path.suffix == ".feather":
                df_to_write.reset_index(drop=True).to_feather(tmp_path)
            else:  # .csv
                _write_dataframe_to_csv(df_to_write, tmp_path, self.settings)

            shutil.move(tmp_path, path)
        finally:
            if os.path.exists(tmp_path):
                try:
                    os.remove(tmp_path)
                except OSError as e:
                    logger.error(f"Failed to remove temporary file {tmp_path}: {e}")

    def _check_nan_rates(self, df: pd.DataFrame, ticker: str, profile: str):
        """Checks for high NaN rates in key indicator columns."""
        try:
            if df is None or df.empty:
                return

            # Build list of indicator columns present in df
            cols = [c for c in df.columns if c.lower() in _INDICATOR_MIN_OBSERVATIONS]
            if not cols:
                return

            # For each column compute a NaN ratio over a recent window but
            # exclude the initial warm-up rows where the indicator cannot exist.
            recent_window = min(len(df), 120)
            warnings: list[tuple[str, float]] = []
            for col in cols:
                try:
                    series = pd.to_numeric(df[col], errors="coerce").reset_index(
                        drop=True
                    )
                except Exception:
                    continue

                lookback = int(_INDICATOR_MIN_OBSERVATIONS.get(col.lower(), 0))

                # If series is shorter than lookback, skip â€” indicator not applicable
                # This handles newly listed stocks where NaN is expected
                if lookback and len(series) <= lookback:
                    continue

                # If the entire series is NaN, warn (indicator missing entirely)
                # But only after confirming we have sufficient data length
                if series.isna().all():
                    warnings.append((col, 1.0))
                    continue

                # Consider only the recent portion where values should be populated
                eval_series = series.tail(recent_window)
                try:
                    nan_ratio = float(eval_series.isna().mean())
                except Exception:
                    nan_ratio = 1.0

                if nan_ratio >= 0.99:
                    warnings.append((col, nan_ratio))

            if warnings:
                # Log a single warning summarizing affected columns
                parts = ", ".join(f"{c}:{r:.2%}" for c, r in warnings)
                msg = f"{self._ui_prefix} âš ï¸ {ticker} {profile} cache: NaNç‡é«˜ ({parts})"
                self._warn_once(ticker, profile, f"nan_rate:{parts}", msg)
        except Exception as e:
            logger.error(f"{self._ui_prefix} NaNç‡ãƒã‚§ãƒƒã‚¯å¤±æ•—: {e}")

    def _check_column_dtypes(self, df: pd.DataFrame, ticker: str, profile: str):
        """Checks for incorrect dtypes in OHLCV columns."""
        for col in ["open", "high", "low", "close", "volume"]:
            if col in df.columns and not pd.api.types.is_numeric_dtype(df[col]):
                dtype_repr = describe_dtype(df[col])
                msg = f"{self._ui_prefix} âš ï¸ {ticker} {profile} cache: {col}å‹ä¸ä¸€è‡´ ({dtype_repr})"
                self._warn_once(ticker, profile, f"dtype:{col}:{dtype_repr}", msg)

    def _check_non_positive_prices(self, df: pd.DataFrame, ticker: str, profile: str):
        """Checks if price columns contain only non-positive values."""
        for col in ["close", "high", "low"]:
            if col in df.columns:
                vals = pd.to_numeric(df[col], errors="coerce")
                if not vals.empty and (vals <= 0).all():
                    msg = (
                        f"{self._ui_prefix} âš ï¸ {ticker} {profile} cache: {col}å…¨ã¦éæ­£å€¤"
                    )
                    self._warn_once(ticker, profile, f"non_positive:{col}", msg)

    def _perform_health_check(
        self, df: pd.DataFrame, ticker: str, profile: str
    ) -> None:
        """Performs a series of health checks on the DataFrame."""
        if df is None or df.empty:
            return
        try:
            # Perform NaN rate checks first
            self._check_nan_rates(df, ticker, profile)
            self._check_column_dtypes(df, ticker, profile)
            self._check_non_positive_prices(df, ticker, profile)
        except Exception as e:
            msg = f"{self._ui_prefix} âš ï¸ {ticker} {profile} cache: å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯å¤±æ•— ({e})"
            self._warn_once(
                ticker, profile, f"healthcheck_error:{type(e).__name__}", msg
            )

    def upsert_both(self, ticker: str, new_rows: pd.DataFrame) -> None:
        """Upserts new rows into both 'full' and 'rolling' caches."""
        for profile in ("full", "rolling"):
            self._upsert_one(ticker, new_rows, profile)

    def _upsert_one(self, ticker: str, new_rows: pd.DataFrame, profile: str) -> None:
        if new_rows is not None and not new_rows.empty and "date" in new_rows.columns:
            new_rows = new_rows.copy()
            new_rows["date"] = pd.to_datetime(new_rows["date"], errors="coerce")
            new_rows = new_rows.dropna(subset=["date"])

        cur = self.read(ticker, profile)
        if cur is None or cur.empty:
            merged = new_rows.copy() if new_rows is not None else pd.DataFrame()
        else:
            merged = (
                pd.concat([cur, new_rows], ignore_index=True)
                if new_rows is not None
                else cur
            )

        if not merged.empty:
            merged = (
                merged.sort_values("date")
                .drop_duplicates("date")
                .reset_index(drop=True)
            )
            if profile == "rolling":
                merged = self._enforce_rolling_window(merged)
            merged = self._recompute_indicators(merged)

        if not merged.empty:
            self.write_atomic(merged, ticker, profile)

    @property
    def _rolling_target_len(self) -> int:
        return int(self.rolling_cfg.base_lookback_days + self.rolling_cfg.buffer_days)

    def _enforce_rolling_window(self, df: pd.DataFrame) -> pd.DataFrame:
        if "date" not in df.columns or df.empty:
            return df
        target_len = self._rolling_target_len
        return (
            df.tail(target_len).reset_index(drop=True) if len(df) > target_len else df
        )

    def prune_rolling_if_needed(self, anchor_ticker: str = "SPY") -> dict:
        """Prunes the rolling cache if enough new data has been added."""
        try:
            meta_text = self.rolling_meta_path.read_text(encoding="utf-8")
            last_meta = json.loads(meta_text)
        except (FileNotFoundError, json.JSONDecodeError):
            last_meta = {"anchor_rows_at_prune": 0}

        anchor_df = self.read(anchor_ticker, "rolling")
        if anchor_df is None or anchor_df.empty:
            logger.info(f"{self._ui_prefix} rollingæœªæ•´å‚™ã®ãŸã‚pruneã‚¹ã‚­ãƒƒãƒ—")
            return {"pruned_files": 0, "dropped_rows_total": 0}

        cur_rows = len(anchor_df)
        prev_rows = int(last_meta.get("anchor_rows_at_prune", 0))
        progressed = cur_rows - prev_rows

        prune_chunk = int(self.rolling_cfg.prune_chunk_days)
        if progressed < prune_chunk:
            msg = f"{self._ui_prefix} é€²æ—{progressed}å–¶æ¥­æ—¥ (<{prune_chunk}) ã®ãŸã‚pruneä¸è¦"
            logger.info(msg)
            return {"pruned_files": 0, "dropped_rows_total": 0}

        msg = f"{self._ui_prefix} â³ pruneé–‹å§‹: anchor={anchor_ticker}, é€²æ—={progressed}å–¶æ¥­æ—¥"
        logger.info(msg)
        pruned_files, dropped_total = 0, 0

        for path in self.rolling_dir.glob("*.*"):
            if path.name.startswith("_"):
                continue

            df = self.read(path.stem, "rolling")
            if df is None or df.empty:
                continue

            can_drop = len(df) - self._rolling_target_len
            drop_n = min(prune_chunk, can_drop)
            if drop_n > 0:
                new_df = df.iloc[drop_n:].reset_index(drop=True)
                self.write_atomic(new_df, path.stem, "rolling")
                pruned_files += 1
                dropped_total += drop_n

        self.rolling_meta_path.write_text(
            json.dumps({"anchor_rows_at_prune": cur_rows}, indent=2), encoding="utf-8"
        )
        msg = f"{self._ui_prefix} âœ… pruneå®Œäº†: files={pruned_files}, dropped_rows={dropped_total}"
        logger.info(msg)
        return {"pruned_files": pruned_files, "dropped_rows_total": dropped_total}

    def analyze_rolling_gaps(self, system_symbols: list[str] | None = None) -> dict:
        """
        rolling cache ã®æ•´å‚™çŠ¶æ³ã‚’åˆ†æã—ã€æœªæ•´å‚™ã‚·ãƒ³ãƒœãƒ«ã®è©³ç´°ãƒ­ã‚°ã‚’é›†ç´„ã™ã‚‹ã€‚

        Args:
            system_symbols: åˆ†æå¯¾è±¡ã‚·ãƒ³ãƒœãƒ«ã®ãƒªã‚¹ãƒˆã€‚Noneã®å ´åˆã¯base cacheã®å…¨ã‚·ãƒ³ãƒœãƒ«
                ã‚’å¯¾è±¡ã¨ã™ã‚‹ã€‚

        Returns:
            åˆ†æçµæœè¾æ›¸:
            - total_symbols: åˆ†æå¯¾è±¡ã‚·ãƒ³ãƒœãƒ«æ•°
            - available_in_rolling: rolling cacheã«å­˜åœ¨ã™ã‚‹ã‚·ãƒ³ãƒœãƒ«æ•°
            - missing_from_rolling: rolling cacheã«å­˜åœ¨ã—ãªã„ã‚·ãƒ³ãƒœãƒ«æ•°
            - missing_symbols: æœªæ•´å‚™ã‚·ãƒ³ãƒœãƒ«ã®ãƒªã‚¹ãƒˆ
            - coverage_percentage: ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡ï¼ˆ%ï¼‰
        """
        logger.info(f"{self._ui_prefix} ğŸ” rolling cacheæ•´å‚™çŠ¶æ³ã®åˆ†æã‚’é–‹å§‹")

        # åˆ†æå¯¾è±¡ã‚·ãƒ³ãƒœãƒ«ã®æ±ºå®š
        if system_symbols is None:
            # base cacheã‹ã‚‰å…¨ã‚·ãƒ³ãƒœãƒ«ã‚’å–å¾—
            base_files = list(self.full_dir.parent.glob(f"{BASE_SUBDIR}/*.*"))
            system_symbols = [p.stem for p in base_files if not p.name.startswith("_")]
            logger.info(
                f"{self._ui_prefix} base cacheã‹ã‚‰{len(system_symbols)}ã‚·ãƒ³ãƒœãƒ«ã‚’æ¤œå‡º"
            )
        else:
            logger.info(
                f"{self._ui_prefix} æŒ‡å®šã•ã‚ŒãŸ{len(system_symbols)}ã‚·ãƒ³ãƒœãƒ«ã‚’åˆ†æå¯¾è±¡ã¨ã—ã¾ã™"
            )

        available_symbols = []
        missing_symbols = []

        # å„ã‚·ãƒ³ãƒœãƒ«ã®rolling cacheå­˜åœ¨ç¢ºèª
        for symbol in system_symbols:
            rolling_data = self.read(symbol, "rolling")
            if rolling_data is not None and not rolling_data.empty:
                available_symbols.append(symbol)
            else:
                missing_symbols.append(symbol)
                # é›†ç´„ãƒ­ã‚°ã«æœªæ•´å‚™ã‚’å ±å‘Š
                report_rolling_issue("missing_from_analysis", symbol)

        total_symbols = len(system_symbols)
        available_count = len(available_symbols)
        missing_count = len(missing_symbols)
        coverage_percentage = (
            (available_count / total_symbols * 100) if total_symbols > 0 else 0
        )

        # çµæœãƒ­ã‚°
        logger.info(f"{self._ui_prefix} ğŸ“Š åˆ†æå®Œäº†:")
        logger.info(f"{self._ui_prefix}   - åˆ†æå¯¾è±¡: {total_symbols}ã‚·ãƒ³ãƒœãƒ«")
        logger.info(
            f"{self._ui_prefix}   - rolling cacheæ•´å‚™æ¸ˆã¿: {available_count}ã‚·ãƒ³ãƒœãƒ«"
        )
        logger.info(
            f"{self._ui_prefix}   - rolling cacheæœªæ•´å‚™: {missing_count}ã‚·ãƒ³ãƒœãƒ«"
        )
        logger.info(f"{self._ui_prefix}   - ã‚«ãƒãƒ¬ãƒƒã‚¸: {coverage_percentage:.1f}%")

        if missing_symbols:
            # é›†ç´„ãƒ­ã‚°ã«ã‚ˆã‚‹æœªæ•´å‚™ã‚·ãƒ³ãƒœãƒ«å ±å‘Šï¼ˆæ—¢å­˜ã®warningã‚’ç½®ãæ›ãˆï¼‰
            for symbol in missing_symbols[:10]:  # å…ˆé ­10ä»¶ã‚’è©³ç´°å ±å‘Š
                report_rolling_issue("missing_analysis_detailed", symbol)

            # å¾“æ¥ã®å½¢å¼ã®ãƒ­ã‚°ã‚‚æ¡ä»¶ä»˜ãã§ç¶­æŒï¼ˆé›†ç´„ç„¡åŠ¹æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            if not _rolling_issue_aggregator.compact_mode:
                logger.warning(
                    f"{self._ui_prefix} ğŸš¨ æœªæ•´å‚™ã‚·ãƒ³ãƒœãƒ«: {missing_symbols[:10]}"
                )
                if len(missing_symbols) > 10:
                    logger.warning(
                        f"{self._ui_prefix}   ... ä»–{len(missing_symbols) - 10}ã‚·ãƒ³ãƒœãƒ«"
                    )

        return {
            "total_symbols": total_symbols,
            "available_in_rolling": available_count,
            "missing_from_rolling": missing_count,
            "missing_symbols": missing_symbols,
            "coverage_percentage": coverage_percentage,
        }

    def get_rolling_health_summary(self) -> dict:
        """
        rolling cache ã®å¥å…¨æ€§ã‚µãƒãƒªãƒ¼ã‚’å–å¾—ã™ã‚‹ã€‚

        Returns:
            å¥å…¨æ€§ã‚µãƒãƒªãƒ¼è¾æ›¸:
            - meta_exists: ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨
            - meta_content: ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹
            - rolling_files_count: rolling cacheãƒ•ã‚¡ã‚¤ãƒ«æ•°
            - target_length: ç›®æ¨™ãƒ‡ãƒ¼ã‚¿é•·
            - anchor_symbol_status: ã‚¢ãƒ³ã‚«ãƒ¼ã‚·ãƒ³ãƒœãƒ«ï¼ˆSPYï¼‰ã®çŠ¶æ…‹
        """
        logger.info(f"{self._ui_prefix} ğŸ©º rolling cacheå¥å…¨æ€§ãƒã‚§ãƒƒã‚¯ã‚’é–‹å§‹")

        # ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
        meta_exists = self.rolling_meta_path.exists()
        meta_content = {}
        if meta_exists:
            try:
                with open(self.rolling_meta_path, encoding="utf-8") as f:
                    meta_content = json.load(f)
            except (json.JSONDecodeError, OSError) as e:
                logger.warning(f"{self._ui_prefix} ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")

        # rolling cacheãƒ•ã‚¡ã‚¤ãƒ«æ•°
        rolling_files = [
            p for p in self.rolling_dir.glob("*.*") if not p.name.startswith("_")
        ]
        rolling_files_count = len(rolling_files)

        # ç›®æ¨™ãƒ‡ãƒ¼ã‚¿é•·
        target_length = self._rolling_target_len

        # SPYã‚¢ãƒ³ã‚«ãƒ¼ã®çŠ¶æ…‹ç¢ºèª
        spy_data = self.read("SPY", "rolling")
        anchor_status = {
            "exists": spy_data is not None and not spy_data.empty,
            "rows": len(spy_data) if spy_data is not None else 0,
            "meets_target": False,
        }
        if spy_data is not None:
            anchor_status["meets_target"] = len(spy_data) >= target_length

        result = {
            "meta_exists": meta_exists,
            "meta_content": meta_content,
            "rolling_files_count": rolling_files_count,
            "target_length": target_length,
            "anchor_symbol_status": anchor_status,
        }

        logger.info(f"{self._ui_prefix} âœ… å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯å®Œäº†:")
        logger.info(
            f"{self._ui_prefix}   - ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«: {'å­˜åœ¨' if meta_exists else 'ä¸åœ¨'}"
        )
        logger.info(f"{self._ui_prefix}   - rolling files: {rolling_files_count}å€‹")
        spy_status = "æ­£å¸¸" if anchor_status["meets_target"] else "è¦ç¢ºèª"
        logger.info(
            f"{self._ui_prefix}   - SPYçŠ¶æ…‹: {spy_status} ({anchor_status['rows']}è¡Œ)"
        )

        return result

    def read_batch_parallel(
        self,
        symbols: list[str],
        profile: str = "rolling",
        max_workers: int | None = None,
        fallback_profile: str | None = "full",
        progress_callback=None,
    ) -> dict[str, pd.DataFrame]:
        """
        ã‚·ãƒ³ãƒœãƒ«ã®ãƒªã‚¹ãƒˆã‚’ä¸¦åˆ—ã§èª­ã¿è¾¼ã¿ã€è¾æ›¸ã¨ã—ã¦è¿”ã™ã€‚
        Phase2ãƒœãƒˆãƒ«ãƒãƒƒã‚¯è§£æ¶ˆã®ãŸã‚ã®ã‚­ãƒ¼æ©Ÿèƒ½ã€‚

        Args:
            symbols: èª­ã¿è¾¼ã‚€ã‚·ãƒ³ãƒœãƒ«ã®ãƒªã‚¹ãƒˆ
            profile: æœ€åˆã«è©¦è¡Œã™ã‚‹ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ« ("rolling", "full")
            max_workers: ThreadPoolExecutorã®ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã€‚Noneã®å ´åˆã¯è‡ªå‹•æ±ºå®š
            fallback_profile: æœ€åˆã®profileã§å¤±æ•—ã—ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            progress_callback: é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯(loaded_count, total_count)

        Returns:
            {symbol: DataFrame} ã®è¾æ›¸ã€‚èª­ã¿è¾¼ã¿å¤±æ•—ã—ãŸã‚·ãƒ³ãƒœãƒ«ã¯é™¤å¤–ã•ã‚Œã‚‹
        """
        if not symbols:
            return {}

        # å°ã•ãªãƒãƒƒãƒã‚µã‚¤ã‚ºã§ã¯ä¸¦åˆ—å‡¦ç†ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒå¤§ãã„ãŸã‚ã€
        # ã—ãã„å€¤ä»¥ä¸‹ã§ã¯ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«å‡¦ç†ã‚’é¸æŠ
        if len(symbols) <= 20:
            return self._read_batch_sequential(
                symbols, profile, fallback_profile, progress_callback
            )

        if max_workers is None:
            cpu_count = os.cpu_count() or 4
            # I/Oãƒã‚¦ãƒ³ãƒ‰ãªã®ã§ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’é©åº¦ã«åˆ¶é™
            max_workers = min(max(2, cpu_count // 2), min(8, len(symbols)))

        result = {}
        total_symbols = len(symbols)
        loaded_count = 0

        def _read_single(symbol: str) -> tuple[str, pd.DataFrame | None]:
            """å˜ä¸€ã‚·ãƒ³ãƒœãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰"""
            df = None
            try:
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯ã‚’é«˜é€ŸåŒ–
                df = self._read_optimized(symbol, profile)
                if df is None or df.empty:
                    if fallback_profile and fallback_profile != profile:
                        df = self._read_optimized(symbol, fallback_profile)
                return symbol, df
            except Exception:
                return symbol, None

        try:
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«å‡¦ç†ã—ã¦ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’åˆ¶å¾¡
                chunk_size = max(10, len(symbols) // max_workers)
                chunks = [
                    symbols[i : i + chunk_size]
                    for i in range(0, len(symbols), chunk_size)
                ]

                for chunk in chunks:
                    future_to_symbol = {
                        executor.submit(_read_single, symbol): symbol
                        for symbol in chunk
                    }

                    # ãƒãƒ£ãƒ³ã‚¯çµæœã‚’åé›†
                    for future in as_completed(future_to_symbol):
                        symbol, df = future.result()
                        if df is not None and not df.empty:
                            result[symbol] = df
                        loaded_count += 1
                        if progress_callback:
                            try:
                                progress_callback(loaded_count, total_symbols)
                            except Exception:
                                pass

        except Exception as e:
            # ä¸¦åˆ—å‡¦ç†å¤±æ•—æ™‚ã¯ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            logger.warning(f"ä¸¦åˆ—èª­ã¿è¾¼ã¿å¤±æ•—ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {e}")
            return self._read_batch_sequential(
                symbols, profile, fallback_profile, progress_callback
            )

        return result

    def _read_batch_sequential(
        self,
        symbols: list[str],
        profile: str,
        fallback_profile: str | None = None,
        progress_callback=None,
    ) -> dict[str, pd.DataFrame]:
        """æœ€é©åŒ–ã•ã‚ŒãŸã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«èª­ã¿è¾¼ã¿"""
        result = {}
        loaded_count = 0

        for symbol in symbols:
            try:
                df = self._read_optimized(symbol, profile)
                if df is None or df.empty:
                    if fallback_profile and fallback_profile != profile:
                        df = self._read_optimized(symbol, fallback_profile)
                if df is not None and not df.empty:
                    result[symbol] = df
                loaded_count += 1
                if progress_callback:
                    try:
                        progress_callback(loaded_count, len(symbols))
                    except Exception:
                        pass
            except Exception:
                loaded_count += 1
                if progress_callback:
                    try:
                        progress_callback(loaded_count, len(symbols))
                    except Exception:
                        pass
        return result

    def _read_optimized(self, symbol: str, profile: str) -> pd.DataFrame | None:
        """æœ€é©åŒ–ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡é‡è¦–ï¼‰"""
        try:
            base = self.full_dir if profile == "full" else self.rolling_dir
            path = self._detect_path(base, symbol)

            if not path or not path.exists():
                return None

            # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼åˆ¥ã®æœ€é©åŒ–èª­ã¿è¾¼ã¿
            if path.suffix == ".parquet":
                df = pd.read_parquet(path)
            elif path.suffix == ".feather":
                df = pd.read_feather(path)
            else:  # CSV
                # CSVã¯èª­ã¿è¾¼ã¿æ™‚ã«å‹ã‚’æŒ‡å®šã—ã¦é«˜é€ŸåŒ–
                df = pd.read_csv(
                    path,
                    dtype={
                        "Volume": "float32",
                        "Close": "float32",
                        "High": "float32",
                        "Low": "float32",
                        "Open": "float32",
                    },
                    parse_dates=(
                        ["Date"]
                        if "Date" in pd.read_csv(path, nrows=0).columns
                        else None
                    ),
                )

            if df is None or df.empty:
                return None

            # è»½é‡ãªåˆ—æ­£è¦åŒ–
            df.columns = [c.lower() for c in df.columns]
            if df.columns.has_duplicates:
                df = df.loc[:, ~df.columns.duplicated(keep="first")]

            return df
        except Exception:
            return None

    def optimize_dataframe_memory(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        DataFrameã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æœ€é©åŒ–ã™ã‚‹ã€‚
        Phase2ãƒœãƒˆãƒ«ãƒãƒƒã‚¯è§£æ¶ˆã®ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿å‹ã®æœ€é©åŒ–ã‚’å®Ÿè¡Œã€‚

        Args:
            df: æœ€é©åŒ–å¯¾è±¡ã®DataFrame

        Returns:
            ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã•ã‚ŒãŸDataFrame
        """
        if df is None or df.empty:
            return df

        optimized = df.copy()

        # æ•°å€¤ã‚«ãƒ©ãƒ ã®å‹æœ€é©åŒ–
        for col in optimized.columns:
            if col.lower() in ["date"]:
                # æ—¥ä»˜åˆ—ã¯ãã®ã¾ã¾
                continue

            if optimized[col].dtype == "object":
                # ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå‹ã§æ•°å€¤å¤‰æ›å¯èƒ½ãªã‚‚ã®ã‚’å¤‰æ›
                try:
                    numeric = pd.to_numeric(optimized[col], errors="coerce")
                    if not numeric.isna().all():
                        optimized[col] = numeric
                except Exception:
                    continue

            # æµ®å‹•å°æ•°ç‚¹æ•°ã®å‹æœ€é©åŒ–
            if optimized[col].dtype == "float64":
                try:
                    # float32ã§è¡¨ç¾å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
                    col_min = optimized[col].min()
                    col_max = optimized[col].max()
                    if (pd.isna(col_min) or col_min >= np.finfo(np.float32).min) and (
                        pd.isna(col_max) or col_max <= np.finfo(np.float32).max
                    ):
                        optimized[col] = optimized[col].astype("float32")
                except Exception:
                    pass

            # æ•´æ•°ã®å‹æœ€é©åŒ–
            elif optimized[col].dtype == "int64":
                try:
                    col_min = optimized[col].min()
                    col_max = optimized[col].max()

                    # int32ã§ååˆ†ã‹ãƒã‚§ãƒƒã‚¯
                    if (
                        col_min >= np.iinfo(np.int32).min
                        and col_max <= np.iinfo(np.int32).max
                    ):
                        optimized[col] = optimized[col].astype("int32")
                    # int16ã§ååˆ†ã‹ãƒã‚§ãƒƒã‚¯
                    elif (
                        col_min >= np.iinfo(np.int16).min
                        and col_max <= np.iinfo(np.int16).max
                    ):
                        optimized[col] = optimized[col].astype("int16")
                except Exception:
                    pass

        return optimized

    def remove_unnecessary_columns(
        self, df: pd.DataFrame, keep_columns: list[str] | None = None
    ) -> pd.DataFrame:
        """
        ä¸è¦ãªã‚«ãƒ©ãƒ ã‚’å‰Šé™¤ã—ã¦ãƒ¡ãƒ¢ãƒªã‚’ç¯€ç´„ã™ã‚‹ã€‚

        Args:
            df: å¯¾è±¡DataFrame
            keep_columns: ä¿æŒã™ã‚‹ã‚«ãƒ©ãƒ ã®ãƒªã‚¹ãƒˆã€‚Noneã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé‡è¦ã‚«ãƒ©ãƒ ã‚’ä½¿ç”¨

        Returns:
            å¿…è¦ã‚«ãƒ©ãƒ ã®ã¿ã®DataFrame
        """
        if df is None or df.empty:
            return df

        if keep_columns is None:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®é‡è¦ã‚«ãƒ©ãƒ ï¼ˆSystem1-7ã§ä½¿ç”¨ã•ã‚Œã‚‹ä¸»è¦æŒ‡æ¨™ï¼‰
            keep_columns = [
                "date",
                "Date",
                "open",
                "high",
                "low",
                "close",
                "volume",
                "Open",
                "High",
                "Low",
                "Close",
                "Volume",
                "adjusted_close",
                "sma25",
                "sma50",
                "sma100",
                "sma150",
                "sma200",
                "atr10",
                "atr20",
                "atr40",
                "atr50",
                "rsi3",
                "rsi4",
                "adx7",
                "roc200",
                "dollarvolume20",
                "dollarvolume50",
                "avgvolume50",
                "atr_ratio",
                "atr_pct",
                "return_3d",
                "return_6d",
                "uptwodays",
                "twodayup",
                "hv50",
                "min_50",
                "max_70",
            ]

        # å­˜åœ¨ã™ã‚‹ã‚«ãƒ©ãƒ ã®ã¿ã‚’ä¿æŒ
        available_columns = [col for col in keep_columns if col in df.columns]

        if available_columns:
            return df[available_columns].copy()
        else:
            # ä¿æŒã™ã‚‹ã‚«ãƒ©ãƒ ãŒä¸€ã¤ã‚‚è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å…ƒã®DataFrameã‚’è¿”ã™
            return df


def _base_dir() -> Path:
    settings = get_settings(create_dirs=True)
    base = Path(settings.DATA_CACHE_DIR) / BASE_SUBDIR
    base.mkdir(parents=True, exist_ok=True)
    return base


def compute_base_indicators(df: pd.DataFrame) -> pd.DataFrame:
    """OHLCVã®DataFrameã«å…±é€šãƒ™ãƒ¼ã‚¹æŒ‡æ¨™ã‚’ä»˜åŠ ã—ã¦è¿”ã™ã€‚"""
    if df is None or df.empty:
        return df

    x = df.copy()

    # Normalize column names
    rename_map = {c: c.lower() for c in x.columns}
    x = x.rename(columns=rename_map)

    # Ensure 'Date' column and set as index
    if "date" in x.columns:
        x = x.rename(columns={"date": "Date"})
    if "Date" in x.columns:
        x["Date"] = pd.to_datetime(x["Date"], errors="coerce")
        x = x.dropna(subset=["Date"]).sort_values("Date").set_index("Date")

    # Standardize OHLCV column names
    ohlcv_map = {
        "open": "Open",
        "high": "High",
        "low": "Low",
        "adjusted_close": "Close",
        "adj_close": "Close",
        "adjclose": "Close",
        "close": "Close",
        "volume": "Volume",
        "vol": "Volume",
    }
    final_rename = {c: ohlcv_map[c] for c in x.columns if c in ohlcv_map}
    x = x.rename(columns=final_rename)

    required = {"High", "Low", "Close"}
    if not required.issubset(x.columns):
        missing_cols = required - set(x.columns)
        logger.warning(
            f"{__name__}: å¿…é ˆåˆ—æ¬ è½ã®ãŸã‚ã‚¤ãƒ³ã‚¸è¨ˆç®—ã‚’ã‚¹ã‚­ãƒƒãƒ—: missing={missing_cols}"
        )
        return x.reset_index()

    close = pd.to_numeric(x["Close"], errors="coerce")
    high = pd.to_numeric(x["High"], errors="coerce")
    low = pd.to_numeric(x["Low"], errors="coerce")
    vol = None
    if "Volume" in x.columns:
        vol = pd.to_numeric(x["Volume"], errors="coerce")

    # SMA/EMA - å¤§æ–‡å­—çµ±ä¸€
    for n in [25, 50, 100, 150, 200]:
        x[f"SMA{n}"] = close.rolling(n).mean()
    for n in [20, 50]:
        x[f"EMA{n}"] = close.ewm(span=n, adjust=False).mean()

    # ATR - å¤§æ–‡å­—çµ±ä¸€
    tr = pd.concat(
        [high - low, (high - close.shift()).abs(), (low - close.shift()).abs()], axis=1
    ).max(axis=1)
    for n in [10, 14, 20, 40, 50]:
        x[f"ATR{n}"] = tr.rolling(n).mean()

    # RSI (Wilder) - å¤§æ–‡å­—çµ±ä¸€
    def _rsi(s: pd.Series, n: int) -> pd.Series:
        delta = s.diff()
        gain = delta.clip(lower=0).ewm(alpha=1 / n, adjust=False).mean()
        loss = -delta.clip(upper=0).ewm(alpha=1 / n, adjust=False).mean()
        rs = gain / loss.replace(0, np.nan)
        return 100 - (100 / (1 + rs))

    for n in [3, 4, 14]:
        x[f"RSI{n}"] = _rsi(close, n)

    # ROC & HV - å¤§æ–‡å­—çµ±ä¸€
    x["ROC200"] = close.pct_change(200) * 100.0
    log_ret = (close / close.shift(1)).apply(np.log)
    std_dev = log_ret.rolling(50).std()
    x["HV50"] = std_dev * np.sqrt(252) * 100.0

    # DollarVolume - å¤§æ–‡å­—çµ±ä¸€
    if vol is not None:
        x["DollarVolume20"] = (close * vol).rolling(20).mean()
        x["DollarVolume50"] = (close * vol).rolling(50).mean()

    return x.reset_index()


def get_indicator_column_flexible(df: pd.DataFrame, indicator: str) -> pd.Series | None:
    """æŒ‡æ¨™åˆ—ã‚’å¤§æ–‡å­—ãƒ»å°æ–‡å­—ä¸¡å¯¾å¿œã§å–å¾—ã€‚å¤§æ–‡å­—å„ªå…ˆã€å°æ–‡å­—ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‚

    Args:
        df: å¯¾è±¡DataFrame
        indicator: æŒ‡æ¨™åï¼ˆä¾‹: "ATR10"ï¼‰

    Returns:
        è©²å½“ã™ã‚‹åˆ—ã®Seriesã€å­˜åœ¨ã—ãªã„å ´åˆã¯None
    """
    # å¤§æ–‡å­—å„ªå…ˆ
    if indicator in df.columns:
        return df[indicator]

    # å°æ–‡å­—ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    lower_indicator = indicator.lower()
    if lower_indicator in df.columns:
        return df[lower_indicator]

    return None


def standardize_indicator_columns(df: pd.DataFrame) -> pd.DataFrame:
    """æŒ‡æ¨™åˆ—åã‚’å¤§æ–‡å­—ã«æ¨™æº–åŒ–ã€‚æ—¢å­˜ã®å°æ–‡å­—åˆ—ã¯å‰Šé™¤ã€‚

    Args:
        df: å¯¾è±¡DataFrame

    Returns:
        æ¨™æº–åŒ–ã•ã‚ŒãŸDataFrame
    """
    result = df.copy()

    # æ¨™æº–åŒ–ã™ã‚‹æŒ‡æ¨™ã®ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆå°æ–‡å­— -> å¤§æ–‡å­—ï¼‰
    indicator_mapping = {
        "atr10": "ATR10",
        "atr14": "ATR14",
        "atr20": "ATR20",
        "atr40": "ATR40",
        "atr50": "ATR50",
        "sma25": "SMA25",
        "sma50": "SMA50",
        "sma100": "SMA100",
        "sma150": "SMA150",
        "sma200": "SMA200",
        "ema20": "EMA20",
        "ema50": "EMA50",
        "rsi3": "RSI3",
        "rsi4": "RSI4",
        "rsi14": "RSI14",
        "roc200": "ROC200",
        "hv50": "HV50",
        "dollarvolume20": "DollarVolume20",
        "dollarvolume50": "DollarVolume50",
        "avgvolume50": "AvgVolume50",
        "adx7": "ADX7",
        "drop3d": "Drop3D",
        "return_pct": "Return_Pct",
        # è¿½åŠ ã®ãƒãƒƒãƒ”ãƒ³ã‚°
        "atr_ratio": "ATR_Ratio",
        "atr_pct": "ATR_Pct",
        "return_3d": "Return_3D",
        "return_6d": "Return_6D",
        "uptwodays": "UpTwoDays",
        "twodayup": "TwoDayUp",
    }

    # å°æ–‡å­— -> å¤§æ–‡å­—ã¸ã®å¤‰æ›ã¨é‡è¤‡å‰Šé™¤
    for old_name, new_name in indicator_mapping.items():
        if old_name in result.columns and new_name not in result.columns:
            # å°æ–‡å­—åˆ—ã‚’å¤§æ–‡å­—ã«ãƒªãƒãƒ¼ãƒ 
            result = result.rename(columns={old_name: new_name})
        elif old_name in result.columns and new_name in result.columns:
            # ä¸¡æ–¹å­˜åœ¨ã™ã‚‹å ´åˆã¯å°æ–‡å­—åˆ—ã‚’å‰Šé™¤
            result = result.drop(columns=[old_name])

    return result


def base_cache_path(symbol: str) -> Path:
    return _base_dir() / f"{safe_filename(symbol)}.csv"


def save_base_cache(
    symbol: str, df: pd.DataFrame, settings: Settings | None = None
) -> Path:
    """Saves the base cache DataFrame to a CSV file."""
    path = base_cache_path(symbol)
    df_reset = df.reset_index() if df.index.name is not None else df
    path.parent.mkdir(parents=True, exist_ok=True)

    # Backwards-compatible: if settings is not provided, obtain global settings
    if settings is None:
        settings = get_settings(create_dirs=True)

    round_dec = getattr(getattr(settings, "cache", None), "round_decimals", None)
    df_to_write = round_dataframe(df_reset, round_dec)

    _write_dataframe_to_csv(df_to_write, path, settings)
    return path


_DEFAULT_CACHE_MANAGER: CacheManager | None = None


def _get_default_cache_manager() -> CacheManager:
    """Returns a module-level singleton CacheManager instance."""
    global _DEFAULT_CACHE_MANAGER
    if _DEFAULT_CACHE_MANAGER is None:
        settings = get_settings(create_dirs=False)
        _DEFAULT_CACHE_MANAGER = CacheManager(settings)
    return _DEFAULT_CACHE_MANAGER


def _read_legacy_cache(symbol: str) -> pd.DataFrame | None:
    """Reads from the old cache location for backward compatibility."""
    legacy_path = Path("data_cache") / f"{safe_filename(symbol)}.csv"
    if not legacy_path.exists():
        return None
    try:
        return pd.read_csv(legacy_path)
    except Exception:
        return None


def load_base_cache(
    symbol: str,
    *,
    rebuild_if_missing: bool = True,
    cache_manager: CacheManager | None = None,
    min_last_date: pd.Timestamp | None = None,
    allowed_recent_dates: Iterable[object] | None = None,
    prefer_precomputed_indicators: bool = True,
) -> pd.DataFrame | None:
    """Loads base cache, with optional freshness validation and rebuilding."""
    cm = cache_manager or _get_default_cache_manager()
    path = base_cache_path(symbol)
    df: pd.DataFrame | None = None

    if path.exists():
        try:
            df = pd.read_csv(path, parse_dates=["Date"])
            df = df.dropna(subset=["Date"]).sort_values("Date").set_index("Date")
        except Exception:
            df = None

    if df is not None:
        last_date = df.index[-1] if not df.empty else None
        is_stale = False
        if allowed_recent_dates and last_date:
            allowed_set = {
                pd.Timestamp(cast(Any, d)).normalize()
                for d in allowed_recent_dates
                if d is not None and not pd.isna(d)
            }
            if last_date.normalize() not in allowed_set:
                is_stale = True
        if not is_stale and min_last_date and last_date:
            if last_date.normalize() < pd.Timestamp(min_last_date).normalize():
                is_stale = True

        if not is_stale:
            return df.reset_index()

        if not rebuild_if_missing:
            return df.reset_index()

        msg = f"{__name__}: base cache for {symbol} is stale, rebuilding."
        logger.info(msg)
        df = None  # Force rebuild

    if df is None and rebuild_if_missing:
        raw = (
            cm.read(symbol, "full")
            or cm.read(symbol, "rolling")
            or _read_legacy_cache(symbol)
        )
        if raw is not None and not raw.empty:
            # If caller prefers to reuse precomputed indicator columns and
            # the raw frame appears to contain indicator columns, avoid
            # calling expensive `compute_base_indicators` and save the raw
            # data as the base cache directly.
            try:
                lc_cols = {c.lower() for c in raw.columns}
            except Exception:
                lc_cols = set()

            has_any_indicator = any((col in lc_cols) for col in MAIN_INDICATOR_COLUMNS)
            if prefer_precomputed_indicators and has_any_indicator:
                out = raw.copy()
                # Normalize date column name to 'Date' for base cache consistency
                if "date" in out.columns and "Date" not in out.columns:
                    out = out.rename(columns={"date": "Date"})
                # If index is a DatetimeIndex and there's no Date column, expose it
                if "Date" not in out.columns and isinstance(
                    out.index, pd.DatetimeIndex
                ):
                    try:
                        out = out.reset_index()
                        # ensure the date column is named 'Date'
                        if out.columns[0].lower() == "index":
                            out = out.rename(columns={out.columns[0]: "Date"})
                    except Exception:
                        pass
                try:
                    save_base_cache(symbol, out, cm.settings)
                except Exception:
                    # fallback to computing indicators if saving fails
                    out = compute_base_indicators(raw)
                    save_base_cache(symbol, out, cm.settings)
                return out

            # otherwise compute indicators normally
            out = compute_base_indicators(raw)
            save_base_cache(symbol, out, cm.settings)
            return out

    return df.reset_index() if df is not None else None
