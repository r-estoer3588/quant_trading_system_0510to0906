# ============================================================================
# ğŸ§  Context Note
# ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥å±¤ã®ç·åˆç®¡ç†ã€‚éšå±¤æ§‹é€ : full_backup â†’ baseï¼ˆæŒ‡æ¨™ä»˜ä¸æ¸ˆï¼‰ â†’ rollingï¼ˆå½“æ—¥ç”¨ï¼‰
#
# å‰ææ¡ä»¶ï¼š
#   - ã™ã¹ã¦ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ I/O ã¯ CacheManager çµŒç”±ã®ã¿ã€‚data_cache/ ã®ç›´å‚ç…§ãƒ»ç›´æ¥æ›¸è¾¼ç¦æ­¢
#   - rolling: å½“æ—¥ã‚’å«ã‚€ç›´è¿‘ N æ—¥ã€‚base: æŒ‡æ¨™ä»˜ä¸æ¸ˆãƒ•ãƒ«ã‚»ãƒƒãƒˆã€‚full_backup: åŸæœ¬
#   - è§£æ±ºé †åº: today ã¯ rolling â†’ base â†’ full_backupã€‚backtest ã¯ base â†’ full_backup
#   - Feather ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå›ºæœ‰ã®åˆ¶ç´„ï¼ˆæ—¥ä»˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼‰ã‚’å³æ ¼ã«å®ˆã‚‹
#
# ãƒ­ã‚¸ãƒƒã‚¯å˜ä½ï¼š
#   load()       â†’ å¤šå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰é©åˆ‡ã«é¸æŠãƒ»ãƒ­ãƒ¼ãƒ‰
#   save()       â†’ éšå±¤åˆ¥ã«é©åˆ‡ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ä¿å­˜
#   validate()   â†’ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼†ã‚¹ã‚­ãƒ¼ãƒæ¤œè¨¼
#
# Copilot ã¸ï¼š
#   â†’ ã‚­ãƒ£ãƒƒã‚·ãƒ¥å±¤è¨­è¨ˆã®å¤‰æ›´ã¯ç¦æ­¢ã€‚æ‹¡å¼µã¯æ–°ãƒ¡ã‚½ãƒƒãƒ‰ã§å¯¾å¿œ
#   â†’ ãƒãƒ¼ã‚¸ãƒ§ãƒ³äº’æ›æ€§ãƒã‚§ãƒƒã‚¯ã¯å¿…ãšå«ã‚ã‚‹
#   â†’ "ä»Šæ—¥ã®ãƒ‡ãƒ¼ã‚¿ãŒãªã„" ã¯æ­£å¸¸ã€‚ã‚¨ãƒ©ãƒ¼ã¨æ··åŒã™ã‚‹ãª
# ============================================================================

from __future__ import annotations

import logging
import os

# ruff: noqa: E501
# flake8: noqa: E501
from collections.abc import Callable, Iterable
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import cast

import numpy as np
import pandas as pd

from common.cache_format import safe_filename
from common.cache_io import CacheFileManager
from common.cache_validation import perform_cache_health_check
from common.cache_warnings import report_rolling_issue
from common.indicators_common import add_indicators
from config.settings import Settings, get_settings

logger = logging.getLogger(__name__)

BASE_SUBDIR = "base"

# åŸºæœ¬åˆ—ã®å®šç¾©ã‚’çµ±ä¸€
BASIC_OHLCV_COLS = {"date", "open", "high", "low", "close", "volume", "raw_close"}
BASIC_COLS_WITH_CASE = {
    "date",
    "Date",
    "open",
    "high",
    "low",
    "close",
    "volume",
    "Open",
    "High",
    "Low",
    "Close",
    "Volume",
    "raw_close",
}

# åˆ—åã®å¤§æ–‡å­—å°æ–‡å­—å¤‰æ›ãƒãƒƒãƒ—
CASE_MAP = {
    "open": "Open",
    "high": "High",
    "low": "Low",
    "close": "Close",
    "volume": "Volume",
}


class CacheManager:
    """
    äºŒå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ï¼ˆfull / rollingï¼‰ã€‚
    ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ï¼šå…¥å‡ºåŠ›ãƒ»æ¤œè¨¼ãƒ»è­¦å‘Šã¯å°‚ç”¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«å§”è­²ã€‚
    """

    def __init__(self, settings: Settings):
        self.settings = settings
        self.full_dir = Path(settings.cache.full_dir)
        self.rolling_dir = Path(settings.cache.rolling_dir)
        self.rolling_cfg = settings.cache.rolling
        self.rolling_meta_path = self.rolling_dir / self.rolling_cfg.meta_file

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.full_dir.mkdir(parents=True, exist_ok=True)
        self.rolling_dir.mkdir(parents=True, exist_ok=True)

        # å…¥å‡ºåŠ›ç®¡ç†
        self.file_manager = CacheFileManager(settings)

    def _read_base_and_tail(
        self, ticker: str, tail_rows: int = 330
    ) -> pd.DataFrame | None:
        """baseã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã¿ã€rollingç›¸å½“ã®è¡Œæ•°ã§tailå‡¦ç†ã‚’è¡Œã†ã€‚
        baseãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯full_backupã‹ã‚‰ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‚"""
        try:
            # ã¾ãšbaseãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰èª­ã¿è¾¼ã¿ã‚’è©¦ã™
            base_dir = self.full_dir.parent / "base"
            path = self.file_manager.detect_path(base_dir, ticker)

            df = None
            if path.exists():
                df = self.file_manager.read_with_fallback(path, ticker, "base")

            # baseã«è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€full_backupã‹ã‚‰èª­ã¿è¾¼ã¿
            if df is None or df.empty:
                path = self.file_manager.detect_path(self.full_dir, ticker)
                if path.exists():
                    df = self.file_manager.read_with_fallback(path, ticker, "full")

            if df is None or df.empty:
                return None

            # tailå‡¦ç†ã§rollingç›¸å½“ã®ã‚µã‚¤ã‚ºã«
            tail_df = df.tail(tail_rows)
            return cast(pd.DataFrame, tail_df)

        except Exception as e:
            logger.warning(f"Failed to read base and tail for {ticker}: {e}")
            return None

    def _recompute_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """Recalculate derived indicator columns when base OHLC data is updated."""
        if df is None or df.empty or "date" not in df.columns:
            return df

        required = {"open", "high", "low", "close"}
        if not required.issubset(set(df.columns)):
            return df

        base = df.copy()
        base["date"] = pd.to_datetime(base["date"], errors="coerce")
        base = base.dropna(subset=["date"]).sort_values("date").reset_index(drop=True)
        if base.empty:
            return df

        for col in ("open", "high", "low", "close", "volume"):
            if col in base.columns:
                base[col] = pd.to_numeric(base[col], errors="coerce")

        base_renamed = base.rename(
            columns={k: v for k, v in CASE_MAP.items() if k in base.columns}
        )
        base_renamed["Date"] = base_renamed["date"]

        # æ—¢å­˜ã®æŒ‡æ¨™åˆ—ã‚’å‰Šé™¤ã—ã¦å¼·åˆ¶çš„ã«å†è¨ˆç®—ã‚’å®Ÿè¡Œ
        indicator_cols = [
            col for col in base_renamed.columns if col not in BASIC_COLS_WITH_CASE
        ]
        if indicator_cols:
            base_renamed = base_renamed.drop(columns=indicator_cols)

        try:
            enriched = add_indicators(base_renamed)
            enriched = enriched.drop(columns=["Date"], errors="ignore")
            # æŒ‡æ¨™åˆ—ã®æ¨™æº–åŒ–ã¯è¡Œã‚ãªã„ï¼ˆå°æ–‡å­—ã‚’ç¶­æŒï¼‰
            # enriched = standardize_indicator_columns(enriched)
            # åŸºæœ¬åˆ—ï¼ˆdate, open, highç­‰ï¼‰ã®ã¿å°æ–‡å­—ã«å¤‰æ›
            enriched.columns = [
                c.lower() if c.lower() in BASIC_OHLCV_COLS else c
                for c in enriched.columns
            ]
            enriched["date"] = pd.to_datetime(
                enriched.get("date", base["date"]), errors="coerce"
            )

            # Overwrite indicator columns with freshly computed values while
            # preserving original OHLCV and date columns. This ensures appended
            # rows receive correct indicator values and existing indicators are
            # consistent with the latest OHLC history.

            # Start with OHLCV columns only from the original df
            ohlcv_cols = [col for col in BASIC_OHLCV_COLS if col in df.columns]
            combined = df[ohlcv_cols].copy().reset_index(drop=True)

            # Add all indicator columns from enriched
            for col, series in enriched.items():
                if col in BASIC_OHLCV_COLS:
                    # Skip OHLCV columns - already copied
                    continue
                # Add or replace indicator columns from enriched (ä½ç½®ãƒ™ãƒ¼ã‚¹ã§ä»£å…¥)
                combined[col] = series.values

            # drop any duplicated columns just in case
            return combined.loc[:, ~combined.columns.duplicated(keep="first")]
        except Exception as e:
            logger.error(f"Failed to recompute indicators: {e}")
            return df

    def read(self, ticker: str, profile: str) -> pd.DataFrame | None:
        """æŒ‡å®šãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€ã€‚"""
        original_ticker = ticker
        if profile == "rolling":
            # rollingå„ªå…ˆã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
            path = self.file_manager.detect_path(self.rolling_dir, ticker)
            df = self.file_manager.read_with_fallback(path, ticker, profile)

            # ã‚µãƒ‹ã‚¿ã‚¤ã‚ºç„¡åŠ¹åŒ–ãƒ¢ãƒ¼ãƒ‰æ™‚ / ã‚ã‚‹ã„ã¯ä»¥å‰ã®ãƒ•ã‚¡ã‚¤ãƒ«å‘½åäº’æ›ç”¨ã«å…ƒåæ¢ç´¢ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if (df is None or df.empty) and ticker != original_ticker:
                try:
                    legacy_path = self.file_manager.detect_path(
                        self.rolling_dir, original_ticker
                    )
                    if legacy_path.exists():
                        alt = self.file_manager.read_with_fallback(
                            legacy_path, original_ticker, profile
                        )
                        if alt is not None and not alt.empty:
                            df = alt
                except Exception:
                    pass

            if df is None or df.empty:
                # baseã‹ã‚‰rollingç›¸å½“ã‚’ç”Ÿæˆ
                report_rolling_issue("missing_rolling", ticker, "fallback to base+tail")
                df = self._read_base_and_tail(ticker)

                if df is not None and not df.empty:
                    # rollingå½¢å¼ã§ä¿å­˜
                    try:
                        self.file_manager.write_atomic(df, path, ticker, profile)
                        logger.debug(f"Generated rolling cache for {ticker}")
                    except Exception as e:
                        logger.warning(
                            f"Failed to save generated rolling for {ticker}: {e}"
                        )

                # If rolling exists but lacks essential indicators, optionally
                # attempt to recompute them on read to self-heal broken caches.
                try:
                    recompute_flag = bool(
                        getattr(
                            self.settings.cache.rolling,
                            "recompute_indicators_on_read",
                            True,
                        )
                    )
                except Exception:
                    recompute_flag = True

                if df is not None and recompute_flag:
                    try:
                        required_indicators = ["drop3d", "atr_ratio", "dollarvolume20"]
                        missing = [
                            c
                            for c in required_indicators
                            if c not in df.columns or df[c].isna().all()
                        ]
                        if missing:
                            logger.info(
                                f"Rolling cache for {ticker} missing indicators {missing}; attempting recompute"
                            )
                            recomputed = self._recompute_indicators(df)
                            # Validate recompute produced usable values for the required indicators
                            ok = True
                            for c in required_indicators:
                                if (
                                    c not in recomputed.columns
                                    or recomputed[c].dropna().empty
                                ):
                                    ok = False
                                    break
                            if ok:
                                try:
                                    # Persist recomputed rolling cache and use it for return
                                    self.write_atomic(recomputed, ticker, "rolling")
                                    df = recomputed
                                    logger.info(
                                        f"Recomputed and saved rolling cache for {ticker}"
                                    )
                                except (
                                    Exception
                                ) as e:  # pragma: no cover - best-effort save
                                    logger.warning(
                                        f"Failed to save recomputed rolling for {ticker}: {e}"
                                    )
                            else:
                                logger.warning(
                                    f"Recompute did not produce required indicators for {ticker}: {missing}"
                                )
                    except Exception as e:  # pragma: no cover - defensive
                        logger.exception(
                            f"Error during recompute indicators for {ticker}: {e}"
                        )

                return cast(pd.DataFrame | None, df)

            # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯ï¼ˆãŸã ã—ä¸Šå ´é–“ã‚‚ãªã„éŠ˜æŸ„ã¯æ­£å¸¸ãªã‚±ãƒ¼ã‚¹ã¨ã—ã¦æ‰±ã†ï¼‰
            if len(df) < self.rolling_cfg.base_lookback_days:
                # ä¸Šå ´é–“ã‚‚ãªã„éŠ˜æŸ„ï¼ˆãƒ‡ãƒ¼ã‚¿æœŸé–“ãŒçŸ­ã„ï¼‰ã¯è­¦å‘Šãƒ¬ãƒ™ãƒ«ã‚’ä¸‹ã’ã‚‹
                if len(df) < 100:  # æ˜ã‚‰ã‹ã«ä¸Šå ´é–“ã‚‚ãªã„å ´åˆ
                    logger.debug(
                        f"æ–°è¦ä¸Šå ´éŠ˜æŸ„ {ticker}: rows={len(df)}, expected>={self.rolling_cfg.base_lookback_days} (æ­£å¸¸)"
                    )
                else:
                    report_rolling_issue(
                        "insufficient_data",
                        ticker,
                        f"rows={len(df)}, expected>={self.rolling_cfg.base_lookback_days}",
                    )

            return cast(pd.DataFrame | None, df)

        elif profile == "full":
            path = self.file_manager.detect_path(self.full_dir, ticker)
            return cast(
                pd.DataFrame | None,
                self.file_manager.read_with_fallback(path, ticker, profile),
            )

        else:
            raise ValueError(f"Unsupported profile: {profile}")

    def write_atomic(self, df: pd.DataFrame, ticker: str, profile: str) -> None:
        """æŒ‡å®šãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒˆãƒŸãƒƒã‚¯æ›¸ãè¾¼ã¿ã€‚"""
        if profile == "rolling":
            dir_path = self.rolling_dir
        elif profile == "full":
            dir_path = self.full_dir
        else:
            raise ValueError(f"Unsupported profile: {profile}")

        path = self.file_manager.detect_path(dir_path, ticker)

        # å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆç’°å¢ƒå¤‰æ•°ã§æŠ‘åˆ¶å¯èƒ½ï¼‰
        try:
            _silent = (os.getenv("CACHE_HEALTH_SILENT") or "").strip().lower() in {
                "1",
                "true",
                "yes",
                "on",
            }
        except Exception:
            _silent = False
        if not _silent:
            perform_cache_health_check(df, ticker, profile)

        # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
        optimized_df = self.file_manager.optimize_dataframe_memory(df)

        # æ›¸ãè¾¼ã¿
        self.file_manager.write_atomic(optimized_df, path, ticker, profile)

    def upsert_both(self, ticker: str, new_rows: pd.DataFrame) -> None:
        """full ã¨ rolling ä¸¡æ–¹ã« upsertï¼ˆæ›´æ–°ãƒ»æŒ¿å…¥ï¼‰å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹ã€‚"""
        self._upsert_one(ticker, new_rows, "full")
        self._upsert_one(ticker, new_rows, "rolling")

    def _upsert_one(self, ticker: str, new_rows: pd.DataFrame, profile: str) -> None:
        """å˜ä¸€ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾ã™ã‚‹ upsert å‡¦ç†ã€‚"""
        if new_rows is None or new_rows.empty:
            return

        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        existing = self.read(ticker, profile)

        if existing is None or existing.empty:
            # æ–°è¦ä½œæˆ
            to_save = new_rows.copy()
        else:
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®åˆ—é‡è¤‡ãƒã‚§ãƒƒã‚¯ã¨ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if existing.columns.duplicated().any():
                print(
                    f"[WARNING] Existing data has duplicate columns for {ticker}. Cleaning up..."
                )
                existing = existing.loc[:, ~existing.columns.duplicated()]

            # new_rows ã‹ã‚‰ã‚‚æŒ‡æ¨™åˆ—ã‚’å‰Šé™¤ã—ã¦ OHLCV ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä¿æŒ
            new_rows_clean = new_rows[
                [col for col in BASIC_OHLCV_COLS if col in new_rows.columns]
            ].copy()

            # ãƒãƒ¼ã‚¸å‡¦ç† (åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ã®ã¿)
            combined = pd.concat([existing, new_rows_clean], ignore_index=True)
            if "date" in combined.columns:
                combined["date"] = pd.to_datetime(combined["date"], errors="coerce")
                combined = combined.dropna(subset=["date"])
                combined = combined.drop_duplicates(subset=["date"], keep="last")
                combined = combined.sort_values("date").reset_index(drop=True)
            to_save = combined

        # rollingåˆ¶é™é©ç”¨
        if profile == "rolling":
            to_save = self._enforce_rolling_window(to_save)

        # æŒ‡æ¨™å†è¨ˆç®—
        to_save = self._recompute_indicators(to_save)

        # ä¿å­˜
        self.write_atomic(to_save, ticker, profile)

    @property
    def _ui_prefix(self) -> str:
        return "[CacheManager]"

    def _enforce_rolling_window(self, df: pd.DataFrame) -> pd.DataFrame:
        """rolling ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºåˆ¶é™ã‚’é©ç”¨ã™ã‚‹ã€‚"""
        if df is None or df.empty:
            return df
        max_rows = self.rolling_cfg.base_lookback_days + self.rolling_cfg.buffer_days
        return df.tail(max_rows)

    def _validate_symbol_data(
        self,
        symbol: str,
        df: pd.DataFrame,
        reference_date: pd.Timestamp,
        min_rows: int,
        max_stale_days: int,
    ) -> str | None:
        """ã‚·ãƒ³ãƒœãƒ«ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ã‚’è¡Œã„ã€å•é¡ŒãŒã‚ã‚‹å ´åˆã¯åˆ†é¡ã‚’è¿”ã™"""
        if df is None or df.empty:
            return "missing"

        if "date" not in df.columns:
            return f"insufficient:{symbol}(no_date_col)"

        df["date"] = pd.to_datetime(df["date"], errors="coerce")
        valid_dates = df["date"].dropna()

        if len(valid_dates) == 0:
            return f"insufficient:{symbol}(no_valid_dates)"

        if len(df) < min_rows:
            return f"insufficient:{symbol}(rows={len(df)})"

        latest_date = valid_dates.max()
        if pd.notna(latest_date) and pd.notna(reference_date):
            days_behind = (reference_date - latest_date).days
            if days_behind > max_stale_days:
                return f"stale:{symbol}({days_behind}d)"

        return None  # æ­£å¸¸

    def _get_reference_date(self, anchor_ticker: str = "SPY") -> pd.Timestamp:
        """åŸºæº–æ—¥ä»˜ã‚’å–å¾—ã™ã‚‹ï¼ˆSPYã®æœ€æ–°æ—¥ä»˜ã¾ãŸã¯Nowï¼‰"""
        anchor_df = self.read(anchor_ticker, "rolling")
        if (
            anchor_df is not None
            and not anchor_df.empty
            and "date" in anchor_df.columns
        ):
            anchor_df["date"] = pd.to_datetime(anchor_df["date"], errors="coerce")
            reference_date = anchor_df["date"].max()
            if pd.notna(reference_date):
                return pd.Timestamp(reference_date)
        return pd.Timestamp.now().normalize()

    def prune_rolling_if_needed(self, anchor_ticker: str = "SPY") -> dict:
        """Rolling cache ã®å®¹é‡ç®¡ç†ã¨ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã€‚"""
        try:
            # ã‚¢ãƒ³ã‚«ãƒ¼éŠ˜æŸ„ã®æœ€æ–°æ—¥ä»˜ã‚’å–å¾—
            anchor_latest = self._get_reference_date(anchor_ticker)

            # Rolling ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§
            rolling_files = list(self.rolling_dir.glob("*.csv")) + list(
                self.rolling_dir.glob("*.feather")
            )
            if not rolling_files:
                return {
                    "status": "success",
                    "message": "ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãªã—",
                    "pruned": 0,
                }

            pruned_count = 0
            staleness_threshold = self.rolling_cfg.max_staleness_days

            for file_path in rolling_files:
                ticker_name = file_path.stem
                try:
                    df = self.file_manager.read_with_fallback(
                        file_path, ticker_name, "rolling"
                    )
                    if df is None or df.empty or "date" not in df.columns:
                        continue

                    df["date"] = pd.to_datetime(df["date"], errors="coerce")
                    file_latest = df["date"].max()

                    if pd.isna(file_latest):
                        continue

                    days_stale = (anchor_latest - file_latest).days
                    if days_stale > staleness_threshold:
                        file_path.unlink()
                        pruned_count += 1
                        logger.info(
                            f"Pruned stale rolling cache: {ticker_name} ({days_stale} days stale)"
                        )

                except Exception as e:
                    logger.warning(f"ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°å‡¦ç†ä¸­ã‚¨ãƒ©ãƒ¼ {ticker_name}: {e}")
                    continue

            return {
                "status": "success",
                "message": f"ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†: {pruned_count} ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤",
                "pruned": pruned_count,
                "anchor_date": anchor_latest.strftime("%Y-%m-%d"),
            }

        except Exception as e:
            return {"status": "error", "message": f"ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}"}

    def analyze_rolling_gaps(self, system_symbols: list[str] | None = None) -> dict:
        """Rolling cache ã®ã‚®ãƒ£ãƒƒãƒ—åˆ†æã‚’å®Ÿè¡Œã€‚"""
        try:
            if system_symbols is None:
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚·ãƒ³ãƒœãƒ«å–å¾—
                try:
                    from common.symbols_manifest import load_symbol_manifest

                    manifest = load_symbol_manifest(self.full_dir)
                    system_symbols = manifest
                except Exception:
                    system_symbols = []

            if not system_symbols:
                return {
                    "status": "error",
                    "message": "åˆ†æå¯¾è±¡ã‚·ãƒ³ãƒœãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“",
                }

            missing_files = []
            insufficient_data = []
            stale_data = []
            healthy_count = 0

            # SPY ã‚’åŸºæº–æ—¥ä»˜ã¨ã—ã¦ä½¿ç”¨
            reference_date = self._get_reference_date("SPY")

            min_required_rows = self.rolling_cfg.base_lookback_days
            max_stale_days = self.rolling_cfg.max_staleness_days

            for symbol in system_symbols:
                try:
                    df = self.read(symbol, "rolling")
                    validation_result: str | None = None
                    if df is None:
                        validation_result = "missing"
                    else:
                        validation_result = self._validate_symbol_data(
                            symbol,
                            df,
                            reference_date,
                            min_required_rows,
                            max_stale_days,
                        )

                    if validation_result is None:
                        healthy_count += 1
                    elif validation_result == "missing":
                        missing_files.append(symbol)
                    elif validation_result.startswith("insufficient:"):
                        insufficient_data.append(validation_result.split(":", 1)[1])
                    elif validation_result.startswith("stale:"):
                        stale_data.append(validation_result.split(":", 1)[1])

                except Exception as e:
                    logger.warning(f"ã‚®ãƒ£ãƒƒãƒ—åˆ†æã‚¨ãƒ©ãƒ¼ {symbol}: {e}")
                    missing_files.append(f"{symbol}(error)")

            return {
                "status": "success",
                "total_symbols": len(system_symbols),
                "healthy": healthy_count,
                "missing_files": len(missing_files),
                "insufficient_data": len(insufficient_data),
                "stale_data": len(stale_data),
                "missing_list": missing_files[:10],  # æœ€åˆã®10ä»¶ã®ã¿
                "insufficient_list": insufficient_data[:10],
                "stale_list": stale_data[:10],
                "reference_date": (
                    reference_date.strftime("%Y-%m-%d")
                    if pd.notna(reference_date)
                    else "N/A"
                ),
            }

        except Exception as e:
            return {"status": "error", "message": f"ã‚®ãƒ£ãƒƒãƒ—åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}"}

    def get_rolling_health_summary(self) -> dict:
        """Rolling cache ã®å¥åº·çŠ¶æ…‹ã‚µãƒãƒªãƒ¼ã‚’å–å¾—ã€‚"""
        try:
            logger.info("Rolling health summary: start")

            # ãƒ­ãƒ¼ãƒªãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§
            rolling_files = list(self.rolling_dir.glob("*.csv")) + list(
                self.rolling_dir.glob("*.feather")
            )
            total_files = len(rolling_files)

            # æ—¢å­˜ã®é›†è¨ˆï¼ˆå¾Œæ–¹äº’æ›ç”¨ï¼‰
            readable_files = 0
            total_rows = 0
            date_range_info: dict[str, dict] = {}
            for file_path in rolling_files[:20]:  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                try:
                    ticker = file_path.stem
                    df = self.file_manager.read_with_fallback(
                        file_path, ticker, "rolling"
                    )
                    if df is not None and not df.empty:
                        readable_files += 1
                        total_rows += len(df)
                        if "date" in df.columns:
                            df["date"] = pd.to_datetime(df["date"], errors="coerce")
                            valid_dates = df["date"].dropna()
                            if len(valid_dates) > 0:
                                date_range_info[ticker] = {
                                    "start": valid_dates.min().strftime("%Y-%m-%d"),
                                    "end": valid_dates.max().strftime("%Y-%m-%d"),
                                    "rows": len(df),
                                }
                except Exception:
                    continue

            # UI æœŸå¾…ã‚­ãƒ¼: ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
            meta_exists = bool(self.rolling_meta_path.exists())
            meta_content = None
            if meta_exists:
                try:
                    import json

                    with self.rolling_meta_path.open("r", encoding="utf-8") as fh:
                        meta_content = json.load(fh)
                except Exception as e:
                    logger.warning(
                        "Corrupt or unreadable rolling meta file: %s err=%s",
                        self.rolling_meta_path,
                        e,
                    )
                    meta_content = None

            # ç›®æ¨™ãƒ‡ãƒ¼ã‚¿é•·ï¼ˆå±æ€§åã®äº’æ›: base_lookback_days or lookback_daysï¼‰
            target_length = getattr(self.rolling_cfg, "base_lookback_days", None)
            if target_length is None:
                target_length = getattr(self.rolling_cfg, "lookback_days", None)
            if target_length is None:
                target_length = 250  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

            # ã‚¢ãƒ³ã‚«ãƒ¼éŠ˜æŸ„ï¼ˆSPYï¼‰çŠ¶æ³
            anchor_df: pd.DataFrame | None = None
            try:
                anchor_df = self.read("SPY", "rolling")
            except Exception:
                anchor_df = None
            anchor_exists = bool(
                isinstance(anchor_df, pd.DataFrame)
                and not getattr(anchor_df, "empty", True)
            )
            anchor_rows = (
                int(len(anchor_df)) if isinstance(anchor_df, pd.DataFrame) else 0
            )
            latest_date_str = None
            if isinstance(anchor_df, pd.DataFrame) and "date" in anchor_df.columns:
                try:
                    ad = anchor_df.copy()
                    ad["date"] = pd.to_datetime(ad["date"], errors="coerce")
                    latest = ad["date"].dropna().max()
                    if pd.notna(latest):
                        latest_date_str = latest.strftime("%Y-%m-%d")
                except Exception:
                    latest_date_str = None

            anchor_symbol_status = {
                "symbol": "SPY",
                "exists": anchor_exists,
                "rows": anchor_rows,
                "latest": latest_date_str,
                "meets_target": bool(anchor_rows >= int(target_length or 0)),
            }

            # UI äº’æ›: rolling_files_count ã‚’æä¾›
            rolling_files_count = total_files

            # è¿”å´ï¼ˆUIæœŸå¾… + æ—¢å­˜ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼‰
            result = {
                "status": "success",
                # æ—¢å­˜ã‚­ãƒ¼ï¼ˆäº’æ›ä¿æŒï¼‰
                "total_files": total_files,
                "readable_files": readable_files,
                "sample_total_rows": total_rows,
                "avg_rows_per_file": (
                    total_rows / readable_files if readable_files > 0 else 0
                ),
                "sample_date_ranges": date_range_info,
                # UI æœŸå¾…ã‚­ãƒ¼
                "meta_exists": meta_exists,
                "meta_content": meta_content,
                "rolling_files_count": rolling_files_count,
                "anchor_symbol_status": anchor_symbol_status,
                "target_length": int(target_length),
            }

            if total_files == 0:
                result["message"] = "Rolling cache ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“"

            logger.info(
                "Rolling health summary: done", extra={"total_files": total_files}
            )
            return result

        except Exception as e:
            return {"status": "error", "message": f"å¥åº·çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}"}

    def read_batch_parallel(
        self,
        symbols: list[str],
        profile: str = "rolling",
        max_workers: int | None = None,
        fallback_profile: str | None = "full",
        progress_callback: Callable[[int, int], None] | None = None,
    ) -> dict[str, pd.DataFrame]:
        """è¤‡æ•°éŠ˜æŸ„ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¸¦åˆ—ã§èª­ã¿è¾¼ã‚€ã€‚"""
        if not symbols:
            return {}

        if max_workers is None:
            max_workers = min(8, len(symbols))

        results = {}
        completed = 0

        def read_single(symbol: str) -> tuple[str, pd.DataFrame | None]:
            df = self.read(symbol, profile)
            if df is None and fallback_profile:
                df = self.read(symbol, fallback_profile)
            return symbol, df

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_symbol = {
                executor.submit(read_single, sym): sym for sym in symbols
            }

            for future in as_completed(future_to_symbol):
                symbol, df = future.result()
                if df is not None:
                    results[symbol] = df

                completed += 1
                if progress_callback and completed % 50 == 0:
                    progress_callback(completed, len(symbols))

        return results

    def optimize_dataframe_memory(self, df: pd.DataFrame) -> pd.DataFrame:
        """DataFrameã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æœ€é©åŒ–ã™ã‚‹ï¼ˆå§”è­²ï¼‰ã€‚"""
        return cast(pd.DataFrame, self.file_manager.optimize_dataframe_memory(df))

    def remove_unnecessary_columns(
        self, df: pd.DataFrame, keep_columns: list[str] | None = None
    ) -> pd.DataFrame:
        """ä¸è¦ãªåˆ—ã‚’é™¤å»ã™ã‚‹ï¼ˆå§”è­²ï¼‰ã€‚"""
        return cast(
            pd.DataFrame, self.file_manager.remove_unnecessary_columns(df, keep_columns)
        )


def _base_dir() -> Path:
    settings = get_settings(create_dirs=True)
    base = Path(settings.DATA_CACHE_DIR) / BASE_SUBDIR
    base.mkdir(parents=True, exist_ok=True)
    return base


def compute_base_indicators(df: pd.DataFrame) -> pd.DataFrame:
    """OHLCVã®DataFrameã«å…±é€šãƒ™ãƒ¼ã‚¹æŒ‡æ¨™ã‚’ä»˜åŠ ã—ã¦è¿”ã™ã€‚"""
    if df is None or df.empty:
        return df

    x = df.copy()

    # Normalize column names
    rename_map = {c: c.lower() for c in x.columns}
    x = x.rename(columns=rename_map)

    # Ensure 'Date' column and set as index
    if "date" in x.columns:
        x = x.rename(columns={"date": "Date"})
    if "Date" in x.columns:
        x["Date"] = pd.to_datetime(x["Date"], errors="coerce")
        x = x.dropna(subset=["Date"]).sort_values("Date").set_index("Date")

    # Standardize OHLCV column names with priority to avoid duplicates
    # Priority: adjusted_close > adjclose > close for Close column
    close_candidates = ["adjusted_close", "adjclose", "close"]
    close_source = None
    for candidate in close_candidates:
        if candidate in x.columns:
            close_source = candidate
            break

    # Build rename map without duplicates
    ohlcv_map = {}
    if "open" in x.columns:
        ohlcv_map["open"] = "Open"
    if "high" in x.columns:
        ohlcv_map["high"] = "High"
    if "low" in x.columns:
        ohlcv_map["low"] = "Low"
    if close_source:
        ohlcv_map[close_source] = "Close"
    if "volume" in x.columns:
        ohlcv_map["volume"] = "Volume"
    elif "vol" in x.columns:
        ohlcv_map["vol"] = "Volume"

    # Rename and drop duplicates
    x = x.rename(columns=ohlcv_map)
    # é˜²å¾¡çš„ã«åˆ—é‡è¤‡ã‚’é™¤å»ï¼ˆå¤§å°æ–‡å­—é•ã„ã®é‡è¤‡ã‚„äºŒé‡ãƒªãƒãƒ¼ãƒ å¯¾ç­–ï¼‰
    if getattr(x, "columns", None) is not None:
        try:
            x = x.loc[:, ~x.columns.duplicated(keep="last")]
        except Exception:
            pass

    # Drop unused close-related columns to prevent duplicates
    cols_to_drop = [c for c in close_candidates if c in x.columns and c != close_source]
    if cols_to_drop:
        x = x.drop(columns=cols_to_drop, errors="ignore")

    required = {"High", "Low", "Close"}
    if not required.issubset(x.columns):
        missing_cols = required - set(x.columns)
        logger.warning(
            f"{__name__}: å¿…é ˆåˆ—æ¬ è½ã®ãŸã‚ã‚¤ãƒ³ã‚¸è¨ˆç®—ã‚’ã‚¹ã‚­ãƒƒãƒ—: missing={missing_cols}"
        )
        return x.reset_index()

    close = pd.to_numeric(x["Close"], errors="coerce")
    high = pd.to_numeric(x["High"], errors="coerce")
    low = pd.to_numeric(x["Low"], errors="coerce")
    vol = None
    if "Volume" in x.columns:
        vol = pd.to_numeric(x["Volume"], errors="coerce")

    # SMA/EMA - å¤§æ–‡å­—çµ±ä¸€
    for n in [25, 50, 100, 150, 200]:
        x[f"SMA{n}"] = close.rolling(n).mean()
    for n in [20, 50]:
        x[f"EMA{n}"] = close.ewm(span=n, adjust=False).mean()

    # ATR - å¤§æ–‡å­—çµ±ä¸€
    tr = pd.concat(
        [high - low, (high - close.shift()).abs(), (low - close.shift()).abs()], axis=1
    ).max(axis=1)
    for n in [10, 14, 20, 40, 50]:
        x[f"ATR{n}"] = tr.rolling(n).mean()

    # RSI (Wilder) - å¤§æ–‡å­—çµ±ä¸€
    def _rsi(s: pd.Series, n: int) -> pd.Series:
        delta = s.diff()
        gain = delta.clip(lower=0).ewm(alpha=1 / n, adjust=False).mean()
        loss = -delta.clip(upper=0).ewm(alpha=1 / n, adjust=False).mean()
        rs = gain / loss.replace(0, np.nan)
        return cast(pd.Series, 100 - (100 / (1 + rs)))

    for n in [3, 4, 14]:
        x[f"RSI{n}"] = _rsi(close, n)

    # ROC & HV - å¤§æ–‡å­—çµ±ä¸€
    x["ROC200"] = close.pct_change(200, fill_method=None) * 100.0
    log_ret = (close / close.shift(1)).apply(np.log)
    std_dev = log_ret.rolling(50).std()
    x["HV50"] = std_dev * np.sqrt(252) * 100.0

    # DollarVolume - å¤§æ–‡å­—çµ±ä¸€
    if vol is not None:
        x["DollarVolume20"] = (close * vol).rolling(20).mean()
        x["DollarVolume50"] = (close * vol).rolling(50).mean()

    return x.reset_index()


def get_indicator_column_flexible(df: pd.DataFrame, indicator: str) -> pd.Series | None:
    """å¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã›ãšã«æŒ‡æ¨™åˆ—ã‚’å–å¾—ã™ã‚‹ã€‚"""
    if df is None or df.empty:
        return None

    # å®Œå…¨ä¸€è‡´ã‚’æœ€åˆã«è©¦è¡Œ
    if indicator in df.columns:
        return df[indicator]

    # å°æ–‡å­—å¤‰æ›ã§æ¤œç´¢
    lower_indicator = indicator.lower()
    for col in df.columns:
        if col.lower() == lower_indicator:
            return df[col]

    return None


def standardize_indicator_columns(df: pd.DataFrame) -> pd.DataFrame:
    """æŒ‡æ¨™åˆ—åã‚’æ¨™æº–å½¢å¼ã«çµ±ä¸€ã™ã‚‹ã€‚"""
    if df is None or df.empty:
        return df

    result = df.copy()

    # æ¨™æº–åŒ–ãƒãƒƒãƒ—ï¼ˆå°æ–‡å­— -> æ¨™æº–å½¢å¼ï¼‰
    standard_map = {
        "sma25": "SMA25",
        "sma50": "SMA50",
        "sma100": "SMA100",
        "sma150": "SMA150",
        "sma200": "SMA200",
        "ema20": "EMA20",
        "ema50": "EMA50",
        "atr10": "ATR10",
        "atr14": "ATR14",
        "atr20": "ATR20",
        "atr40": "ATR40",
        "atr50": "ATR50",
        "rsi3": "RSI3",
        "rsi4": "RSI4",
        "rsi14": "RSI14",
        "adx7": "ADX7",
        "roc200": "ROC200",
        "hv50": "HV50",
        "dollarvolume20": "DollarVolume20",
        "dollarvolume50": "DollarVolume50",
        "avgvolume50": "AvgVolume50",
    }

    rename_dict = {}
    for col in result.columns:
        col_lower = col.lower()
        if col_lower in standard_map:
            rename_dict[col] = standard_map[col_lower]

    if rename_dict:
        result = result.rename(columns=rename_dict)

    return result


def base_cache_path(symbol: str) -> Path:
    return _base_dir() / f"{safe_filename(symbol)}.csv"


def save_base_cache(
    symbol: str,
    df: pd.DataFrame,
    settings: Settings | None = None,
) -> Path:
    """Base ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ feather å½¢å¼ã§ä¿å­˜ã—ã€ãƒ‘ã‚¹ã‚’è¿”ã™ã€‚"""
    if settings is None:
        settings = get_settings(create_dirs=True)

    # ãƒ™ãƒ¼ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨featherãƒ‘ã‚¹
    base_dir = Path(settings.DATA_CACHE_DIR) / "base"
    base_dir.mkdir(parents=True, exist_ok=True)
    path = base_dir / f"{safe_filename(symbol)}.feather"
    tmp_path = path.with_suffix(path.suffix + ".tmp")

    # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
    df_reset = (
        df.reset_index()
        if hasattr(df, "index") and getattr(df.index, "name", None) is not None
        else df
    )
    # åˆ—åã‚’å°æ–‡å­—åŒ–ã—ã€é‡è¤‡åˆ—ã‚’æ’é™¤ï¼ˆFeatherã¯é‡è¤‡åˆ—åã‚’è¨±å®¹ã—ãªã„ï¼‰
    df_reset = df_reset.rename(columns={c: str(c).lower() for c in df_reset.columns})
    try:
        df_reset = df_reset.loc[:, ~df_reset.columns.duplicated(keep="last")]
    except Exception:
        pass

    # è¨­å®šã«åŸºã¥ãä¸¸ã‚å‡¦ç†
    try:
        round_dec = getattr(getattr(settings, "cache", None), "round_decimals", None)
    except Exception:
        round_dec = None

    if round_dec is not None:
        from common.cache_format import round_dataframe

        df_reset = round_dataframe(df_reset, round_dec)

    try:
        # featherå½¢å¼ã§ã‚¢ãƒˆãƒŸãƒƒã‚¯æ›¸ãè¾¼ã¿
        df_reset.to_feather(tmp_path)
        tmp_path.replace(path)
    except Exception as e:
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
        if tmp_path.exists():
            tmp_path.unlink()
        raise RuntimeError(f"feather æ›¸ãè¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {path}") from e

    return path


_DEFAULT_CACHE_MANAGER: CacheManager | None = None


def _get_default_cache_manager() -> CacheManager:
    global _DEFAULT_CACHE_MANAGER
    if _DEFAULT_CACHE_MANAGER is None:
        settings = get_settings(create_dirs=True)
        _DEFAULT_CACHE_MANAGER = CacheManager(settings)
    return _DEFAULT_CACHE_MANAGER


def _read_legacy_cache(symbol: str) -> pd.DataFrame | None:
    """Legacy cache ã‹ã‚‰èª­ã¿è¾¼ã‚€ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰ã€‚"""
    try:
        legacy_path = Path("data_cache") / f"{safe_filename(symbol)}.csv"
        if legacy_path.exists():
            return pd.read_csv(legacy_path)
    except Exception:
        pass
    return None


def load_base_cache(
    symbol: str,
    *,
    rebuild_if_missing: bool = True,
    cache_manager: CacheManager | None = None,
    min_last_date: pd.Timestamp | None = None,
    allowed_recent_dates: Iterable[object] | None = None,
    prefer_precomputed_indicators: bool = True,
) -> pd.DataFrame | None:
    """Base ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã‚€ï¼ˆä¸‹ä½äº’æ›æ€§ã®ãŸã‚ä¿æŒï¼‰ã€‚"""
    if cache_manager is None:
        cache_manager = _get_default_cache_manager()

    try:
        # ã¾ãš base ã‹ã‚‰èª­ã¿è¾¼ã¿
        base_dir = cache_manager.full_dir.parent / "base"
        if base_dir.exists():
            df = cache_manager.read(symbol, "full")  # full ã¨ã—ã¦èª­ã¿è¾¼ã¿
            if df is not None and not df.empty:
                return df

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: legacy
        return _read_legacy_cache(symbol)

    except Exception as e:
        logger.warning(f"load_base_cache failed for {symbol}: {e}")
        return None
