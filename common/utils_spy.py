from __future__ import annotations

import os
from collections.abc import Iterable
from datetime import time as dtime
from pathlib import Path
import sys
from zoneinfo import ZoneInfo

import pandas as pd
import pandas_market_calendars as mcal
import streamlit as st
from ta.trend import SMAIndicator

from common.i18n import tr
from config.settings import get_settings


_NY_TIMEZONE = ZoneInfo("America/New_York")


def _ui_enabled() -> bool:
    """Return True when running under Streamlit UI.

    Heuristic only: avoid querying Streamlit runtime context directly to prevent
    'missing ScriptRunContext' warnings. Prefer environment flag or command hint.
    """
    try:
        v = (os.getenv("STREAMLIT_SERVER_ENABLED") or "").strip().lower()
        if v in {"1", "true", "yes"}:
            return True
    except Exception:
        pass
    try:
        argv = " ".join(sys.argv).lower()
        if "streamlit" in argv:
            return True
    except Exception:
        pass
    return False


def _st_emit(kind: str, *args, **kwargs) -> None:
    """Safely emit Streamlit UI calls only when UI context is active.

    In CLI/batch runs, this becomes a no-op to avoid ScriptRunContext warnings.
    """
    if not _ui_enabled():
        return
    try:
        fn = getattr(st, kind, None)
        if callable(fn):
            fn(*args, **kwargs)
    except Exception:
        # Silently ignore UI errors in non-critical paths
        return


def _candidate_spy_paths(root: Path) -> list[Path]:
    """SPY.csv „ÇíÊé¢„ÅôÂÄôË£ú„Éë„Çπ„ÇíËøî„Åô„ÄÇ

    Ôºà„Éê„ÉÉ„ÇØ„ÉÜ„Çπ„Éà„Å™„Å©Â∫ÉÊúüÈñì„ÅåÂøÖË¶Å„Å™Â†¥Èù¢„Åß„ÅØ base „ÇíÂÑ™ÂÖà„Åó„ÄÅ
     ÂΩìÊó•„Ç∑„Ç∞„Éä„É´„Å™„Å©„Åß„ÅØ rolling „ÇíÂÑ™ÂÖà„Åô„ÇãË®≠Ë®à„Å†„Åå„ÄÅ
     Êú¨Èñ¢Êï∞„ÅØÂçòÁ¥î„Å™ÂÄôË£úÂàóÊåô„ÅÆ„Åø„ÇíË°å„ÅÜÔºâ
    Â§ßÊñáÂ≠óÂ∞èÊñáÂ≠ó„ÅÆÈÅï„ÅÑ„ÇÇÂê∏Âèé„Åô„ÇãÔºà„ÇØ„É≠„Çπ„Éó„É©„ÉÉ„Éà„Éï„Ç©„Éº„É†„ÅÆ„Åü„ÇÅÔºâ„ÄÇ
    """

    def _case_insensitive_find(dir_path: Path, name: str) -> Path | None:
        try:
            if not dir_path.exists():
                return None
            for fn in os.listdir(dir_path):
                if fn.lower() == name.lower():
                    return dir_path / fn
        except Exception:
            return None
        return None

    names = ("SPY.csv",)
    dirs: Iterable[Path] = (
        root / "base",
        root / "full",
        root / "rolling",
    )
    out: list[Path] = []
    for d in dirs:
        for nm in names:
            p = _case_insensitive_find(d, nm)
            if p is not None:
                out.append(p)
    return out


def _read_daily_csv_any_datecol(path: Path) -> pd.DataFrame:
    """Êó•Ë∂≥CSV„Çí 'Date' „Åæ„Åü„ÅØ 'date' „ÅÑ„Åö„Çå„Åß„ÇÇË™≠„ÅøËæº„ÇÅ„Çã„Çà„ÅÜ„Å´„Åô„Çã„ÄÇ"""
    df = pd.read_csv(path)
    date_col = None
    if "Date" in df.columns:
        date_col = "Date"
    elif "date" in df.columns:
        date_col = "date"
    if date_col is None:
        raise ValueError("date/Date Âàó„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì")
    df[date_col] = pd.to_datetime(df[date_col])
    df = df.sort_values(date_col).set_index(date_col)
    return df


def get_spy_data_cached_v2(folder: str = "data_cache", mode: str = "backtest"):
    """
    SPY.csv „Çí„Ç≠„É£„ÉÉ„Ç∑„É•„Åã„ÇâË™≠„ÅøËæº„ÇÄ„ÄÇ
    - mode="backtest": data_cache/base ‚Üí data_cache/full_backup „ÅÆÈ†ÜÔºàrolling „ÅØÊé¢Á¥¢„Åó„Å™„ÅÑÔºâ
    - mode="today": data_cache/rolling ‚Üí data_cache/base ‚Üí data_cache/full_backup „ÅÆÈ†Ü
    - „Ç≠„É£„ÉÉ„Ç∑„É•„ÅåË¶ã„Å§„Åã„Çâ„Å™„ÅÑÂ†¥Âêà„ÅØ„Ç®„É©„Éº„ÇíËøî„Åô
    """
    try:
        settings = get_settings(create_dirs=True)
        root = Path(settings.DATA_CACHE_DIR)
        full_dir = Path(getattr(settings.cache, "full_dir", root / "full_backup"))
        rolling_dir = Path(getattr(settings.cache, "rolling_dir", root / "rolling"))
    except Exception:
        root = Path(folder)
        full_dir = root / "full_backup"
        rolling_dir = root / "rolling"

    # Êé¢Á¥¢È†Ü„Çí mode „ÅßÂàá„ÇäÊõø„Åà
    base_dir = root / "base"
    mode_lower = str(mode).lower()
    if mode_lower == "today":
        search_dirs: list[Path] = [rolling_dir, base_dir, full_dir]
    else:
        # backtest: rolling „ÅØÊé¢Á¥¢„Åó„Å™„ÅÑ
        search_dirs = [base_dir, full_dir]

    def _find_case_insensitive(d: Path, name: str) -> Path | None:
        try:
            if not d.exists():
                return None
            for fn in os.listdir(d):
                if fn.lower() == name.lower():
                    return d / fn
        except Exception:
            return None
        return None

    path: Path | None = None
    for d in search_dirs:
        p = _find_case_insensitive(d, "SPY.csv")
        if p is not None:
            path = p
            break
    if path is None or not path.exists():
        _st_emit(
            "error", tr("‚ùå SPY.csv „ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì (base/full_backup/rolling „ÇíÁ¢∫Ë™ç)")
        )
        return None

    # backtest ÊôÇ„ÅØ full_backup „ÅÆÂ≠òÂú®„ÇíÂøÖÈ†à„Å®„Åó„ÄÅÁÑ°„Åë„Çå„Å∞„Ç®„É©„Éº„É°„ÉÉ„Çª„Éº„Ç∏„ÇíË°®Á§∫
    if mode_lower != "today":
        has_full_backup = _find_case_insensitive(full_dir, "SPY.csv") is not None
        if not has_full_backup:
            try:
                _st_emit(
                    "error",
                    tr(
                        "‚ö† SPY „ÅÆ full_backup „ÅåÂ≠òÂú®„Åó„Åæ„Åõ„Çì: {p}",
                        p=str(full_dir / "SPY.csv"),
                    ),
                )
            except Exception:
                pass

    try:
        df = _read_daily_csv_any_datecol(Path(path))

        # Áõ¥ËøëÊÉÖÂ†±„ÅÆË°®Á§∫ÔºàUI„ÅåÁÑ°„ÅÑÂ†¥Èù¢„Åß„ÅØÁÑ°Ë¶ñ„Åï„Çå„ÇãÔºâ
        try:
            _st_emit(
                "write", tr("‚úÖ SPY„Ç≠„É£„ÉÉ„Ç∑„É•ÊúÄÁµÇÊó•: {d}", d=str(df.index[-1].date()))
            )
        except Exception:
            pass

        # NYSE ÊúÄÊñ∞Âñ∂Ê•≠Êó•
        today = pd.Timestamp.today().normalize()
        latest_trading_day = get_latest_nyse_trading_day(today)
        try:
            _st_emit(
                "write", tr("üóìÔ∏è Áõ¥Ëøë„ÅÆNYSEÂñ∂Ê•≠Êó•: {d}", d=str(latest_trading_day.date()))
            )
        except Exception:
            pass

        # 1„Å§Ââç„ÅÆÂñ∂Ê•≠Êó•ÔºàÂΩìÊó•Âñ∂Ê•≠ÊôÇÈñìÂ∏Ø„ÅÆÂΩ±Èüø„ÇíÈÅø„Åë„ÇãÔºâ
        try:
            nyse = mcal.get_calendar("NYSE")
            sched = nyse.schedule(
                start_date=today - pd.Timedelta(days=7),
                end_date=today,
            )
            valid = pd.to_datetime(sched.index).normalize()
            prev_trading_day = valid[-2] if len(valid) >= 2 else latest_trading_day
        except Exception:
            prev_trading_day = latest_trading_day

        # Á±≥Êù±ÈÉ®ÊôÇÈñì„ÇíÂèñÂæó
        try:
            ny_time = pd.Timestamp.now(tz="America/New_York").time()
        except Exception:
            ny_time = dtime(18, 0)

        # Âè§„ÅÑÂ†¥Âêà„ÅØË≠¶Âëä„ÅÆ„ÅøË°®Á§∫
        if df.index[-1].normalize() < prev_trading_day and ny_time >= dtime(18, 0):
            try:
                _st_emit("warning", tr("‚ö† SPY„Ç≠„É£„ÉÉ„Ç∑„É•„ÅåÂè§„ÅÑÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„Åô"))
            except Exception:
                pass
        else:
            try:
                _st_emit("write", tr("‚úÖ SPY„Ç≠„É£„ÉÉ„Ç∑„É•„ÅØÊúâÂäπ"))
            except Exception:
                pass

        return df

    except Exception as e:
        _st_emit("error", tr("‚ùå SPYË™≠„ÅøËæº„ÅøÂ§±Êïó: {e}", e=str(e)))
        return None


def _normalize_to_naive_day(ts: pd.Timestamp | None) -> pd.Timestamp:
    """Normalize timestamp to tz-naive midnight."""

    if ts is None:
        ts = pd.Timestamp.now(tz=_NY_TIMEZONE)
    else:
        ts = pd.Timestamp(ts)
        tz = getattr(ts, "tzinfo", None)
        if tz is not None:
            try:
                ts = ts.tz_convert(_NY_TIMEZONE)
            except (TypeError, ValueError, AttributeError):
                try:
                    ts = pd.Timestamp(ts.to_pydatetime().astimezone(_NY_TIMEZONE))
                except Exception:
                    ts = pd.Timestamp(ts.to_pydatetime().replace(tzinfo=None))
    tz = getattr(ts, "tzinfo", None)
    if tz is not None:
        try:
            ts = ts.tz_localize(None)
        except (TypeError, ValueError, AttributeError):
            try:
                ts = ts.tz_convert(None)
            except Exception:
                ts = pd.Timestamp(ts.to_pydatetime().replace(tzinfo=None))
    return ts.normalize()


def get_latest_nyse_trading_day(today: pd.Timestamp | None = None) -> pd.Timestamp:
    nyse = mcal.get_calendar("NYSE")
    today_naive = _normalize_to_naive_day(today)
    sched = nyse.schedule(
        start_date=today_naive - pd.Timedelta(days=7),
        end_date=today_naive + pd.Timedelta(days=1),
    )
    valid_days = pd.to_datetime(sched.index).normalize()
    return valid_days[valid_days <= today_naive].max()


def get_next_nyse_trading_day(current: pd.Timestamp | None = None) -> pd.Timestamp:
    """NYË®ºÂà∏ÂèñÂºïÊâÄ„ÅÆÁøåÂñ∂Ê•≠Êó•„ÇíËøî„Åô„ÄÇ"""

    nyse = mcal.get_calendar("NYSE")
    current_naive = _normalize_to_naive_day(current)
    sched = nyse.schedule(
        start_date=current_naive,
        end_date=current_naive + pd.Timedelta(days=10),
    )
    valid_days = pd.to_datetime(sched.index).normalize()
    future_days = valid_days[valid_days > current_naive]
    if future_days.empty:
        raise ValueError("No upcoming NYSE trading day found")
    return future_days.min()


def get_signal_target_trading_day(now: pd.Timestamp | None = None) -> pd.Timestamp:
    """Determine the trading day to target when running today's signal extraction."""

    def _ensure_ny_timestamp(ts: pd.Timestamp | None) -> pd.Timestamp:
        if ts is None:
            return pd.Timestamp.now(tz="America/New_York")
        raw = pd.Timestamp(ts)
        tzinfo = getattr(raw, "tzinfo", None)
        if tzinfo is None:
            try:
                localized = raw.tz_localize(
                    "America/New_York", ambiguous="NaT", nonexistent="NaT"
                )
                if pd.isna(localized):  # type: ignore[truthy-bool]
                    raise ValueError
                return localized
            except Exception:
                return raw.tz_localize("UTC").tz_convert("America/New_York")
        try:
            return raw.tz_convert("America/New_York")
        except Exception:
            return raw.tz_localize("America/New_York")

    try:
        ny_now = _ensure_ny_timestamp(now)
    except Exception:
        ny_now = pd.Timestamp.now(tz="America/New_York")

    latest = get_latest_nyse_trading_day(ny_now)
    target = latest

    try:
        ny_date = ny_now.date()
    except Exception:
        ny_date = None
    try:
        latest_date = pd.Timestamp(latest).date()
    except Exception:
        latest_date = None
    try:
        ny_time = ny_now.time()
    except Exception:
        ny_time = None

    need_next = False
    if ny_date is not None and latest_date is not None and ny_date > latest_date:
        need_next = True
    if ny_time is not None and ny_time >= dtime(16, 0):
        need_next = True

    if need_next:
        try:
            target = get_next_nyse_trading_day(latest)
        except Exception:
            target = latest

    return pd.Timestamp(target).normalize()


def resolve_signal_entry_date(base_date) -> pd.Timestamp | pd.NaT:
    """„Ç∑„Ç∞„Éä„É´Êó•„Åã„ÇâÁøåÂñ∂Ê•≠Êó•ÔºàÂèñÂºï‰∫àÂÆöÊó•Ôºâ„ÇíÁÆóÂá∫„Åô„Çã„ÄÇ

    - base_date „ÅåÊ¨†Êêç„ÉªÂ§âÊèõ‰∏çÂèØ„ÅÆÂ†¥Âêà„ÅØ NaT „ÇíËøî„Åô„ÄÇ
    - get_next_nyse_trading_day „ÅÆÁµêÊûú„ÅØÂ∏∏„Å´ tz-naive „Å™Êó•‰ªò„Å´Ê≠£Ë¶èÂåñ„Åô„Çã„ÄÇ
    """

    if base_date is None or (isinstance(base_date, float) and pd.isna(base_date)):
        return pd.NaT
    try:
        ts = pd.Timestamp(base_date)
    except Exception:
        return pd.NaT
    if pd.isna(ts):
        return pd.NaT
    ts = _normalize_to_naive_day(ts)
    try:
        entry_candidate = get_next_nyse_trading_day(ts)
    except Exception:
        return pd.NaT
    if pd.isna(entry_candidate):
        return pd.NaT
    entry_ts = pd.Timestamp(entry_candidate)
    try:
        entry_ts = entry_ts.tz_localize(None)
    except Exception:
        pass
    return entry_ts.normalize()


def get_spy_data_cached(folder: str = "data_cache"):
    """
    Êóß„Éê„Éº„Ç∏„Éß„É≥„ÅÆ SPY „Ç≠„É£„ÉÉ„Ç∑„É•Ë™≠„ÅøËæº„ÅøÈñ¢Êï∞„ÄÇ
    - v2 „É™„Çæ„É´„Éê„Çí‰ΩøÁî®Ôºàbacktest: base‚Üífull_backup, today: rolling‚Üíbase‚Üífull_backupÔºâ
    """
    # v2 ÂÆüË£Ö„Å∏ÂßîË≠≤ÔºàÊé¢Á¥¢È†ÜÂàáÊõø„ÅØ v2 „Å´ÈõÜÁ¥ÑÔºâ
    try:
        return get_spy_data_cached_v2(folder)
    except Exception:
        # v2 ÂÅ¥„Å´‰∏ÄÊú¨Âåñ„Åô„Çã„Åü„ÇÅ„ÄÅÂæìÊù•„ÅÆ data_cache Áõ¥‰∏ã„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ„ÅØÂªÉÊ≠¢
        return get_spy_data_cached_v2(folder)


def _find_spy_csv(dir_path: Path) -> Path | None:
    try:
        if not dir_path.exists():
            return None
        for fn in os.listdir(dir_path):
            if fn.lower() == "spy.csv":
                return dir_path / fn
    except Exception:
        return None
    return None


def _persist_spy_with_indicators(spy_df: pd.DataFrame) -> None:
    """Persist augmented SPY data to rolling/base cache for reuse."""

    if spy_df is None or getattr(spy_df, "empty", True):
        return
    try:
        settings = get_settings(create_dirs=True)
        root = Path(settings.DATA_CACHE_DIR)
        rolling_dir = Path(getattr(settings.cache, "rolling_dir", root / "rolling"))
        base_dir = root / "base"
    except Exception:
        root = Path("data_cache")
        rolling_dir = root / "rolling"
        base_dir = root / "base"

    for target_dir in (rolling_dir, base_dir):
        path = _find_spy_csv(target_dir)
        if path is None or not path.exists():
            continue
        try:
            df_to_save = spy_df.copy()
            if "Date" in df_to_save.columns:
                df_to_save["Date"] = pd.to_datetime(
                    df_to_save["Date"], errors="coerce"
                ).dt.normalize()
                df_to_save = df_to_save.dropna(subset=["Date"]).sort_values("Date")
                df_to_save.to_csv(path, index=False)
            else:
                idx = pd.to_datetime(df_to_save.index, errors="coerce").normalize()
                df_to_save = df_to_save.loc[~idx.isna()].copy()
                df_to_save.index = pd.Index(idx[~idx.isna()])
                df_to_save.sort_index(inplace=True)
                df_to_save.to_csv(path, index_label="Date")
        except Exception:
            continue
        else:
            break


def get_spy_with_indicators(spy_df=None):
    """
    SPY „Å´ SMA100 / SMA200 „Çí‰ªò‰∏éÔºà„Éï„Ç£„É´„Çø„Éº„ÅØÊà¶Áï•ÂÅ¥„ÅßÂà§ÂÆö„Åô„ÇãÔºâ
    """

    loaded_from_cache = False
    if spy_df is None:
        # Prefer today cache (rolling) for the latest values, fallback to base/full.
        loaded_from_cache = True
        spy_df = get_spy_data_cached_v2(mode="today")
        if spy_df is None:
            spy_df = get_spy_data_cached_v2()

    if spy_df is not None and not getattr(spy_df, "empty", True):
        if isinstance(spy_df, pd.Series):
            spy_df = spy_df.to_frame().T
        elif not isinstance(spy_df, pd.DataFrame):
            try:
                spy_df = pd.DataFrame(spy_df)
            except Exception:
                return spy_df

        spy_df = spy_df.copy()

        if isinstance(spy_df.columns, pd.MultiIndex):
            flattened_cols: list[str] = []
            for col in spy_df.columns:
                if isinstance(col, tuple):
                    flattened = next((part for part in col if part not in (None, "")), None)
                    if flattened is None:
                        flattened = col[-1] if col else ""
                    flattened_cols.append(flattened)
                else:
                    flattened_cols.append(col)
            spy_df.columns = flattened_cols

        # Close ÂàóÂêç„ÅÆ„Å∞„Çâ„Å§„Åç„Å´ÂØæÂøúÔºàclose/AdjClose/adjusted_close Á≠âÔºâ
        if "Close" not in spy_df.columns:
            if "close" in spy_df.columns:
                spy_df["Close"] = spy_df["close"]
            elif "AdjClose" in spy_df.columns:
                spy_df["Close"] = spy_df["AdjClose"]
            elif "adjclose" in spy_df.columns:
                spy_df["Close"] = spy_df["adjclose"]
            elif "adjusted_close" in spy_df.columns:
                spy_df["Close"] = spy_df["adjusted_close"]
            else:
                try:
                    _st_emit(
                        "warning",
                        tr(
                            "‚ùóSPY„ÅÆÁµÇÂÄ§Âàó„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì: {cols}",
                            cols=str(list(spy_df.columns)),
                        ),
                    )
                except Exception:
                    pass
                return spy_df

        close_series = spy_df["Close"]
        if isinstance(close_series, pd.DataFrame):
            try:
                close_series = close_series.iloc[:, 0]
            except Exception:
                close_series = close_series.squeeze(axis=1)

        if isinstance(close_series, pd.Series):
            if len(close_series) == len(spy_df.index):
                close_series = pd.Series(close_series.to_numpy(), index=spy_df.index)
            else:
                close_series = close_series.reindex(spy_df.index)
        else:
            try:
                close_series = pd.Series(close_series, index=spy_df.index)
            except Exception:
                close_series = pd.Series(close_series)
                if len(close_series) == len(spy_df.index):
                    close_series.index = spy_df.index
                else:
                    return spy_df

        close_numeric = pd.to_numeric(close_series, errors="coerce")

        spy_df["SMA100"] = SMAIndicator(
            close_numeric,
            window=100,
        ).sma_indicator()
        spy_df["SMA200"] = SMAIndicator(
            close_numeric,
            window=200,
        ).sma_indicator()

        if loaded_from_cache:
            try:
                _persist_spy_with_indicators(spy_df)
            except Exception:
                pass

    return spy_df
