"""ä»Šæ—¥ã®ã‚·ã‚°ãƒŠãƒ«æŠ½å‡ºãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ç”¨ã„ã‚‹åŸºç¤ãƒ»æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ç¾¤ã€‚

run_all_systems_today.py ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿è²¬å‹™ã‚’åˆ†é›¢ï¼ˆè²¬å‹™åˆ†å‰²ï¼‰:
  - basic_data èª­ã¿è¾¼ã¿ï¼ˆsymbol_data / rolling / base éšå±¤å¯¾å¿œï¼‰
  - indicator_data èª­ã¿è¾¼ã¿
  - æ–°é®®åº¦åˆ¤å®šãƒ»ä¸¦åˆ—å‡¦ç†ãƒ»é€²æ—ãƒ¬ãƒãƒ¼ãƒˆ

æ³¨æ„: å…¬é–‹ API ã¯ run_all_systems_today.py ã¨äº’æ›ã€‚
      ä¾å­˜: CacheManager, Settings, pandas, threadingï¼ˆå¤–éƒ¨ UI ã‚³ãƒ¼ãƒ«ä¸å«ï¼‰
"""

from __future__ import annotations

import os
import time
from collections.abc import Callable
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
from typing import Any

import pandas as pd

from common.cache_manager import CacheManager
from common.rate_limited_logging import create_rate_limited_logger

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒ­ã‚¬ãƒ¼
_rate_limited_logger = None


def _get_rate_limited_logger():
    """ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒ­ã‚¬ãƒ¼ã‚’å–å¾—ã€‚"""
    global _rate_limited_logger
    if _rate_limited_logger is None:
        _rate_limited_logger = create_rate_limited_logger("today_data_loader", 3.0)
    return _rate_limited_logger


__all__ = [
    "_extract_last_cache_date",
    "_recent_trading_days",
    "_build_rolling_from_base",
    "load_basic_data",
    "load_indicator_data",
]

# ----------------------------- ãƒ‡ãƒ¼ã‚¿æ“ä½œãƒ˜ãƒ«ãƒ‘ ----------------------------- #


def _extract_last_cache_date(df: pd.DataFrame) -> pd.Timestamp | None:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æœ€çµ‚æ—¥ä»˜ã‚’æŠ½å‡ºã€‚"""
    if df is None or getattr(df, "empty", True):
        return None
    for col in ("date", "Date"):
        if col in df.columns:
            try:
                values = pd.to_datetime(df[col].to_numpy(), errors="coerce")
                values = values.dropna()
                if not values.empty:
                    return pd.Timestamp(values[-1]).normalize()
            except Exception:
                continue
    try:
        idx = pd.to_datetime(df.index.to_numpy(), errors="coerce")
        mask = ~pd.isna(idx)
        if mask.any():
            return pd.Timestamp(idx[mask][-1]).normalize()
    except Exception:
        pass
    return None


def _recent_trading_days(today: pd.Timestamp | None, max_back: int) -> list[pd.Timestamp]:
    """ä»Šæ—¥ã‹ã‚‰æœ€å¤§ max_back å–¶æ¥­æ—¥ã‚’é¡ã£ã¦æ—¥ä»˜ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆã€‚"""
    if today is None:
        return []
    try:
        from common.utils_spy import get_latest_nyse_trading_day
    except ImportError:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å˜ç´”ãªæ—¥ä»˜æ¸›ç®—
        dates = []
        current = pd.Timestamp(today).normalize()
        for _i in range(max_back + 1):
            dates.append(current)
            current = current - pd.Timedelta(days=1)
        return dates

    out: list[pd.Timestamp] = []
    seen: set[pd.Timestamp] = set()
    current = pd.Timestamp(today).normalize()
    steps = max(0, int(max_back))
    for _ in range(steps + 1):
        if current in seen:
            break
        out.append(current)
        seen.add(current)
        prev_candidate = get_latest_nyse_trading_day(current - pd.Timedelta(days=1))
        prev_candidate = pd.Timestamp(prev_candidate).normalize()
        if prev_candidate == current:
            break
        current = prev_candidate
    return out


def _build_rolling_from_base(
    symbol: str,
    base_df: pd.DataFrame,
    target_len: int,
    cache_manager: CacheManager | None = None,
) -> pd.DataFrame | None:
    """base ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ rolling å½¢å¼ï¼ˆå°»å°¾åˆ‡ã‚Šï¼‰ã«å¤‰æ›ã€å¿…è¦ãªã‚‰ä¿å­˜ã€‚"""
    if base_df is None or getattr(base_df, "empty", True):
        return None
    try:
        work = base_df.copy()
    except Exception:
        work = base_df
    if work.index.name is not None:
        work = work.reset_index()
    if "Date" in work.columns:
        work["date"] = pd.to_datetime(work["Date"].to_numpy(), errors="coerce")
    elif "date" in work.columns:
        work["date"] = pd.to_datetime(work["date"].to_numpy(), errors="coerce")
    else:
        return None
    work = work.dropna(subset=["date"]).sort_values("date")
    col_map = {
        "Open": "open",
        "High": "high",
        "Low": "low",
        "Close": "close",
        "AdjClose": "adjusted_close",
        "Adj Close": "adjusted_close",
        "Volume": "volume",
    }
    try:
        for src, dst in list(col_map.items()):
            if src in work.columns:
                work = work.rename(columns={src: dst})
    except Exception:
        pass
    sliced = work.tail(int(target_len)).reset_index(drop=True)
    if sliced.empty:
        return None
    if cache_manager is not None:
        try:
            cache_manager.write_atomic(sliced, symbol, "rolling")
        except Exception:
            pass
    return sliced


def _normalize_ohlcv(df: pd.DataFrame) -> pd.DataFrame:
    """åˆ—åã‚’å¤§æ–‡å­—OHLCVã«çµ±ä¸€ã€‚"""
    col_map = {
        "open": "Open",
        "high": "High",
        "low": "Low",
        "close": "Close",
        "volume": "Volume",
        "adj_close": "AdjClose",
        "adjusted_close": "AdjClose",
    }
    try:
        return df.rename(columns={k: v for k, v in col_map.items() if k in df.columns})
    except Exception:
        return df


# ----------------------------- åŸºç¤ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ ----------------------------- #


def load_basic_data(
    symbols: list[str],
    cache_manager: CacheManager,
    settings: Any,
    symbol_data: dict[str, pd.DataFrame] | None,
    *,
    today: pd.Timestamp | None = None,
    freshness_tolerance: int | None = None,
    base_cache: dict[str, pd.DataFrame] | None = None,
    log_callback: Callable[[str, bool], None] | None = None,
    ui_log_callback: Callable[[str], None] | None = None,
) -> dict[str, pd.DataFrame]:
    """
    åŸºç¤ãƒ‡ãƒ¼ã‚¿ï¼ˆOHLCV + åŸºæœ¬æŒ‡æ¨™ï¼‰ã‚’èª­ã¿è¾¼ã¿ã€‚

    èª­ã¿è¾¼ã¿é †åº:
    1. symbol_data (äº‹å‰æä¾›)
    2. rolling ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    3. base ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ rolling ç”Ÿæˆ

    Args:
        symbols: å¯¾è±¡ã‚·ãƒ³ãƒœãƒ«ä¸€è¦§
        cache_manager: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        settings: è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        symbol_data: äº‹å‰ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
        today: ä»Šæ—¥ã®æ—¥ä»˜ï¼ˆæ–°é®®åº¦åˆ¤å®šç”¨ï¼‰
        freshness_tolerance: è¨±å®¹ã•ã‚Œã‚‹é™³è…åŒ–æ—¥æ•°
        base_cache: ãƒ™ãƒ¼ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ (æœªä½¿ç”¨, äº’æ›æ€§ç¶­æŒ)
        log_callback: ãƒ­ã‚°ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
        ui_log_callback: UI ãƒ­ã‚°ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯

    Returns:
        {symbol: DataFrame} ã®è¾æ›¸
    """

    def _log(msg: str, ui: bool = True) -> None:
        if log_callback:
            log_callback(msg, ui)

    def _emit_ui_log(msg: str) -> None:
        if ui_log_callback:
            ui_log_callback(msg)

    data: dict[str, pd.DataFrame] = {}
    total_syms = len(symbols)
    start_ts = time.perf_counter()
    chunk = 500

    if freshness_tolerance is None:
        try:
            freshness_tolerance = int(settings.cache.rolling.max_staleness_days)
        except Exception:
            freshness_tolerance = 2
    freshness_tolerance = max(0, int(freshness_tolerance))

    # target length è©¦ç®—ã¯æœªä½¿ç”¨ã®ãŸã‚å‰Šé™¤ï¼ˆä»¥å‰ã®ãƒ­ã‚¸ãƒƒã‚¯æ®‹éª¸ï¼‰

    stats_lock = Lock()
    stats: dict[str, int] = {}

    def _record_stat(key: str) -> None:
        with stats_lock:
            stats[key] = stats.get(key, 0) + 1

    recent_allowed: set[pd.Timestamp] = set()
    if today is not None and freshness_tolerance >= 0:
        try:
            recent_allowed = {
                pd.Timestamp(d).normalize()
                for d in _recent_trading_days(pd.Timestamp(today), freshness_tolerance)
            }
        except Exception:
            recent_allowed = set()

    gap_probe_days = max(freshness_tolerance + 5, 10)

    def _estimate_gap_days(
        today_dt: pd.Timestamp | None, last_dt: pd.Timestamp | None
    ) -> int | None:
        if today_dt is None or last_dt is None:
            return None
        try:
            recent = _recent_trading_days(pd.Timestamp(today_dt), gap_probe_days)
        except Exception:
            recent = []
        for offset, dt in enumerate(recent):
            if dt == last_dt:
                return offset
        try:
            return max(0, int((pd.Timestamp(today_dt) - pd.Timestamp(last_dt)).days))
        except Exception:
            return None

    def _pick_symbol_data(sym: str) -> pd.DataFrame | None:
        try:
            if not symbol_data or sym not in symbol_data:
                return None
            df = symbol_data.get(sym)
            if df is None or getattr(df, "empty", True):
                return None
            x = df.copy()
            if x.index.name is not None:
                x = x.reset_index()
            if "date" in x.columns:
                x["date"] = pd.to_datetime(x["date"].to_numpy(), errors="coerce")
            elif "Date" in x.columns:
                x["date"] = pd.to_datetime(x["Date"].to_numpy(), errors="coerce")
            else:
                return None
            col_map = {
                "Open": "open",
                "High": "high",
                "Low": "low",
                "Close": "close",
                "Adj Close": "adjusted_close",
                "AdjClose": "adjusted_close",
                "Volume": "volume",
            }
            for k, v in list(col_map.items()):
                if k in x.columns:
                    x = x.rename(columns={k: v})
            required = {"date", "close"}
            if not required.issubset(set(x.columns)):
                return None
            x = x.dropna(subset=["date"]).sort_values("date")
            return x
        except Exception:
            return None

    def _normalize_loaded(df: pd.DataFrame | None) -> pd.DataFrame | None:
        if df is None or getattr(df, "empty", True):
            return None
        try:
            if "Date" not in df.columns:
                work = df.copy()
                if "date" in work.columns:
                    work["Date"] = pd.to_datetime(work["date"].to_numpy(), errors="coerce")
                else:
                    work["Date"] = pd.to_datetime(work.index.to_numpy(), errors="coerce")
                df = work
            df["Date"] = pd.to_datetime(df["Date"].to_numpy(), errors="coerce").normalize()
        except Exception:
            pass
        normalized = _normalize_ohlcv(df)
        try:
            fill_cols = [
                c for c in ("Open", "High", "Low", "Close", "Volume") if c in normalized.columns
            ]
            if fill_cols:
                normalized = normalized.copy()
                try:
                    filled = normalized[fill_cols].apply(pd.to_numeric, errors="coerce")
                except Exception:
                    filled = normalized[fill_cols]
                normalized.loc[:, fill_cols] = filled.ffill().bfill()
        except Exception:
            pass
        try:
            if "Date" in normalized.columns:
                normalized = normalized.dropna(subset=["Date"])
        except Exception:
            pass
        return normalized

    env_parallel = (os.environ.get("BASIC_DATA_PARALLEL", "") or "").strip().lower()
    try:
        env_parallel_threshold = int(os.environ.get("BASIC_DATA_PARALLEL_THRESHOLD", "200"))
    except Exception:
        env_parallel_threshold = 200
    if env_parallel in ("1", "true", "yes"):
        use_parallel = total_syms > 1
    elif env_parallel in ("0", "false", "no"):
        use_parallel = False
    else:
        use_parallel = total_syms >= max(0, env_parallel_threshold)

    max_workers: int | None = None
    if use_parallel and total_syms > 0:
        try:
            env_workers = (os.environ.get("BASIC_DATA_MAX_WORKERS", "") or "").strip()
            if env_workers:
                max_workers = int(env_workers)
        except Exception:
            max_workers = None
        if max_workers is None:
            try:
                cfg_workers = getattr(settings.cache.rolling, "load_max_workers", None)
                if cfg_workers:
                    max_workers = int(cfg_workers)
            except Exception:
                pass
        if max_workers is None:
            cpu_count = os.cpu_count() or 4
            max_workers = max(4, cpu_count * 2)
        max_workers = max(1, min(int(max_workers), total_syms))
        if _log:
            _log(f"ğŸ§µ åŸºç¤ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ä¸¦åˆ—åŒ–: workers={max_workers}")

    def _load_one(sym: str) -> tuple[str, pd.DataFrame | None]:
        try:
            source: str | None = None
            df = _pick_symbol_data(sym)
            rebuild_reason: str | None = None
            last_seen_date: pd.Timestamp | None = None
            # gap_days: ä¸ä½¿ç”¨ï¼ˆçµ±è¨ˆæ©Ÿèƒ½ç°¡ç´ åŒ–ã«ã‚ˆã‚Šå‰Šé™¤ï¼‰
            if df is None or getattr(df, "empty", True):
                df = cache_manager.read(sym, "rolling")
            else:
                source = "prefetched"
            if df is None or getattr(df, "empty", True):
                source = None
            if df is None or getattr(df, "empty", True):
                needs_rebuild = True
            else:
                needs_rebuild = False
            if df is not None and not getattr(df, "empty", True) and source is None:
                source = "rolling"
            if df is not None and not getattr(df, "empty", True):
                last_seen_date = _extract_last_cache_date(df)
                if last_seen_date is None:
                    rebuild_reason = rebuild_reason or "missing_date"
                    needs_rebuild = True
                else:
                    last_seen_date = pd.Timestamp(last_seen_date).normalize()
                    if (
                        today is not None
                        and recent_allowed
                        and last_seen_date not in recent_allowed
                    ):
                        rebuild_reason = "stale"
                        # gap_days = _estimate_gap_days(pd.Timestamp(today), last_seen_date)
                        needs_rebuild = True
            if needs_rebuild:
                # å€‹åˆ¥ãƒ­ã‚°ã‚’æŠ‘åˆ¶ï¼ˆã‚µãƒãƒªãƒ¼è¡¨ç¤ºã«çµ±åˆï¼‰
                _record_stat("manual_rebuild_required")
                _record_stat("failed")
                return sym, None
            normalized = _normalize_loaded(df)
            if normalized is not None and not getattr(normalized, "empty", True):
                _record_stat(source or "rolling")
                return sym, normalized
            _record_stat("failed")
            return sym, None
        except Exception:
            _record_stat("failed")
            return sym, None

    def _report_progress(done: int) -> None:
        if done <= 0 or chunk <= 0:
            return
        if done % chunk != 0:
            return
        try:
            elapsed = max(0.001, time.perf_counter() - start_ts)
            rate = done / elapsed
            remain = max(0, total_syms - done)
            eta_sec = int(remain / rate) if rate > 0 else 0
            m, s = divmod(eta_sec, 60)
            msg = f"ğŸ“¦ åŸºç¤ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰é€²æ—: {done}/{total_syms} | ETA {m}åˆ†{s}ç§’"

            # é€²æ—ãƒ­ã‚°ã¯DEBUGãƒ¬ãƒ™ãƒ«ã§ãƒ¬ãƒ¼ãƒˆåˆ¶é™é©ç”¨
            try:
                rate_logger = _get_rate_limited_logger()
                rate_logger.debug_rate_limited(
                    f"ğŸ“¦ åŸºç¤ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰é€²æ—: {done}/{total_syms}",
                    interval=2.0,
                    message_key="åŸºç¤ãƒ‡ãƒ¼ã‚¿é€²æ—",
                )
            except Exception:
                pass
            if _emit_ui_log:
                _emit_ui_log(msg)
        except Exception:
            if _log:
                _log(f"ğŸ“¦ åŸºç¤ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰é€²æ—: {done}/{total_syms}", ui=False)
            if _emit_ui_log:
                _emit_ui_log(f"ğŸ“¦ åŸºç¤ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰é€²æ—: {done}/{total_syms}")

    processed = 0
    if use_parallel and max_workers and total_syms > 1:
        # æ–°ã—ã„ä¸¦åˆ—ãƒãƒƒãƒèª­ã¿è¾¼ã¿ã‚’ä½¿ç”¨ï¼ˆPhase2æœ€é©åŒ–ï¼‰
        try:
            if _log:
                _log(f"ğŸš€ ä¸¦åˆ—ãƒãƒƒãƒèª­ã¿è¾¼ã¿é–‹å§‹: {total_syms}ã‚·ãƒ³ãƒœãƒ«, workers={max_workers}")

            def progress_callback_internal(loaded, total):
                nonlocal processed
                processed = loaded
                _report_progress(processed)

            # CacheManagerã®ä¸¦åˆ—èª­ã¿è¾¼ã¿æ©Ÿèƒ½ã‚’æ´»ç”¨
            parallel_data = cache_manager.read_batch_parallel(
                symbols=symbols,
                profile="rolling",
                max_workers=max_workers,
                fallback_profile="full",
                progress_callback=progress_callback_internal,
            )

            # çµæœã‚’æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«åˆã‚ã›ã¦å‡¦ç†
            for sym, df in parallel_data.items():
                if df is not None and not getattr(df, "empty", True):
                    # æ—¢å­˜ã®_normalize_loadedã¨åŒæ§˜ã®å‡¦ç†ã‚’é©ç”¨
                    normalized = _normalize_loaded(df)
                    if normalized is not None and not getattr(normalized, "empty", True):
                        data[sym] = normalized
                        _record_stat("rolling")
                    else:
                        _record_stat("failed")
                else:
                    _record_stat("failed")

            if _log:
                _log(f"âœ… ä¸¦åˆ—ãƒãƒƒãƒèª­ã¿è¾¼ã¿å®Œäº†: {len(data)}/{total_syms}ä»¶æˆåŠŸ")

        except Exception as e:
            # ä¸¦åˆ—å‡¦ç†å¤±æ•—æ™‚ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if _log:
                _log(f"âš ï¸ ä¸¦åˆ—ãƒãƒƒãƒèª­ã¿è¾¼ã¿å¤±æ•—ã€å¾“æ¥å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {e}")
            data.clear()
            processed = 0
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = {executor.submit(_load_one, sym): sym for sym in symbols}
                for fut in as_completed(futures):
                    try:
                        sym, df = fut.result()
                    except Exception:
                        sym, df = futures[fut], None
                    if df is not None and not getattr(df, "empty", True):
                        data[sym] = df
                    processed += 1
                    _report_progress(processed)
    else:
        for sym in symbols:
            sym, df = _load_one(sym)
            if df is not None and not getattr(df, "empty", True):
                data[sym] = df
            processed += 1
            _report_progress(processed)

    try:
        total_elapsed = max(0.0, time.perf_counter() - start_ts)
        total_int = int(total_elapsed)
        m, s = divmod(total_int, 60)
        done_msg = f"ğŸ“¦ åŸºç¤ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰å®Œäº†: {len(data)}/{total_syms} | æ‰€è¦ {m}åˆ†{s}ç§’" + (
            " | ä¸¦åˆ—=ON" if use_parallel and max_workers else " | ä¸¦åˆ—=OFF"
        )
        if _log:
            _log(done_msg)
        if _emit_ui_log:
            _emit_ui_log(done_msg)
    except Exception:
        if _log:
            _log(f"ğŸ“¦ åŸºç¤ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰å®Œäº†: {len(data)}/{total_syms}")
        if _emit_ui_log:
            _emit_ui_log(f"ğŸ“¦ åŸºç¤ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰å®Œäº†: {len(data)}/{total_syms}")

    try:
        summary_map = {
            "prefetched": "äº‹å‰ä¾›çµ¦",
            "rolling": "rollingå†åˆ©ç”¨",
            "manual_rebuild_required": "æ‰‹å‹•å¯¾å¿œ",
            "failed": "å¤±æ•—",
        }
        summary_parts = [
            f"{label}={stats.get(key, 0)}" for key, label in summary_map.items() if stats.get(key)
        ]
        if summary_parts:
            try:
                rate_logger = _get_rate_limited_logger()
                rate_logger.debug_rate_limited(
                    "ğŸ“Š åŸºç¤ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰å†…è¨³: " + " / ".join(summary_parts),
                    interval=5.0,
                    message_key="åŸºç¤ãƒ‡ãƒ¼ã‚¿å†…è¨³",
                )
            except Exception:
                pass
    except Exception:
        pass

    return data


# ----------------------------- æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ ----------------------------- #


def load_indicator_data(
    symbols: list[str],
    cache_manager: CacheManager,
    settings: Any,
    symbol_data: dict[str, pd.DataFrame] | None,
    *,
    log_callback: Callable[[str, bool], None] | None = None,
    ui_log_callback: Callable[[str], None] | None = None,
) -> dict[str, pd.DataFrame]:
    """
    æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ï¼ˆäº‹å‰è¨ˆç®—æ¸ˆã¿æŒ‡æ¨™å«ã‚€å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼‰ã‚’èª­ã¿è¾¼ã¿ã€‚

    Args:
        symbols: å¯¾è±¡ã‚·ãƒ³ãƒœãƒ«ä¸€è¦§
        cache_manager: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        settings: è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        symbol_data: äº‹å‰ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
        log_callback: ãƒ­ã‚°ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
        ui_log_callback: UI ãƒ­ã‚°ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯

    Returns:
        {symbol: DataFrame} ã®è¾æ›¸
    """

    def _log(msg: str, ui: bool = True) -> None:
        if log_callback:
            log_callback(msg, ui)

    def _emit_ui_log(msg: str) -> None:
        if ui_log_callback:
            ui_log_callback(msg)

    data: dict[str, pd.DataFrame] = {}
    total_syms = len(symbols)
    start_ts = time.time()
    chunk = 500

    # å€‹åˆ¥éŠ˜æŸ„ã”ã¨ã® "â›” rollingæœªæ•´å‚™" ãƒ­ã‚°ã¯å†—é•·ã«ãªã‚‹ãŸã‚æ—¢å®šã§æŠ‘åˆ¶ã—ã€
    # ãƒ«ãƒ¼ãƒ—çµ‚äº†å¾Œã«ã‚µãƒãƒªãƒ¼ã®ã¿ã‚’å‡ºåŠ›ã™ã‚‹æ–¹é‡ã«å¤‰æ›´ã€‚
    # æ—§æŒ™å‹•ã‚’å¾©æ´»ã•ã›ãŸã„å ´åˆã¯ç’°å¢ƒå¤‰æ•° ROLLING_MISSING_VERBOSE=1 ã‚’è¨­å®šã€‚
    missing_symbols: list[str] = []
    # ç†ç”±åˆ¥ã‚«ã‚¦ãƒ³ã‚¿ (ç”Ÿæˆå¤±æ•—/é•·ã•ä¸è¶³ãªã©) ã‚’åé›†ã—æœ€çµ‚ã‚µãƒãƒªãƒ¼ã«è¼‰ã›ã‚‹
    missing_reasons: dict[str, int] = {}
    verbose_missing = os.environ.get("ROLLING_MISSING_VERBOSE", "").strip().lower() in {
        "1",
        "true",
        "yes",
        "on",
    }

    for idx, sym in enumerate(symbols, start=1):
        try:
            df = None
            try:
                if symbol_data and sym in symbol_data:
                    df = symbol_data.get(sym)
                    if df is not None and not df.empty:
                        x = df.copy()
                        if x.index.name is not None:
                            x = x.reset_index()
                        if "date" in x.columns:
                            x["date"] = pd.to_datetime(x["date"].to_numpy(), errors="coerce")
                        elif "Date" in x.columns:
                            x["date"] = pd.to_datetime(x["Date"].to_numpy(), errors="coerce")
                        col_map = {
                            "Open": "open",
                            "High": "high",
                            "Low": "low",
                            "Close": "close",
                            "Adj Close": "adjusted_close",
                            "AdjClose": "adjusted_close",
                            "Volume": "volume",
                        }
                        for k, v in list(col_map.items()):
                            if k in x.columns:
                                x = x.rename(columns={k: v})
                        required = {"date", "close"}
                        if required.issubset(set(x.columns)):
                            x = x.dropna(subset=["date"]).sort_values("date")
                            df = x
                        else:
                            df = None
                    else:
                        df = None
            except Exception:
                df = None
            if df is None or df.empty:
                df = cache_manager.read(sym, "rolling")

            try:
                target_len = int(
                    settings.cache.rolling.base_lookback_days + settings.cache.rolling.buffer_days
                )
            except Exception:
                target_len = 300  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

            needs_rebuild = df is None or getattr(df, "empty", True)
            if needs_rebuild:
                # ç†ç”±ã¯ã¾ã¨ã‚ã¦ä½¿ã‚ãªã„ãŒã€å°†æ¥ã®è©³ç´°é›†ç´„ç”¨é€”ã«ä¿æŒã™ã‚‹ãªã‚‰ã‚¿ãƒ—ãƒ«æ‹¡å¼µå¯
                if df is None or getattr(df, "empty", True):
                    reason_desc = "rollingæœªç”Ÿæˆ"
                else:
                    try:
                        reason_desc = f"len={len(df)}/{target_len}"
                    except Exception:
                        reason_desc = "è¡Œæ•°ä¸è¶³"
                missing_symbols.append(sym)
                missing_reasons[reason_desc] = missing_reasons.get(reason_desc, 0) + 1
                if verbose_missing and _log:
                    from common.cache_warnings import get_rolling_issue_aggregator

                    agg = get_rolling_issue_aggregator()
                    # CacheManager å´ã§ "missing_rolling" ãŒæ—¢ã«è¨˜éŒ²ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯äºŒé‡å ±å‘Šã‚’æŠ‘æ­¢
                    if not agg.has_issue("missing_rolling", sym):
                        _log(
                            f"â›” rollingæœªæ•´å‚™: {sym} ({reason_desc}) â†’ æ‰‹å‹•æ›´æ–°ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„",
                            ui=False,
                        )
                continue

            if df is not None and not df.empty:
                try:
                    if "Date" not in df.columns:
                        if "date" in df.columns:
                            df = df.copy()
                            df["Date"] = pd.to_datetime(df["date"].to_numpy(), errors="coerce")
                        else:
                            df = df.copy()
                            df["Date"] = pd.to_datetime(df.index.to_numpy(), errors="coerce")
                    df["Date"] = pd.to_datetime(df["Date"].to_numpy(), errors="coerce").normalize()
                except Exception:
                    pass
                df = _normalize_ohlcv(df)
                data[sym] = df
        except Exception:
            continue

        if total_syms > 0 and idx % chunk == 0:
            try:
                elapsed = max(0.001, time.time() - start_ts)
                rate = idx / elapsed
                remain = max(0, total_syms - idx)
                eta_sec = int(remain / rate) if rate > 0 else 0
                m, s = divmod(eta_sec, 60)
                msg = f"ğŸ§® æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰é€²æ—: {idx}/{total_syms} | ETA {m}åˆ†{s}ç§’"

                # é€²æ—ãƒ­ã‚°ã¯DEBUGãƒ¬ãƒ™ãƒ«ã§ãƒ¬ãƒ¼ãƒˆåˆ¶é™é©ç”¨
                try:
                    rate_logger = _get_rate_limited_logger()
                    rate_logger.debug_rate_limited(
                        f"ğŸ§® æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰é€²æ—: {idx}/{total_syms}",
                        interval=2.0,
                        message_key="æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿é€²æ—",
                    )
                except Exception:
                    pass
                if _emit_ui_log:
                    _emit_ui_log(msg)
            except Exception:
                if _log:
                    _log(f"ğŸ§® æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰é€²æ—: {idx}/{total_syms}", ui=False)
                if _emit_ui_log:
                    _emit_ui_log(f"ğŸ§® æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰é€²æ—: {idx}/{total_syms}")

    # ãƒ«ãƒ¼ãƒ—çµ‚äº†å¾Œã« missing ã®ã‚µãƒãƒªãƒ¼ã‚’ãƒãƒƒãƒè¡¨ç¤º
    if missing_symbols and _log:
        try:
            total_missing = len(missing_symbols)
            # 10%åˆ»ã¿ï¼ˆæœ€ä½1ä»¶ï¼‰ã§åˆ†å‰²ã—ã¦è¦‹ã‚„ã™ã•ç¢ºä¿
            batch_size = max(1, int(total_missing * 0.1))
            for i in range(0, total_missing, batch_size):
                batch = missing_symbols[i : i + batch_size]
                symbols_str = ", ".join(batch)
                _log(
                    f"âš ï¸ rollingæœªæ•´å‚™ ({i+1}ã€œ{min(i+batch_size, total_missing)}/{total_missing}): {symbols_str}",
                    ui=False,
                )
            # ç†ç”±åˆ¥åˆ†å¸ƒã‚’æ•´å½¢
            if missing_reasons:
                try:
                    reason_parts = [
                        f"{k}={v}"
                        for k, v in sorted(missing_reasons.items(), key=lambda x: (-x[1], x[0]))
                    ]
                    reason_str = " / ".join(reason_parts)
                except Exception:
                    reason_str = ""
            else:
                reason_str = ""
            base_summary = f"ğŸ’¡ rollingæœªæ•´å‚™ã®è¨ˆ{total_missing}éŠ˜æŸ„ã¯è‡ªå‹•çš„ã«ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸï¼ˆbase/full_backupã‹ã‚‰ã®å†è©¦è¡Œã¯ä¸è¦ï¼‰"
            if reason_str:
                base_summary += f" | å†…è¨³: {reason_str}"
            _log(base_summary, ui=False)
        except Exception:
            pass

    try:
        total_elapsed = max(0.0, time.time() - start_ts)
        total_int = int(total_elapsed)
        m, s = divmod(total_int, 60)
        done_msg = f"ğŸ§® æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰å®Œäº†: {len(data)}/{total_syms} | æ‰€è¦ {m}åˆ†{s}ç§’"
        if _log:
            _log(done_msg)
        if _emit_ui_log:
            _emit_ui_log(done_msg)
    except Exception:
        if _log:
            _log(f"ğŸ§® æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰å®Œäº†: {len(data)}/{total_syms}")
        if _emit_ui_log:
            _emit_ui_log(f"ğŸ§® æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰å®Œäº†: {len(data)}/{total_syms}")

    return data
