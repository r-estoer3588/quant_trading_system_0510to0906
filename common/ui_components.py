"""
å…±é€šUIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆï¼ˆUTF-8ãƒ»æ—¥æœ¬èªå¯¾å¿œï¼‰ã€‚
æ—¢å­˜ã®å…¬é–‹APIï¼ˆé–¢æ•°åãƒ»æˆ»ã‚Šå€¤ï¼‰ã¯ç¶­æŒã—ã¤ã¤ã€å„ãƒ•ã‚§ãƒ¼ã‚ºï¼ˆãƒ‡ãƒ¼ã‚¿å–å¾—/ã‚¤ãƒ³ã‚¸è¨ˆç®—/å€™è£œæŠ½å‡º/ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆï¼‰ã§
UIManagerï¼ˆä»»æ„ï¼‰ã«é€²æ—ã¨ãƒ­ã‚°ã‚’å‡ºåŠ›ã§ãã‚‹ã‚ˆã†ã«ã—ã¦ã„ã‚‹ã€‚
"""

from __future__ import annotations

from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
import os
import time
from typing import Any, cast

import matplotlib as mpl
from matplotlib import font_manager as _font_manager
import matplotlib.pyplot as plt
import pandas as pd
import streamlit as st

from common.cache_format import round_dataframe
from common.utils import get_cached_data, safe_filename
from config.settings import get_settings

try:
    # è¨­å®šã‹ã‚‰UIãƒ•ãƒ©ã‚°ã‚’å‚ç…§ï¼ˆå¤±æ•—æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‹•ä½œã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    from config.settings import get_settings

    _APP_SETTINGS = get_settings(create_dirs=True)
except Exception:
    _APP_SETTINGS = None
from common.cache_manager import base_cache_path, load_base_cache
from common.holding_tracker import generate_holding_matrix
import common.i18n as i18n
from common.logging_utils import log_with_progress
from scripts.tickers_loader import get_all_tickers

# äº’æ›ç”¨ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼ˆæ—¢å­˜ã‚³ãƒ¼ãƒ‰ã® tr(...) å‘¼ã³å‡ºã—ã‚’ç¶­æŒï¼‰
tr = i18n.tr

# æ˜ç¤ºçš„å…¬é–‹APIå¢ƒç•Œ: tests/test_public_api_exports.py ã® EXPECTED_EXPORTS ã¨åŒæœŸã™ã‚‹ã“ã¨ã€‚
__all__ = [
    "run_backtest_app",
    "prepare_backtest_data",
    "fetch_data",
    "show_results",
    "show_signal_trade_summary",
    "clean_date_column",
    "save_signal_and_trade_logs",
]


# ------------------------------
# Type overloads for static checkers
# ------------------------------
# overloads removed - keep concrete implementations only


# overloads removed - keep concrete implementations only


# overloads removed - keep concrete implementations only


# overloads removed - keep concrete implementations only


# overloads removed - keep concrete implementations only


# overloads removed - keep concrete implementations only


# æ—¥æœ¬èªè¡¨ç¤ºã®ãŸã‚ã®ãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆWindowså‘ã‘å„ªå…ˆï¼‰
def _set_japanese_font_fallback() -> None:
    """æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ã®ã‚‚ã®ã ã‘ã«è¨­å®šã—ã¦è­¦å‘Šã‚’å›é¿ã™ã‚‹ã€‚"""
    try:
        preferred = [
            "Noto Sans JP",
            "IPAexGothic",
            "Yu Gothic",
            "Meiryo",
            "MS Gothic",
            "Yu Gothic UI",
            "MS PGothic",
            "Hiragino Sans",
            "Hiragino Kaku Gothic ProN",
            "TakaoGothic",
            "DejaVu Sans",
        ]
        available = {f.name for f in _font_manager.fontManager.ttflist}
        chosen = [name for name in preferred if name in available]
        if not chosen:
            chosen = ["DejaVu Sans"]
        mpl.rcParams["font.family"] = chosen
        mpl.rcParams["axes.unicode_minus"] = False
    except Exception:
        pass


_set_japanese_font_fallback()

# matplotlib.font_manager ã®å†—é•·ãª INFO ã‚’æŠ‘åˆ¶
logging.getLogger("matplotlib.font_manager").setLevel(logging.ERROR)


# ------------------------------
# Small utilities
# ------------------------------
def clean_date_column(df: pd.DataFrame, col_name: str = "Date") -> pd.DataFrame:
    if col_name in df.columns:
        df = df.copy()
        df[col_name] = pd.to_datetime(df[col_name], errors="coerce")
        df = df.dropna(subset=[col_name])
    return df


def default_log_callback(
    processed: int, total: int, start_time: float, prefix: str = "ğŸ“Š çŠ¶æ³"
) -> str:
    elapsed = time.time() - start_time
    remain = (elapsed / processed) * (total - processed) if processed else 0
    return (
        f"{prefix}: {processed}/{total} ä»¶ | çµŒé: {int(elapsed // 60)}åˆ†{int(elapsed % 60)}ç§’"
        f" / æ®‹ã‚Šç›®å®‰: ç´„{int(remain // 60)}åˆ†{int(remain % 60)}ç§’"
    )


# ------------------------------
# Data fetch
# ------------------------------
def _mtime_or_zero(path: str) -> float:
    try:
        return os.path.getmtime(path)
    except Exception:
        return 0.0


@st.cache_data(show_spinner=False)
def _load_symbol_cached(
    symbol: str, *, base_path: str, base_mtime: float, raw_path: str, raw_mtime: float
) -> tuple[str, pd.DataFrame | None]:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°æ™‚åˆ»ã‚’ã‚­ãƒ¼ã«å«ã‚ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã€ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°ã§è‡ªå‹•ç„¡åŠ¹åŒ–ã€‚
    æˆ»ã‚Šå€¤ã¯ (symbol, DataFrame|None)
    """
    try:
        df = load_base_cache(
            symbol, rebuild_if_missing=True, prefer_precomputed_indicators=True
        )
        if df is not None and not df.empty:
            return symbol, df
    except Exception:
        pass
    if os.path.exists(raw_path):
        return symbol, get_cached_data(symbol)
    return symbol, None


def load_symbol(
    symbol: str, cache_dir: str = "data_cache"
) -> tuple[str, pd.DataFrame | None]:
    base_path = str(base_cache_path(symbol))
    raw_path = os.path.join(cache_dir, f"{safe_filename(symbol)}.csv")
    return _load_symbol_cached(
        symbol,
        base_path=base_path,
        base_mtime=_mtime_or_zero(base_path),
        raw_path=raw_path,
        raw_mtime=_mtime_or_zero(raw_path),
    )


def fetch_data(
    symbols, max_workers: int = 8, ui_manager=None, enable_debug_logs: bool = True
) -> dict[str, pd.DataFrame]:
    data_dict: dict[str, pd.DataFrame] = {}
    total = len(symbols)
    # UIManagerã®ãƒ•ã‚§ãƒ¼ã‚ºï¼ˆfetchï¼‰ãŒã‚ã‚Œã°ãã“ã¸å‡ºåŠ›
    phase = ui_manager.phase("fetch") if ui_manager else None
    if phase:
        progress_bar = phase.progress_bar
        log_area = phase.log_area
        # ãƒ•ã‚§ãƒ¼ã‚ºé…ä¸‹ã«ã€Œno dataã€ç”¨ã®åˆ¥ã‚¹ãƒ­ãƒƒãƒˆã‚’ç¢ºä¿ï¼ˆæœªä½œæˆãªã‚‰ç”Ÿæˆï¼‰
        no_data_area = phase.no_data_area if hasattr(phase, "no_data_area") else None
        if no_data_area is None:
            try:
                no_data_area = phase.container.empty()
            except Exception:
                no_data_area = st.empty()
            try:
                phase.no_data_area = no_data_area
            except Exception:
                pass
        try:
            phase.info(tr("fetch: start | {total} symbols", total=total))
        except Exception:
            pass
    else:
        # UIManager ãªã—ã®å ´åˆ: å®Ÿè¡Œæ™‚ã«å‹•çš„ç”Ÿæˆ
        fetch_info_placeholder = st.empty()
        fetch_progress_placeholder = st.empty()
        fetch_log_placeholder = st.empty()
        no_data_placeholder = st.empty()

        fetch_info_placeholder.info(tr("fetch: start | {total} symbols", total=total))
        progress_bar = fetch_progress_placeholder.progress(0)
        log_area = fetch_log_placeholder
        # ãƒ•ã‚§ãƒ¼ã‚ºæœªä½¿ç”¨æ™‚ã¯ç›´ä¸‹ã«no-dataç”¨ã‚¹ãƒ­ãƒƒãƒˆã‚’ç”¨æ„
        no_data_area = no_data_placeholder
    buffer, skipped, start_time = [], [], time.time()

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(load_symbol, sym): sym for sym in symbols}
        for i, future in enumerate(as_completed(futures), 1):
            sym, df = future.result()
            if df is not None and not df.empty:
                data_dict[sym] = df
                buffer.append(sym)
            else:
                skipped.append(sym)

            if i % 50 == 0 or i == total:
                log_with_progress(
                    i,
                    total,
                    start_time,
                    prefix="ãƒ‡ãƒ¼ã‚¿å–å¾—",
                    batch=50,
                    log_func=(
                        (lambda msg: (log_area.text(msg), None)[1])
                        if hasattr(log_area, "text")
                        else None
                    ),
                    progress_func=(
                        (lambda val: (progress_bar.progress(val), None)[1])
                        if hasattr(progress_bar, "progress")
                        else None
                    ),
                    extra_msg=(f"éŠ˜æŸ„: {', '.join(buffer)}" if buffer else None),
                    silent=not enable_debug_logs,
                )
                buffer.clear()

    try:
        progress_bar.empty()
    except Exception:
        pass
    if skipped:
        try:
            # use i18n message for skipped count, append symbols list
            # tr ã¯ kwargs ã‚’å—ã‘ã¦ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¸ˆã¿æ–‡å­—åˆ—ã‚’è¿”ã™ã®ã§ .format ã¯ä¸è¦
            msg = tr("âš ï¸ no data: {n} symbols", n=len(skipped))
            # é•·å¤§ãªãƒªã‚¹ãƒˆã‚’é¿ã‘ã‚‹ãŸã‚ã€ä»£è¡¨ã®ã¿ï¼ˆå…ˆé ­10ä»¶ï¼‰ã‚’è¡¨ç¤º
            _sample = list(skipped)[:10]
            msg = msg + "\n" + ", ".join(_sample)
            _rest = len(skipped) - len(_sample)
            if _rest > 0:
                msg += f"\n... (+{_rest} more)"
            # å–å¾—ãƒ­ã‚°ã‚’ä¸Šæ›¸ãã›ãšã€ä¸‹ã®è¡Œã«è¡¨ç¤º
            no_data_area.text(msg)
        except Exception:
            pass
    return data_dict


# ------------------------------
# Prepare + candidates
# ------------------------------


def prepare_backtest_data(
    strategy,
    symbols,
    system_name: str = "SystemX",
    spy_df: pd.DataFrame | None = None,
    ui_manager=None,
    use_process_pool: bool = False,
    enable_debug_logs: bool = True,
    fast_mode: bool = False,
    **kwargs,
):
    # 1) fetch
    if use_process_pool:
        data_dict = None
    else:
        data_dict = fetch_data(
            symbols, ui_manager=ui_manager, enable_debug_logs=enable_debug_logs
        )
        if not data_dict:
            st.error(tr("no valid data"))
            return None, None, None

    # 2) indicators (delegated to strategy)
    # indicators ãƒ•ã‚§ãƒ¼ã‚º
    ind_phase = ui_manager.phase("indicators") if ui_manager else None
    if ind_phase:
        try:
            ind_phase.info(tr("indicators: computing..."))
        except Exception:
            pass
        ind_progress = ind_phase.progress_bar
        ind_log = ind_phase.log_area
    else:
        # UIManager ãªã—ã®å ´åˆ: å®Ÿè¡Œæ™‚ã«å‹•çš„ç”Ÿæˆ
        ind_info_placeholder = st.empty()
        ind_progress_placeholder = st.empty()
        ind_log_placeholder = st.empty()

        ind_info_placeholder.info(tr("indicators: computing..."))
        ind_progress = ind_progress_placeholder.progress(0)
        ind_log = ind_log_placeholder

    if fast_mode and data_dict:
        # å±¥æ­´ã‚’æœ«å°¾120è¡Œã«çŸ­ç¸®ï¼ˆå­˜åœ¨ã™ã‚‹æ—¥ä»˜æ•°ã«å¿œã˜ã¦ï¼‰
        try:
            trimmed = {}
            for _sym, _df in data_dict.items():
                if hasattr(_df, "tail"):
                    trimmed[_sym] = _df.tail(120)
                else:
                    trimmed[_sym] = _df
            data_dict = trimmed
        except Exception:
            pass

    call_input = data_dict if not use_process_pool else symbols

    # é€²æ—ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ï¼ˆæ–‡å­—åˆ—ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚«ã‚¦ãƒ³ãƒˆã«å¤‰æ›ï¼‰
    progress_counter = {"count": 0}
    total_symbols = len(symbols)

    def _update_progress(msg_or_tuple):
        """é€²æ—æ›´æ–°ï¼šæ–‡å­—åˆ—ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¾ãŸã¯(done, total)ã‚¿ãƒ—ãƒ«ã®ä¸¡æ–¹ã«å¯¾å¿œ"""
        try:
            if isinstance(msg_or_tuple, tuple) and len(msg_or_tuple) == 2:
                # (done, total) å½¢å¼
                done, total = msg_or_tuple
                if total > 0:
                    ind_progress.progress(done / total)
            else:
                # æ–‡å­—åˆ—ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼ï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼ï¼‰
                progress_counter["count"] += 1
                if total_symbols > 0:
                    ind_progress.progress(progress_counter["count"] / total_symbols)
        except Exception:
            pass

    call_kwargs = dict(
        progress_callback=_update_progress,
        log_callback=lambda msg: ind_log.text(str(msg)),
        skip_callback=lambda msg: ind_log.text(str(msg)),
        fast_mode=fast_mode,
        **kwargs,
    )
    if use_process_pool:
        # cast to Any to satisfy narrow type checkers used in the repo
        call_kwargs["use_process_pool"] = cast(Any, True)

    try:
        prepared_dict = strategy.prepare_data(call_input, **call_kwargs)
    except TypeError:
        # å¾Œæ–¹äº’æ›: æœªå¯¾å¿œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¦å†è©¦è¡Œ
        for _k in ["skip_callback", "use_process_pool", "fast_mode"]:
            call_kwargs.pop(_k, None)
        prepared_dict = strategy.prepare_data(call_input, **call_kwargs)
    try:
        ind_progress.empty()
    except Exception:
        pass

    # 3) candidates
    # candidates ãƒ•ã‚§ãƒ¼ã‚º
    cand_phase = ui_manager.phase("candidates") if ui_manager else None
    if cand_phase:
        try:
            cand_phase.info(tr("candidates: extracting..."))
        except Exception:
            pass
        cand_progress = cand_phase.progress_bar
    else:
        # UIManager ãªã—ã®å ´åˆ: å®Ÿè¡Œæ™‚ã«å‹•çš„ç”Ÿæˆ
        cand_info_placeholder = st.empty()
        cand_progress_placeholder = st.empty()

        cand_info_placeholder.info(tr("candidates: extracting..."))
        cand_progress = cand_progress_placeholder.progress(0)

    merged_df = None
    # ã™ã¹ã¦ã®ã‚·ã‚¹ãƒ†ãƒ ã¯ strategy.generate_candidates ã‚’ä½¿ã†çµ±ä¸€ãƒ‘ã‚¹
    try:
        if system_name == "System4" and spy_df is not None:
            candidates_by_date = strategy.generate_candidates(
                prepared_dict,
                market_df=spy_df,
                **kwargs,
            )
        else:
            candidates_by_date = strategy.generate_candidates(
                prepared_dict,
                **kwargs,
            )
    except (TypeError, ValueError) as e:
        st.error(f"å€™è£œæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return prepared_dict, None, None

    # æ­£å¸¸ç³»ã§ã‚‚ (dict, df) ã‚’è¿”ã™å®Ÿè£…ãŒã‚ã‚‹ãŸã‚å¾Œæ®µã§æ­£è¦åŒ–
    if isinstance(candidates_by_date, tuple) and len(candidates_by_date) == 2:
        candidates_by_date, merged_df = candidates_by_date
    try:
        cand_progress.empty()
    except Exception:
        pass

    if not candidates_by_date:
        st.warning(tr("{system_name}: no candidates"))
        return prepared_dict, None, None

    return prepared_dict, candidates_by_date, merged_df


# ------------------------------
# Backtest execution (common wrapper)
# ------------------------------
def run_backtest_with_logging(
    strategy,
    prepared_dict,
    candidates_by_date,
    capital,
    system_name: str = "SystemX",
    ui_manager=None,
):
    bt_phase = ui_manager.phase("backtest") if ui_manager else None
    if bt_phase:
        try:
            bt_phase.info(tr("backtest: running..."))
        except Exception:
            pass
        progress = bt_phase.progress_bar
        log_area = bt_phase.log_area
        # è³‡é‡‘æ¨ç§»ã¯æœ€æ–°è¡Œã®ã¿ã€ã‚¨ã‚¯ã‚¹ãƒ‘ãƒ³ãƒ€ãƒ¼ã¯ä½¿ã‚ãšå˜ä¸€ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ã«å‡ºåŠ›
        fund_log_area = (
            bt_phase.fund_log_area
            if hasattr(bt_phase, "fund_log_area")
            else bt_phase.container.empty()
        )
        try:
            bt_phase.fund_log_area = fund_log_area
        except Exception:
            pass
    else:
        # UIManager ãªã—ã®å ´åˆ: å®Ÿè¡Œæ™‚ã«å‹•çš„ç”Ÿæˆ
        bt_info_placeholder = st.empty()
        bt_progress_placeholder = st.empty()
        bt_log_placeholder = st.empty()
        bt_fund_log_placeholder = st.empty()

        bt_info_placeholder.info(tr("backtest: running..."))
        progress = bt_progress_placeholder.progress(0)
        log_area = bt_log_placeholder
        fund_log_area = bt_fund_log_placeholder

    debug_logs: list[str] = []

    def handle_log(msg):
        if isinstance(msg, str) and msg.startswith("ğŸ’°"):
            # attempt to localize capital/active segments while preserving date
            import re

            s = str(msg)
            # Capital: 3812.31 USD -> è³‡é‡‘: 3812.31 USD
            s = re.sub(r"Capital:\s*([0-9\.,]+)\s*USD", r"è³‡é‡‘: \1 USD", s)
            # Active: 0 -> ä¿æœ‰ãƒã‚¸ã‚·ãƒ§ãƒ³: 0
            s = re.sub(r"Active:\s*([0-9]+)", r"ä¿æœ‰ãƒã‚¸ã‚·ãƒ§ãƒ³: \1", s)
            debug_logs.append(s)
            # æœ€æ–°è¡Œã®ã¿ã‚’è¡¨ç¤ºï¼ˆå·®ã—æ›¿ãˆï¼‰
            fund_log_area.text(s)
        else:
            log_area.text(str(msg))

    results_df = strategy.run_backtest(
        prepared_dict,
        candidates_by_date,
        capital,
        on_progress=lambda i, total, start: log_with_progress(
            i,
            total,
            start,
            prefix="bt",
            log_func=(
                (lambda msg: (log_area.text(msg), None)[1])
                if hasattr(log_area, "text")
                else None
            ),
            progress_func=(
                (lambda val: (progress.progress(val), None)[1])
                if hasattr(progress, "progress")
                else None
            ),
            unit="days",
        ),
        on_log=lambda msg: handle_log(msg),
    )

    try:
        progress.empty()
    except Exception:
        pass

    # ãƒ­ã‚°ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¸ä¿æŒï¼ˆãƒªãƒ©ãƒ³ã—ã¦ã‚‚è¡¨ç¤ºã§ãã‚‹ã‚ˆã†ã«ï¼‰
    st.session_state[f"{system_name}_debug_logs"] = list(debug_logs)

    # ã‚·ã‚¹ãƒ†ãƒ å›ºæœ‰ã®ãƒ‡ãƒãƒƒã‚°ãƒ•ãƒ©ã‚°ã‚’ãƒã‚§ãƒƒã‚¯
    debug_key = f"{system_name}_show_debug_logs"
    show_debug = st.session_state.get(debug_key, True)

    if show_debug and debug_logs:
        # ãƒ­ã‚°ã¯ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆãƒ»ãƒ•ã‚§ãƒ¼ã‚ºã®ã‚³ãƒ³ãƒ†ãƒŠå†…ã«é…ç½®ï¼ˆã‚·ã‚¹ãƒ†ãƒ ã”ã¨ã«ã¾ã¨ã¾ã‚‹ã‚ˆã†ã«ï¼‰
        parent = bt_phase.container if bt_phase else st.container()
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æœ›: å–å¼•ãƒ­ã‚°ã¯ã‚¨ã‚¯ã‚¹ãƒ‘ãƒ³ãƒ€ãƒ¼ã§æŠ˜ã‚ŠãŸãŸã¿è¡¨ç¤º
        title = f"ğŸ’° {tr('trade logs')}"
        with parent.expander(title, expanded=False):
            # text_area ã®æ–¹ãŒè¡Œé–“ãƒ»ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã§è¦–èªæ€§ãŒé«˜ã„
            st.text_area(
                "Logs",
                "\n".join(debug_logs),
                height=300,
            )

    # çµæœã‚‚ä½µã›ã¦ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜ï¼ˆUIå±¤ã§ã‚‚ä¿å­˜ã™ã‚‹ãŒäºŒé‡ã§ã‚‚å®‰å…¨ï¼‰
    st.session_state[f"{system_name}_results_df"] = results_df
    return results_df


# ------------------------------
# App entry for a single system tab
# ------------------------------


def run_backtest_app(
    strategy,
    system_name: str = "SystemX",
    limit_symbols: int = 10,
    system_title: str | None = None,
    spy_df: pd.DataFrame | None = None,
    ui_manager=None,
    **kwargs,
):
    st.title(system_title or f"{system_name} backtest")
    # ãƒ¢ãƒ¼ãƒ‰è¡¨ç¤ºç”¨ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ç›´ä¸‹ï¼‰
    mode_caption_placeholder = st.empty()

    # --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«è¨­å®šUIã‚’çµ±åˆ ---
    with st.sidebar:
        st.subheader(tr("backtest settings"))

        debug_key = f"{system_name}_show_debug_logs"
        if debug_key not in st.session_state:
            st.session_state[debug_key] = True
        st.checkbox(tr("show debug logs"), key=debug_key)

        # è‡ªå‹•ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹åˆ©ç”¨ãƒ•ãƒ©ã‚° (æ™®é€šæ ªãƒ¦ãƒ‹ãƒãƒ¼ã‚¹) å¾©å…ƒ
        auto_key = f"{system_name}_auto"
        if auto_key not in st.session_state:
            st.session_state[auto_key] = True
        use_auto = st.checkbox(
            tr("auto symbols (common stocks)"),
            value=st.session_state[auto_key],
            key=auto_key,
        )
        # æ™®é€šæ ª å…¨éŠ˜æŸ„ã‚’ä¸€æ‹¬åˆ©ç”¨ã™ã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆåˆ¶é™æ•°å…¥åŠ›ã‚’ç„¡è¦–ã™ã‚‹ï¼‰
        all_common_key = f"{system_name}_use_all_common"
        st.checkbox(
            tr("use full common stocks universe"), value=True, key=all_common_key
        )
        # Fast Preview / æŒ™å‹•ç¢ºèªãƒ¢ãƒ¼ãƒ‰ (MVP)
        fast_key = f"{system_name}_fast_mode"
        if fast_key not in st.session_state:
            st.session_state[fast_key] = False
        from common.i18n import get_language as _get_lang  # é…å»¶ import

        st.checkbox(
            (
                "Fast Preview Mode"
                if _get_lang() == "en"
                else tr("fast preview mode (mvp)")
            ),
            key=fast_key,
            help=(
                "Skip heavy indicators & shorten lookback (~120d) for quicker approximate preview"
                if _get_lang() == "en"
                else "é‡ã„æŒ‡æ¨™ã‚’çœç•¥ã—å±¥æ­´ã‚’ç´„120æ—¥ã«çŸ­ç¸®ã—ã¦é«˜é€Ÿã«è¿‘ä¼¼çµæœã‚’è¡¨ç¤º"
            ),
        )

        _init_cap = int(st.session_state.get(f"{system_name}_capital_saved", 100000))
        capital = st.number_input(
            tr("capital (USD)"),
            min_value=1000,
            value=_init_cap,
            step=100,
            key=f"{system_name}_capital",
        )

        # å¸¸ã«é€šå¸¸æ ªï¼ˆæ™®é€šæ ªã®ã¿ã€~6200éŠ˜æŸ„ï¼‰ã‚’ä½¿ç”¨ï¼ˆ11800å…¨éŠ˜æŸ„ã‚ªãƒ—ã‚·ãƒ§ãƒ³å»ƒæ­¢ï¼‰
        # ç°¡æ˜“ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆãƒ—ãƒ­ã‚»ã‚¹å†…ï¼‰: æ¯å›ãƒ­ãƒ¼ãƒ‰ã‚’é¿ã‘ã‚‹
        global _COMMON_STOCKS_CACHE  # type: ignore
        if "_COMMON_STOCKS_CACHE" not in globals():  # åˆå›å®šç¾©
            _COMMON_STOCKS_CACHE = None  # type: ignore

        if _COMMON_STOCKS_CACHE is None:
            try:
                from scripts.tickers_loader import get_common_stocks_only

                _COMMON_STOCKS_CACHE = list(get_common_stocks_only())  # type: ignore
            except ImportError:
                _COMMON_STOCKS_CACHE = list(get_all_tickers())  # type: ignore
            except Exception:
                _COMMON_STOCKS_CACHE = list(get_all_tickers())  # type: ignore

        all_tickers = _COMMON_STOCKS_CACHE  # type: ignore

        max_allowed = len(all_tickers)
        default_value = min(10, max_allowed)
        # å…¨éŠ˜æŸ„é¸æŠæ™‚ã®éŠ˜æŸ„ç·æ•°è¡¨ç¤º & ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è­¦å‘Š + éå»ãƒ­ã‚°ã‹ã‚‰æ¨å®šæ™‚é–“
        if st.session_state.get(all_common_key, False):
            st.caption(tr("using all common stocks: {n} symbols", n=max_allowed))
            if max_allowed > 3000:
                st.warning(
                    tr(
                        "large universe may slow processing (>{th} symbols)",
                        th=3000,
                    )
                )
            # éå»ã®å®Ÿè¡Œæ™‚é–“ãƒ­ã‚°(JSONL)ã‹ã‚‰æ¨å®šï¼ˆå˜ç´”ã«åŒUniverseã‚µã‚¤ã‚ºè¿‘å‚ã‚’å¯¾è±¡ï¼‰
            try:
                import json
                import math
                from pathlib import Path
                import statistics

                settings = get_settings(create_dirs=True)
                perf_dir = Path(settings.LOGS_DIR) / "perf_estimates"
                perf_dir.mkdir(parents=True, exist_ok=True)
                history_path = perf_dir / f"{system_name}_universe_times.jsonl"

                # æ—¢å­˜ãƒ­ã‚°èª­ã¿è¾¼ã¿
                sizes: list[int] = []
                durations: list[float] = []
                if history_path.exists():
                    with history_path.open("r", encoding="utf-8") as fh:
                        for line in fh:
                            try:
                                rec = json.loads(line)
                                sizes.append(int(rec.get("size", 0)))
                                durations.append(float(rec.get("seconds", 0.0)))
                            except Exception:
                                continue
                # è¿‘å‚ (Â±5%) ã®ã‚µã‚¤ã‚ºã‚µãƒ³ãƒ—ãƒ«ã‚’æŠ½å‡º
                est_seconds: float | None = None
                p25 = p75 = None
                if sizes:
                    target_min = max_allowed * 0.95
                    target_max = max_allowed * 1.05
                    filt = [
                        d
                        for s, d in zip(sizes, durations)
                        if target_min <= s <= target_max and d > 0
                    ]
                    if len(filt) >= 3:
                        filt.sort()
                        p25 = filt[max(0, math.floor(len(filt) * 0.25) - 1)]
                        p75 = filt[min(len(filt) - 1, math.floor(len(filt) * 0.75))]
                        est_seconds = statistics.median(filt)
                if est_seconds is not None and p25 is not None and p75 is not None:
                    st.caption(
                        tr(
                            "estimated processing time: median {m:.1f}s (p25={p25:.1f}s / p75={p75:.1f}s)",
                            m=est_seconds,
                            p25=p25,
                            p75=p75,
                        )
                    )
            except Exception:
                pass

        if system_name != "System7" and not st.session_state.get(all_common_key, False):
            limit_symbols = st.number_input(
                tr("symbol limit"),
                min_value=1,
                max_value=max_allowed,
                value=default_value,
                step=1,
                key=f"{system_name}_limit",
            )
            # å…¨éŠ˜æŸ„ä½¿ç”¨ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯å»ƒæ­¢ï¼ˆä¸Šé™æŒ‡å®šã®ã¿ï¼‰

        symbols_input = None
        if not use_auto:
            symbols_input = st.text_input(
                tr("symbols (comma separated)"),
                "AAPL,MSFT,TSLA,NVDA,META",
                key=f"{system_name}_symbols_main",
            )

        # é€šçŸ¥ãƒˆã‚°ãƒ«ï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ã¸ç§»å‹•ï¼‰
        if system_name in (
            "System1",
            "System2",
            "System3",
            "System4",
            "System5",
            "System6",
            "System7",
        ):
            _notify_key = f"{system_name}_notify_backtest"
            if _notify_key not in st.session_state:
                st.session_state[_notify_key] = True
            _label = tr("ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœã‚’é€šçŸ¥ã™ã‚‹ï¼ˆWebhookï¼‰")
            try:
                _use_toggle = hasattr(st, "toggle")
            except Exception:
                _use_toggle = False
            if _use_toggle:
                st.toggle(_label, key=_notify_key)
            else:
                st.checkbox(_label, key=_notify_key)
            try:
                import os as _os

                if not (
                    _os.getenv("DISCORD_WEBHOOK_URL") or _os.getenv("SLACK_BOT_TOKEN")
                ):
                    st.caption(tr("Webhook/Bot è¨­å®šãŒæœªè¨­å®šã§ã™ï¼ˆ.env ã‚’ç¢ºèªï¼‰"))
            except Exception:
                pass

    # --- ãƒ¡ã‚¤ãƒ³é ˜åŸŸ: å‰å›å®Ÿè¡Œçµæœã®è¡¨ç¤º/ã‚¯ãƒªã‚¢ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿æŒï¼‰ ---
    key_results = f"{system_name}_results_df"
    key_prepared = f"{system_name}_prepared_dict"
    key_cands = f"{system_name}_candidates_by_date"
    key_capital = f"{system_name}_capital"
    key_capital_saved = f"{system_name}_capital_saved"
    key_merged = f"{system_name}_merged_df"
    key_debug = f"{system_name}_debug_logs"

    has_prev = any(
        k in st.session_state
        for k in [key_results, key_cands, f"{system_name}_capital_saved"]
    )
    if has_prev:
        with st.expander("å‰å›ã®çµæœï¼ˆãƒªãƒ©ãƒ³ã§ã‚‚ä¿æŒï¼‰", expanded=False):
            prev_res = st.session_state.get(key_results)
            prev_cap = st.session_state.get(
                key_capital_saved, st.session_state.get(key_capital, 0)
            )
            if prev_res is not None and getattr(prev_res, "empty", False) is False:
                show_results(prev_res, prev_cap, system_name, key_context="prev")
            dbg = st.session_state.get(key_debug)
            if dbg:
                st.markdown("**ä¿å­˜æ¸ˆã¿ å–å¼•ãƒ­ã‚°**")
                st.text("\n".join(map(str, dbg)))
            if st.button(tr("ä¿å­˜æ¸ˆã¿çµæœã‚’ã‚¯ãƒªã‚¢"), key=f"{system_name}_clear_saved"):
                for k in [
                    key_results,
                    key_prepared,
                    key_cands,
                    key_capital_saved,
                    key_capital,
                    key_merged,
                    key_debug,
                ]:
                    if k in st.session_state:
                        del st.session_state[k]
                rerun = getattr(st, "experimental_rerun", None)
                if callable(rerun):
                    try:
                        rerun()
                    except Exception:
                        pass

    if st.button(tr("clear streamlit cache"), key=f"{system_name}_clear_cache"):
        st.cache_data.clear()
        st.success(tr("cache cleared"))

    # ã‚·ãƒ³ãƒœãƒ«é¸æŠå‡¦ç†ï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ã§å®šç¾©æ¸ˆã¿ã®å¤‰æ•°ã‚’ä½¿ç”¨ï¼‰
    if system_name == "System7":
        symbols = ["SPY"]
    elif use_auto:
        if st.session_state.get(f"{system_name}_use_all_common", False):
            symbols = all_tickers  # å…¨æ™®é€šæ ª
        else:
            symbols = all_tickers[:limit_symbols]
    else:
        if not symbols_input:
            st.error(tr("please input symbols"))
            return None, None, None, None, None
        symbols = [s.strip().upper() for s in symbols_input.split(",")]

    run_clicked = st.button(tr("run"), key=f"{system_name}_run")
    fast_mode_flag = bool(st.session_state.get(f"{system_name}_fast_mode", False))
    # ã‚¿ã‚¤ãƒˆãƒ«ä¸‹ã«ç¾åœ¨ãƒ¢ãƒ¼ãƒ‰ã‚’è¡¨ç¤ºï¼ˆå³æ™‚æ›´æ–°ï¼‰
    try:
        from common.i18n import get_language as _get_lang2

        if fast_mode_flag:
            if _get_lang2() == "en":
                mode_caption_placeholder.caption("Mode: Fast Preview (approximate)")
            else:
                mode_caption_placeholder.caption(
                    tr("fast preview mode enabled (approximate results)")
                )
        else:
            if _get_lang2() == "en":
                mode_caption_placeholder.caption("Mode: Normal")
            else:
                mode_caption_placeholder.caption(tr("mode: normal"))
    except Exception:
        pass

    # å®Ÿè¡Œãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯å¾Œã«å‹•çš„ã«ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ç”Ÿæˆ
    if run_clicked:
        # ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œé ˜åŸŸã‚’å‹•çš„ç”Ÿæˆï¼ˆå®Ÿè¡Œå‰ã¯è¡¨ç¤ºã•ã‚Œãªã„ï¼‰
        result_area = st.container()
        with result_area:
            # --- timing start (stdout) ---
            from datetime import datetime as _dt

            _t_start = _dt.now()
            try:
                import json
                import os
                import pathlib

                use_color = os.getenv("BACKTEST_COLOR", "1") != "0"
                use_json = os.getenv("BACKTEST_JSON", "1") != "0"
                mode_txt = "FAST" if fast_mode_flag else "NORMAL"
                # run_id: 1 å›ã® UI å®Ÿè¡Œå˜ä½
                import uuid

                run_id = os.getenv("BACKTEST_RUN_ID") or uuid.uuid4().hex[:12]
                os.environ["BACKTEST_RUN_ID"] = run_id  # Downstream (Slack) ä½¿ç”¨å‘ã‘
                # ANSI colors
                C = {
                    "reset": "\u001b[0m",
                    "cyan": "\u001b[36m",
                    "green": "\u001b[32m",
                    "yellow": "\u001b[33m",
                    "magenta": "\u001b[35m",
                    "bold": "\u001b[1m",
                }
                if not use_color:
                    for k in list(C.keys()):
                        C[k] = ""
                start_lines = [
                    "",
                    f"{C['cyan']}=============================={C['reset']}",
                    f"{C['bold']}ğŸš€ ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆé–‹å§‹{C['reset']}: {system_name}",
                    f"ğŸ•’ é–‹å§‹æ™‚åˆ»: {_t_start:%Y-%m-%d %H:%M:%S}",
                    f"ğŸ“Š å¯¾è±¡ã‚·ãƒ³ãƒœãƒ«æ•°: {len(symbols)}",
                    f"ğŸ†” Run ID: {run_id}",
                    f"ãƒ¢ãƒ¼ãƒ‰: {mode_txt}",
                    f"{C['cyan']}=============================={C['reset']}",
                ]
                print("\n".join(start_lines), flush=True)
                try:
                    # UI å´ã«ã‚‚ Run ID ã‚’è¡¨ç¤ºã™ã‚‹å°ãƒ‘ãƒãƒ«
                    st.info(f"Run ID: {run_id}")
                except Exception:
                    pass
                if use_json:
                    try:
                        settings_local = get_settings(create_dirs=True)
                        log_dir = (
                            pathlib.Path(settings_local.LOGS_DIR) / "backtest_events"
                        )
                    except Exception:
                        log_dir = pathlib.Path("logs") / "backtest_events"
                    log_dir.mkdir(parents=True, exist_ok=True)
                    rec = {
                        "event": "start",
                        "system": system_name,
                        "timestamp": _t_start.isoformat(),
                        "symbols": len(symbols),
                        "mode": mode_txt,
                        "run_id": run_id,
                        "status": "running",
                        "exception": None,
                    }
                    with (log_dir / f"{system_name.lower()}_events.jsonl").open(
                        "a", encoding="utf-8"
                    ) as jf:
                        jf.write(json.dumps(rec, ensure_ascii=False) + "\n")
            except Exception:
                pass
            prepared_dict, candidates_by_date, merged_df = prepare_backtest_data(
                strategy,
                symbols,
                system_name=system_name,
                spy_df=spy_df,
                ui_manager=ui_manager,
                enable_debug_logs=st.session_state.get(debug_key, True),
                fast_mode=fast_mode_flag,
                **kwargs,
            )
            if candidates_by_date is None:
                return None, None, None, None, None

            results_df = run_backtest_with_logging(
                strategy,
                prepared_dict,
                candidates_by_date,
                capital,
                system_name,
                ui_manager=ui_manager,
            )
            # fast_mode åˆ—ä»˜ä¸ï¼ˆå¾Œå·¥ç¨‹ã§åˆ©ç”¨è€…ãŒè­˜åˆ¥ã§ãã‚‹ã‚ˆã†ã«ï¼‰
            try:
                if results_df is not None and not results_df.empty:
                    results_df = results_df.copy()
                    results_df["mode"] = "fast" if fast_mode_flag else "normal"
            except Exception:
                pass
            show_results(results_df, capital, system_name, key_context="curr")

            # fast_mode å®Ÿè¡Œå¾Œã«é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã§ã®å†å®Ÿè¡Œãƒœã‚¿ãƒ³ã‚’æä¾›
            try:
                if fast_mode_flag:
                    if st.button(
                        tr("re-run in normal mode"), key=f"{system_name}_rerun_normal"
                    ):
                        # ãƒ•ãƒ©ã‚°ã‚’ False ã«ã—ã¦å†å®Ÿè¡Œã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼æ“ä½œç°¡ç•¥åŒ–ã€‚
                        st.session_state[f"{system_name}_fast_mode"] = False
                        rerun = getattr(st, "experimental_rerun", None)
                        if callable(rerun):
                            try:
                                rerun()
                            except Exception:
                                pass
            except Exception:
                pass

            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¸ä¿å­˜ï¼ˆãƒªãƒ©ãƒ³å¯¾ç­–ï¼‰
            st.session_state[key_results] = results_df
            st.session_state[key_prepared] = prepared_dict
            st.session_state[key_cands] = candidates_by_date
            st.session_state[key_capital_saved] = capital
            if merged_df is not None:
                st.session_state[key_merged] = merged_df

            if system_name == "System1":
                return results_df, merged_df, prepared_dict, capital, candidates_by_date
            else:
                # --- timing end (stdout) ---
                try:
                    import json
                    import os
                    import pathlib

                    use_color = os.getenv("BACKTEST_COLOR", "1") != "0"
                    use_json = os.getenv("BACKTEST_JSON", "1") != "0"
                    run_id = os.getenv("BACKTEST_RUN_ID") or "unknown"
                    _t_end = _dt.now()
                    _elapsed = (_t_end - _t_start).total_seconds()
                    h = int(_elapsed // 3600)
                    m = int((_elapsed % 3600) // 60)
                    s = _elapsed % 60
                    trades_cnt = 0
                    try:
                        if (
                            results_df is not None
                            and hasattr(results_df, "empty")
                            and not results_df.empty
                        ):
                            trades_cnt = len(results_df)
                    except Exception:
                        trades_cnt = 0
                    C = {
                        "reset": "\u001b[0m",
                        "cyan": "\u001b[36m",
                        "green": "\u001b[32m",
                        "yellow": "\u001b[33m",
                        "magenta": "\u001b[35m",
                        "bold": "\u001b[1m",
                    }
                    if not use_color:
                        for k in list(C.keys()):
                            C[k] = ""
                    mode_txt = "FAST" if fast_mode_flag else "NORMAL"
                    # è¦–èªæ€§å‘ä¸Š: 0åŸ‹ã‚ H:MM:SS ã¨åˆè¨ˆç§’ã€ä¸¡æ–¹ã‚’è¡¨ç¤º
                    total_fmt = f"{h}:{m:02d}:{int(s):02d}"  # ä¾‹ 0:03:52
                    # é•·æ™‚é–“è‰²ä»˜ã‘(> 15åˆ†)ã§æ³¨æ„ã‚’å¼•ã
                    warn = use_color and _elapsed > 900
                    elapsed_line = (
                        f"â±ï¸ æ‰€è¦æ™‚é–“: {total_fmt} (åˆè¨ˆ {_elapsed:.2f} ç§’)"
                        if not warn
                        else f"â±ï¸ æ‰€è¦æ™‚é–“: {C['yellow']}{total_fmt}{C['reset']} (åˆè¨ˆ {_elapsed:.2f} ç§’)"
                    )
                    end_lines = [
                        "",
                        f"{C['green']}=============================={C['reset']}",
                        f"{C['bold']}âœ… ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Œäº†{C['reset']}: {system_name}",
                        f"ğŸ•’ çµ‚äº†æ™‚åˆ»: {_t_end:%Y-%m-%d %H:%M:%S}",
                        elapsed_line,
                        f"ğŸ“Š å–å¼•ä»¶æ•°: {trades_cnt} / ã‚·ãƒ³ãƒœãƒ«æ•°: {len(symbols)}",
                        f"ğŸ†” Run ID: {run_id}",
                        f"ãƒ¢ãƒ¼ãƒ‰: {mode_txt}",
                        f"{C['green']}=============================={C['reset']}",
                        "",
                    ]
                    print("\n".join(end_lines), flush=True)
                    if use_json:
                        try:
                            settings_local = get_settings(create_dirs=True)
                            log_dir = (
                                pathlib.Path(settings_local.LOGS_DIR)
                                / "backtest_events"
                            )
                        except Exception:
                            log_dir = pathlib.Path("logs") / "backtest_events"
                        log_dir.mkdir(parents=True, exist_ok=True)
                        rec = {
                            "event": "end",
                            "system": system_name,
                            "timestamp": _t_end.isoformat(),
                            "elapsed_sec": round(_elapsed, 3),
                            "elapsed_hms": {"h": h, "m": m, "s": round(s, 3)},
                            "symbols": len(symbols),
                            "trades": trades_cnt,
                            "mode": mode_txt,
                            "run_id": run_id,
                            "status": "success",
                            "exception": None,
                        }
                        with (log_dir / f"{system_name.lower()}_events.jsonl").open(
                            "a", encoding="utf-8"
                        ) as jf:
                            jf.write(json.dumps(rec, ensure_ascii=False) + "\n")
                except Exception:
                    pass
                return results_df, None, prepared_dict, capital, candidates_by_date
    return None, None, None, None, None


# ------------------------------
# Heatmap silent export helper
# ------------------------------
def _export_holding_heatmap_silent(matrix, system_name: str) -> None:
    """Export holding matrix to CSV (and optionally PNG in future) without rendering.

    The UI no longer renders the heatmap directly to avoid vertical clutter and
    performance overhead. We still persist a CSV so users can download or use it
    for offline analysis. PNG export can be added later if required.
    """
    from pathlib import Path

    try:
        from .logging_utils import get_logger  # type: ignore
    except Exception:  # pragma: no cover - fallback when logger util unavailable
        get_logger = None  # type: ignore

    try:
        settings = get_settings(create_dirs=True)
        base_dir = Path(getattr(settings.cache, "base_dir", "data_cache"))  # type: ignore[arg-type]
        out_dir = base_dir / "exports" / "holding_heatmaps"
        out_dir.mkdir(parents=True, exist_ok=True)
        out_path = out_dir / f"holding_status_{system_name}.csv"
        matrix.to_csv(out_path)
        if get_logger:
            try:
                logger = get_logger()
                logger.info(
                    "heatmap csv exported",
                    extra={"system": system_name, "path": str(out_path)},
                )
            except Exception:
                pass
    except Exception:
        # éè‡´å‘½: å¤±æ•—ã—ã¦ã‚‚ UI ã§ warning æ¸ˆã¿
        pass


# ------------------------------
# Rendering helpers
# ------------------------------
def summarize_results(results_df: pd.DataFrame, capital: float):
    # é˜²å¾¡: å¿…é ˆåˆ—ä¸è¶³æ™‚ã¯ç©ºã‚µãƒãƒªè¿”å´ï¼ˆå‘¼ã³å‡ºã—å´ã§infoè¡¨ç¤ºæ¸ˆï¼‰
    required_cols = {"entry_date", "exit_date"}
    if (
        results_df is None
        or results_df.empty
        or not required_cols.issubset(set(results_df.columns))
    ):
        return {"trades": 0, "total_return": 0.0, "win_rate": 0.0, "max_dd": 0.0}, pd.DataFrame(columns=["cumulative_pnl"])  # type: ignore
    df = results_df.copy()

    # æ—¥ä»˜ã‚’ç¢ºå®Ÿã«æ—¥æ™‚å‹ã«
    df["entry_date"] = pd.to_datetime(df["entry_date"])
    df["exit_date"] = pd.to_datetime(df["exit_date"])

    # åŸºæœ¬é›†è¨ˆ
    df = df.sort_values("exit_date").reset_index(drop=True)
    trades = len(df)
    total_return = float(df["pnl"].sum()) if "pnl" in df.columns else 0.0
    wins = int((df["pnl"] > 0).sum()) if "pnl" in df.columns else 0
    win_rate = (wins / trades * 100.0) if trades > 0 else 0.0

    # exit_date åŸºæº–ã§ç´¯ç©PnL ã‚’ä½œæˆï¼ˆã‚°ãƒ©ãƒ•ç”¨ï¼‰
    df2 = df.copy()
    if "pnl" in df2.columns:
        df2["cumulative_pnl"] = df2["pnl"].cumsum()
    else:
        df2["cumulative_pnl"] = 0.0

    # æ—¥æ¬¡ä¿æœ‰çŠ¶æ…‹ãƒ»ã‚¨ã‚¯ã‚¤ãƒ†ã‚£ç­‰ï¼ˆç°¡æ˜“ç‰ˆï¼‰
    # cumulative_pnl ã‹ã‚‰ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³ã‚’è¨ˆç®—
    try:
        cum = df2["cumulative_pnl"].astype(float)
        dd_series = cum - cum.cummax()
        max_dd = float(abs(dd_series.min()))
    except Exception:
        max_dd = 0.0

    summary = {
        "trades": int(trades),
        "total_return": float(total_return),
        "win_rate": float(win_rate),
        "max_dd": float(max_dd),
    }

    # å‘¼ã³å‡ºã—å…ƒã¯ (summary, df2) ã‚’æœŸå¾…ã—ã¦ã„ã‚‹ãŸã‚è¿”ã™
    return summary, df2


def show_results(
    results_df: pd.DataFrame,
    capital: float,
    system_name: str = "SystemX",
    *,
    key_context: str = "main",
):
    # è¿½åŠ é˜²å¾¡: results_dfãŒæœŸå¾…åˆ—ã‚’æ¬ ãå ´åˆã¯æ—©æœŸreturnã§UIå´©å£Šé˜²æ­¢
    minimal_cols = {"entry_date", "exit_date"}
    if (
        results_df is None
        or results_df.empty
        or not minimal_cols.issubset(set(results_df.columns))
    ):
        st.info(i18n.tr("no trades"))
        return

    st.success(i18n.tr("backtest finished"))
    st.subheader(i18n.tr("results"))
    st.dataframe(results_df)

    # ãƒ‡ãƒãƒƒã‚°: åˆ—åãƒ»å‹ãƒ»å…ˆé ­æ•°è¡Œã‚’è¡¨ç¤ºï¼ˆmax drawdown ãŒ0ã®åŸå› ç¢ºèªç”¨ã€ç¢ºèªå¾Œã¯å‰Šé™¤ã—ã¦ãã ã•ã„ï¼‰
    # removed debug: results_df.head()
    # removed debug: results_df.columns
    # removed debug: results_df.dtypes

    # ä¸€éƒ¨ç’°å¢ƒã§ summarize_results ãŒ 2 å¼•æ•°ç‰ˆã§ãƒ©ãƒƒãƒ—ã•ã‚Œã¦ã„ã‚‹ã“ã¨ãŒã‚ã‚‹ãŸã‚ã€
    # system_name å›ºæœ‰ã®ãƒ‡ãƒãƒƒã‚°ãƒ•ãƒ©ã‚°ã‚’ä¸€æ™‚çš„ã«å…±é€šã‚­ãƒ¼ã¸ã‚³ãƒ”ãƒ¼ã—ã¦ã‹ã‚‰å‘¼ã³å‡ºã™
    try:
        prev_flag = st.session_state.get("show_debug_logs", None)
        # system_name å›ºæœ‰ãƒ•ãƒ©ã‚°ãŒã‚ã‚Œã°å„ªå…ˆã—ã¦ä¸€æ™‚çš„ã«ã‚»ãƒƒãƒˆ
        sys_flag = st.session_state.get(f"{system_name}_show_debug_logs", None)
        if sys_flag is not None:
            st.session_state["show_debug_logs"] = sys_flag
    except Exception:
        prev_flag = None

    # äº’æ›å‘¼ã³å‡ºã—ï¼ˆ2 å¼•æ•°ç‰ˆã§ã‚‚å‹•ä½œã™ã‚‹ã‚ˆã†ã«ã™ã‚‹ï¼‰
    summary, df2 = summarize_results(results_df, capital)
    # æœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³ã‚’å†è¨ˆç®—ã—ã¦ summary ã«åæ˜ ï¼ˆè¡¨ç¤ºã®ã‚¼ãƒ­ã‚’é˜²æ­¢ï¼‰
    try:
        cum = df2["cumulative_pnl"].astype(float)
        dd_series = cum - cum.cummax()
        max_dd_val = float(abs(dd_series.min()))
        try:
            summary["max_dd"] = max_dd_val
        except Exception:
            pass
    except Exception:
        pass

    # ãƒ•ãƒ©ã‚°ã‚’å…ƒã«æˆ»ã™
    try:
        if prev_flag is None:
            if "show_debug_logs" in st.session_state:
                del st.session_state["show_debug_logs"]
        else:
            st.session_state["show_debug_logs"] = prev_flag
    except Exception:
        pass

    # Series/Dict ã„ãšã‚Œã«ã‚‚å®‰å…¨ã«å¯¾å¿œã—ã€æ¬ æã‚­ãƒ¼ã¯ 0 æ‰±ã„
    if isinstance(summary, pd.Series):
        summary = summary.to_dict()
    col1, col2, col3, col4 = st.columns(4)
    col1.metric("å–å¼•æ•°", int(summary.get("trades", 0)))
    col2.metric("åˆè¨ˆæç›Š", f"{float(summary.get('total_return', 0.0)):.2f}")
    col3.metric("å‹ç‡ (%)", f"{float(summary.get('win_rate', 0.0)):.2f}")
    col4.metric("æœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³", f"{float(summary.get('max_dd', 0.0)):.2f}")

    st.subheader(i18n.tr("cumulative pnl"))
    # æ—¥æœ¬èªã‚’è»¸ãƒ©ãƒ™ãƒ«ã«ä½¿ã†éš›ã®ãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®šï¼ˆç’°å¢ƒã«ã‚ã‚‹ãƒ•ã‚©ãƒ³ãƒˆã‚’å„ªå…ˆã—ã¦é¸æŠï¼‰
    try:
        _set_japanese_font_fallback()
    except Exception:
        pass
    plt.figure(figsize=(10, 4))
    plt.plot(df2["exit_date"], df2["cumulative_pnl"], label="CumPnL")
    # Drawdownï¼ˆç´¯ç©æç›Šã®ãƒ”ãƒ¼ã‚¯ã‹ã‚‰ã®ä¸‹è½ï¼‰ã‚’èµ¤ç·šã§é‡ã­ã‚‹
    try:
        cum = df2["cumulative_pnl"].astype(float)
        dd = cum - cum.cummax()
        plt.plot(df2["exit_date"], dd, color="red", linewidth=1.2, label="Drawdown")
    except Exception:
        pass
    plt.xlabel(i18n.tr("date"))
    plt.ylabel(i18n.tr("pnl"))
    plt.legend()
    # streamlit.pyplot ã«ã¯ Figure ã‚’æ¸¡ã™ï¼ˆplt ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãã®ã‚‚ã®ã‚’æ¸¡ã•ãªã„ï¼‰
    try:
        fig = plt.gcf()
        st.pyplot(fig)
    except Exception:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç›´æ¥æ¸¡ã™ã®ã¯é¿ã‘ã‚‹ãŒã‚¨ãƒ©ãƒ¼æ™‚ã¯ç„¡è¦–
        pass

    st.subheader(i18n.tr("yearly summary"))
    if len(df2) > 0:
        yearly = (
            df2.groupby(df2["exit_date"].dt.to_period("Y"))["pnl"].sum().reset_index()
        )
        yearly["æç›Š"] = yearly["pnl"].round(2)
        yearly["ãƒªã‚¿ãƒ¼ãƒ³(%)"] = yearly["pnl"] / (capital if capital else 1) * 100
        yearly = yearly.rename(columns={"exit_date": "å¹´"})
        st.dataframe(
            yearly[["å¹´", "æç›Š", "ãƒªã‚¿ãƒ¼ãƒ³(%)"]].style.format(
                {"æç›Š": "{:.2f}", "ãƒªã‚¿ãƒ¼ãƒ³(%)": "{:.1f}%"}
            )
        )
    else:
        st.info("ãƒˆãƒ¬ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

    st.subheader(i18n.tr("monthly summary"))
    if len(df2) > 0:
        monthly = (
            df2.groupby(df2["exit_date"].dt.to_period("M"))["pnl"].sum().reset_index()
        )
        monthly["æç›Š"] = monthly["pnl"].round(2)
        monthly["ãƒªã‚¿ãƒ¼ãƒ³(%)"] = monthly["pnl"] / (capital if capital else 1) * 100
        monthly = monthly.rename(columns={"exit_date": "æœˆ"})
        st.dataframe(
            monthly[["æœˆ", "æç›Š", "ãƒªã‚¿ãƒ¼ãƒ³(%)"]].style.format(
                {"æç›Š": "{:.2f}", "ãƒªã‚¿ãƒ¼ãƒ³(%)": "{:.1f}%"}
            )
        )
    else:
        st.info("ãƒˆãƒ¬ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

    # (ä»•æ§˜å¤‰æ›´) ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã¯ UI ã¸æç”»ã›ãšã€å¾Œç¶šã®ã‚µã‚¤ãƒ¬ãƒ³ãƒˆã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã®ã¿å®Ÿè¡Œ
    # åˆ©ç”¨è€…ã¸ã¯è»½é‡ãªå®Œäº†æƒ…å ±ã®ã¿æä¾›ã€‚
    try:
        export_start = time.time()
        holding_matrix = generate_holding_matrix(df2)
        _export_holding_heatmap_silent(holding_matrix, system_name)
        st.caption(i18n.tr("heatmap generated"))
        st.session_state[f"{system_name}_heatmap_export_secs"] = round(
            time.time() - export_start, 3
        )
    except Exception as _e:
        # å¤±æ•—ã—ã¦ã‚‚è‡´å‘½çš„ã§ãªã„ãŸã‚ warning ã®ã¿ã«ç•™ã‚ã‚‹
        st.warning(f"heatmap export skipped: {_e}")


def show_signal_trade_summary(
    source_df, trades_df, system_name: str, display_name: str | None = None
):
    if system_name == "System1" and isinstance(source_df, pd.DataFrame):
        signal_counts = source_df["symbol"].value_counts().reset_index()
        signal_counts.columns = ["symbol", "Signal_Count"]
    else:
        signal_counts = {
            sym: int(df.get("setup", pd.Series(dtype=int)).sum())
            for sym, df in (source_df or {}).items()
        }
        signal_counts = pd.DataFrame(
            signal_counts.items(), columns=["symbol", "Signal_Count"]
        )

    if trades_df is not None and not trades_df.empty:
        trade_counts = (
            trades_df.groupby("symbol").size().reset_index(name="Trade_Count")
        )
    else:
        trade_counts = pd.DataFrame(columns=["symbol", "Trade_Count"])

    summary_df = pd.merge(signal_counts, trade_counts, on="symbol", how="outer").fillna(
        0
    )
    summary_df["Signal_Count"] = summary_df["Signal_Count"].astype(int)
    summary_df["Trade_Count"] = summary_df["Trade_Count"].astype(int)

    label = f"{display_name or system_name} signalç™ºç”Ÿä»¶æ•° / ãƒˆãƒ¬ãƒ¼ãƒ‰ç™ºç”Ÿä»¶æ•°"
    with st.expander(label, expanded=False):
        st.dataframe(summary_df.sort_values("Signal_Count", ascending=False))
    return summary_df


def extract_zero_reason_from_logs(logs: list[str] | None) -> str | None:
    """ãƒ­ã‚°é…åˆ—ã‹ã‚‰å€™è£œ0ä»¶ã®ç†ç”±ã‚’æŠ½å‡ºã—ã¦è¿”ã™ï¼ˆè¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã° Noneï¼‰ã€‚

    å¯¾å¿œãƒ‘ã‚¿ãƒ¼ãƒ³:
    - "å€™è£œ0ä»¶ç†ç”±: ..."
    - "ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸æˆç«‹: ..."
    """
    if not logs:
        return None
    import re as _re

    for ln in reversed(list(logs)):
        if not ln:
            continue
        m = _re.search(r"å€™è£œ0ä»¶ç†ç”±[:ï¼š]\s*(.+)$", ln)
        if m:
            return m.group(1).strip()
        m2 = _re.search(r"ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸æˆç«‹[:ï¼š]\s*(.+)$", ln)
        if m2:
            return m2.group(1).strip()
    return None


def display_roc200_ranking(
    ranking_df: pd.DataFrame,
    years: int = 5,
    top_n: int = 10,
    title: str = "System1 ROC200ãƒ©ãƒ³ã‚­ãƒ³ã‚°",
):
    if ranking_df is None or ranking_df.empty:
        st.info(tr("ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"))
        return
    df = ranking_df.copy()
    df["Date"] = (
        pd.to_datetime(df["Date"]) if "Date" in df.columns else pd.to_datetime(df.index)
    )
    df = df.reset_index(drop=True)
    if "ROC200_Rank" not in df.columns and "ROC200" in df.columns:
        df["ROC200_Rank"] = df.groupby("Date")["ROC200"].rank(
            ascending=False, method="first"
        )
    if years:
        start_date = pd.Timestamp.now() - pd.DateOffset(years=years)
        df = df[df["Date"] >= start_date]
    if top_n:
        df = df.groupby("Date").head(top_n)
    df = df.sort_values(["Date", "ROC200_Rank"], ascending=[True, True])
    with st.expander(f"{title} (ç›´è¿‘{years}å¹´ / ä¸Šä½{top_n}ä»¶)", expanded=False):
        st.dataframe(
            df.reset_index(drop=True)[["Date", "ROC200_Rank", "symbol"]],
            hide_index=False,
        )


# ------------------------------
# Save helpers
# ------------------------------


def save_signal_and_trade_logs(signal_counts_df, results, system_name, capital):
    # download_button keyè¡çªå›é¿: key_contextã‚’å«ã‚ä¸€æ„æ€§å¼·åŒ–
    key_suffix = f"{system_name}_{int(capital)}"
    today_str = pd.Timestamp.today().strftime("%Y-%m-%d_%H%M")
    save_dir = "results_csv"
    os.makedirs(save_dir, exist_ok=True)
    sig_dir = os.path.join(save_dir, "signals")
    os.makedirs(sig_dir, exist_ok=True)
    trade_dir = os.path.join(save_dir, "trades")
    os.makedirs(trade_dir, exist_ok=True)

    if signal_counts_df is not None and not signal_counts_df.empty:
        signal_path = os.path.join(
            sig_dir, f"{system_name}_signals_{today_str}_{int(capital)}.csv"
        )
        try:
            settings = get_settings(create_dirs=True)
            round_dec = getattr(settings.cache, "round_decimals", None)
        except Exception:
            round_dec = None
        try:
            out_df = round_dataframe(signal_counts_df, round_dec)
        except Exception:
            out_df = signal_counts_df
        out_df.to_csv(signal_path, index=False)
        st.write(tr("ã‚·ã‚°ãƒŠãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {signal_path}", signal_path=signal_path))
        # å³æ™‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        st.download_button(
            label=f"{system_name} ã‚·ã‚°ãƒŠãƒ«CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=out_df.to_csv(index=False).encode("utf-8"),
            file_name=f"{system_name}_signals_{today_str}_{int(capital)}.csv",
            mime="text/csv",
            key=f"{key_suffix}_download_signals_csv",
        )

    trades_df = pd.DataFrame(results) if isinstance(results, list) else results
    if trades_df is not None and not trades_df.empty:
        # ç”»é¢å†…ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆå‘¼ã³å‡ºã—å…ƒã§ã‚¨ã‚¯ã‚¹ãƒ‘ãƒ³ãƒ€ãƒ¼å†…ã«ã„ã‚‹æƒ³å®šï¼‰
        try:
            preferred_cols = [
                "entry_date",
                "exit_date",
                "symbol",
                "action",
                "price",
                "qty",
                "pnl",
            ]
            cols = [c for c in preferred_cols if c in trades_df.columns]
            st.dataframe(trades_df[cols] if cols else trades_df)
        except Exception:
            pass
        trade_path = os.path.join(
            trade_dir, f"{system_name}_trades_{today_str}_{int(capital)}.csv"
        )
        try:
            try:
                settings = get_settings(create_dirs=True)
                round_dec = getattr(settings.cache, "round_decimals", None)
            except Exception:
                round_dec = None
            try:
                out_trades = round_dataframe(trades_df, round_dec)
            except Exception:
                out_trades = trades_df
            out_trades.to_csv(trade_path, index=False)
            st.write(tr("ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {trade_path}", trade_path=trade_path))
            # å³æ™‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            st.download_button(
                label=f"{system_name} ãƒˆãƒ¬ãƒ¼ãƒ‰CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=out_trades.to_csv(index=False).encode("utf-8"),
                file_name=f"{system_name}_trades_{today_str}_{int(capital)}.csv",
                mime="text/csv",
                key=f"{key_suffix}_download_trades_csv",
            )
        except Exception:
            # æ›¸ãè¾¼ã¿/ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—ã—ã¦ã‚‚å‡¦ç†ã‚’ç¶™ç¶š
            pass


###############################
# Deprecated/Removed Features #
###############################
# save_prepared_data_cache: å®Œå…¨æ’¤å» (2025-10). UI ã‹ã‚‰ã‚‚å‘¼å‡ºã—å‰Šé™¤æ¸ˆã¿ã€‚


def display_cache_health_dashboard() -> None:
    """
    rolling cacheã®å¥å…¨æ€§ã‚’è¡¨ç¤ºã™ã‚‹ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã€‚
    """
    st.subheader("ğŸ©º Cache Health Dashboard")

    from common.cache_manager import CacheManager
    from config.settings import get_settings

    try:
        settings = get_settings(create_dirs=True)
        cache_manager = CacheManager(settings)

        # å¥å…¨æ€§ã‚µãƒãƒªãƒ¼å–å¾—
        health_summary = cache_manager.get_rolling_health_summary()

        # ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«çŠ¶æ³
        st.write("### ğŸ“‹ ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«çŠ¶æ³")
        col1, col2 = st.columns(2)

        with col1:
            meta_status = "âœ… å­˜åœ¨" if health_summary["meta_exists"] else "âŒ ä¸åœ¨"
            st.metric("ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«", meta_status)

        with col2:
            st.metric("Rolling Files", f"{health_summary['rolling_files_count']}å€‹")

        # SPY ã‚¢ãƒ³ã‚«ãƒ¼çŠ¶æ³
        st.write("### âš“ SPY ã‚¢ãƒ³ã‚«ãƒ¼çŠ¶æ³")
        anchor_status = health_summary["anchor_symbol_status"]
        col1, col2, col3 = st.columns(3)

        with col1:
            anchor_exists = "âœ… å­˜åœ¨" if anchor_status["exists"] else "âŒ ä¸åœ¨"
            st.metric("SPYå­˜åœ¨", anchor_exists)

        with col2:
            st.metric("ãƒ‡ãƒ¼ã‚¿è¡Œæ•°", f"{anchor_status['rows']:,}")

        with col3:
            target_status = "âœ… ååˆ†" if anchor_status["meets_target"] else "âš ï¸ ä¸è¶³"
            st.metric("ç›®æ¨™é”æˆ", target_status)

        # ç›®æ¨™ãƒ‡ãƒ¼ã‚¿é•·
        st.write("### ğŸ¯ ç›®æ¨™è¨­å®š")
        st.metric("ç›®æ¨™ãƒ‡ãƒ¼ã‚¿é•·", f"{health_summary['target_length']}æ—¥")

        # ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹è©³ç´°
        if health_summary["meta_exists"] and health_summary["meta_content"]:
            st.write("### ğŸ“„ ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«è©³ç´°")
            st.json(health_summary["meta_content"])

        # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒœã‚¿ãƒ³
        st.write("### âš¡ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³")
        col1, col2 = st.columns(2)

        with col1:
            if st.button("ğŸ”„ Rolling Cache åˆ†æå®Ÿè¡Œ"):
                with st.spinner("åˆ†æä¸­..."):
                    analysis_result = cache_manager.analyze_rolling_gaps()
                    _display_cache_analysis_results(analysis_result)

        with col2:
            if st.button("ğŸ§¹ Rolling Cache Pruneå®Ÿè¡Œ"):
                with st.spinner("Pruneå®Ÿè¡Œä¸­..."):
                    prune_result = cache_manager.prune_rolling_if_needed()
                    st.success(
                        f"âœ… Pruneå®Œäº†: {prune_result['pruned_files']}ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†"
                    )

    except Exception as e:
        st.error(f"Cache health dashboard ã‚¨ãƒ©ãƒ¼: {str(e)}")
        logging.error(f"Cache health dashboard error: {e}")


def _display_cache_analysis_results(analysis_result: dict) -> None:
    """Cacheåˆ†æçµæœã‚’è¡¨ç¤ºã™ã‚‹å†…éƒ¨ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã€‚"""
    st.write("### ğŸ“Š Rolling Cache åˆ†æçµæœ")

    # ã‚µãƒãƒªãƒ¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("ç·ã‚·ãƒ³ãƒœãƒ«æ•°", analysis_result["total_symbols"])

    with col2:
        st.metric("æ•´å‚™æ¸ˆã¿", analysis_result["available_in_rolling"])

    with col3:
        st.metric("æœªæ•´å‚™", analysis_result["missing_from_rolling"])

    with col4:
        coverage = analysis_result["coverage_percentage"]
        st.metric("ã‚«ãƒãƒ¬ãƒƒã‚¸", f"{coverage:.1f}%")

    # ã‚«ãƒãƒ¬ãƒƒã‚¸çŠ¶æ³ã®è¦–è¦šåŒ–
    if coverage >= 90:
        st.success("ğŸ‰ Rolling cacheæ•´å‚™çŠ¶æ³ã¯è‰¯å¥½ã§ã™")
    elif coverage >= 70:
        st.warning("âš ï¸ Rolling cacheæ•´å‚™ç‡ã®æ”¹å–„ã‚’æ¨å¥¨ã—ã¾ã™")
    else:
        st.error("ğŸš¨ Rolling cacheæ•´å‚™ãŒä¸ååˆ†ã§ã™")

    # æœªæ•´å‚™ã‚·ãƒ³ãƒœãƒ«ã®è¡¨ç¤º
    missing_symbols = analysis_result.get("missing_symbols", [])
    if missing_symbols:
        st.write("### âŒ æœªæ•´å‚™ã‚·ãƒ³ãƒœãƒ«")

        if len(missing_symbols) <= 20:
            # 20å€‹ä»¥ä¸‹ãªã‚‰å…¨ã¦è¡¨ç¤º
            st.write(", ".join(missing_symbols))
        else:
            # å¤šã„å ´åˆã¯å±•é–‹å¯èƒ½ã«ã™ã‚‹
            with st.expander(f"æœªæ•´å‚™ã‚·ãƒ³ãƒœãƒ«ä¸€è¦§ ({len(missing_symbols)}å€‹)"):
                # 10å€‹ãšã¤åŒºåˆ‡ã£ã¦è¡¨ç¤º
                for i in range(0, len(missing_symbols), 10):
                    chunk = missing_symbols[i : i + 10]
                    st.write(", ".join(chunk))


def display_system_cache_coverage() -> None:
    """
    ã‚·ã‚¹ãƒ†ãƒ åˆ¥ã®cache coverageçŠ¶æ³ã‚’è¡¨ç¤ºã™ã‚‹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã€‚
    """
    st.subheader("ğŸ¯ Systemåˆ¥ Cache Coverage")

    from common.cache_manager import CacheManager
    from common.system_groups import analyze_system_symbols_coverage
    from config.settings import get_settings
    from scripts.tickers_loader import get_all_tickers

    try:
        settings = get_settings(create_dirs=True)
        cache_manager = CacheManager(settings)

        # å…¨ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‹ã‚‰å„ã‚·ã‚¹ãƒ†ãƒ ç”¨ã®ã‚·ãƒ³ãƒœãƒ«ãƒãƒƒãƒ—ã‚’æ§‹ç¯‰
        # å®Ÿè£…ã§ã¯å„ã‚·ã‚¹ãƒ†ãƒ ã«å›ºæœ‰ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯ãŒå¿…è¦ã ãŒã€
        # ã“ã“ã§ã¯ç°¡ç•¥åŒ–ã—ã¦å…¨ã‚·ãƒ³ãƒœãƒ«ã‚’ä½¿ç”¨
        all_tickers = get_all_tickers()
        system_symbols_map = {}
        for system_num in range(1, 8):
            # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€å„ã‚·ã‚¹ãƒ†ãƒ å›ºæœ‰ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¡ä»¶ã‚’é©ç”¨
            system_symbols_map[f"system{system_num}"] = all_tickers[:500]  # ç°¡ç•¥åŒ–

        # å…¨ä½“ã®cacheåˆ†æ
        overall_analysis = cache_manager.analyze_rolling_gaps()

        # ã‚·ã‚¹ãƒ†ãƒ åˆ¥ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æ
        coverage_analysis = analyze_system_symbols_coverage(
            system_symbols_map, overall_analysis
        )

        # ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        st.write("### ğŸ“ˆ ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥ã‚µãƒãƒªãƒ¼")
        group_data = coverage_analysis["by_group"]

        for group_name in ["long", "short"]:
            if group_name in group_data:
                group_stats = group_data[group_name]
                col1, col2, col3, col4 = st.columns(4)

                group_display = (
                    "Long Systems" if group_name == "long" else "Short Systems"
                )
                st.write(f"**{group_display}**")

                with col1:
                    st.metric("ç·ã‚·ãƒ³ãƒœãƒ«", group_stats["total_symbols"])

                with col2:
                    st.metric("æ•´å‚™æ¸ˆã¿", group_stats["available"])

                with col3:
                    st.metric("æœªæ•´å‚™", group_stats["missing"])

                with col4:
                    coverage = group_stats["coverage_percentage"]
                    status = group_stats["status"]
                    st.metric("çŠ¶æ³", f"{status} {coverage:.1f}%")

        # ã‚·ã‚¹ãƒ†ãƒ åˆ¥è©³ç´°
        st.write("### ğŸ” ã‚·ã‚¹ãƒ†ãƒ åˆ¥è©³ç´°")
        system_data = coverage_analysis["by_system"]

        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å½¢å¼ã§è¡¨ç¤º
        df_data = []
        for system_name in [f"system{i}" for i in range(1, 8)]:
            if system_name in system_data:
                stats = system_data[system_name]
                df_data.append(
                    {
                        "ã‚·ã‚¹ãƒ†ãƒ ": system_name.upper(),
                        "ç·ã‚·ãƒ³ãƒœãƒ«": stats["total_symbols"],
                        "æ•´å‚™æ¸ˆã¿": stats["available"],
                        "æœªæ•´å‚™": stats["missing"],
                        "ã‚«ãƒãƒ¬ãƒƒã‚¸": f"{stats['coverage_percentage']:.1f}%",
                        "çŠ¶æ³": stats["status"],
                    }
                )

        if df_data:
            df = pd.DataFrame(df_data)
            st.dataframe(df, width="stretch")

        # è©³ç´°åˆ†æç”¨ã®å±•é–‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        with st.expander("ğŸ“‹ è©³ç´°åˆ†æçµæœ"):
            st.json(coverage_analysis)

    except Exception as e:
        st.error(f"System cache coverage ã‚¨ãƒ©ãƒ¼: {str(e)}")
        logging.error(f"System cache coverage error: {e}")


def display_cache_recommendations(analysis_result: dict) -> None:
    """
    Cacheåˆ†æçµæœã«åŸºã¥ãæ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¡¨ç¤ºã™ã‚‹ã€‚
    """
    from common.system_groups import format_cache_coverage_report

    # åˆ†æçµæœã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    report = format_cache_coverage_report(
        analysis_result["total_symbols"],
        analysis_result["available_in_rolling"],
        analysis_result["missing_from_rolling"],
        analysis_result["coverage_percentage"],
        analysis_result.get("missing_symbols", []),
    )

    # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º
    st.write(f"### {report['status']} ç·åˆè©•ä¾¡")
    st.write(f"**å„ªå…ˆåº¦**: {report['priority']}")

    # ã‚µãƒãƒªãƒ¼æƒ…å ±
    summary = report["summary"]
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("ç·æ•°", summary["total"])
    with col2:
        st.metric("æ•´å‚™æ¸ˆã¿", summary["available"])
    with col3:
        st.metric("æœªæ•´å‚™", summary["missing"])
    with col4:
        st.metric("ã‚«ãƒãƒ¬ãƒƒã‚¸", summary["coverage"])

    # æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
    st.write("### ğŸ’¡ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³")
    for recommendation in report["recommendations"]:
        st.write(f"- {recommendation}")

    # æœªæ•´å‚™ã‚·ãƒ³ãƒœãƒ«ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
    if report["missing_symbols_preview"]:
        st.write("### ğŸ” æœªæ•´å‚™ã‚·ãƒ³ãƒœãƒ«ï¼ˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰")
        for symbol in report["missing_symbols_preview"]:
            st.write(f"- {symbol}")
