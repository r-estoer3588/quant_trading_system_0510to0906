"""
å…±é€šUIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆï¼ˆUTF-8ãƒ»æ—¥æœ¬èªå¯¾å¿œï¼‰ã€‚
æ—¢å­˜ã®å…¬é–‹APIï¼ˆé–¢æ•°åãƒ»æˆ»ã‚Šå€¤ï¼‰ã¯ç¶­æŒã—ã¤ã¤ã€å„ãƒ•ã‚§ãƒ¼ã‚ºï¼ˆãƒ‡ãƒ¼ã‚¿å–å¾—/ã‚¤ãƒ³ã‚¸è¨ˆç®—/å€™è£œæŠ½å‡º/ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆï¼‰ã§
UIManagerï¼ˆä»»æ„ï¼‰ã«é€²æ—ã¨ãƒ­ã‚°ã‚’å‡ºåŠ›ã§ãã‚‹ã‚ˆã†ã«ã—ã¦ã„ã‚‹ã€‚
"""

from __future__ import annotations

import logging
import os
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Any, cast

import matplotlib as mpl
import matplotlib.pyplot as plt
import pandas as pd
import streamlit as st
from matplotlib import font_manager as _font_manager

from common.cache_format import round_dataframe
from common.utils import get_cached_data, safe_filename
from config.settings import get_settings

try:
    # è¨­å®šã‹ã‚‰UIãƒ•ãƒ©ã‚°ã‚’å‚ç…§ï¼ˆå¤±æ•—æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‹•ä½œã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    from config.settings import get_settings

    _APP_SETTINGS = get_settings(create_dirs=True)
except Exception:
    _APP_SETTINGS = None
import common.i18n as i18n
from common.cache_manager import base_cache_path, load_base_cache
from common.holding_tracker import display_holding_heatmap, generate_holding_matrix
from core.system1 import generate_roc200_ranking_system1
from scripts.tickers_loader import get_all_tickers

# äº’æ›ç”¨ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼ˆæ—¢å­˜ã‚³ãƒ¼ãƒ‰ã® tr(...) å‘¼ã³å‡ºã—ã‚’ç¶­æŒï¼‰
tr = i18n.tr


# ------------------------------
# Type overloads for static checkers
# ------------------------------
# overloads removed - keep concrete implementations only


# overloads removed - keep concrete implementations only


# overloads removed - keep concrete implementations only


# overloads removed - keep concrete implementations only


# overloads removed - keep concrete implementations only


# overloads removed - keep concrete implementations only


# æ—¥æœ¬èªè¡¨ç¤ºã®ãŸã‚ã®ãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆWindowså‘ã‘å„ªå…ˆï¼‰
def _set_japanese_font_fallback() -> None:
    """æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ã®ã‚‚ã®ã ã‘ã«è¨­å®šã—ã¦è­¦å‘Šã‚’å›é¿ã™ã‚‹ã€‚"""
    try:
        preferred = [
            "Noto Sans JP",
            "IPAexGothic",
            "Yu Gothic",
            "Meiryo",
            "MS Gothic",
            "Yu Gothic UI",
            "MS PGothic",
            "Hiragino Sans",
            "Hiragino Kaku Gothic ProN",
            "TakaoGothic",
            "DejaVu Sans",
        ]
        available = {f.name for f in _font_manager.fontManager.ttflist}
        chosen = [name for name in preferred if name in available]
        if not chosen:
            chosen = ["DejaVu Sans"]
        mpl.rcParams["font.family"] = chosen
        mpl.rcParams["axes.unicode_minus"] = False
    except Exception:
        pass


_set_japanese_font_fallback()

# matplotlib.font_manager ã®å†—é•·ãª INFO ã‚’æŠ‘åˆ¶
logging.getLogger("matplotlib.font_manager").setLevel(logging.ERROR)


# ------------------------------
# Small utilities
# ------------------------------
def clean_date_column(df: pd.DataFrame, col_name: str = "Date") -> pd.DataFrame:
    if col_name in df.columns:
        df = df.copy()
        df[col_name] = pd.to_datetime(df[col_name], errors="coerce")
        df = df.dropna(subset=[col_name])
    return df


def log_with_progress(
    i: int,
    total: int,
    start_time: float,
    *,
    prefix: str = "é€²æ—",
    batch: int = 50,
    log_area=None,
    progress_bar=None,
    extra_msg: str | None = None,
    unit: str = "ä»¶",
) -> None:
    if i % batch == 0 or i == total:
        elapsed = time.time() - start_time
        remain = (elapsed / i) * (total - i) if i > 0 else 0
        msg = (
            f"{prefix}: {i}/{total} {unit} | çµŒé: {int(elapsed // 60)}åˆ†{int(elapsed % 60)}ç§’"
            f" / æ®‹ã‚Šç›®å®‰: ç´„{int(remain // 60)}åˆ†{int(remain % 60)}ç§’"
        )
        if extra_msg:
            msg += f"\n{extra_msg}"
        try:
            if log_area is not None:
                log_area.text(msg)
        except Exception:
            pass
        try:
            if progress_bar is not None:
                progress_bar.progress(0 if total == 0 else i / total)
        except Exception:
            pass


def default_log_callback(
    processed: int, total: int, start_time: float, prefix: str = "ğŸ“Š çŠ¶æ³"
) -> str:
    elapsed = time.time() - start_time
    remain = (elapsed / processed) * (total - processed) if processed else 0
    return (
        f"{prefix}: {processed}/{total} ä»¶ | çµŒé: {int(elapsed // 60)}åˆ†{int(elapsed % 60)}ç§’"
        f" / æ®‹ã‚Šç›®å®‰: ç´„{int(remain // 60)}åˆ†{int(remain % 60)}ç§’"
    )


# ------------------------------
# Data fetch
# ------------------------------
def _mtime_or_zero(path: str) -> float:
    try:
        return os.path.getmtime(path)
    except Exception:
        return 0.0


@st.cache_data(show_spinner=False)
def _load_symbol_cached(
    symbol: str, *, base_path: str, base_mtime: float, raw_path: str, raw_mtime: float
) -> tuple[str, pd.DataFrame | None]:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°æ™‚åˆ»ã‚’ã‚­ãƒ¼ã«å«ã‚ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã€ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°ã§è‡ªå‹•ç„¡åŠ¹åŒ–ã€‚
    æˆ»ã‚Šå€¤ã¯ (symbol, DataFrame|None)
    """
    try:
        df = load_base_cache(
            symbol, rebuild_if_missing=True, prefer_precomputed_indicators=True
        )
        if df is not None and not df.empty:
            return symbol, df
    except Exception:
        pass
    if os.path.exists(raw_path):
        return symbol, get_cached_data(symbol)
    return symbol, None


def load_symbol(
    symbol: str, cache_dir: str = "data_cache"
) -> tuple[str, pd.DataFrame | None]:
    base_path = str(base_cache_path(symbol))
    raw_path = os.path.join(cache_dir, f"{safe_filename(symbol)}.csv")
    return _load_symbol_cached(
        symbol,
        base_path=base_path,
        base_mtime=_mtime_or_zero(base_path),
        raw_path=raw_path,
        raw_mtime=_mtime_or_zero(raw_path),
    )


def fetch_data(
    symbols, max_workers: int = 8, ui_manager=None
) -> dict[str, pd.DataFrame]:
    data_dict: dict[str, pd.DataFrame] = {}
    total = len(symbols)
    # UIManagerã®ãƒ•ã‚§ãƒ¼ã‚ºï¼ˆfetchï¼‰ãŒã‚ã‚Œã°ãã“ã¸å‡ºåŠ›
    phase = ui_manager.phase("fetch") if ui_manager else None
    if phase:
        progress_bar = phase.progress_bar
        log_area = phase.log_area
        # ãƒ•ã‚§ãƒ¼ã‚ºé…ä¸‹ã«ã€Œno dataã€ç”¨ã®åˆ¥ã‚¹ãƒ­ãƒƒãƒˆã‚’ç¢ºä¿ï¼ˆæœªä½œæˆãªã‚‰ç”Ÿæˆï¼‰
        no_data_area = phase.no_data_area if hasattr(phase, "no_data_area") else None
        if no_data_area is None:
            try:
                no_data_area = phase.container.empty()
            except Exception:
                no_data_area = st.empty()
            try:
                phase.no_data_area = no_data_area
            except Exception:
                pass
        try:
            phase.info(tr("fetch: start | {total} symbols", total=total))
        except Exception:
            pass
    else:
        st.info(tr("fetch: start | {total} symbols", total=total))
        progress_bar = st.progress(0)
        log_area = st.empty()
        # ãƒ•ã‚§ãƒ¼ã‚ºæœªä½¿ç”¨æ™‚ã¯ç›´ä¸‹ã«no-dataç”¨ã‚¹ãƒ­ãƒƒãƒˆã‚’ç”¨æ„
        no_data_area = st.empty()
    buffer, skipped, start_time = [], [], time.time()

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(load_symbol, sym): sym for sym in symbols}
        for i, future in enumerate(as_completed(futures), 1):
            sym, df = future.result()
            if df is not None and not df.empty:
                data_dict[sym] = df
                buffer.append(sym)
            else:
                skipped.append(sym)

            if i % 50 == 0 or i == total:
                log_with_progress(
                    i,
                    total,
                    start_time,
                    prefix="ãƒ‡ãƒ¼ã‚¿å–å¾—",
                    batch=50,
                    log_area=log_area,
                    progress_bar=progress_bar,
                    extra_msg=(f"éŠ˜æŸ„: {', '.join(buffer)}" if buffer else None),
                )
                buffer.clear()

    try:
        progress_bar.empty()
    except Exception:
        pass
    if skipped:
        try:
            # use i18n message for skipped count, append symbols list
            # tr ã¯ kwargs ã‚’å—ã‘ã¦ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¸ˆã¿æ–‡å­—åˆ—ã‚’è¿”ã™ã®ã§ .format ã¯ä¸è¦
            msg = tr("âš ï¸ no data: {n} symbols", n=len(skipped))
            # é•·å¤§ãªãƒªã‚¹ãƒˆã‚’é¿ã‘ã‚‹ãŸã‚ã€ä»£è¡¨ã®ã¿ï¼ˆå…ˆé ­10ä»¶ï¼‰ã‚’è¡¨ç¤º
            _sample = list(skipped)[:10]
            msg = msg + "\n" + ", ".join(_sample)
            _rest = len(skipped) - len(_sample)
            if _rest > 0:
                msg += f"\n... (+{_rest} more)"
            # å–å¾—ãƒ­ã‚°ã‚’ä¸Šæ›¸ãã›ãšã€ä¸‹ã®è¡Œã«è¡¨ç¤º
            no_data_area.text(msg)
        except Exception:
            pass
    return data_dict


# ------------------------------
# Prepare + candidates
# ------------------------------


def prepare_backtest_data(
    strategy,
    symbols,
    system_name: str = "SystemX",
    spy_df: pd.DataFrame | None = None,
    ui_manager=None,
    use_process_pool: bool = False,
    **kwargs,
):
    # 1) fetch
    if use_process_pool:
        data_dict = None
    else:
        data_dict = fetch_data(symbols, ui_manager=ui_manager)
        if not data_dict:
            st.error(tr("no valid data"))
            return None, None, None

    # 2) indicators (delegated to strategy)
    # indicators ãƒ•ã‚§ãƒ¼ã‚º
    ind_phase = ui_manager.phase("indicators") if ui_manager else None
    if ind_phase:
        try:
            ind_phase.info(tr("indicators: computing..."))
        except Exception:
            pass
        ind_progress = ind_phase.progress_bar
        ind_log = ind_phase.log_area
    else:
        st.info(tr("indicators: computing..."))
        ind_progress = st.progress(0)
        ind_log = st.empty()
    start_time = time.time()
    call_input = data_dict if not use_process_pool else symbols
    call_kwargs = dict(
        progress_callback=lambda done, total: ind_progress.progress(
            0 if total == 0 else done / total
        ),
        log_callback=lambda msg: ind_log.text(str(msg)),
        skip_callback=lambda msg: ind_log.text(str(msg)),
        **kwargs,
    )
    if use_process_pool:
        # cast to Any to satisfy narrow type checkers used in the repo
        call_kwargs["use_process_pool"] = cast(Any, True)

    try:
        prepared_dict = strategy.prepare_data(call_input, **call_kwargs)
    except TypeError:
        # å¤ã„æˆ¦ç•¥å®Ÿè£…ã¨ã®å¾Œæ–¹äº’æ›: skip_callback/use_process_pool æœªå¯¾å¿œã®æˆ¦ç•¥ã«å†è©¦è¡Œ
        call_kwargs.pop("skip_callback", None)
        call_kwargs.pop("use_process_pool", None)
        prepared_dict = strategy.prepare_data(call_input, **call_kwargs)
    try:
        ind_progress.empty()
    except Exception:
        pass

    # 3) candidates
    # candidates ãƒ•ã‚§ãƒ¼ã‚º
    cand_phase = ui_manager.phase("candidates") if ui_manager else None
    if cand_phase:
        try:
            cand_phase.info(tr("candidates: extracting..."))
        except Exception:
            pass
        cand_log = cand_phase.log_area
        cand_progress = cand_phase.progress_bar
    else:
        st.info(tr("candidates: extracting..."))
        cand_log = st.empty()
        cand_progress = st.progress(0)
    start_time = time.time()

    merged_df = None
    if system_name == "System1":
        if spy_df is None or spy_df.empty:
            st.error(tr("System1 requires SPY data for market filter"))
            return prepared_dict, None, None
        candidates_by_date, merged_df = generate_roc200_ranking_system1(
            prepared_dict,
            spy_df,
            on_progress=lambda i, total, start: log_with_progress(
                i,
                total,
                start,
                prefix="ğŸ“ˆ ROC200ãƒ©ãƒ³ã‚­ãƒ³ã‚°",
                log_area=cand_log,
                progress_bar=cand_progress,
                unit=tr("days"),
            ),
            on_log=None,
        )
    else:
        # generic path (System2â€“7)
        try:
            candidates_by_date = strategy.generate_candidates(
                prepared_dict,
                progress_callback=lambda done, total: log_with_progress(
                    done,
                    total,
                    start_time,
                    prefix="candidates",
                    log_area=cand_log,
                    progress_bar=cand_progress,
                ),
                log_callback=lambda msg: cand_log.text(str(msg)),
                **kwargs,
            )
        except (TypeError, ValueError):
            # æˆ»ã‚Šå€¤ã®å½¢ or å¼•æ•°ä¸ä¸€è‡´ï¼ˆä¾‹: System4 ã® market_dfï¼‰ã«å¯¾å¿œ
            if system_name == "System4" and spy_df is not None:
                ret = strategy.generate_candidates(
                    prepared_dict,
                    market_df=spy_df,
                    **kwargs,
                )
            else:
                ret = strategy.generate_candidates(
                    prepared_dict,
                    **kwargs,
                )
            if isinstance(ret, tuple) and len(ret) == 2:
                candidates_by_date, merged_df = ret
            else:
                candidates_by_date = ret
    # æ­£å¸¸ç³»ã§ã‚‚ (dict, df) ã‚’è¿”ã™å®Ÿè£…ãŒã‚ã‚‹ãŸã‚å¾Œæ®µã§æ­£è¦åŒ–
    if isinstance(candidates_by_date, tuple) and len(candidates_by_date) == 2:
        candidates_by_date, merged_df = candidates_by_date
    try:
        cand_progress.empty()
    except Exception:
        pass

    if not candidates_by_date:
        st.warning(tr("{system_name}: no candidates"))
        return prepared_dict, None, None

    return prepared_dict, candidates_by_date, merged_df


# ------------------------------
# Backtest execution (common wrapper)
# ------------------------------
def run_backtest_with_logging(
    strategy,
    prepared_dict,
    candidates_by_date,
    capital,
    system_name: str = "SystemX",
    ui_manager=None,
):
    bt_phase = ui_manager.phase("backtest") if ui_manager else None
    if bt_phase:
        try:
            bt_phase.info(tr("backtest: running..."))
        except Exception:
            pass
        progress = bt_phase.progress_bar
        log_area = bt_phase.log_area
        # è³‡é‡‘æ¨ç§»ã¯æœ€æ–°è¡Œã®ã¿ã€ã‚¨ã‚¯ã‚¹ãƒ‘ãƒ³ãƒ€ãƒ¼ã¯ä½¿ã‚ãšå˜ä¸€ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ã«å‡ºåŠ›
        fund_log_area = (
            bt_phase.fund_log_area
            if hasattr(bt_phase, "fund_log_area")
            else bt_phase.container.empty()
        )
        try:
            bt_phase.fund_log_area = fund_log_area
        except Exception:
            pass
    else:
        st.info(tr("backtest: running..."))
        progress = st.progress(0)
        log_area = st.empty()
        fund_log_area = st.empty()
    # debug_area is not used directly here; keep UI placeholder via st.empty() when needed
    _ = st.empty()
    debug_logs: list[str] = []

    def handle_log(msg):
        if isinstance(msg, str) and msg.startswith("ğŸ’°"):
            # attempt to localize capital/active segments while preserving date
            import re

            s = str(msg)
            # Capital: 3812.31 USD -> è³‡é‡‘: 3812.31 USD
            s = re.sub(r"Capital:\s*([0-9\.,]+)\s*USD", r"è³‡é‡‘: \1 USD", s)
            # Active: 0 -> ä¿æœ‰ãƒã‚¸ã‚·ãƒ§ãƒ³: 0
            s = re.sub(r"Active:\s*([0-9]+)", r"ä¿æœ‰ãƒã‚¸ã‚·ãƒ§ãƒ³: \1", s)
            debug_logs.append(s)
            # æœ€æ–°è¡Œã®ã¿ã‚’è¡¨ç¤ºï¼ˆå·®ã—æ›¿ãˆï¼‰
            fund_log_area.text(s)
        else:
            log_area.text(str(msg))

    results_df = strategy.run_backtest(
        prepared_dict,
        candidates_by_date,
        capital,
        on_progress=lambda i, total, start: log_with_progress(
            i,
            total,
            start,
            prefix="bt",
            log_area=log_area,
            progress_bar=progress,
            unit="days",
        ),
        on_log=lambda msg: handle_log(msg),
    )

    try:
        progress.empty()
    except Exception:
        pass

    # ãƒ­ã‚°ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¸ä¿æŒï¼ˆãƒªãƒ©ãƒ³ã—ã¦ã‚‚è¡¨ç¤ºã§ãã‚‹ã‚ˆã†ã«ï¼‰
    st.session_state[f"{system_name}_debug_logs"] = list(debug_logs)

    if st.session_state.get("show_debug_logs", True) and debug_logs:
        # ãƒ­ã‚°ã¯ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆãƒ»ãƒ•ã‚§ãƒ¼ã‚ºã®ã‚³ãƒ³ãƒ†ãƒŠå†…ã«é…ç½®ï¼ˆã‚·ã‚¹ãƒ†ãƒ ã”ã¨ã«ã¾ã¨ã¾ã‚‹ã‚ˆã†ã«ï¼‰
        parent = bt_phase.container if bt_phase else st.container()
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æœ›: å–å¼•ãƒ­ã‚°ã¯ã‚¨ã‚¯ã‚¹ãƒ‘ãƒ³ãƒ€ãƒ¼ã§æŠ˜ã‚ŠãŸãŸã¿è¡¨ç¤º
        title = f"ğŸ’° {tr('trade logs')}"
        with parent.expander(title, expanded=False):
            # text_area ã®æ–¹ãŒè¡Œé–“ãƒ»ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã§è¦–èªæ€§ãŒé«˜ã„
            st.text_area(
                "Logs",
                "\n".join(debug_logs),
                height=300,
            )

    # çµæœã‚‚ä½µã›ã¦ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜ï¼ˆUIå±¤ã§ã‚‚ä¿å­˜ã™ã‚‹ãŒäºŒé‡ã§ã‚‚å®‰å…¨ï¼‰
    st.session_state[f"{system_name}_results_df"] = results_df
    return results_df


# ------------------------------
# App entry for a single system tab
# ------------------------------


def run_backtest_app(
    strategy,
    system_name: str = "SystemX",
    limit_symbols: int = 10,
    system_title: str | None = None,
    spy_df: pd.DataFrame | None = None,
    ui_manager=None,
    **kwargs,
):
    st.title(system_title or f"{system_name} backtest")

    # --- å‰å›å®Ÿè¡Œçµæœã®è¡¨ç¤º/ã‚¯ãƒªã‚¢ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿æŒï¼‰ ---
    key_results = f"{system_name}_results_df"
    key_prepared = f"{system_name}_prepared_dict"
    key_cands = f"{system_name}_candidates_by_date"
    key_capital = f"{system_name}_capital"
    key_capital_saved = f"{system_name}_capital_saved"
    key_merged = f"{system_name}_merged_df"
    key_debug = f"{system_name}_debug_logs"

    has_prev = any(
        k in st.session_state
        for k in [key_results, key_cands, f"{system_name}_capital_saved"]
    )
    if has_prev:
        with st.expander("å‰å›ã®çµæœï¼ˆãƒªãƒ©ãƒ³ã§ã‚‚ä¿æŒï¼‰", expanded=False):
            prev_res = st.session_state.get(key_results)
            prev_cap = st.session_state.get(
                key_capital_saved, st.session_state.get(key_capital, 0)
            )
            if prev_res is not None and getattr(prev_res, "empty", False) is False:
                show_results(prev_res, prev_cap, system_name, key_context="prev")
            dbg = st.session_state.get(key_debug)
            if dbg:
                # Streamlit ã®åˆ¶ç´„ã«ã‚ˆã‚Š Expander åŒå£°ã®å…¥ã‚Œå­ã¯ä¸å¯
                # å†…å´ã® expander ã‚’é€šå¸¸è¡¨ç¤ºã«å¤‰æ›´
                st.markdown("**ä¿å­˜æ¸ˆã¿ å–å¼•ãƒ­ã‚°**")
                st.text("\n".join(map(str, dbg)))
            if st.button(tr("ä¿å­˜æ¸ˆã¿çµæœã‚’ã‚¯ãƒªã‚¢"), key=f"{system_name}_clear_saved"):
                for k in [
                    key_results,
                    key_prepared,
                    key_cands,
                    key_capital_saved,
                    key_capital,
                    key_merged,
                    key_debug,
                ]:
                    if k in st.session_state:
                        del st.session_state[k]
                # å‹ãƒã‚§ãƒƒã‚«ãƒ¼ã‚„å¤ã„ Streamlit å®Ÿè£…ã«å¯¾å¿œã™ã‚‹ãŸã‚å­˜åœ¨ã‚’ç¢ºèªã—ã¦ã‹ã‚‰å‘¼ã³å‡ºã™
                rerun = getattr(st, "experimental_rerun", None)
                if callable(rerun):
                    try:
                        rerun()
                    except Exception:
                        pass

    if st.button(tr("clear streamlit cache"), key=f"{system_name}_clear_cache"):
        st.cache_data.clear()
        st.success(tr("cache cleared"))

    debug_key = f"{system_name}_show_debug_logs"
    if debug_key not in st.session_state:
        st.session_state[debug_key] = True
    st.checkbox(tr("show debug logs"), key=debug_key)

    use_auto = st.checkbox(
        tr("auto symbols (all tickers)"), value=True, key=f"{system_name}_auto"
    )

    # é€šå¸¸æ ªã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    use_common_stocks_only = st.checkbox(
        tr("æ™®é€šæ ªã®ã¿ï¼ˆç´„6,200éŠ˜æŸ„ã€ETFãƒ»å„ªå…ˆæ ªé™¤å¤–ï¼‰"),
        value=False,
        key=f"{system_name}_common_only",
    )

    _init_cap = int(st.session_state.get(key_capital_saved, 1000))
    capital = st.number_input(
        tr("capital (USD)"),
        min_value=1000,
        value=_init_cap,
        step=100,
        key=f"{system_name}_capital",
    )

    # ãƒ†ã‚£ãƒƒã‚«ãƒ¼ãƒªã‚¹ãƒˆå–å¾—ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚ªãƒ—ã‚·ãƒ§ãƒ³è€ƒæ…®ï¼‰
    if use_common_stocks_only:
        try:
            from scripts.tickers_loader import get_common_stocks_only

            all_tickers = get_common_stocks_only()
            st.info(f"é€šå¸¸æ ªãƒ•ã‚£ãƒ«ã‚¿é©ç”¨: {len(all_tickers)}éŠ˜æŸ„")
        except ImportError as e:
            st.error(f"é€šå¸¸æ ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—: {e}")
            all_tickers = get_all_tickers()
        except Exception as e:
            st.warning(f"é€šå¸¸æ ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¤±æ•—: {e}")
            st.info("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…¨éŠ˜æŸ„ã‚’ä½¿ç”¨ã—ã¾ã™")
            all_tickers = get_all_tickers()
    else:
        all_tickers = get_all_tickers()

    max_allowed = len(all_tickers)
    
    # System6ç”¨ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ç‰¹åˆ¥ã«è¨­å®š
    if system_name == "System6":
        default_value = min(500, max_allowed)   # System6ã¯500ãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆä¿å®ˆçš„ï¼‰
    else:
        default_value = min(10, max_allowed)    # ä»–ã®ã‚·ã‚¹ãƒ†ãƒ ã¯10ãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

    if system_name != "System7":
        # ãƒ†ã‚¹ãƒˆç”¨ã§ã‚‚ä½¿ã„ã‚„ã™ã„ã‚ˆã†ã«æœ€å°å€¤ã‚’1ã«ã€åˆ»ã¿å¹…ã‚’1ã«å¤‰æ›´
        limit_symbols = st.number_input(
            tr("symbol limit"),
            min_value=1,
            max_value=max_allowed,
            value=default_value,
            step=1,
            key=f"{system_name}_limit",
        )
        if st.checkbox(tr("use all symbols"), key=f"{system_name}_all"):
            limit_symbols = max_allowed

    symbols_input = None
    if not use_auto:
        symbols_input = st.text_input(
            tr("symbols (comma separated)"),
            "AAPL,MSFT,TSLA,NVDA,META",
            key=f"{system_name}_symbols_main",
        )

    if system_name == "System7":
        symbols = ["SPY"]
    elif use_auto:
        symbols = all_tickers[:limit_symbols]
    else:
        if not symbols_input:
            st.error(tr("please input symbols"))
            return None, None, None, None, None
        symbols = [s.strip().upper() for s in symbols_input.split(",")]

    # System1 å°‚ç”¨: å®Ÿè¡Œãƒœã‚¿ãƒ³ã®ç›´å‰ã«é€šçŸ¥ãƒˆã‚°ãƒ«ã‚’é…ç½®
    if system_name in (
        "System1",
        "System2",
        "System3",
        "System4",
        "System5",
        "System6",
        "System7",
    ):
        _notify_key = f"{system_name}_notify_backtest"
        if _notify_key not in st.session_state:
            st.session_state[_notify_key] = True
        _label = tr("ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœã‚’é€šçŸ¥ã™ã‚‹ï¼ˆWebhookï¼‰")
        try:
            _use_toggle = hasattr(st, "toggle")
        except Exception:
            _use_toggle = False
        if _use_toggle:
            st.toggle(_label, key=_notify_key)
        else:
            st.checkbox(_label, key=_notify_key)
        try:
            import os as _os  # local alias to avoid top imports churn

            if not (_os.getenv("DISCORD_WEBHOOK_URL") or _os.getenv("SLACK_BOT_TOKEN")):
                st.caption(tr("Webhook/Bot è¨­å®šãŒæœªè¨­å®šã§ã™ï¼ˆ.env ã‚’ç¢ºèªï¼‰"))
        except Exception:
            pass

    run_clicked = st.button(tr("run"), key=f"{system_name}_run")
    result_area = st.container()
    if run_clicked:
        with result_area:
            prepared_dict, candidates_by_date, merged_df = prepare_backtest_data(
                strategy,
                symbols,
                system_name=system_name,
                spy_df=spy_df,
                ui_manager=ui_manager,
                **kwargs,
            )
            if candidates_by_date is None:
                return None, None, None, None, None

            results_df = run_backtest_with_logging(
                strategy,
                prepared_dict,
                candidates_by_date,
                capital,
                system_name,
                ui_manager=ui_manager,
            )
            show_results(results_df, capital, system_name, key_context="curr")

            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¸ä¿å­˜ï¼ˆãƒªãƒ©ãƒ³å¯¾ç­–ï¼‰
            st.session_state[key_results] = results_df
            st.session_state[key_prepared] = prepared_dict
            st.session_state[key_cands] = candidates_by_date
            st.session_state[key_capital_saved] = capital
            if merged_df is not None:
                st.session_state[key_merged] = merged_df

            if system_name == "System1":
                return results_df, merged_df, prepared_dict, capital, candidates_by_date
            else:
                return results_df, None, prepared_dict, capital, candidates_by_date

    return None, None, None, None, None


# ------------------------------
# Rendering helpers
# ------------------------------
def summarize_results(results_df: pd.DataFrame, capital: float):
    df = results_df.copy()

    # æ—¥ä»˜ã‚’ç¢ºå®Ÿã«æ—¥æ™‚å‹ã«
    df["entry_date"] = pd.to_datetime(df["entry_date"])
    df["exit_date"] = pd.to_datetime(df["exit_date"])

    # åŸºæœ¬é›†è¨ˆ
    df = df.sort_values("exit_date").reset_index(drop=True)
    trades = len(df)
    total_return = float(df["pnl"].sum()) if "pnl" in df.columns else 0.0
    wins = int((df["pnl"] > 0).sum()) if "pnl" in df.columns else 0
    win_rate = (wins / trades * 100.0) if trades > 0 else 0.0

    # exit_date åŸºæº–ã§ç´¯ç©PnL ã‚’ä½œæˆï¼ˆã‚°ãƒ©ãƒ•ç”¨ï¼‰
    df2 = df.copy()
    if "pnl" in df2.columns:
        df2["cumulative_pnl"] = df2["pnl"].cumsum()
    else:
        df2["cumulative_pnl"] = 0.0

    # æ—¥æ¬¡ä¿æœ‰çŠ¶æ…‹ãƒ»ã‚¨ã‚¯ã‚¤ãƒ†ã‚£ç­‰ï¼ˆç°¡æ˜“ç‰ˆï¼‰
    # cumulative_pnl ã‹ã‚‰ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³ã‚’è¨ˆç®—
    try:
        cum = df2["cumulative_pnl"].astype(float)
        dd_series = cum - cum.cummax()
        max_dd = float(abs(dd_series.min()))
    except Exception:
        max_dd = 0.0

    summary = {
        "trades": int(trades),
        "total_return": float(total_return),
        "win_rate": float(win_rate),
        "max_dd": float(max_dd),
    }

    # å‘¼ã³å‡ºã—å…ƒã¯ (summary, df2) ã‚’æœŸå¾…ã—ã¦ã„ã‚‹ãŸã‚è¿”ã™
    return summary, df2


def show_results(
    results_df: pd.DataFrame,
    capital: float,
    system_name: str = "SystemX",
    *,
    key_context: str = "main",
):
    if results_df is None or results_df.empty:
        st.info(i18n.tr("no trades"))
        return

    st.success(i18n.tr("backtest finished"))
    st.subheader(i18n.tr("results"))
    st.dataframe(results_df)

    # ãƒ‡ãƒãƒƒã‚°: åˆ—åãƒ»å‹ãƒ»å…ˆé ­æ•°è¡Œã‚’è¡¨ç¤ºï¼ˆmax drawdown ãŒ0ã®åŸå› ç¢ºèªç”¨ã€ç¢ºèªå¾Œã¯å‰Šé™¤ã—ã¦ãã ã•ã„ï¼‰
    # removed debug: results_df.head()
    # removed debug: results_df.columns
    # removed debug: results_df.dtypes

    # ä¸€éƒ¨ç’°å¢ƒã§ summarize_results ãŒ 2 å¼•æ•°ç‰ˆã§ãƒ©ãƒƒãƒ—ã•ã‚Œã¦ã„ã‚‹ã“ã¨ãŒã‚ã‚‹ãŸã‚ã€
    # system_name å›ºæœ‰ã®ãƒ‡ãƒãƒƒã‚°ãƒ•ãƒ©ã‚°ã‚’ä¸€æ™‚çš„ã«å…±é€šã‚­ãƒ¼ã¸ã‚³ãƒ”ãƒ¼ã—ã¦ã‹ã‚‰å‘¼ã³å‡ºã™
    try:
        prev_flag = st.session_state.get("show_debug_logs", None)
        # system_name å›ºæœ‰ãƒ•ãƒ©ã‚°ãŒã‚ã‚Œã°å„ªå…ˆã—ã¦ä¸€æ™‚çš„ã«ã‚»ãƒƒãƒˆ
        sys_flag = st.session_state.get(f"{system_name}_show_debug_logs", None)
        if sys_flag is not None:
            st.session_state["show_debug_logs"] = sys_flag
    except Exception:
        prev_flag = None

    # äº’æ›å‘¼ã³å‡ºã—ï¼ˆ2 å¼•æ•°ç‰ˆã§ã‚‚å‹•ä½œã™ã‚‹ã‚ˆã†ã«ã™ã‚‹ï¼‰
    summary, df2 = summarize_results(results_df, capital)
    # æœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³ã‚’å†è¨ˆç®—ã—ã¦ summary ã«åæ˜ ï¼ˆè¡¨ç¤ºã®ã‚¼ãƒ­ã‚’é˜²æ­¢ï¼‰
    try:
        cum = df2["cumulative_pnl"].astype(float)
        dd_series = cum - cum.cummax()
        max_dd_val = float(abs(dd_series.min()))
        try:
            summary["max_dd"] = max_dd_val
        except Exception:
            pass
    except Exception:
        pass

    # ãƒ•ãƒ©ã‚°ã‚’å…ƒã«æˆ»ã™
    try:
        if prev_flag is None:
            if "show_debug_logs" in st.session_state:
                del st.session_state["show_debug_logs"]
        else:
            st.session_state["show_debug_logs"] = prev_flag
    except Exception:
        pass

    # Series/Dict ã„ãšã‚Œã«ã‚‚å®‰å…¨ã«å¯¾å¿œã—ã€æ¬ æã‚­ãƒ¼ã¯ 0 æ‰±ã„
    if isinstance(summary, pd.Series):
        summary = summary.to_dict()
    col1, col2, col3, col4 = st.columns(4)
    col1.metric("å–å¼•æ•°", int(summary.get("trades", 0)))
    col2.metric("åˆè¨ˆæç›Š", f"{float(summary.get('total_return', 0.0)):.2f}")
    col3.metric("å‹ç‡ (%)", f"{float(summary.get('win_rate', 0.0)):.2f}")
    col4.metric("æœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³", f"{float(summary.get('max_dd', 0.0)):.2f}")

    st.subheader(i18n.tr("cumulative pnl"))
    # æ—¥æœ¬èªã‚’è»¸ãƒ©ãƒ™ãƒ«ã«ä½¿ã†éš›ã®ãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®šï¼ˆç’°å¢ƒã«ã‚ã‚‹ãƒ•ã‚©ãƒ³ãƒˆã‚’å„ªå…ˆã—ã¦é¸æŠï¼‰
    try:
        _set_japanese_font_fallback()
    except Exception:
        pass
    plt.figure(figsize=(10, 4))
    plt.plot(df2["exit_date"], df2["cumulative_pnl"], label="CumPnL")
    # Drawdownï¼ˆç´¯ç©æç›Šã®ãƒ”ãƒ¼ã‚¯ã‹ã‚‰ã®ä¸‹è½ï¼‰ã‚’èµ¤ç·šã§é‡ã­ã‚‹
    try:
        cum = df2["cumulative_pnl"].astype(float)
        dd = cum - cum.cummax()
        plt.plot(df2["exit_date"], dd, color="red", linewidth=1.2, label="Drawdown")
    except Exception:
        pass
    plt.xlabel(i18n.tr("date"))
    plt.ylabel(i18n.tr("pnl"))
    plt.legend()
    # streamlit.pyplot ã«ã¯ Figure ã‚’æ¸¡ã™ï¼ˆplt ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãã®ã‚‚ã®ã‚’æ¸¡ã•ãªã„ï¼‰
    try:
        fig = plt.gcf()
        st.pyplot(fig)
    except Exception:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç›´æ¥æ¸¡ã™ã®ã¯é¿ã‘ã‚‹ãŒã‚¨ãƒ©ãƒ¼æ™‚ã¯ç„¡è¦–
        pass

    st.subheader(i18n.tr("yearly summary"))
    yearly = df2.groupby(df2["exit_date"].dt.to_period("Y"))["pnl"].sum().reset_index()
    yearly["æç›Š"] = yearly["pnl"].round(2)
    yearly["ãƒªã‚¿ãƒ¼ãƒ³(%)"] = yearly["pnl"] / (capital if capital else 1) * 100
    yearly = yearly.rename(columns={"exit_date": "å¹´"})
    st.dataframe(
        yearly[["å¹´", "æç›Š", "ãƒªã‚¿ãƒ¼ãƒ³(%)"]].style.format(
            {"æç›Š": "{:.2f}", "ãƒªã‚¿ãƒ¼ãƒ³(%)": "{:.1f}%"}
        )
    )
    st.subheader(i18n.tr("monthly summary"))
    monthly = df2.groupby(df2["exit_date"].dt.to_period("M"))["pnl"].sum().reset_index()
    monthly["æç›Š"] = monthly["pnl"].round(2)
    monthly["ãƒªã‚¿ãƒ¼ãƒ³(%)"] = monthly["pnl"] / (capital if capital else 1) * 100
    monthly = monthly.rename(columns={"exit_date": "æœˆ"})
    st.dataframe(
        monthly[["æœˆ", "æç›Š", "ãƒªã‚¿ãƒ¼ãƒ³(%)"]].style.format(
            {"æç›Š": "{:.2f}", "ãƒªã‚¿ãƒ¼ãƒ³(%)": "{:.1f}%"}
        )
    )

    st.subheader(i18n.tr("holdings heatmap (by day)"))
    progress_heatmap = st.progress(0)
    heatmap_log = st.empty()
    start_time = time.time()
    unique_dates = sorted(df2["entry_date"].dt.normalize().unique())
    total_dates = len(unique_dates)
    for i, _date in enumerate(unique_dates, 1):
        _ = df2[(df2["entry_date"] <= _date) & (df2["exit_date"] >= _date)]
        log_with_progress(
            i,
            total_dates,
            start_time,
            prefix="heatmap",
            batch=10,
            log_area=heatmap_log,
            progress_bar=progress_heatmap,
            unit="days",
        )
        time.sleep(0.005)
    heatmap_log.text(i18n.tr("drawing heatmap..."))
    holding_matrix = generate_holding_matrix(df2)
    display_holding_heatmap(
        holding_matrix, title=f"{system_name} - {i18n.tr('holdings heatmap (by day)')}"
    )
    heatmap_log.success(tr("heatmap generated"))
    # unique-key download button to avoid DuplicateElementId across tabs/systems
    try:
        settings = get_settings(create_dirs=True)
        round_dec = getattr(settings.cache, "round_decimals", None)
    except Exception:
        round_dec = None
    try:
        hm_out = round_dataframe(holding_matrix, round_dec)
    except Exception:
        hm_out = holding_matrix
    csv_bytes = hm_out.to_csv().encode("utf-8")
    if getattr(getattr(_APP_SETTINGS, "ui", None), "show_download_buttons", True):
        st.download_button(
            label=(i18n.tr("download holdings csv")),
            data=csv_bytes,
            file_name=f"holding_status_{system_name}.csv",
            mime="text/csv",
            key=f"{system_name}_{key_context}_download_holding_csv",
        )
    try:
        progress_heatmap.empty()
    except Exception:
        pass


def show_signal_trade_summary(
    source_df, trades_df, system_name: str, display_name: str | None = None
):
    if system_name == "System1" and isinstance(source_df, pd.DataFrame):
        signal_counts = source_df["symbol"].value_counts().reset_index()
        signal_counts.columns = ["symbol", "Signal_Count"]
    else:
        signal_counts = {
            sym: int(df.get("setup", pd.Series(dtype=int)).sum())
            for sym, df in (source_df or {}).items()
        }
        signal_counts = pd.DataFrame(
            signal_counts.items(), columns=["symbol", "Signal_Count"]
        )

    if trades_df is not None and not trades_df.empty:
        trade_counts = (
            trades_df.groupby("symbol").size().reset_index(name="Trade_Count")
        )
    else:
        trade_counts = pd.DataFrame(columns=["symbol", "Trade_Count"])

    summary_df = pd.merge(signal_counts, trade_counts, on="symbol", how="outer").fillna(
        0
    )
    summary_df["Signal_Count"] = summary_df["Signal_Count"].astype(int)
    summary_df["Trade_Count"] = summary_df["Trade_Count"].astype(int)

    label = f"{display_name or system_name} signalç™ºç”Ÿä»¶æ•° / ãƒˆãƒ¬ãƒ¼ãƒ‰ç™ºç”Ÿä»¶æ•°"
    with st.expander(label, expanded=False):
        st.dataframe(summary_df.sort_values("Signal_Count", ascending=False))
    return summary_df


def extract_zero_reason_from_logs(logs: list[str] | None) -> str | None:
    """ãƒ­ã‚°é…åˆ—ã‹ã‚‰å€™è£œ0ä»¶ã®ç†ç”±ã‚’æŠ½å‡ºã—ã¦è¿”ã™ï¼ˆè¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã° Noneï¼‰ã€‚

    å¯¾å¿œãƒ‘ã‚¿ãƒ¼ãƒ³:
    - "å€™è£œ0ä»¶ç†ç”±: ..."
    - "ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸æˆç«‹: ..."
    """
    if not logs:
        return None
    import re as _re

    for ln in reversed(list(logs)):
        if not ln:
            continue
        m = _re.search(r"å€™è£œ0ä»¶ç†ç”±[:ï¼š]\s*(.+)$", ln)
        if m:
            return m.group(1).strip()
        m2 = _re.search(r"ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸æˆç«‹[:ï¼š]\s*(.+)$", ln)
        if m2:
            return m2.group(1).strip()
    return None


def display_roc200_ranking(
    ranking_df: pd.DataFrame,
    years: int = 5,
    top_n: int = 10,
    title: str = "System1 ROC200ãƒ©ãƒ³ã‚­ãƒ³ã‚°",
):
    if ranking_df is None or ranking_df.empty:
        st.info(tr("ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"))
        return
    df = ranking_df.copy()
    df["Date"] = (
        pd.to_datetime(df["Date"]) if "Date" in df.columns else pd.to_datetime(df.index)
    )
    df = df.reset_index(drop=True)
    if "ROC200_Rank" not in df.columns and "ROC200" in df.columns:
        df["ROC200_Rank"] = df.groupby("Date")["ROC200"].rank(
            ascending=False, method="first"
        )
    if years:
        start_date = pd.Timestamp.now() - pd.DateOffset(years=years)
        df = df[df["Date"] >= start_date]
    if top_n:
        df = df.groupby("Date").head(top_n)
    df = df.sort_values(["Date", "ROC200_Rank"], ascending=[True, True])
    with st.expander(f"{title} (ç›´è¿‘{years}å¹´ / ä¸Šä½{top_n}ä»¶)", expanded=False):
        st.dataframe(
            df.reset_index(drop=True)[["Date", "ROC200_Rank", "symbol"]],
            hide_index=False,
        )


# ------------------------------
# Save helpers
# ------------------------------


def save_signal_and_trade_logs(signal_counts_df, results, system_name, capital):
    today_str = pd.Timestamp.today().strftime("%Y-%m-%d_%H%M")
    save_dir = "results_csv"
    os.makedirs(save_dir, exist_ok=True)
    sig_dir = os.path.join(save_dir, "signals")
    os.makedirs(sig_dir, exist_ok=True)
    trade_dir = os.path.join(save_dir, "trades")
    os.makedirs(trade_dir, exist_ok=True)

    if signal_counts_df is not None and not signal_counts_df.empty:
        signal_path = os.path.join(
            sig_dir, f"{system_name}_signals_{today_str}_{int(capital)}.csv"
        )
        try:
            settings = get_settings(create_dirs=True)
            round_dec = getattr(settings.cache, "round_decimals", None)
        except Exception:
            round_dec = None
        try:
            out_df = round_dataframe(signal_counts_df, round_dec)
        except Exception:
            out_df = signal_counts_df
        out_df.to_csv(signal_path, index=False)
        st.write(tr("ã‚·ã‚°ãƒŠãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {signal_path}", signal_path=signal_path))
        # å³æ™‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        st.download_button(
            label=f"{system_name} ã‚·ã‚°ãƒŠãƒ«CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=out_df.to_csv(index=False).encode("utf-8"),
            file_name=f"{system_name}_signals_{today_str}_{int(capital)}.csv",
            mime="text/csv",
            key=f"{system_name}_download_signals_csv",
        )

    trades_df = pd.DataFrame(results) if isinstance(results, list) else results
    if trades_df is not None and not trades_df.empty:
        # ç”»é¢å†…ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆå‘¼ã³å‡ºã—å…ƒã§ã‚¨ã‚¯ã‚¹ãƒ‘ãƒ³ãƒ€ãƒ¼å†…ã«ã„ã‚‹æƒ³å®šï¼‰
        try:
            preferred_cols = [
                "entry_date",
                "exit_date",
                "symbol",
                "action",
                "price",
                "qty",
                "pnl",
            ]
            cols = [c for c in preferred_cols if c in trades_df.columns]
            st.dataframe(trades_df[cols] if cols else trades_df)
        except Exception:
            pass
        trade_path = os.path.join(
            trade_dir, f"{system_name}_trades_{today_str}_{int(capital)}.csv"
        )
        try:
            try:
                settings = get_settings(create_dirs=True)
                round_dec = getattr(settings.cache, "round_decimals", None)
            except Exception:
                round_dec = None
            try:
                out_trades = round_dataframe(trades_df, round_dec)
            except Exception:
                out_trades = trades_df
            out_trades.to_csv(trade_path, index=False)
            st.write(tr("ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {trade_path}", trade_path=trade_path))
            # å³æ™‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            st.download_button(
                label=f"{system_name} ãƒˆãƒ¬ãƒ¼ãƒ‰CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=out_trades.to_csv(index=False).encode("utf-8"),
                file_name=f"{system_name}_trades_{today_str}_{int(capital)}.csv",
                mime="text/csv",
                key=f"{system_name}_download_trades_csv",
            )
        except Exception:
            # æ›¸ãè¾¼ã¿/ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—ã—ã¦ã‚‚å‡¦ç†ã‚’ç¶™ç¶š
            pass


def save_prepared_data_cache(
    data_dict: dict[str, pd.DataFrame], system_name: str = "SystemX"
):
    """Save prepared per-symbol CSVs under `data_cache/` (Streamlit UI helper).

    This implementation attempts to round numeric columns according to
    `settings.cache.round_decimals` before writing. Failures fall back to
    writing the unrounded DataFrame.
    """
    st.info(tr("{system_name} ã®æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ä¸­...", system_name=system_name))
    if not data_dict:
        st.warning(tr("ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"))
        return
    total = len(data_dict)
    progress_bar = st.progress(0)
    for i, (sym, df) in enumerate(data_dict.items(), 1):
        path = os.path.join("data_cache", f"{safe_filename(sym)}.csv")
        try:
            try:
                settings = get_settings(create_dirs=True)
                round_dec = getattr(settings.cache, "round_decimals", None)
            except Exception:
                round_dec = None
            try:
                out_df = round_dataframe(df, round_dec)
            except Exception:
                out_df = df
            try:
                out_df.to_csv(path)
            except Exception:
                df.to_csv(path)
        except Exception:
            # Ignore failures and continue
            pass
        progress_bar.progress(0 if total == 0 else i / total)
    st.write(tr("{total}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ", total=total))
    try:
        progress_bar.empty()
    except Exception:
        pass


def display_cache_health_dashboard() -> None:
    """
    rolling cacheã®å¥å…¨æ€§ã‚’è¡¨ç¤ºã™ã‚‹ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã€‚
    """
    st.subheader("ğŸ©º Cache Health Dashboard")

    from common.cache_manager import CacheManager
    from config.settings import get_settings

    try:
        settings = get_settings(create_dirs=True)
        cache_manager = CacheManager(settings)

        # å¥å…¨æ€§ã‚µãƒãƒªãƒ¼å–å¾—
        health_summary = cache_manager.get_rolling_health_summary()

        # ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«çŠ¶æ³
        st.write("### ğŸ“‹ ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«çŠ¶æ³")
        col1, col2 = st.columns(2)

        with col1:
            meta_status = "âœ… å­˜åœ¨" if health_summary["meta_exists"] else "âŒ ä¸åœ¨"
            st.metric("ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«", meta_status)

        with col2:
            st.metric("Rolling Files", f"{health_summary['rolling_files_count']}å€‹")

        # SPY ã‚¢ãƒ³ã‚«ãƒ¼çŠ¶æ³
        st.write("### âš“ SPY ã‚¢ãƒ³ã‚«ãƒ¼çŠ¶æ³")
        anchor_status = health_summary["anchor_symbol_status"]
        col1, col2, col3 = st.columns(3)

        with col1:
            anchor_exists = "âœ… å­˜åœ¨" if anchor_status["exists"] else "âŒ ä¸åœ¨"
            st.metric("SPYå­˜åœ¨", anchor_exists)

        with col2:
            st.metric("ãƒ‡ãƒ¼ã‚¿è¡Œæ•°", f"{anchor_status['rows']:,}")

        with col3:
            target_status = "âœ… ååˆ†" if anchor_status["meets_target"] else "âš ï¸ ä¸è¶³"
            st.metric("ç›®æ¨™é”æˆ", target_status)

        # ç›®æ¨™ãƒ‡ãƒ¼ã‚¿é•·
        st.write("### ğŸ¯ ç›®æ¨™è¨­å®š")
        st.metric("ç›®æ¨™ãƒ‡ãƒ¼ã‚¿é•·", f"{health_summary['target_length']}æ—¥")

        # ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹è©³ç´°
        if health_summary["meta_exists"] and health_summary["meta_content"]:
            st.write("### ğŸ“„ ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«è©³ç´°")
            st.json(health_summary["meta_content"])

        # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒœã‚¿ãƒ³
        st.write("### âš¡ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³")
        col1, col2 = st.columns(2)

        with col1:
            if st.button("ğŸ”„ Rolling Cache åˆ†æå®Ÿè¡Œ"):
                with st.spinner("åˆ†æä¸­..."):
                    analysis_result = cache_manager.analyze_rolling_gaps()
                    _display_cache_analysis_results(analysis_result)

        with col2:
            if st.button("ğŸ§¹ Rolling Cache Pruneå®Ÿè¡Œ"):
                with st.spinner("Pruneå®Ÿè¡Œä¸­..."):
                    prune_result = cache_manager.prune_rolling_if_needed()
                    st.success(
                        f"âœ… Pruneå®Œäº†: {prune_result['pruned_files']}ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†"
                    )

    except Exception as e:
        st.error(f"Cache health dashboard ã‚¨ãƒ©ãƒ¼: {str(e)}")
        logging.error(f"Cache health dashboard error: {e}")


def _display_cache_analysis_results(analysis_result: dict) -> None:
    """Cacheåˆ†æçµæœã‚’è¡¨ç¤ºã™ã‚‹å†…éƒ¨ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã€‚"""
    st.write("### ğŸ“Š Rolling Cache åˆ†æçµæœ")

    # ã‚µãƒãƒªãƒ¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("ç·ã‚·ãƒ³ãƒœãƒ«æ•°", analysis_result["total_symbols"])

    with col2:
        st.metric("æ•´å‚™æ¸ˆã¿", analysis_result["available_in_rolling"])

    with col3:
        st.metric("æœªæ•´å‚™", analysis_result["missing_from_rolling"])

    with col4:
        coverage = analysis_result["coverage_percentage"]
        st.metric("ã‚«ãƒãƒ¬ãƒƒã‚¸", f"{coverage:.1f}%")

    # ã‚«ãƒãƒ¬ãƒƒã‚¸çŠ¶æ³ã®è¦–è¦šåŒ–
    if coverage >= 90:
        st.success("ğŸ‰ Rolling cacheæ•´å‚™çŠ¶æ³ã¯è‰¯å¥½ã§ã™")
    elif coverage >= 70:
        st.warning("âš ï¸ Rolling cacheæ•´å‚™ç‡ã®æ”¹å–„ã‚’æ¨å¥¨ã—ã¾ã™")
    else:
        st.error("ğŸš¨ Rolling cacheæ•´å‚™ãŒä¸ååˆ†ã§ã™")

    # æœªæ•´å‚™ã‚·ãƒ³ãƒœãƒ«ã®è¡¨ç¤º
    missing_symbols = analysis_result.get("missing_symbols", [])
    if missing_symbols:
        st.write("### âŒ æœªæ•´å‚™ã‚·ãƒ³ãƒœãƒ«")

        if len(missing_symbols) <= 20:
            # 20å€‹ä»¥ä¸‹ãªã‚‰å…¨ã¦è¡¨ç¤º
            st.write(", ".join(missing_symbols))
        else:
            # å¤šã„å ´åˆã¯å±•é–‹å¯èƒ½ã«ã™ã‚‹
            with st.expander(f"æœªæ•´å‚™ã‚·ãƒ³ãƒœãƒ«ä¸€è¦§ ({len(missing_symbols)}å€‹)"):
                # 10å€‹ãšã¤åŒºåˆ‡ã£ã¦è¡¨ç¤º
                for i in range(0, len(missing_symbols), 10):
                    chunk = missing_symbols[i : i + 10]
                    st.write(", ".join(chunk))


def display_system_cache_coverage() -> None:
    """
    ã‚·ã‚¹ãƒ†ãƒ åˆ¥ã®cache coverageçŠ¶æ³ã‚’è¡¨ç¤ºã™ã‚‹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã€‚
    """
    st.subheader("ğŸ¯ Systemåˆ¥ Cache Coverage")

    from common.cache_manager import CacheManager
    from common.system_groups import analyze_system_symbols_coverage
    from config.settings import get_settings
    from scripts.tickers_loader import get_all_tickers

    try:
        settings = get_settings(create_dirs=True)
        cache_manager = CacheManager(settings)

        # å…¨ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‹ã‚‰å„ã‚·ã‚¹ãƒ†ãƒ ç”¨ã®ã‚·ãƒ³ãƒœãƒ«ãƒãƒƒãƒ—ã‚’æ§‹ç¯‰
        # å®Ÿè£…ã§ã¯å„ã‚·ã‚¹ãƒ†ãƒ ã«å›ºæœ‰ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯ãŒå¿…è¦ã ãŒã€
        # ã“ã“ã§ã¯ç°¡ç•¥åŒ–ã—ã¦å…¨ã‚·ãƒ³ãƒœãƒ«ã‚’ä½¿ç”¨
        all_tickers = get_all_tickers()
        system_symbols_map = {}
        for system_num in range(1, 8):
            # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€å„ã‚·ã‚¹ãƒ†ãƒ å›ºæœ‰ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¡ä»¶ã‚’é©ç”¨
            system_symbols_map[f"system{system_num}"] = all_tickers[:500]  # ç°¡ç•¥åŒ–

        # å…¨ä½“ã®cacheåˆ†æ
        overall_analysis = cache_manager.analyze_rolling_gaps()

        # ã‚·ã‚¹ãƒ†ãƒ åˆ¥ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æ
        coverage_analysis = analyze_system_symbols_coverage(
            system_symbols_map, overall_analysis
        )

        # ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        st.write("### ğŸ“ˆ ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥ã‚µãƒãƒªãƒ¼")
        group_data = coverage_analysis["by_group"]

        for group_name in ["long", "short"]:
            if group_name in group_data:
                group_stats = group_data[group_name]
                col1, col2, col3, col4 = st.columns(4)

                group_display = (
                    "Long Systems" if group_name == "long" else "Short Systems"
                )
                st.write(f"**{group_display}**")

                with col1:
                    st.metric("ç·ã‚·ãƒ³ãƒœãƒ«", group_stats["total_symbols"])

                with col2:
                    st.metric("æ•´å‚™æ¸ˆã¿", group_stats["available"])

                with col3:
                    st.metric("æœªæ•´å‚™", group_stats["missing"])

                with col4:
                    coverage = group_stats["coverage_percentage"]
                    status = group_stats["status"]
                    st.metric("çŠ¶æ³", f"{status} {coverage:.1f}%")

        # ã‚·ã‚¹ãƒ†ãƒ åˆ¥è©³ç´°
        st.write("### ğŸ” ã‚·ã‚¹ãƒ†ãƒ åˆ¥è©³ç´°")
        system_data = coverage_analysis["by_system"]

        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å½¢å¼ã§è¡¨ç¤º
        df_data = []
        for system_name in [f"system{i}" for i in range(1, 8)]:
            if system_name in system_data:
                stats = system_data[system_name]
                df_data.append(
                    {
                        "ã‚·ã‚¹ãƒ†ãƒ ": system_name.upper(),
                        "ç·ã‚·ãƒ³ãƒœãƒ«": stats["total_symbols"],
                        "æ•´å‚™æ¸ˆã¿": stats["available"],
                        "æœªæ•´å‚™": stats["missing"],
                        "ã‚«ãƒãƒ¬ãƒƒã‚¸": f"{stats['coverage_percentage']:.1f}%",
                        "çŠ¶æ³": stats["status"],
                    }
                )

        if df_data:
            df = pd.DataFrame(df_data)
            st.dataframe(df, use_container_width=True)

        # è©³ç´°åˆ†æç”¨ã®å±•é–‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        with st.expander("ğŸ“‹ è©³ç´°åˆ†æçµæœ"):
            st.json(coverage_analysis)

    except Exception as e:
        st.error(f"System cache coverage ã‚¨ãƒ©ãƒ¼: {str(e)}")
        logging.error(f"System cache coverage error: {e}")


def display_cache_recommendations(analysis_result: dict) -> None:
    """
    Cacheåˆ†æçµæœã«åŸºã¥ãæ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¡¨ç¤ºã™ã‚‹ã€‚
    """
    from common.system_groups import format_cache_coverage_report

    # åˆ†æçµæœã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    report = format_cache_coverage_report(
        analysis_result["total_symbols"],
        analysis_result["available_in_rolling"],
        analysis_result["missing_from_rolling"],
        analysis_result["coverage_percentage"],
        analysis_result.get("missing_symbols", []),
    )

    # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º
    st.write(f"### {report['status']} ç·åˆè©•ä¾¡")
    st.write(f"**å„ªå…ˆåº¦**: {report['priority']}")

    # ã‚µãƒãƒªãƒ¼æƒ…å ±
    summary = report["summary"]
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("ç·æ•°", summary["total"])
    with col2:
        st.metric("æ•´å‚™æ¸ˆã¿", summary["available"])
    with col3:
        st.metric("æœªæ•´å‚™", summary["missing"])
    with col4:
        st.metric("ã‚«ãƒãƒ¬ãƒƒã‚¸", summary["coverage"])

    # æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
    st.write("### ğŸ’¡ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³")
    for recommendation in report["recommendations"]:
        st.write(f"- {recommendation}")

    # æœªæ•´å‚™ã‚·ãƒ³ãƒœãƒ«ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
    if report["missing_symbols_preview"]:
        st.write("### ğŸ” æœªæ•´å‚™ã‚·ãƒ³ãƒœãƒ«ï¼ˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰")
        for symbol in report["missing_symbols_preview"]:
            st.write(f"- {symbol}")
