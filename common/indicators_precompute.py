from __future__ import annotations

# DEPRECATED: „Åì„ÅÆ„Éï„Ç°„Ç§„É´„ÅØÂÖ±ÊúâÊåáÊ®ôÂâçË®àÁÆó„Éï„Çß„Éº„Ç∫„ÅÆÂâäÈô§„Å´„Çà„Çä‰∏ÄÊôÇÁöÑ„Å´ÁÑ°ÂäπÂåñ„Åï„Çå„Å¶„ÅÑ„Åæ„Åô
# rolling cache „Åß„ÅÆ‰∫ãÂâçË®àÁÆó„Çí‰ΩøÁî®„Åó„Å¶„Åè„Å†„Åï„ÅÑ
raise NotImplementedError(
    "indicators_precompute.py „ÅØÁÑ°ÂäπÂåñ„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ"
    "rolling cache (scripts/build_rolling_with_indicators.py) „Çí‰ΩøÁî®„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"
)

# ‰ª•‰∏ã„ÅØ‰øùÊåÅ„ÅÆ„Åü„ÇÅÊÆã„Åó„Åæ„Åô„Åå‰ΩøÁî®„Åï„Çå„Åæ„Åõ„Çì

import time as _t
from collections.abc import Callable
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path

import pandas as pd

from common.indicators_common import add_indicators  # Used in deprecated code paths

# Note: add_indicators is referenced but deprecated, using placeholder

try:
    from config.settings import get_settings
except Exception:  # pragma: no cover
    get_settings = None  # type: ignore

try:
    from common.cache_manager import standardize_indicator_columns
except Exception:  # pragma: no cover
    standardize_indicator_columns = None  # type: ignore

# ÂÖ±ÊúâÂâçË®àÁÆó„Åß‰ªò‰∏é„Åô„Çã‰∏ª„Å™ÊåáÊ®ôÔºàË™¨ÊòéÁî®Ôºâ„ÄÇ
# „Ç§„É≥„Éù„Éº„ÉàÊôÇÁÇπ„ÅßÂèÇÁÖßÂèØËÉΩ„Å´„Åó„ÄÅÂëº„Å≥Âá∫„ÅóÂÅ¥„ÅÆ from ... import „ÇíÂÆâÂÖ®Âåñ„Åô„Çã„ÄÇ
PRECOMPUTED_INDICATORS = (
    # ATR Á≥ª
    "ATR10",
    "ATR20",
    "ATR40",
    "ATR50",
    # ÁßªÂãïÂπ≥Âùá
    "SMA25",
    "SMA50",
    "SMA100",
    "SMA150",
    "SMA200",
    # „É¢„É°„É≥„Çø„É†/„Ç™„Ç∑„É¨„Éº„Çø„Éº
    "ROC200",
    "RSI3",
    "RSI4",
    "ADX7",
    # ÊµÅÂãïÊÄß„Éª„Éú„É©„ÉÜ„Ç£„É™„ÉÜ„Ç£Á≠â
    "DollarVolume20",
    "DollarVolume50",
    "AvgVolume50",
    "ATR_Ratio",
    "ATR_Pct",
    # Ê¥æÁîü„ÉªË£úÂä©ÊåáÊ®ô
    "Return_3D",
    "Return_6D",
    "Return_Pct",  # Êñ∞Ë¶èËøΩÂä†Ôºö„É™„Çø„Éº„É≥Áéá
    "UpTwoDays",
    "TwoDayUp",
    "Drop3D",  # Êñ∞Ë¶èËøΩÂä†Ôºö3Êó•Èñì‰∏ãËêΩÁéá
    "HV50",
    "min_50",
    "max_70",
)


def _ensure_price_columns_upper(df: pd.DataFrame) -> pd.DataFrame:
    x = df.copy()
    # Êó¢„Å´Â§ßÊñáÂ≠ó„Åå„ÅÇ„Çå„Å∞Â∞äÈáç„Åó„ÄÅÁÑ°„ÅÑÂ†¥Âêà„ÅÆ„ÅøÂ∞èÊñáÂ≠ó„Åã„ÇâË£úÂÆå
    if "Open" not in x.columns and "open" in x.columns:
        x["Open"] = x["open"]
    if "High" not in x.columns and "high" in x.columns:
        x["High"] = x["high"]
    if "Low" not in x.columns and "low" in x.columns:
        x["Low"] = x["low"]
    if "Close" not in x.columns and "close" in x.columns:
        x["Close"] = x["close"]
    if "Volume" not in x.columns and "volume" in x.columns:
        x["Volume"] = x["volume"]
    return x


def precompute_shared_indicators(
    basic_data: dict[str, pd.DataFrame],
    *,
    log: Callable[[str], None] | None = None,
    parallel: bool = False,
    max_workers: int | None = None,
) -> dict[str, pd.DataFrame]:
    """
    basic_data „ÅÆÂêÑ DataFrame „Å´ÂÖ±Êúâ„Ç§„É≥„Ç∏„Ç±„Éº„ÇøÂàó„Çí‰ªò‰∏é„Åó„Å¶Ëøî„Åô„ÄÇ

    - ÂÖ•Âäõ„ÅØ„É≠„Éº„É™„É≥„Ç∞/„Éô„Éº„ÇπÁî±Êù•„ÅÆÊúÄÂ∞è„Ç´„É©„É†ÔºàÂ∞èÊñáÂ≠óÔºâ„Åß„ÇÇÂèØ„ÄÇ
    - ‰æ°Ê†ºÁ≥ª„Ç´„É©„É†ÔºàOpen/High/Low/Close/VolumeÔºâ„ÅØÂ§ßÊñáÂ≠ó„ÇíË£úÂÆå„Åó„Å¶„Åã„ÇâË®àÁÆó„ÄÇ
    - Âá∫Âäõ„ÅØÂÖÉ„ÅÆ DataFrame „Å´ `add_indicators` „ÅßËøΩÂä†„Åï„Çå„ÅüÂàó„ÇíÁµêÂêà„ÄÇ
    - Êó¢Â≠òÂàó„ÅØ‰∏äÊõ∏„Åç„Åó„Å™„ÅÑÊñπÈáùÔºàÂêåÂêç„ÅåÂ≠òÂú®„Åô„Çå„Å∞„Åù„ÅÆ„Åæ„ÅæÊÆã„ÅôÔºâ„ÄÇ
    """
    if not basic_data:
        return basic_data
    out: dict[str, pd.DataFrame] = {}
    total = len(basic_data)
    start_ts = _t.time()
    CHUNK = 500

    # ÂàùÂõû„É≠„Ç∞„ÇíÂç≥ÊôÇÂá∫ÂäõÔºàËµ∑ÂãïÁ¢∫Ë™çÁî®Ôºâ
    if callable(log):
        try:
            log(f"üßÆ ÂÖ±ÊúâÊåáÊ®ô ÂâçË®àÁÆó: 0/{total} | Ëµ∑Âãï‰∏≠‚Ä¶")
        except Exception:
            pass

    # ÂÖ±ÊúâÂâçË®àÁÆó„Åß‰ªò‰∏é„Åô„Çã‰∏ª„Å™ÊåáÊ®ô„ÅÆÂêçÁß∞‰∏ÄË¶ßÔºà„É≠„Ç∞Ë°®Á§∫Áî®Ôºâ
    # add_indicators() „ÅåÂÆüÈöõ„ÅÆË®àÁÆó„ÇíÊãÖ„ÅÜ„Åü„ÇÅ„ÄÅ„Åì„ÅÆ‰∏ÄË¶ß„ÅØË™¨ÊòéÁî®„Å´ÈôêÂÆö
    # „Åó„ÄÅÊåôÂãï„ÅÆÂàá„ÇäÊõø„Åà„Å´„ÅØÂΩ±Èüø„Åó„Åæ„Åõ„Çì„ÄÇ
    global PRECOMPUTED_INDICATORS
    PRECOMPUTED_INDICATORS = (
        # ATR Á≥ª
        "ATR10",
        "ATR20",
        "ATR40",
        "ATR50",
        # ÁßªÂãïÂπ≥Âùá
        "SMA25",
        "SMA50",
        "SMA100",
        "SMA150",
        "SMA200",
        # „É¢„É°„É≥„Çø„É†/„Ç™„Ç∑„É¨„Éº„Çø„Éº
        "ROC200",
        "RSI3",
        "RSI4",
        "ADX7",
        # ÊµÅÂãïÊÄß„Éª„Éú„É©„ÉÜ„Ç£„É™„ÉÜ„Ç£Á≠â
        "DollarVolume20",
        "DollarVolume50",
        "AvgVolume50",
        "ATR_Ratio",
        "ATR_Pct",
        # Ê¥æÁîü„ÉªË£úÂä©ÊåáÊ®ô
        "Return_3D",
        "Return_6D",
        "Return_Pct",  # Êñ∞Ë¶èËøΩÂä†Ôºö„É™„Çø„Éº„É≥Áéá
        "UpTwoDays",
        "TwoDayUp",
        "Drop3D",  # Êñ∞Ë¶èËøΩÂä†Ôºö3Êó•Èñì‰∏ãËêΩÁéá
        "HV50",
        "min_50",
        "max_70",
    )

    # ÂÖ±Êúâ„Ç§„É≥„Ç∏„Ç±„Éº„Çø„ÅÆ„Ç≠„É£„ÉÉ„Ç∑„É•Ê†ºÁ¥çÂ†¥ÊâÄÔºàË®≠ÂÆö > Êó¢ÂÆöÔºâ
    def _cache_dir() -> Path:
        try:
            settings = get_settings(create_dirs=True) if get_settings else None
            base = Path(settings.outputs.signals_dir) if settings else Path("data_cache/signals")
        except Exception:
            base = Path("data_cache/signals")
        p = base / "shared_indicators"
        try:
            p.mkdir(parents=True, exist_ok=True)
        except Exception:
            pass
        return p

    cdir = _cache_dir()

    def _read_cache(sym: str) -> pd.DataFrame | None:
        for ext in (".feather", ".parquet"):
            fp = cdir / f"{sym}{ext}"
            if fp.exists():
                try:
                    if ext == ".feather":
                        df = pd.read_feather(fp)
                    else:
                        df = pd.read_parquet(fp)
                    if df is not None and not df.empty:
                        # Date Ê≠£Ë¶èÂåñ
                        col = "Date" if "Date" in df.columns else None
                        if col:
                            df[col] = pd.to_datetime(df[col], errors="coerce").dt.normalize()
                        return df
                except Exception:
                    continue
        return None

    def _write_cache(sym: str, df: pd.DataFrame) -> None:
        try:
            # Feather „ÇíÂÑ™ÂÖà„ÄÅParquet „Çí„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ‰øùÂ≠ò
            fp = cdir / f"{sym}.feather"
            df.reset_index(drop=True).to_feather(fp)
        except Exception:
            try:
                fp2 = cdir / f"{sym}.parquet"
                df.to_parquet(fp2, index=False)
            except Exception:
                pass

    def _calc(sym_df: tuple[str, pd.DataFrame]) -> tuple[str, pd.DataFrame]:
        sym, df = sym_df
        try:
            if df is None or getattr(df, "empty", True):
                return sym, df
            work = _ensure_price_columns_upper(df)
            # Êó¢Â≠ò„Ç≠„É£„ÉÉ„Ç∑„É•„ÇíË™≠„ÅøËæº„Åø„ÄÅÂ∑ÆÂàÜË°å„Å†„ÅëÂÜçË®àÁÆó
            cached = _read_cache(sym)
            if cached is not None and not cached.empty:
                # Date Âü∫Ê∫ñ„ÅßÂ∑ÆÂàÜ
                try:
                    src = work.copy()
                    if "Date" in src.columns:
                        src_dates = pd.to_datetime(src["Date"], errors="coerce").dt.normalize()
                    else:
                        src_dates = pd.to_datetime(src.index, errors="coerce").normalize()
                        src = src.reset_index(drop=True)
                        src["Date"] = src_dates
                    cached_local = cached.copy()
                    if "Date" in cached_local.columns:
                        cached_dates = pd.to_datetime(
                            cached_local["Date"], errors="coerce"
                        ).dt.normalize()
                    else:
                        cached_dates = pd.to_datetime(
                            cached_local.index, errors="coerce"
                        ).normalize()
                        cached_local = cached_local.reset_index(drop=True)
                        cached_local["Date"] = cached_dates
                    last = cached_dates.max()
                    src_latest = src_dates.max()
                    use_cached_only = (
                        pd.notna(last)
                        and pd.notna(src_latest)
                        and src_latest <= last
                        and len(cached_local) == len(src)
                    )
                    if use_cached_only:
                        ind_df = cached_local
                        ind_df.attrs["_precompute_skip_cache"] = True
                    else:
                        # ÂÆâÂÖ®„Å´ÊñáËÑà„Çí‰ªò„Åë„Å¶ÂÜçË®àÁÆóÔºàÊúÄÂ§ß„ÅÆÂøÖË¶ÅÁ™ì„ÅØ 200 „Å®ÊÉ≥ÂÆö + 10% ‰ΩôË£ïÔºâ
                        ctx_days = 220
                        src_recent = src[src["Date"] >= (last - pd.Timedelta(days=ctx_days))]
                        # Â∑ÆÂàÜÂÜçË®àÁÆó
                        recomputed = add_indicators(src_recent)
                        # ‰ª•Ââç„ÅÆÊúÄÁµÇÊó•„Çà„ÇäÊñ∞„Åó„ÅÑË°å„Å†„Åë„ÇíÊé°Áî®
                        recomputed_new = recomputed[recomputed["Date"] > last]
                        # FutureWarning ÂõûÈÅø: Á©∫/ÂÖ®NA„ÅÆ„Éï„É¨„Éº„É†„ÅØ concat „Åã„ÇâÈô§Â§ñ
                        is_empty = recomputed_new is None or getattr(recomputed_new, "empty", True)
                        is_all_na = False
                        try:
                            if not is_empty:
                                is_all_na = bool(recomputed_new.count().sum() == 0)
                        except Exception:
                            is_all_na = False
                        if is_empty or is_all_na:
                            ind_df = cached_local
                        else:
                            merged = pd.concat([cached_local, recomputed_new], ignore_index=True)
                            ind_df = merged
                except Exception:
                    ind_df = add_indicators(work)
            else:
                ind_df = add_indicators(work)
            new_cols = [c for c in ind_df.columns if c not in df.columns]
            if new_cols:
                merged = df.copy()
                for c in new_cols:
                    merged[c] = ind_df[c]
                # ÊåáÊ®ôÂàó„ÅÆÊ®ôÊ∫ñÂåñ„ÇíÈÅ©Áî®
                if standardize_indicator_columns:
                    merged = standardize_indicator_columns(merged)
                if getattr(ind_df, "attrs", {}).get("_precompute_skip_cache"):
                    try:
                        merged.attrs["_precompute_skip_cache"] = True
                    except Exception:
                        pass
                return sym, merged
            # Êñ∞Ë¶èÊåáÊ®ô„Åå„Å™„ÅÑÂ†¥Âêà„Åß„ÇÇÊ®ôÊ∫ñÂåñ„ÇíÈÅ©Áî®
            if standardize_indicator_columns:
                df = standardize_indicator_columns(df)
            return sym, df
        except Exception:
            # „Ç®„É©„ÉºÊôÇ„ÇÇÊ®ôÊ∫ñÂåñ„ÇíÈÅ©Áî®
            if standardize_indicator_columns:
                df = standardize_indicator_columns(df)
            return sym, df

    # ‰∏¶ÂàóÊåáÂÆö„Åå„ÅÇ„Çå„Å∞‰ª∂Êï∞„Å´Èñ¢„Çè„Çâ„Åö‰∏¶ÂàóÂÆüË°å„Åô„ÇãÔºà„ÉØ„Éº„Ç´„ÉºÊï∞„ÅØÈäòÊüÑÊï∞„ÇíË∂Ö„Åà„Å™„ÅÑÔºâ
    if parallel:
        workers = max_workers or min(32, (total // 1000) + 8)
        workers = max(1, min(int(workers), int(total)))
        with ThreadPoolExecutor(max_workers=workers) as ex:
            futures = {ex.submit(_calc, item): item[0] for item in basic_data.items()}
            done = 0
            for fut in as_completed(futures):
                sym, res = fut.result()
                out[sym] = res
                # „Ç≠„É£„ÉÉ„Ç∑„É•Êõ∏„ÅçËæº„ÅøÔºàÊñ∞Ë¶èÂàó„ÇÇÂê´„ÇÄ„ÉÜ„Éº„Éñ„É´Ôºâ
                try:
                    skip_cache = bool(getattr(res, "attrs", {}).get("_precompute_skip_cache"))
                except Exception:
                    skip_cache = False
                try:
                    if not skip_cache and res is not None and not getattr(res, "empty", True):
                        _write_cache(sym, res)
                except Exception:
                    pass
                done += 1
                if log and (done % CHUNK == 0 or done == total):
                    try:
                        elapsed = max(0.001, _t.time() - start_ts)
                        rate = done / elapsed
                        remain = max(0, total - done)
                        eta_sec = int(remain / rate) if rate > 0 else 0
                        m, s = divmod(eta_sec, 60)
                        log(f"üßÆ ÂÖ±ÊúâÊåáÊ®ô ÂâçË®àÁÆó: {done}/{total} | ETA {m}ÂàÜ{s}Áßí")
                    except Exception:
                        try:
                            log(f"üßÆ ÂÖ±ÊúâÊåáÊ®ô ÂâçË®àÁÆó: {done}/{total}")
                        except Exception:
                            pass
    else:
        for idx, item in enumerate(basic_data.items(), start=1):
            sym, res = _calc(item)
            out[sym] = res
            try:
                skip_cache = bool(getattr(res, "attrs", {}).get("_precompute_skip_cache"))
            except Exception:
                skip_cache = False
            try:
                if not skip_cache and res is not None and not getattr(res, "empty", True):
                    _write_cache(sym, res)
            except Exception:
                pass
            if log and (idx % CHUNK == 0 or idx == total):
                try:
                    elapsed = max(0.001, _t.time() - start_ts)
                    rate = idx / elapsed
                    remain = max(0, total - idx)
                    eta_sec = int(remain / rate) if rate > 0 else 0
                    m, s = divmod(eta_sec, 60)
                    log(f"üßÆ ÂÖ±ÊúâÊåáÊ®ô ÂâçË®àÁÆó: {idx}/{total} | ETA {m}ÂàÜ{s}Áßí")
                except Exception:
                    try:
                        log(f"üßÆ ÂÖ±ÊúâÊåáÊ®ô ÂâçË®àÁÆó: {idx}/{total}")
                    except Exception:
                        pass
    return out


__all__ = ["precompute_shared_indicators", "PRECOMPUTED_INDICATORS"]
