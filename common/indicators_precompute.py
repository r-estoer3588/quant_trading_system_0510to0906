from __future__ import annotations

# DEPRECATED: ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯å…±æœ‰æŒ‡æ¨™å‰è¨ˆç®—ãƒ•ã‚§ãƒ¼ã‚ºã®å‰Šé™¤ã«ã‚ˆã‚Šä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™
# rolling cache ã§ã®äº‹å‰è¨ˆç®—ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
raise NotImplementedError(
    "indicators_precompute.py ã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚"
    "rolling cache (scripts/build_rolling_with_indicators.py) ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚"
)

# ä»¥ä¸‹ã¯ä¿æŒã®ãŸã‚æ®‹ã—ã¾ã™ãŒä½¿ç”¨ã•ã‚Œã¾ã›ã‚“

import time as _t
from collections.abc import Callable
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path

import pandas as pd

from common.indicators_common import add_indicators  # Used in deprecated code paths

# Note: add_indicators is referenced but deprecated, using placeholder

try:
    from config.settings import get_settings
except Exception:  # pragma: no cover
    get_settings = None  # type: ignore

try:
    from common.cache_manager import standardize_indicator_columns
except Exception:  # pragma: no cover
    standardize_indicator_columns = None  # type: ignore

# å…±æœ‰å‰è¨ˆç®—ã§ä»˜ä¸ã™ã‚‹ä¸»ãªæŒ‡æ¨™ï¼ˆèª¬æ˜ç”¨ï¼‰ã€‚
# ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ™‚ç‚¹ã§å‚ç…§å¯èƒ½ã«ã—ã€å‘¼ã³å‡ºã—å´ã® from ... import ã‚’å®‰å…¨åŒ–ã™ã‚‹ã€‚
PRECOMPUTED_INDICATORS = (
    # ATR ç³»
    "ATR10",
    "ATR20",
    "ATR40",
    "ATR50",
    # ç§»å‹•å¹³å‡
    "SMA25",
    "SMA50",
    "SMA100",
    "SMA150",
    "SMA200",
    # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ /ã‚ªã‚·ãƒ¬ãƒ¼ã‚¿ãƒ¼
    "ROC200",
    "RSI3",
    "RSI4",
    "ADX7",
    # æµå‹•æ€§ãƒ»ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç­‰
    "DollarVolume20",
    "DollarVolume50",
    "AvgVolume50",
    "ATR_Ratio",
    "ATR_Pct",
    # æ´¾ç”Ÿãƒ»è£œåŠ©æŒ‡æ¨™
    "Return_3D",
    "Return_6D",
    "Return_Pct",  # æ–°è¦è¿½åŠ ï¼šãƒªã‚¿ãƒ¼ãƒ³ç‡
    "UpTwoDays",
    "TwoDayUp",
    "Drop3D",  # æ–°è¦è¿½åŠ ï¼š3æ—¥é–“ä¸‹è½ç‡
    "HV50",
    "min_50",
    "max_70",
)


def _ensure_price_columns_upper(df: pd.DataFrame) -> pd.DataFrame:
    x = df.copy()
    # æ—¢ã«å¤§æ–‡å­—ãŒã‚ã‚Œã°å°Šé‡ã—ã€ç„¡ã„å ´åˆã®ã¿å°æ–‡å­—ã‹ã‚‰è£œå®Œ
    if "Open" not in x.columns and "open" in x.columns:
        x["Open"] = x["open"]
    if "High" not in x.columns and "high" in x.columns:
        x["High"] = x["high"]
    if "Low" not in x.columns and "low" in x.columns:
        x["Low"] = x["low"]
    if "Close" not in x.columns and "close" in x.columns:
        x["Close"] = x["close"]
    if "Volume" not in x.columns and "volume" in x.columns:
        x["Volume"] = x["volume"]
    return x


def precompute_shared_indicators(
    basic_data: dict[str, pd.DataFrame],
    *,
    log: Callable[[str], None] | None = None,
    parallel: bool = False,
    max_workers: int | None = None,
) -> dict[str, pd.DataFrame]:
    """
    basic_data ã®å„ DataFrame ã«å…±æœ‰ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿åˆ—ã‚’ä»˜ä¸ã—ã¦è¿”ã™ã€‚

    - å…¥åŠ›ã¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°/ãƒ™ãƒ¼ã‚¹ç”±æ¥ã®æœ€å°ã‚«ãƒ©ãƒ ï¼ˆå°æ–‡å­—ï¼‰ã§ã‚‚å¯ã€‚
    - ä¾¡æ ¼ç³»ã‚«ãƒ©ãƒ ï¼ˆOpen/High/Low/Close/Volumeï¼‰ã¯å¤§æ–‡å­—ã‚’è£œå®Œã—ã¦ã‹ã‚‰è¨ˆç®—ã€‚
    - å‡ºåŠ›ã¯å…ƒã® DataFrame ã« `add_indicators` ã§è¿½åŠ ã•ã‚ŒãŸåˆ—ã‚’çµåˆã€‚
    - æ—¢å­˜åˆ—ã¯ä¸Šæ›¸ãã—ãªã„æ–¹é‡ï¼ˆåŒåãŒå­˜åœ¨ã™ã‚Œã°ãã®ã¾ã¾æ®‹ã™ï¼‰ã€‚
    """
    if not basic_data:
        return basic_data
    out: dict[str, pd.DataFrame] = {}
    total = len(basic_data)
    start_ts = _t.time()
    CHUNK = 500

    # åˆå›ãƒ­ã‚°ã‚’å³æ™‚å‡ºåŠ›ï¼ˆèµ·å‹•ç¢ºèªç”¨ï¼‰
    if callable(log):
        try:
            log(f"ğŸ§® å…±æœ‰æŒ‡æ¨™ å‰è¨ˆç®—: 0/{total} | èµ·å‹•ä¸­â€¦")
        except Exception:
            pass

    # å…±æœ‰å‰è¨ˆç®—ã§ä»˜ä¸ã™ã‚‹ä¸»ãªæŒ‡æ¨™ã®åç§°ä¸€è¦§ï¼ˆãƒ­ã‚°è¡¨ç¤ºç”¨ï¼‰
    # add_indicators() ãŒå®Ÿéš›ã®è¨ˆç®—ã‚’æ‹…ã†ãŸã‚ã€ã“ã®ä¸€è¦§ã¯èª¬æ˜ç”¨ã«é™å®š
    # ã—ã€æŒ™å‹•ã®åˆ‡ã‚Šæ›¿ãˆã«ã¯å½±éŸ¿ã—ã¾ã›ã‚“ã€‚
    global PRECOMPUTED_INDICATORS
    PRECOMPUTED_INDICATORS = (
        # ATR ç³»
        "ATR10",
        "ATR20",
        "ATR40",
        "ATR50",
        # ç§»å‹•å¹³å‡
        "SMA25",
        "SMA50",
        "SMA100",
        "SMA150",
        "SMA200",
        # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ /ã‚ªã‚·ãƒ¬ãƒ¼ã‚¿ãƒ¼
        "ROC200",
        "RSI3",
        "RSI4",
        "ADX7",
        # æµå‹•æ€§ãƒ»ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç­‰
        "DollarVolume20",
        "DollarVolume50",
        "AvgVolume50",
        "ATR_Ratio",
        "ATR_Pct",
        # æ´¾ç”Ÿãƒ»è£œåŠ©æŒ‡æ¨™
        "Return_3D",
        "Return_6D",
        "Return_Pct",  # æ–°è¦è¿½åŠ ï¼šãƒªã‚¿ãƒ¼ãƒ³ç‡
        "UpTwoDays",
        "TwoDayUp",
        "Drop3D",  # æ–°è¦è¿½åŠ ï¼š3æ—¥é–“ä¸‹è½ç‡
        "HV50",
        "min_50",
        "max_70",
    )

    # å…±æœ‰ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ ¼ç´å ´æ‰€ï¼ˆè¨­å®š > æ—¢å®šï¼‰
    def _cache_dir() -> Path:
        try:
            settings = get_settings(create_dirs=True) if get_settings else None
            base = Path(settings.outputs.signals_dir) if settings else Path("data_cache/signals")
        except Exception:
            base = Path("data_cache/signals")
        p = base / "shared_indicators"
        try:
            p.mkdir(parents=True, exist_ok=True)
        except Exception:
            pass
        return p

    cdir = _cache_dir()

    def _read_cache(sym: str) -> pd.DataFrame | None:
        for ext in (".feather", ".parquet"):
            fp = cdir / f"{sym}{ext}"
            if fp.exists():
                try:
                    if ext == ".feather":
                        df = pd.read_feather(fp)
                    else:
                        df = pd.read_parquet(fp)
                    if df is not None and not df.empty:
                        # Date æ­£è¦åŒ–
                        col = "Date" if "Date" in df.columns else None
                        if col:
                            df[col] = pd.to_datetime(df[col], errors="coerce").dt.normalize()
                        return df
                except Exception:
                    continue
        return None

    def _write_cache(sym: str, df: pd.DataFrame) -> None:
        try:
            # Feather ã‚’å„ªå…ˆã€Parquet ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä¿å­˜
            fp = cdir / f"{sym}.feather"
            df.reset_index(drop=True).to_feather(fp)
        except Exception:
            try:
                fp2 = cdir / f"{sym}.parquet"
                df.to_parquet(fp2, index=False)
            except Exception:
                pass

    def _calc(sym_df: tuple[str, pd.DataFrame]) -> tuple[str, pd.DataFrame]:
        sym, df = sym_df
        try:
            if df is None or getattr(df, "empty", True):
                return sym, df
            work = _ensure_price_columns_upper(df)
            # æ—¢å­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã¿ã€å·®åˆ†è¡Œã ã‘å†è¨ˆç®—
            cached = _read_cache(sym)
            if cached is not None and not cached.empty:
                # Date åŸºæº–ã§å·®åˆ†
                try:
                    src = work.copy()
                    if "Date" in src.columns:
                        src_dates = pd.to_datetime(src["Date"], errors="coerce").dt.normalize()
                    else:
                        src_dates = pd.to_datetime(src.index, errors="coerce").normalize()
                        src = src.reset_index(drop=True)
                        src["Date"] = src_dates
                    cached_local = cached.copy()
                    if "Date" in cached_local.columns:
                        cached_dates = pd.to_datetime(
                            cached_local["Date"], errors="coerce"
                        ).dt.normalize()
                    else:
                        cached_dates = pd.to_datetime(
                            cached_local.index, errors="coerce"
                        ).normalize()
                        cached_local = cached_local.reset_index(drop=True)
                        cached_local["Date"] = cached_dates
                    last = cached_dates.max()
                    src_latest = src_dates.max()
                    use_cached_only = (
                        pd.notna(last)
                        and pd.notna(src_latest)
                        and src_latest <= last
                        and len(cached_local) == len(src)
                    )
                    if use_cached_only:
                        ind_df = cached_local
                        ind_df.attrs["_precompute_skip_cache"] = True
                    else:
                        # å®‰å…¨ã«æ–‡è„ˆã‚’ä»˜ã‘ã¦å†è¨ˆç®—ï¼ˆæœ€å¤§ã®å¿…è¦çª“ã¯ 200 ã¨æƒ³å®š + 10% ä½™è£•ï¼‰
                        ctx_days = 220
                        src_recent = src[src["Date"] >= (last - pd.Timedelta(days=ctx_days))]
                        # å·®åˆ†å†è¨ˆç®—
                        recomputed = add_indicators(src_recent)
                        # ä»¥å‰ã®æœ€çµ‚æ—¥ã‚ˆã‚Šæ–°ã—ã„è¡Œã ã‘ã‚’æ¡ç”¨
                        recomputed_new = recomputed[recomputed["Date"] > last]
                        # FutureWarning å›é¿: ç©º/å…¨NAã®ãƒ•ãƒ¬ãƒ¼ãƒ ã¯ concat ã‹ã‚‰é™¤å¤–
                        is_empty = recomputed_new is None or getattr(recomputed_new, "empty", True)
                        is_all_na = False
                        try:
                            if not is_empty:
                                is_all_na = bool(recomputed_new.count().sum() == 0)
                        except Exception:
                            is_all_na = False
                        if is_empty or is_all_na:
                            ind_df = cached_local
                        else:
                            merged = pd.concat([cached_local, recomputed_new], ignore_index=True)
                            ind_df = merged
                except Exception:
                    ind_df = add_indicators(work)
            else:
                ind_df = add_indicators(work)
            new_cols = [c for c in ind_df.columns if c not in df.columns]
            if new_cols:
                merged = df.copy()
                for c in new_cols:
                    merged[c] = ind_df[c]
                # æŒ‡æ¨™åˆ—ã®æ¨™æº–åŒ–ã‚’é©ç”¨
                if standardize_indicator_columns:
                    merged = standardize_indicator_columns(merged)
                if getattr(ind_df, "attrs", {}).get("_precompute_skip_cache"):
                    try:
                        merged.attrs["_precompute_skip_cache"] = True
                    except Exception:
                        pass
                return sym, merged
            # æ–°è¦æŒ‡æ¨™ãŒãªã„å ´åˆã§ã‚‚æ¨™æº–åŒ–ã‚’é©ç”¨
            if standardize_indicator_columns:
                df = standardize_indicator_columns(df)
            return sym, df
        except Exception:
            # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚æ¨™æº–åŒ–ã‚’é©ç”¨
            if standardize_indicator_columns:
                df = standardize_indicator_columns(df)
            return sym, df

    # ä¸¦åˆ—æŒ‡å®šãŒã‚ã‚Œã°ä»¶æ•°ã«é–¢ã‚ã‚‰ãšä¸¦åˆ—å®Ÿè¡Œã™ã‚‹ï¼ˆãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã¯éŠ˜æŸ„æ•°ã‚’è¶…ãˆãªã„ï¼‰
    if parallel:
        workers = max_workers or min(32, (total // 1000) + 8)
        workers = max(1, min(int(workers), int(total)))
        with ThreadPoolExecutor(max_workers=workers) as ex:
            futures = {ex.submit(_calc, item): item[0] for item in basic_data.items()}
            done = 0
            for fut in as_completed(futures):
                sym, res = fut.result()
                out[sym] = res
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›¸ãè¾¼ã¿ï¼ˆæ–°è¦åˆ—ã‚‚å«ã‚€ãƒ†ãƒ¼ãƒ–ãƒ«ï¼‰
                try:
                    skip_cache = bool(getattr(res, "attrs", {}).get("_precompute_skip_cache"))
                except Exception:
                    skip_cache = False
                try:
                    if not skip_cache and res is not None and not getattr(res, "empty", True):
                        _write_cache(sym, res)
                except Exception:
                    pass
                done += 1
                if log and (done % CHUNK == 0 or done == total):
                    try:
                        elapsed = max(0.001, _t.time() - start_ts)
                        rate = done / elapsed
                        remain = max(0, total - done)
                        eta_sec = int(remain / rate) if rate > 0 else 0
                        m, s = divmod(eta_sec, 60)
                        log(f"ğŸ§® å…±æœ‰æŒ‡æ¨™ å‰è¨ˆç®—: {done}/{total} | ETA {m}åˆ†{s}ç§’")
                    except Exception:
                        try:
                            log(f"ğŸ§® å…±æœ‰æŒ‡æ¨™ å‰è¨ˆç®—: {done}/{total}")
                        except Exception:
                            pass
    else:
        for idx, item in enumerate(basic_data.items(), start=1):
            sym, res = _calc(item)
            out[sym] = res
            try:
                skip_cache = bool(getattr(res, "attrs", {}).get("_precompute_skip_cache"))
            except Exception:
                skip_cache = False
            try:
                if not skip_cache and res is not None and not getattr(res, "empty", True):
                    _write_cache(sym, res)
            except Exception:
                pass
            if log and (idx % CHUNK == 0 or idx == total):
                try:
                    elapsed = max(0.001, _t.time() - start_ts)
                    rate = idx / elapsed
                    remain = max(0, total - idx)
                    eta_sec = int(remain / rate) if rate > 0 else 0
                    m, s = divmod(eta_sec, 60)
                    log(f"ğŸ§® å…±æœ‰æŒ‡æ¨™ å‰è¨ˆç®—: {idx}/{total} | ETA {m}åˆ†{s}ç§’")
                except Exception:
                    try:
                        log(f"ğŸ§® å…±æœ‰æŒ‡æ¨™ å‰è¨ˆç®—: {idx}/{total}")
                    except Exception:
                        pass
    return out


__all__ = ["precompute_shared_indicators", "PRECOMPUTED_INDICATORS"]
