from __future__ import annotations

import importlib
import json
import logging
import os
import re
import sys
import time
import uuid
from collections.abc import Callable, Sequence
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from datetime import datetime, timezone, tzinfo
from pathlib import Path
from threading import Lock
from typing import TYPE_CHECKING, Any

try:
    from zoneinfo import ZoneInfo

    def get_zoneinfo(name: str) -> tzinfo:
        return ZoneInfo(name)

except ImportError:
    # Python < 3.9 or Windows without zoneinfo, use UTC as fallback
    def get_zoneinfo(name: str) -> tzinfo:
        _ = name
        return timezone.utc


import pandas as pd
import streamlit as st
from streamlit.runtime.scriptrunner import get_script_run_ctx

# „Éö„Éº„Ç∏Ë®≠ÂÆö„ÇíÊúÄÂàù„Å´ÂÆüË°å
st.set_page_config(page_title="Êú¨Êó•„ÅÆ„Ç∑„Ç∞„Éä„É´", layout="wide")

# sys.path„ÇíÊ≠£„Åó„ÅèË®≠ÂÆö„Åó„Å¶„Åã„Çâimport
try:
    # „Éó„É≠„Ç∏„Çß„ÇØ„Éà„É´„Éº„Éà„Ååsys.path„Å´„Å™„ÅÑÂ†¥Âêà„ÅÆ‰∫ãÂâçÂá¶ÁêÜ
    project_root = Path(__file__).parent.parent.resolve()
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))

    # scripts„Éá„Ç£„É¨„ÇØ„Éà„É™„ÇÇËøΩÂä†
    scripts_dir = project_root / "scripts"
    if str(scripts_dir) not in sys.path:
        sys.path.insert(0, str(scripts_dir))
except Exception:
    pass

from common import broker_alpaca as ba
from common.alpaca_order import submit_orders_df
from common.cache_format import round_dataframe
from common.cache_manager import CacheManager
from common.data_loader import load_price
from common.exit_planner import decide_exit_schedule
from common.notifier import create_notifier
from common.position_age import (
    fetch_entry_dates_from_alpaca,
    load_entry_dates,
    save_entry_dates,
)
from common.profit_protection import evaluate_positions
from common.stage_metrics import (
    DEFAULT_SYSTEM_ORDER,
    GLOBAL_STAGE_METRICS,
    StageMetricsStore,
    StageSnapshot,
)
from common.system_groups import format_group_counts, format_group_counts_and_values
from common.today_signals import LONG_SYSTEMS, SHORT_SYSTEMS
from common.today_signals import run_all_systems_today as compute_today_signals
from common.utils_spy import get_latest_nyse_trading_day, get_signal_target_trading_day
from config.settings import get_settings
from strategies.system1_strategy import System1Strategy
from strategies.system2_strategy import System2Strategy
from strategies.system3_strategy import System3Strategy
from strategies.system4_strategy import System4Strategy
from strategies.system5_strategy import System5Strategy
from strategies.system6_strategy import System6Strategy

# Êù°‰ª∂‰ªò„Åç„Ç§„É≥„Éù„Éº„Éà - alpaca.trading.requests „ÅØÂÆüË°åÊôÇ„ÅÆ„ÅøÂøÖË¶Å
AlpacaTradingRequests: Any | None = None


def _import_alpaca_requests():
    """Runtime-safe importer for `alpaca.trading.requests`.

    Returns the module or None if not importable.
    """
    try:
        return importlib.import_module("alpaca.trading.requests")
    except ImportError:
        return None


# ÂÆüË°åÊôÇ„Å´„Ç§„É≥„Éù„Éº„Éà„ÇíË©¶„Åø„Çã
if not TYPE_CHECKING:
    AlpacaTradingRequests = _import_alpaca_requests()


def _running_in_streamlit() -> bool:
    try:
        if get_script_run_ctx(suppress_warning=True) is not None:
            return True
    except Exception:
        pass
    try:
        flag = (os.environ.get("STREAMLIT_SERVER_ENABLED") or "").strip().lower()
        if flag in {"1", "true", "yes"}:
            return True
    except Exception:
        pass
    try:
        argv_text = " ".join(sys.argv).lower()
        if "streamlit" in argv_text:
            return True
    except Exception:
        pass
    return False


_IS_STREAMLIT_RUNTIME = _running_in_streamlit()

if not _IS_STREAMLIT_RUNTIME:
    if __name__ == "__main__":
        print(
            "„Åì„ÅÆ„Çπ„ÇØ„É™„Éó„Éà„ÅØStreamlit„ÅßÂÆüË°å„Åó„Å¶„Åè„Å†„Åï„ÅÑ: `streamlit run apps/dashboards/app_today_signals.py`"
        )
        raise SystemExit

try:
    # Streamlit „ÅÆÂÆüË°å„Ç≥„É≥„ÉÜ„Ç≠„Çπ„ÉàÊúâÁÑ°„ÇíÂà§ÂÆöÔºà„Çπ„É¨„ÉÉ„ÉâÂ§ñ„Åã„Çâ„ÅÆ UI Âëº„Å≥Âá∫„Åó„ÇíÈò≤„ÅêÔºâ
    def _has_st_ctx() -> bool:
        if not _IS_STREAMLIT_RUNTIME:
            return False
        try:
            return get_script_run_ctx() is not None
        except Exception:
            return False

except Exception:

    def _has_st_ctx() -> bool:
        return _IS_STREAMLIT_RUNTIME


# Streamlit checkbox „ÅÆÈáçË§áIDÂØæÁ≠ñÔºàkeyÊú™ÊåáÂÆöÊôÇ„Å´Ëá™Âãï„Åß‰∏ÄÊÑè„Ç≠„Éº„Çí‰ªò‰∏éÔºâ
try:
    # „É¢„Ç∏„É•„Éº„É´Â±ûÊÄß„ÇíÂÆâÂÖ®„Å´Âá¶ÁêÜ
    original_checkbox = getattr(st, "checkbox", None)

    if original_checkbox is not None and callable(original_checkbox):

        def _unique_checkbox(label, *args, **kwargs):
            if "key" not in kwargs:
                base = f"auto_cb_{abs(hash(str(label))) % 10**8}"
                count_key = f"_{base}_cnt"
                try:
                    cnt = int(st.session_state.get(count_key, 0)) + 1
                except Exception:
                    cnt = 1
                st.session_state[count_key] = cnt
                kwargs["key"] = f"{base}_{cnt}"
            # Âøµ„ÅÆ„Åü„ÇÅÂëº„Å≥Âá∫„ÅóÂâç„Å´ÂÜçÂ∫¶„ÉÅ„Çß„ÉÉ„ÇØ
            if callable(original_checkbox):
                return original_checkbox(label, *args, **kwargs)
            else:
                # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ: ÂÖÉ„ÅÆÈñ¢Êï∞„ÇíÁõ¥Êé•Âëº„Å≥Âá∫„Åó
                return st.checkbox(label, *args, **kwargs)

        # ÂÖÉ„ÅÆ„ÉÅ„Çß„ÉÉ„ÇØ„Éú„ÉÉ„ÇØ„Çπ„Çí‰øùÂ≠ò„Åó„Å¶Êñ∞„Åó„ÅÑÈñ¢Êï∞„ÇíË®≠ÂÆö
        setattr(st, "_orig_checkbox", original_checkbox)
        setattr(st, "checkbox", _unique_checkbox)
except Exception:
    # Â§±Êïó„Åó„Å¶„ÇÇÂæìÊù•Âãï‰Ωú„ÅÆ„Åæ„ÅæÈÄ≤„ÇÅ„Çã
    pass

st.title("üìà Êú¨Êó•„ÅÆ„Ç∑„Ç∞„Éä„É´ÔºàÂÖ®„Ç∑„Çπ„ÉÜ„É†Ôºâ")

settings = get_settings(create_dirs=True)
notifier = create_notifier(platform="slack", fallback=True)
# „Åì„ÅÆÂÆüË°å„É´„Éº„Éó„ÅßÁµêÊûú„ÇíË°®Á§∫„Åó„Åü„Åã„ÅÆ„Éï„É©„Ç∞Ôºà‰øùÂ≠ò„Éú„Çø„É≥Á≠â„Åß„ÅÆ„É™„É©„É≥ÂØæÁ≠ñÔºâ
st.session_state.setdefault("today_shown_this_run", False)


def _reset_shown_flag() -> None:
    """„É™„É©„É≥Âæå„ÅÆÂâçÂõûÁµêÊûúÂÜçË°®Á§∫„ÇíÊúâÂäπ„Å´„Åô„Çã„Éï„É©„Ç∞„Çí„É™„Çª„ÉÉ„Éà„Åô„Çã„ÄÇ"""
    st.session_state["today_shown_this_run"] = False


def _build_position_summary_table(df: pd.DataFrame) -> pd.DataFrame:
    """side√ósystem Âà•„ÅÆ‰øùÊúâ‰ª∂Êï∞„Çµ„Éû„É™„Éº„Çí‰ΩúÊàê„Åô„Çã„ÄÇ"""

    if df.empty:
        return pd.DataFrame()

    work = df.copy()
    allowed_systems = {
        *(s.lower() for s in LONG_SYSTEMS),
        *(s.lower() for s in SHORT_SYSTEMS),
    }

    def _norm_side(value: Any) -> str | None:
        if isinstance(value, str):
            side = value.strip().lower()
            if side in {"long", "short"}:
                return side
        return None

    def _norm_system(value: Any) -> str | None:
        if isinstance(value, str):
            system = value.strip().lower()
            if system in allowed_systems:
                return system
        return None

    work["side_norm"] = work["side"].map(_norm_side)
    work["system_norm"] = work["system"].map(_norm_system)

    invalid_side_mask = work["side_norm"].isna()
    if invalid_side_mask.any():
        invalid_values = sorted(
            {str(v) for v in work.loc[invalid_side_mask, "side"].tolist()}
        )  # noqa: E501
        raise ValueError(f"Êú™ÂØæÂøú„ÅÆside„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åô: {invalid_values}")

    invalid_system_mask = work["system_norm"].isna()
    if invalid_system_mask.any():
        invalid_values = sorted(
            {str(v) for v in work.loc[invalid_system_mask, "system"].tolist()}
        )  # noqa: E501
        raise ValueError(f"Êú™ÂØæÂøú„ÅÆsystem„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åô: {invalid_values}")

    long_conflict_mask = (work["side_norm"] == "long") & (
        ~work["system_norm"].isin(LONG_SYSTEMS)
    )  # noqa: E501
    if long_conflict_mask.any():
        conflict = sorted({str(v) for v in work.loc[long_conflict_mask, "system"].tolist()})
        raise ValueError(f"Long„Çµ„Ç§„Éâ„Å´ÊÉ≥ÂÆöÂ§ñ„ÅÆsystem„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åô: {conflict}")

    short_conflict_mask = (work["side_norm"] == "short") & (
        ~work["system_norm"].isin(SHORT_SYSTEMS)
    )
    if short_conflict_mask.any():
        conflict = sorted({str(v) for v in work.loc[short_conflict_mask, "system"].tolist()})
        raise ValueError(f"Short„Çµ„Ç§„Éâ„Å´ÊÉ≥ÂÆöÂ§ñ„ÅÆsystem„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åô: {conflict}")

    def _sorted_systems(systems: set[str]) -> list[str]:
        def _key(name: str) -> tuple[int, int | str]:
            base = name.strip().lower()
            if base.startswith("system"):
                suffix = base[6:]
                if suffix.isdigit():
                    return (0, int(suffix))
            return (1, base)

        return sorted({s.strip().lower() for s in systems if s}, key=_key)

    long_order = _sorted_systems(LONG_SYSTEMS)
    short_order = _sorted_systems(SHORT_SYSTEMS)
    system_columns: list[str] = []
    for name in [*long_order, *short_order]:
        if name and name not in system_columns:
            system_columns.append(name)
    columns_all = [*system_columns, "ÂêàË®à"]

    def _format_system_label(name: str) -> str:
        base = name.strip().lower()
        if base.startswith("system"):
            suffix = base[6:]
            if suffix.isdigit():
                return f"System{int(suffix)}"
        return name

    def _build_row(side_key: str, allowed: list[str]) -> dict[str, int]:
        subset = work[work["side_norm"] == side_key]
        counts = subset["system_norm"].value_counts()
        row = {col: 0 for col in columns_all}
        for system_name in allowed:
            row[system_name] = int(counts.get(system_name, 0))
        row["ÂêàË®à"] = int(counts.sum())
        return row

    summary_rows: list[dict[str, int]] = []
    index_labels: list[str] = []

    summary_rows.append(_build_row("long", long_order))
    index_labels.append("Long")
    summary_rows.append(_build_row("short", short_order))
    index_labels.append("Short")

    summary = pd.DataFrame(summary_rows, index=index_labels)
    summary = summary.reindex(columns=columns_all, fill_value=0)

    rename_map = {name: _format_system_label(name) for name in system_columns}
    rename_map["ÂêàË®à"] = "ÂêàË®à"
    summary = summary.rename(columns=rename_map)

    summary.index.name = "side"
    summary.columns.name = None

    return summary.astype(int)


def _normalize_price_history(df: pd.DataFrame, rows: int) -> pd.DataFrame | None:
    """„É≠„Éº„ÉâÊ∏à„ÅøÊ†™‰æ°„Éá„Éº„Çø„ÇíUIÁî®„Å´Ê≠£Ë¶èÂåñ„Åô„Çã„ÄÇ"""

    try:
        work = df.copy()
    except Exception:
        return None

    try:
        work.columns = [str(col) for col in work.columns]
    except Exception:
        work = pd.DataFrame(work)
        work.columns = [str(col) for col in work.columns]

    lower_map = {col.lower(): col for col in work.columns}

    # Êó•‰ªòÂàó„ÇíÊ±∫ÂÆöÔºàÂ≠òÂú®„Åó„Å™„ÅÑÂ†¥Âêà„ÅØ index „Åã„ÇâÁîüÊàêÔºâ
    date_col = lower_map.get("date")
    if date_col is not None:
        work["date"] = pd.to_datetime(work[date_col], errors="coerce")
    else:
        try:
            idx = pd.to_datetime(work.index, errors="coerce")
            work = work.assign(date=idx)
        except Exception:
            return None

    work = work.dropna(subset=["date"]).sort_values("date")

    rename_src = {
        "open": "open",
        "high": "high",
        "low": "low",
        "close": "close",
        "volume": "volume",
        "adj close": "adjusted_close",
        "adjclose": "adjusted_close",
    }
    for key, target in rename_src.items():
        col = lower_map.get(key)
        if col is not None:
            work.rename(columns={col: target}, inplace=True)

    try:
        work.columns = [str(col).lower() for col in work.columns]
    except Exception:
        work.columns = [str(col) for col in work.columns]

    if "date" not in work.columns or "close" not in work.columns:
        return None

    # `date` „ÇíÂÖàÈ†≠„Å´Á∂≠ÊåÅ„Åó„Å§„Å§Êó¢Áü•„Ç´„É©„É†„ÇíÂÑ™ÂÖàË°®Á§∫
    known_order = ["date", "open", "high", "low", "close", "volume", "adjusted_close"]
    ordered: list[str] = []
    for col in known_order:
        if col in work.columns:
            ordered.append(col)
    if hasattr(work, "columns") and isinstance(work.columns, pd.Index):
        for col in list(work.columns):
            if col not in ordered:
                ordered.append(col)
        work = work.loc[:, ordered]
    else:
        # work.columns „ÅåÂ≠òÂú®„Åó„Å™„ÅÑÂ†¥Âêà„ÇÑÂèçÂæ©„Åß„Åç„Å™„ÅÑÂ†¥Âêà„ÅØÁ©∫DataFrame„ÇíËøî„Åô
        return pd.DataFrame()

    if rows > 0:
        work = work.tail(rows)

    return work.reset_index(drop=True)


_ROLLING_REQUIRED_COLUMNS = [
    "date",
    "open",
    "high",
    "low",
    "close",
    "volume",
    "sma25",
    "sma50",
    "sma100",
    "sma150",
    "sma200",
    "atr20",
    "roc200",
]

_ROLLING_IMPORTANT_COLUMNS = [
    "ema20",
    "ema50",
    "atr10",
    "atr14",
    "atr40",
    "atr50",
    "adx7",
    "rsi3",
    "rsi14",
    "hv50",
    "return_6d",
    "drop3d",
]

_ROLLING_NAN_THRESHOLD = 0.20
_ROLLING_RECENT_WINDOW = 120
_ROLLING_RECENT_STRICT_WINDOW = 30
_ROLLING_RECENT_STRICT_THRESHOLD = 0.0

# Per-column lookback (rows required before values become available).
# When computing NaN ratios we exclude the initial warm-up rows for indicators
# that naturally produce NaN for the first `lookback-1` rows (e.g. ROC200,
# SMA100). Keys are lower-cased to match `col_map` usage.
_ROLLING_COLUMN_LOOKBACK: dict[str, int] = {
    # price / basic
    "date": 0,
    "open": 0,
    "high": 0,
    "low": 0,
    "close": 0,
    "volume": 0,
    # SMAs
    "sma25": 25,
    "sma50": 50,
    "sma100": 100,
    "sma150": 150,
    "sma200": 200,
    # ATR / ROC
    "atr20": 20,
    "roc200": 200,
    # common optional indicators
    "ema20": 20,
    "ema50": 50,
    "atr10": 10,
    "atr14": 14,
    "atr40": 40,
    "atr50": 50,
    "adx7": 7,
    "rsi3": 3,
    "rsi14": 14,
    "hv50": 50,
    "return_6d": 6,
    "drop3d": 3,
}

# When True, emit a single info log summarizing how many indicator columns
# were skipped because their series length was shorter than the configured
# lookback (useful for debugging false-positive NaN warnings).
_ROLLING_DEBUG_LOG_SKIPPED = False


def _has_recent_valid_window(numeric: pd.Series) -> bool:
    """Return True if recent rows provide enough non-NaN coverage."""

    if numeric.empty:
        return False

    recent_len = int(min(len(numeric), _ROLLING_RECENT_WINDOW))
    if recent_len <= 0:
        return False
    recent = numeric.iloc[-recent_len:]
    try:
        recent_ratio = float(recent.isna().mean())
    except Exception:
        recent_ratio = 1.0
    if recent_ratio <= _ROLLING_NAN_THRESHOLD:
        return True

    strict_len = int(min(len(numeric), _ROLLING_RECENT_STRICT_WINDOW))
    if strict_len <= 0:
        return False
    strict_recent = recent.iloc[-strict_len:]
    try:
        strict_ratio = float(strict_recent.isna().mean())
    except Exception:
        strict_ratio = 1.0
    return strict_ratio <= _ROLLING_RECENT_STRICT_THRESHOLD


def _analyze_rolling_cache(df: pd.DataFrame | None) -> tuple[bool, dict[str, Any]]:
    if df is None or df.empty:
        return False, {"status": "rolling_missing"}
    try:
        columns = list(df.columns)
    except Exception:
        columns = []
    col_map = {str(col).lower(): col for col in columns}
    missing_required = [col for col in _ROLLING_REQUIRED_COLUMNS if col not in col_map]
    missing_optional = [col for col in _ROLLING_IMPORTANT_COLUMNS if col not in col_map]
    nan_required: list[tuple[str, float]] = []
    nan_optional: list[tuple[str, float]] = []
    skipped_lookback_count = 0
    for name in {*_ROLLING_REQUIRED_COLUMNS, *_ROLLING_IMPORTANT_COLUMNS}:
        actual = col_map.get(name)
        if actual is None:
            continue
        try:
            numeric = pd.to_numeric(df[actual], errors="coerce")
        except Exception:
            continue
        # Exclude initial warm-up rows for indicators that naturally produce NaNs
        # by using a per-column lookback. If the series is shorter than lookback,
        # the indicator cannot be computed yet ‚Äî treat it as "not applicable"
        # for NaN-warning purposes (do not flag as NaNÈÅéÂ§ö).
        lookback = _ROLLING_COLUMN_LOOKBACK.get(name, 0)
        try:
            # If a lookback is defined but the series is too short, skip this
            # column entirely (it's not a problem ‚Äî the indicator simply
            # couldn't have been computed yet).
            if lookback and len(numeric) <= lookback:
                skipped_lookback_count += 1
                # mark as not-applicable by continuing to next column
                continue
            if lookback and len(numeric) > lookback:
                # exclude the first (lookback - 1) rows from the recent window
                # so only rows where the indicator could exist are counted.
                effective_start = max(
                    0,
                    len(numeric) - _ROLLING_RECENT_WINDOW,
                    lookback - 1 - (len(numeric) - _ROLLING_RECENT_WINDOW),
                )
                eval_series = numeric.iloc[effective_start:]
                # If eval_series is empty fallback to full-series ratio
                if len(eval_series) > 0:
                    ratio = float(eval_series.isna().mean())
                else:
                    ratio = float(numeric.isna().mean())
            else:
                ratio = float(numeric.isna().mean())
        except Exception:
            ratio = 1.0
        if ratio > _ROLLING_NAN_THRESHOLD:
            if name in _ROLLING_REQUIRED_COLUMNS:
                nan_required.append((name, ratio))
            else:
                nan_optional.append((name, ratio))
    issues: dict[str, Any] = {}
    fatal = False
    if missing_required:
        issues["missing_required"] = missing_required
        fatal = True
    if nan_required or nan_optional:
        issues["nan_columns"] = [*nan_required, *nan_optional]
    if nan_required:
        fatal = True
    if missing_optional:
        issues["missing_optional"] = missing_optional
    if fatal:
        issues.setdefault(
            "status",
            "missing_required" if missing_required else "nan_columns",
        )
        return False, issues
    if missing_optional:
        issues.setdefault("status", "missing_optional")
        return True, issues
    if nan_optional:
        issues.setdefault("status", "nan_optional")
        return True, issues
    # Optionally log a debug summary about skipped lookback-short columns
    if _ROLLING_DEBUG_LOG_SKIPPED and skipped_lookback_count:
        try:
            logger = logging.getLogger("today_signals")
            logger.info("lookbackÊú™Ê∫Ä„Åß„Çπ„Ç≠„ÉÉ„Éó„Åï„Çå„ÅüÂàó„ÅØ%d‰ª∂„Åß„Åó„Åü", skipped_lookback_count)
        except Exception:
            pass

    return True, {}


def _format_nan_columns(values: list[tuple[str, float]]) -> str:
    if not values:
        return ""
    return ", ".join(f"{name}:{ratio:.1%}" for name, ratio in values)


def _issues_to_note(issues: dict[str, Any]) -> str:
    if not issues:
        return ""
    parts: list[str] = []
    missing_required = issues.get("missing_required") or []
    if missing_required:
        parts.append("required=" + ", ".join(str(x) for x in missing_required))
    missing_optional = issues.get("missing_optional") or []
    if missing_optional:
        parts.append("optional=" + ", ".join(str(x) for x in missing_optional))
    nan_columns = issues.get("nan_columns") or []
    if nan_columns:
        parts.append("nan=" + _format_nan_columns(list(nan_columns)))
    return "; ".join(parts)


def _merge_note(base: str, addition: str) -> str:
    parts = [part for part in [base, addition] if part]
    return " / ".join(parts)


def _build_missing_detail(
    symbol: str,
    issues: dict[str, Any],
    rows_before: int,
) -> dict[str, Any]:
    missing_required = issues.get("missing_required") or []
    missing_optional = issues.get("missing_optional") or []
    nan_columns = issues.get("nan_columns") or []
    return {
        "symbol": symbol,
        "status": issues.get("status", "missing"),
        "missing_required": ", ".join(str(x) for x in missing_required),
        "missing_optional": ", ".join(str(x) for x in missing_optional),
        "nan_columns": _format_nan_columns(list(nan_columns)),
        "rows_before": int(rows_before),
        "rows_after": 0,
        "action": "",
        "resolved": False,
        "note": "",
    }


def _build_manual_rebuild_message(symbol: str, detail: dict[str, Any]) -> str:
    status = str(detail.get("status") or "rolling_missing")
    reason_map = {
        "rolling_missing": "rollingÊú™ÁîüÊàê",
        "missing_required": "ÂøÖÈ†àÂàó‰∏çË∂≥",
        "missing_optional": "‰ªªÊÑèÂàó‰∏çË∂≥",
        "nan_columns": "NaNÈÅéÂ§ö",
    }
    reason_label = reason_map.get(status, status)
    parts: list[str] = []
    rows_before = detail.get("rows_before")
    try:
        rows_val = int(rows_before) if rows_before is not None else None
    except Exception:
        rows_val = None
    if rows_val:
        parts.append(f"rows={rows_val}")
    missing_required = str(detail.get("missing_required") or "").strip()
    if missing_required:
        parts.append(f"ÂøÖÈ†à: {missing_required}")
    missing_optional = str(detail.get("missing_optional") or "").strip()
    if missing_optional:
        parts.append(f"‰ªªÊÑè: {missing_optional}")
    nan_columns = str(detail.get("nan_columns") or "").strip()
    if nan_columns:
        parts.append(f"NaN: {nan_columns}")
    message = f"‚õî rollingÊú™Êï¥ÂÇô: {symbol} ({reason_label})"
    if parts:
        message += " | " + ", ".join(parts)
    message += " ÔºàËá™Âãï„Çπ„Ç≠„ÉÉ„ÉóÊ∏à„ÅøÔºâ"
    return message


def _log_manual_rebuild_notice(
    symbol: str,
    detail: dict[str, Any],
    log_fn: Callable[[str], None] | None = None,
) -> str:
    """rollingÊú™Êï¥ÂÇô„É°„ÉÉ„Çª„Éº„Ç∏„ÇíÂá∫Âäõ„ÄÇ

    COMPACT_TODAY_LOGS=1 „ÅÆÂ†¥Âêà:
        - Êóß‰ªïÊßò: ÈäòÊüÑ„Åî„Å®„Å´ "‚õî rollingÊú™Êï¥ÂÇô: SYMBOL (...) ÔºàËá™Âãï„Çπ„Ç≠„ÉÉ„ÉóÊ∏à„ÅøÔºâ" „ÇíÈÄêÊ¨°Âá∫Âäõ„ÅóÂ§ßÈáè„Å´ÂÜóÈï∑Âåñ
        - Êñ∞‰ªïÊßò: `common.cache_warnings.RollingIssueAggregator` „Å∏„Ç´„ÉÜ„Ç¥„É™ manual_rebuild „Å®„Åó„Å¶ÈõÜÁ¥Ñ
            * ÂÖàÈ†≠ N ‰ª∂ (ROLLING_ISSUES_VERBOSE_HEAD, Êó¢ÂÆö=5) „ÅÆ„Åø WARNING
            * ‰ª•Èôç„ÅØ DEBUG „Å´„ÉÄ„Ç¶„É≥„Ç∞„É¨„Éº„ÉâÔºà„É≠„Ç∞ÈáèÂâäÊ∏õÔºâ
            * ÈõÜÁ¥Ñ„Çµ„Éû„É™„Éº„ÅØ‰ªñ„Ç´„ÉÜ„Ç¥„É™„Å®Âêå„Åò‰ªïÁµÑ„Åø„Åß INFO Âá∫Âäõ
    COMPACT_TODAY_LOGS!=1 „ÅÆÂ†¥Âêà„ÅØÂæìÊù•ÈÄö„ÇäÂÖ®Êñá„Çí log_fn „Å∏Âá∫Âäõ„Åô„Çã„ÄÇ
    """
    message = _build_manual_rebuild_message(symbol, detail)

    compact_mode = os.getenv("COMPACT_TODAY_LOGS") == "1"
    if compact_mode:
        # Êó¢Â≠ò aggregator „ÇíÂà©Áî®„Åó„Å¶„Ç´„ÉÜ„Ç¥„É™: manual_rebuild „Å®„Åó„Å¶ÁôªÈå≤
        try:
            from common.cache_warnings import report_rolling_issue  # „É≠„Éº„Ç´„É´ import (ÈÅÖÂª∂)

            # ‰ª£Ë°®ÁöÑ„Å™ÁêÜÁî±„Çí status „Åã„ÇâÊäΩÂá∫„Åó„Å¶„É°„ÉÉ„Çª„Éº„Ç∏Á∏ÆÂ∞è
            status = str(detail.get("status") or "manual_rebuild")
            report_rolling_issue("manual_rebuild", symbol, status)
        except Exception:
            # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ: Áõ¥Êé•„É≠„Ç∞
            if log_fn:
                try:
                    log_fn(message)
                except Exception:
                    pass
        return message

    # Èùû„Ç≥„É≥„Éë„ÇØ„Éà„É¢„Éº„Éâ: ÂæìÊù•ÈÄö„ÇäÂÖ®Êñá„ÇíÂá∫Âäõ
    if log_fn is None:
        return message
    try:
        log_fn(message)
    except Exception:
        pass
    return message


def _collect_symbol_data(
    symbols: list[str],
    *,
    rows: int,
    log_fn: Callable[[str], None] | None = None,
    debug_scan: bool = False,
) -> tuple[dict[str, pd.DataFrame], list[dict[str, Any]]]:
    """ÊåáÂÆö„Ç∑„É≥„Éú„É´„ÅÆÊ†™‰æ°Â±•Ê≠¥„Çí„Åæ„Å®„ÇÅ„Å¶ÂèñÂæó„Åó„ÄÅÊ¨†Êêç„ÇÇË®òÈå≤„Åô„Çã„ÄÇ"""

    start_ts = time.time()
    total = len(symbols)
    if total == 0:
        return {}, []

    step = max(1, total // 20)
    fetched: dict[str, pd.DataFrame] = {}
    malformed: list[str] = []
    missing_details: list[dict[str, Any]] = []

    try:
        env_parallel = (os.environ.get("TODAY_PREFETCH_PARALLEL") or "").strip().lower()
    except Exception:
        env_parallel = ""
    try:
        env_threshold = int(os.environ.get("TODAY_PREFETCH_PARALLEL_THRESHOLD", "200"))
    except Exception:
        env_threshold = 200

    if env_parallel in {"1", "true", "yes"}:
        use_parallel = total > 1
    elif env_parallel in {"0", "false", "no"}:
        use_parallel = False
    else:
        use_parallel = total >= max(0, env_threshold)

    max_workers: int | None = None
    if use_parallel:
        try:
            env_workers_raw = (os.environ.get("TODAY_PREFETCH_MAX_WORKERS") or "").strip()
            if env_workers_raw:
                max_workers = int(env_workers_raw)
        except Exception:
            max_workers = None
        if max_workers is None:
            try:
                cfg_workers = getattr(settings.cache.rolling, "load_max_workers", None)
                if cfg_workers:
                    max_workers = int(cfg_workers)
            except Exception:
                pass
        if max_workers is None:
            cpu_count = os.cpu_count() or 4
            max_workers = max(4, cpu_count * 2)
        max_workers = max(1, min(int(max_workers), total))
        if log_fn:
            try:
                log_fn(f"üßµ Âü∫Á§é„Éá„Éº„Çø„É≠„Éº„Éâ(‰∫ãÂâç„ÉÅ„Çß„ÉÉ„ÇØ)‰∏¶ÂàóÂåñ: workers={max_workers}")
            except Exception:
                pass

    data_lock = Lock()
    missing_lock = Lock()
    malformed_lock = Lock()
    progress_lock = Lock()
    processed = 0

    def _emit_progress(current: int) -> None:
        if log_fn is None:
            return
        if current % step != 0 and current != total:
            return
        try:
            elapsed = int(max(0, time.time() - start_ts))
            minutes, seconds = divmod(elapsed, 60)
            log_fn(f"üì¶ Âü∫Á§é„Éá„Éº„Çø„É≠„Éº„ÉâÈÄ≤Êçó: {current}/{total} | ÁµåÈÅé {minutes}ÂàÜ{seconds}Áßí")
        except Exception:
            try:
                log_fn(f"üì¶ Âü∫Á§é„Éá„Éº„Çø„É≠„Éº„ÉâÈÄ≤Êçó: {current}/{total}")
            except Exception:
                pass

    def _process_symbol(
        sym: str,
    ) -> tuple[str, pd.DataFrame | None, dict[str, Any] | None, str | None, bool]:
        manual_msg: str | None = None
        detail: dict[str, Any] | None = None
        malformed_flag = False
        try:
            df = load_price(sym, cache_profile="rolling")
        except Exception:
            df = None
        rows_before = 0 if df is None else int(len(df))
        ok, issues = _analyze_rolling_cache(df)
        if not ok:
            detail = _build_missing_detail(sym, issues, rows_before)
            if debug_scan:
                detail["action"] = "debug_scan"
                detail["note"] = _issues_to_note(issues)
                return sym, None, detail, None, False
            detail["action"] = "manual_rebuild_required"
            manual_note = _merge_note(
                _issues_to_note(issues),
                "Ëá™Âãï„Çπ„Ç≠„ÉÉ„Éó",
            )
            detail["note"] = manual_note
            manual_msg = _build_manual_rebuild_message(sym, detail)
            return sym, None, detail, manual_msg, False

        if df is None:
            malformed_flag = True
            return sym, None, None, None, malformed_flag

        norm = _normalize_price_history(df, rows)
        if norm is not None and not norm.empty:
            return sym, norm, None, None, False
        malformed_flag = True
        return sym, None, None, None, malformed_flag

    def _handle_result(
        result: tuple[str, pd.DataFrame | None, dict[str, Any] | None, str | None, bool],
    ) -> None:
        nonlocal processed
        sym, norm, detail, manual_msg, malformed_flag = result
        if norm is not None and not getattr(norm, "empty", True):
            with data_lock:
                fetched[sym] = norm
        elif malformed_flag:
            with malformed_lock:
                malformed.append(sym)
        if detail is not None:
            with missing_lock:
                missing_details.append(detail)
        if manual_msg and log_fn and not debug_scan:
            try:
                log_fn(manual_msg)
            except Exception:
                pass
        with progress_lock:
            processed += 1
            _emit_progress(processed)

    if use_parallel and max_workers and total > 1:
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {executor.submit(_process_symbol, sym): sym for sym in symbols}
            for fut in as_completed(futures):
                sym = futures[fut]
                try:
                    result = fut.result()
                except Exception:
                    result = (sym, None, None, None, True)
                _handle_result(result)
    else:
        for sym in symbols:
            result = _process_symbol(sym)
            _handle_result(result)

    if log_fn:
        try:
            elapsed = int(max(0, time.time() - start_ts))
            minutes, seconds = divmod(elapsed, 60)
            log_fn(f"üì¶ Âü∫Á§é„Éá„Éº„Çø„É≠„Éº„ÉâÂÆå‰∫Ü: {len(fetched)}/{total} | ÊâÄË¶Å {minutes}ÂàÜ{seconds}Áßí")
        except Exception:
            pass
        manual_symbols = [
            detail["symbol"]
            for detail in missing_details
            if detail.get("action") == "manual_rebuild_required"
        ]
        if manual_symbols:
            sample = ", ".join(manual_symbols[:5])
            if len(manual_symbols) > 5:
                sample += f" „Åª„Åã{len(manual_symbols) - 5}‰ª∂"
            # „Çà„ÇäË©≥Á¥∞„Å™Áä∂Ê≥ÅË™¨Êòé„ÇíËøΩÂä†
            new_listings = [
                s for s in manual_symbols if len(s) <= 4 and s.isalpha()
            ]  # Êñ∞Ë¶è‰∏äÂ†¥„ÅÆÂèØËÉΩÊÄß
            try:
                base_msg = f"‚ö†Ô∏è rollingÊú™Êï¥ÂÇô: {len(manual_symbols)}ÈäòÊüÑ ‚Üí ÊâãÂãï„Åß„Ç≠„É£„ÉÉ„Ç∑„É•„ÇíÊõ¥Êñ∞„Åó„Å¶„Åè„Å†„Åï„ÅÑ | ‰æã: {sample}"
                if new_listings:
                    base_msg += f" (Êñ∞Ë¶è‰∏äÂ†¥Âê´„ÇÄÂèØËÉΩÊÄß: {len(new_listings)}‰ª∂)"
                log_fn(base_msg)
            except Exception:
                pass
        if malformed:
            sample = ", ".join(malformed[:5])
            if len(malformed) > 5:
                sample += f" „Åª„Åã{len(malformed) - 5}‰ª∂"
            try:
                log_fn(f"‚ö†Ô∏è „Éá„Éº„ÇøÊï¥ÂΩ¢‰∏çÂèØ: {sample}")
            except Exception:
                pass
        if debug_scan:
            try:
                if missing_details:
                    log_fn(f"üß™ Ê¨†ÊêçÊ¥ó„ÅÑÂá∫„ÅóÊ§úÂá∫: {len(missing_details)}‰ª∂")
                else:
                    log_fn("üß™ Ê¨†ÊêçÊ¥ó„ÅÑÂá∫„Åó: ÂïèÈ°å„ÅØÊ§úÂá∫„Åï„Çå„Åæ„Åõ„Çì„Åß„Åó„Åü")
            except Exception:
                pass

    return fetched, missing_details


def _get_today_logger() -> logging.Logger:
    """Êú¨Êó•„ÅÆ„Ç∑„Ç∞„Éä„É´ÂÆüË°åÁî®„É≠„Ç¨„Éº„ÄÇ

    - orchestrator(`scripts.run_all_systems_today`)„ÅåË®≠ÂÆö„Åó„Åü„É≠„Ç∞„Éë„Çπ„Åå„ÅÇ„Çå„Å∞„Åù„Çå„Å´Âêà„Çè„Åõ„Çã
    - ÁÑ°„ÅÑÂ†¥Âêà„ÅØ `TODAY_SIGNALS_LOG_MODE`Ôºàsingle|datedÔºâ„ÇíÂèÇÁÖß
    - Êó¢ÂÆö„ÅØ datedÔºàJST: today_signals_YYYYMMDD_HHMM.logÔºâ
    """
    logger = logging.getLogger("today_signals")
    logger.setLevel(logging.INFO)
    try:
        logger.propagate = False
    except Exception:
        pass

    # „É≠„Ç∞„Éá„Ç£„É¨„ÇØ„Éà„É™
    try:
        log_dir = Path(settings.LOGS_DIR)
    except Exception:
        log_dir = Path("logs")
    try:
        log_dir.mkdir(parents=True, exist_ok=True)
    except Exception:
        pass

    # orchestrator ÂÅ¥„ÅÆË®≠ÂÆö„ÇíÊúÄÂÑ™ÂÖà
    log_path: Path | None = None
    # ÂãïÁöÑ„Ç§„É≥„Éù„Éº„Éà„Åß„Ç®„É©„Éº„ÇíÂõûÈÅø
    try:
        import scripts.run_all_systems_today as _run_today_mod

        sel = getattr(_run_today_mod, "_LOG_FILE_PATH", None)
        if isinstance(sel, Path):
            log_path = sel
    except Exception:
        log_path = None

    # ÁÑ°„Åë„Çå„Å∞Áí∞Â¢ÉÂ§âÊï∞„ÇíË¶ã„Å¶Ê±∫ÂÆö
    if log_path is None:
        try:
            mode_env = (os.environ.get("TODAY_SIGNALS_LOG_MODE") or "").strip().lower()
        except Exception:
            mode_env = ""
        if mode_env == "single":
            log_path = log_dir / "today_signals.log"
        else:
            try:
                jst_now = datetime.now(get_zoneinfo("Asia/Tokyo"))
            except Exception:
                jst_now = datetime.now(get_zoneinfo("UTC"))
            stamp = jst_now.strftime("%Y%m%d_%H%M")
            log_path = log_dir / f"today_signals_{stamp}.log"

    # Êó¢Â≠ò„ÅÆ„Éè„É≥„Éâ„É©„ÇíÊï¥ÁêÜÔºàÁï∞„Å™„Çã„Éï„Ç°„Ç§„É´„Å∏„ÅÆ„Éè„É≥„Éâ„É©„ÅØÈô§ÂéªÔºâ
    try:
        for h in list(logger.handlers):
            try:
                if isinstance(h, logging.FileHandler):
                    base = getattr(h, "baseFilename", None)
                    if base and Path(base) != log_path:
                        logger.removeHandler(h)
                        try:
                            h.close()
                        except Exception:
                            pass
            except Exception:
                pass
    except Exception:
        pass

    # Âêå‰∏Ä„Éï„Ç°„Ç§„É´Âêë„Åë„ÅåÊú™ËøΩÂä†„Å™„ÇâËøΩÂä†
    has_handler = False
    for h in list(logger.handlers):
        try:
            if isinstance(h, logging.FileHandler) and getattr(h, "baseFilename", None):
                if Path(h.baseFilename) == log_path:
                    has_handler = True
                    break
        except Exception:
            continue
    if not has_handler:
        try:
            fh = logging.FileHandler(str(log_path), encoding="utf-8")
            fmt = logging.Formatter(
                "%(asctime)s [%(levelname)s] %(message)s", "%Y-%m-%d %H:%M:%S"
            )  # noqa: E501
            fh.setFormatter(fmt)
            logger.addHandler(fh)
        except Exception:
            pass
    return logger


@dataclass
class RunConfig:
    symbols: list[str]
    capital_long: float
    capital_short: float
    save_csv: bool
    csv_name_mode: str
    notify: bool
    run_parallel: bool
    scan_missing_only: bool = False


@dataclass
class TradeOptions:
    paper_mode: bool
    retries: int
    delay: float
    poll_status: bool
    do_trade: bool
    update_bp_after: bool


class ProgressUI:
    """ÂÖ®‰ΩìÈÄ≤Êçó„Å®„É≠„Ç∞Ë°®Á§∫„ÇíÁÆ°ÁêÜ„Åô„Çã„Éò„É´„Éë„Éº„ÄÇ"""

    def __init__(self, ui_vis: dict[str, Any]):
        self.show_overall = bool(ui_vis.get("overall_progress", True))
        self.show_data_load = bool(ui_vis.get("data_load_progress_lines", True))
        self.phase_title_area = st.empty()
        self.progress_area = st.empty()
        self.progress_bar = st.progress(0) if self.show_overall else None
        # progress_text„ÅØÂâäÈô§Ôºà„Çø„Ç§„Éà„É´„ÅßË°®Á§∫„Åô„Çã„Åü„ÇÅÔºâ
        self.phase_state: dict[str, Any] = {"percent": 0, "label": "ÂØæË±°Ë™≠„ÅøËæº„Åø"}
        self._render_title()

    def set_label(self, label: str) -> None:
        if not self.show_overall:
            return
        self.phase_state["label"] = label
        self._render_title()

    def update(self, done: int, total: int, tag: str) -> None:
        if not self.show_overall or self.progress_bar is None:
            return
        total = max(1, int(total))
        ratio = min(max(int(done), 0), total) / total
        percent = int(ratio * 100)
        self.phase_state["percent"] = percent
        mapped = self._map_phase(tag)
        if mapped:
            self.phase_state["label"] = mapped
        try:
            self.progress_bar.progress(percent)
        except Exception:
            pass
        # „Éó„É≠„Ç∞„É¨„Çπ„Éê„Éº‰∏ã„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„ÅØÂâäÈô§Ôºà„Çø„Ç§„Éà„É´„ÅßË°®Á§∫„Åô„Çã„Åü„ÇÅÔºâ
        self._render_title()

    def update_label_for_stage(self, stage_value: int) -> None:
        if not self.show_overall:
            return
        if stage_value <= 0:
            label = "ÂØæË±°Ê∫ñÂÇô"
        elif stage_value < 10:
            label = "ÂØæË±°Ë™≠„ÅøËæº„Åø"
        elif stage_value < 30:
            label = "„Éï„Ç£„É´„Çø„Éº"
        elif stage_value < 60:
            label = "„Çª„ÉÉ„Éà„Ç¢„ÉÉ„Éó"
        elif stage_value < 90:
            label = "„Éà„É¨„Éº„ÉâÂÄôË£úÈÅ∏ÂÆö"
        else:
            label = "„Ç®„É≥„Éà„É™„Éº"
        self.set_label(label)

    def _render_title(self) -> None:
        if not self.show_overall:
            return
        try:
            percent = int(self.phase_state.get("percent", 0))
            label = str(self.phase_state.get("label", "ÂØæË±°Ë™≠„ÅøËæº„Åø"))
            self.phase_title_area.markdown(f"## ÈÄ≤Êçó {percent}%: {label}„Éï„Çß„Éº„Ç∫")
        except Exception:
            pass

    @staticmethod
    def _map_phase(tag: str) -> str:
        try:
            t = (tag or "").lower()
        except Exception:
            t = ""
        if t in {
            "init",
            "ÂØæË±°Ë™≠„ÅøËæº„Åø:start",
            "load_basic:start",
            "load_basic",
            "load_indicators",
            "spx",
            "spy",
        }:
            return "ÂØæË±°Ë™≠„ÅøËæº„Åø"
        if t in {"filter", "„Éï„Ç£„É´„Çø„Éº"}:
            return "„Éï„Ç£„É´„Çø„Éº"
        if t in {"run_strategies", "setup"} or t.startswith("system"):
            return "„Çª„ÉÉ„Éà„Ç¢„ÉÉ„Éó"
        if t in {"strategies_done", "tradeÂÄôË£ú", "„Éà„É¨„Éº„ÉâÂÄôË£úÈÅ∏ÂÆö"}:
            return "„Éà„É¨„Éº„ÉâÂÄôË£úÈÅ∏ÂÆö"
        if t in {"finalize", "done", "„Ç®„É≥„Éà„É™„Éº"}:
            return "„Ç®„É≥„Éà„É™„Éº"
        return "ÂØæË±°Ë™≠„ÅøËæº„Åø"


class StageTracker:
    """„Ç∑„Çπ„ÉÜ„É†Âà•„ÅÆÈÄ≤Êçó„Å®‰ª∂Êï∞„É°„Éà„É™„ÇØ„Çπ„ÇíÁÆ°ÁêÜ„Åô„Çã„ÄÇ"""

    def __init__(self, ui_vis: dict[str, Any], progress_ui: ProgressUI):
        self.progress_ui = progress_ui
        self.show_ui = bool(ui_vis.get("per_system_progress", True)) and _has_st_ctx()
        self.bars: dict[str, Any] = {}
        self.stage_txt: dict[str, Any] = {}
        self.metrics_txt: dict[str, Any] = {}
        self.states: dict[str, int] = {}
        self.metrics_store = StageMetricsStore(DEFAULT_SYSTEM_ORDER)
        self.stage_counts = self.metrics_store.stage_counts
        # ÊúÄÂæå„Å´Âèó„ÅëÂèñ„Å£„Åü„Çπ„ÉÜ„Éº„Ç∏ÊÉÖÂ†±„ÅÆ„Éá„Éá„É•„Éº„ÉóÁî®„Çø„Ç§„É†„Çπ„Çø„É≥„Éó
        self._last_event: dict[str, tuple[int, int, int, int, int, float]] = {}
        self.universe_total: int | None = None
        self.universe_target: int | None = None
        if self.show_ui:
            sys_cols = st.columns(7)
            sys_labels = [f"System{i}" for i in range(1, 8)]
            for i, col in enumerate(sys_cols, start=1):
                key = f"system{i}"
                try:
                    col.caption(sys_labels[i - 1])
                    self.bars[key] = col.progress(0)
                    self.stage_txt[key] = col.empty()
                    self.metrics_txt[key] = col.empty()
                    self._render_metrics(key)
                except Exception:
                    self.show_ui = False
                    break
        self._initialize_from_store()

    def update_progress(self, name: str, phase: str) -> None:
        if not self.show_ui:
            return
        key = str(name).lower()
        progress_bar = self.bars.get(key)
        if progress_bar is None:
            return

        # „Éï„Çß„Éº„Ç∫„Å´Âøú„Åò„ÅüÈÅ©Âàá„Å™ÂÄ§„ÇíË®≠ÂÆö
        if phase == "start":
            # ÈñãÂßãÊôÇ„ÅØ0%„Åã„ÇâÈñãÂßãÔºà„É™„Çª„ÉÉ„ÉàÔºâ
            value = 0
            self.states[key] = 0  # Áä∂ÊÖã„ÇÇ„É™„Çª„ÉÉ„Éà
        elif phase == "done":
            value = 100
        else:
            # „Åù„ÅÆ‰ªñ„ÅÆ„Éï„Çß„Éº„Ç∫„Åß„ÅØÂÆüÈöõ„ÅÆÈÄ≤ÊçóÂÄ§„ÇíÂèñÂæó
            try:
                snapshot = GLOBAL_STAGE_METRICS.get_snapshot(key)
                if snapshot is not None:
                    value = snapshot.progress
                else:
                    value = self.states.get(key, 0)
            except Exception:
                value = self.states.get(key, 0)

        value = max(0, min(100, int(value)))

        # ÈÄöÂ∏∏ÊôÇ„ÅØÈÄ≤ÊçóÂæåÈÄÄ„ÇíÈò≤„Åê„Åå„ÄÅÈñãÂßãÊôÇÔºàphase="start"Ôºâ„ÅØ„É™„Çª„ÉÉ„Éà„ÇíË®±ÂèØ
        if phase != "start":
            prev = int(self.states.get(key, 0))
            value = max(prev, value)

        self.states[key] = value

        try:
            progress_bar.progress(value)
            self.stage_txt[key].text(f"run {value}%" if value < 100 else "done 100%")
        except Exception:
            pass

    def _initialize_from_store(self) -> None:
        try:
            stored_target = GLOBAL_STAGE_METRICS.get_universe_target()
            if stored_target is not None:
                self.universe_target = int(stored_target)
        except Exception:
            pass
        try:
            snapshots = GLOBAL_STAGE_METRICS.all_snapshots()
        except Exception:
            snapshots = {}
        for sys_name, snapshot in snapshots.items():
            try:
                self._apply_snapshot(sys_name, snapshot)
            except Exception:
                continue

    def _apply_snapshot(self, name: str, snapshot: StageSnapshot) -> None:
        key = str(name).lower()
        counts = self._ensure_counts(key)

        # „Çø„Éº„Ç≤„ÉÉ„ÉàÊï∞„ÅÆË®≠ÂÆöÔºàÂÑ™ÂÖàÂ∫¶È†Ü„ÅßË®≠ÂÆöÔºâ
        if snapshot.target is not None:
            try:
                target_val = int(snapshot.target)
                counts["target"] = target_val
                self.universe_total = target_val
            except Exception:
                pass
        elif snapshot.filter_pass is not None and counts.get("target") is None:
            try:
                fallback_target = int(snapshot.filter_pass)
                counts["target"] = fallback_target
                if self.universe_total is None:
                    self.universe_total = fallback_target
            except Exception:
                pass

        # ÈÄ≤Êçó„Éá„Éº„Çø„ÅÆË®≠ÂÆö
        if snapshot.filter_pass is not None:
            try:
                counts["filter"] = int(snapshot.filter_pass)
                # „Éï„Ç£„É´„Çø„ÉºÈÄöÈÅéÊï∞„ÅåË®≠ÂÆö„Åï„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÄÅ„Çø„Éº„Ç≤„ÉÉ„Éà„Åå„Å™„Åë„Çå„Å∞„Éï„Ç£„É´„Çø„ÉºÊï∞„Çí„Çø„Éº„Ç≤„ÉÉ„Éà„Å®„Åó„Å¶‰ΩøÁî®
                if counts.get("target") is None:
                    counts["target"] = int(snapshot.filter_pass)
                    if self.universe_total is None:
                        self.universe_total = int(snapshot.filter_pass)
            except Exception:
                pass
        if snapshot.setup_pass is not None:
            try:
                counts["setup"] = int(snapshot.setup_pass)
            except Exception:
                pass
        if snapshot.candidate_count is not None:
            try:
                counts["cand"] = self._clamp_trdlist(snapshot.candidate_count)
            except Exception:
                pass
        if snapshot.entry_count is not None:
            try:
                counts["entry"] = int(snapshot.entry_count)
            except Exception:
                pass
        if snapshot.exit_count is not None:
            try:
                counts["exit"] = int(snapshot.exit_count)
            except Exception:
                pass
        self._update_bar(key, snapshot.progress)
        self.progress_ui.update_label_for_stage(snapshot.progress)
        self._render_metrics(key)

    def update_stage(
        self,
        name: str,
        value: int,
        filter_cnt: int | None = None,
        setup_cnt: int | None = None,
        cand_cnt: int | None = None,
        final_cnt: int | None = None,
    ) -> None:
        key = str(name).lower()
        # Áü≠ÊôÇÈñìÂÜÖ„Å´Âêå‰∏ÄÂÜÖÂÆπ„ÅÆÊõ¥Êñ∞„ÅåÊù•„Çã„Å® UI „Åå„Éï„É©„ÉÉ„Çø„É™„É≥„Ç∞„Åô„Çã„Åü„ÇÅ„ÄÅ
        # Âêå‰∏Ä„Ç∑„Çπ„ÉÜ„É†„ÉªÂêå‰∏ÄÂÄ§„ÉªÂêå‰∏Ä„Ç´„Ç¶„É≥„Éà„ÅÆÊõ¥Êñ∞„ÅØ 0.5 Áßí‰ª•ÂÜÖ„ÅØÁÑ°Ë¶ñ„Åô„Çã„ÄÇ
        try:
            last = self._last_event.get(key)
            cur_sig = (
                value,
                int(filter_cnt) if filter_cnt is not None else -1,
                int(setup_cnt) if setup_cnt is not None else -1,
                int(cand_cnt) if cand_cnt is not None else -1,
                int(final_cnt) if final_cnt is not None else -1,
                time.time(),
            )
            if last is not None:
                same = last[0:5] == cur_sig[0:5]
                recent = (cur_sig[5] - last[5]) < 0.5
                if same and recent:
                    return
            self._last_event[key] = cur_sig
        except Exception:
            pass
        snapshot: StageSnapshot | None
        try:
            snapshot = GLOBAL_STAGE_METRICS.record_stage(
                key,
                value,
                filter_cnt,
                setup_cnt,
                cand_cnt,
                final_cnt,
                emit_event=False,
            )
        except Exception:
            snapshot = None
        if snapshot is not None:
            self._apply_snapshot(key, snapshot)
            return
        counts = self._ensure_counts(key)
        if filter_cnt is not None:
            try:
                filter_val = int(filter_cnt)
            except Exception:
                filter_val = None
            if filter_val is not None:
                if value == 0:
                    counts["target"] = filter_val
                    self.universe_total = filter_val
                else:
                    counts["filter"] = filter_val
                    if counts.get("target") is None:
                        counts["target"] = (
                            self.universe_total if self.universe_total is not None else filter_val
                        )
                        if self.universe_total is None:
                            self.universe_total = filter_val
        if setup_cnt is not None:
            counts["setup"] = int(setup_cnt)
        if cand_cnt is not None:
            counts["cand"] = self._clamp_trdlist(cand_cnt)
        if final_cnt is not None:
            counts["entry"] = int(final_cnt)
        self._update_bar(key, value)
        self.progress_ui.update_label_for_stage(value)
        self._render_metrics(key)

    def set_universe_target(self, tgt: int | None) -> None:
        """ÂÖ®‰Ωì„É¶„Éã„Éê„Éº„ÇπÔºàTgtÔºâ„ÇíË®≠ÂÆö„ÄÇUI „Å´Âç≥ÊôÇÂèçÊò†„Åô„Çã„ÄÇ

        - ÂºïÊï∞„Åå None „ÅÆÂ†¥Âêà„ÅØÊó¢ÂÆöÂãï‰ΩúÔºàÂêÑ system „ÅÆ target/filter „ÇíË°®Á§∫Ôºâ„Å´Êàª„Çã„ÄÇ
        - Êï¥Êï∞„Åå‰∏é„Åà„Çâ„Çå„ÅüÂ†¥Âêà„ÄÅÂêÑ system „ÅÆË°®Á§∫‰∏ä„ÅÆ `Tgt` „ÅØ„Åì„ÅÆÂÄ§„ÇíË°®Á§∫„Åô„Çã„ÄÇ
        """
        try:
            if tgt is None:
                self.universe_target = None
                self.universe_total = None
            else:
                self.universe_target = int(tgt)
                self.universe_total = int(tgt)
            GLOBAL_STAGE_METRICS.set_universe_target(self.universe_target)
        except Exception:
            self.universe_target = None
            self.universe_total = None
            try:
                GLOBAL_STAGE_METRICS.set_universe_target(None)
            except Exception:
                pass
        # ÂÖ® system „ÅÆË°®Á§∫„ÇíÊõ¥Êñ∞
        self.refresh_all()

    def update_exit(self, name: str, count: int) -> None:
        key = str(name).lower()
        snapshot: StageSnapshot | None
        try:
            snapshot = GLOBAL_STAGE_METRICS.record_exit(key, count, emit_event=False)
        except Exception:
            snapshot = None
        if snapshot is not None:
            self._apply_snapshot(key, snapshot)
            return
        counts = self._ensure_counts(key)
        counts["exit"] = int(count)
        self._render_metrics(key)

    def finalize_counts(
        self, final_df: pd.DataFrame, per_system: dict[str, pd.DataFrame]
    ) -> None:  # noqa: E501
        for name, counts in self.stage_counts.items():
            snapshot: StageSnapshot | None
            try:
                snapshot = GLOBAL_STAGE_METRICS.get_snapshot(name)
            except Exception:
                snapshot = None
            if snapshot is not None:
                if counts.get("target") is None and snapshot.target is not None:
                    try:
                        counts["target"] = int(snapshot.target)
                        if self.universe_total is None:
                            self.universe_total = int(snapshot.target)
                    except Exception:
                        pass
                if counts.get("filter") is None and snapshot.filter_pass is not None:
                    try:
                        counts["filter"] = int(snapshot.filter_pass)
                    except Exception:
                        pass
                if counts.get("setup") is None and snapshot.setup_pass is not None:
                    try:
                        counts["setup"] = int(snapshot.setup_pass)
                    except Exception:
                        pass
                if counts.get("cand") is None and snapshot.candidate_count is not None:
                    try:
                        counts["cand"] = self._clamp_trdlist(snapshot.candidate_count)
                    except Exception:
                        pass
                if counts.get("entry") is None and snapshot.entry_count is not None:
                    try:
                        counts["entry"] = int(snapshot.entry_count)
                    except Exception:
                        pass
                if counts.get("exit") is None and snapshot.exit_count is not None:
                    try:
                        counts["exit"] = int(snapshot.exit_count)
                    except Exception:
                        pass
        try:
            system_series = (
                final_df["system"].astype(str).str.strip().str.lower()
                if "system" in final_df.columns
                else pd.Series(dtype=str)
            )
        except Exception:
            system_series = pd.Series(dtype=str)
        for name, counts in self.stage_counts.items():
            if counts.get("cand") is None:
                df_sys = per_system.get(name)
                if df_sys is None or df_sys.empty:
                    counts["cand"] = 0
                else:
                    counts["cand"] = self._clamp_trdlist(len(df_sys))
            if counts.get("entry") is None and not system_series.empty:
                try:
                    counts["entry"] = int((system_series == name).sum())
                except Exception:
                    counts["entry"] = counts.get("entry")
            if counts.get("target") is None:
                if self.universe_total is not None:
                    counts["target"] = self.universe_total
                elif counts.get("filter") is not None and counts.get("setup") is None:
                    counts["target"] = counts.get("filter")
            try:
                GLOBAL_STAGE_METRICS.record_stage(
                    name,
                    int(self.states.get(name, 100 if counts.get("entry") is not None else 0)),
                    counts.get("filter"),
                    counts.get("setup"),
                    counts.get("cand"),
                    counts.get("entry"),
                    emit_event=False,
                )
            except Exception:
                pass
        self.refresh_all()

    def apply_exit_counts(self, exit_counts: dict[str, int]) -> None:
        for name, cnt in exit_counts.items():
            if not cnt:
                continue
            snapshot: StageSnapshot | None
            try:
                snapshot = GLOBAL_STAGE_METRICS.record_exit(name, cnt, emit_event=False)
            except Exception:
                snapshot = None
            if snapshot is not None:
                self._apply_snapshot(name, snapshot)
            else:
                self._ensure_counts(name)["exit"] = int(cnt)
        self.refresh_all()

    def refresh_all(self) -> None:
        for name in self.metrics_store.systems():
            self._render_metrics(name)

    def _update_bar(self, key: str, value: int) -> None:
        if not self.show_ui:
            return
        progress_bar = self.bars.get(key)
        if progress_bar is None:
            return
        vv = max(0, min(100, int(value)))
        prev = int(self.states.get(key, 0))
        vv = max(prev, vv)
        self.states[key] = vv
        try:
            progress_bar.progress(vv)
            self.stage_txt[key].text(f"run {vv}%" if vv < 100 else "done 100%")
        except Exception:
            pass

    def _render_metrics(self, key: str) -> None:
        placeholder = self.metrics_txt.get(key)
        if placeholder is None:
            return
        display = self.metrics_store.get_display_metrics(key)
        target_value = (
            self.universe_target if self.universe_target is not None else display.get("target")
        )
        text = "  ".join(
            [
                f"Tgt {self._format_value(target_value)}",
                f"FILpass {self._format_value(display.get('filter'))}",
                f"STUpass {self._format_value(display.get('setup'))}",
                f"TRDlist {self._format_trdlist(display.get('cand'))}",
                f"Entry {self._format_value(display.get('entry'))}",
                f"Exit {self._format_value(display.get('exit'))}",
            ]
        )
        try:
            placeholder.text(text)
        except Exception:
            pass

    def get_display_metrics(self, name: str) -> dict[str, int | None]:
        key = str(name).lower()
        return self.metrics_store.get_display_metrics(key)

    def _ensure_counts(self, name: str) -> dict[str, int | None]:
        return self.metrics_store.ensure_display_metrics(name)

    @staticmethod
    def _format_value(value: Any) -> str:
        return "-" if value is None else str(value)

    @staticmethod
    def _clamp_trdlist(value: Any) -> int | None:
        return StageMetricsStore.clamp_trdlist(value)

    def _format_trdlist(self, value: Any) -> str:
        if value is None:
            return "-"
        try:
            return str(self._clamp_trdlist(value))
        except Exception:
            return "-"


class UILogger:
    """UI„Å®„Éï„Ç°„Ç§„É´Âá∫Âäõ„ÅÆ‰∏°Êñπ„Å∏„É≠„Ç∞„ÇíÊõ∏„ÅçÂá∫„Åô„ÄÇ"""

    def __init__(self, start_time: float, progress_ui: ProgressUI):
        self.start_time = start_time
        self.progress_ui = progress_ui
        self.log_lines: list[str] = []
        # „É≠„Ç∞„Éá„Éá„É•„Éº„ÉóÁî®ÔºàÁü≠ÊôÇÈñì„Å´Âêå‰∏Ä„É°„ÉÉ„Çª„Éº„Ç∏„ÅåÊù•„Åü„ÇâÊäëÊ≠¢Ôºâ
        self._last_log: dict[str, float] = {}

    def log(self, msg: str, no_timestamp: bool = False) -> None:
        forwarded_from_cli = False
        try:
            import scripts.run_all_systems_today as _run_today_mod

            forwarding_flag = getattr(_run_today_mod, "_LOG_FORWARDING", None)
            if forwarding_flag is not None:
                forwarded_from_cli = bool(forwarding_flag.get())
        except Exception:
            forwarded_from_cli = False
        structured_mode = False
        parsed_msg: str | None = None
        iso_ts: str | None = None
        rel_prefix: str | None = None
        if not no_timestamp:
            # STRUCTURED_UI_LOGS=1 „ÅÆ„Å®„Åç„ÄÅ„Ç®„É≥„Ç∏„É≥ÂÅ¥„Åã„ÇâÊ∏°„Åï„Çå„Çã JSON ÂΩ¢Âºè„ÇíÂÑ™ÂÖàÁöÑ„Å´Ëß£Èáà
            if os.environ.get("STRUCTURED_UI_LOGS") == "1":
                try:
                    import json as _json

                    if isinstance(msg, str) and msg.startswith("{") and '"msg"' in msg:
                        obj = _json.loads(msg)
                        # ÊúÄ‰ΩéÈôê 'msg' „Åå„ÅÇ„Çã„Åì„Å®
                        raw_inner = obj.get("msg")
                        if isinstance(raw_inner, str):
                            structured_mode = True
                            parsed_msg = raw_inner
                            # ISO ÊôÇÂàª
                            iso_candidate = obj.get("iso")
                            if isinstance(iso_candidate, str):
                                iso_ts = iso_candidate
                            # Áõ∏ÂØæÊôÇÈñìÔºà„Ç®„Éù„ÉÉ„ÇØ„Çí start_time „Å®„ÅÆÂ∑ÆÂàÜ„ÅßË®àÁÆóÔºâ
                            ts_val = obj.get("ts")
                            if isinstance(ts_val, (int, float)):
                                try:
                                    rel_elapsed = max(0, (ts_val / 1000.0) - self.start_time)
                                    mm, ss = divmod(int(rel_elapsed), 60)
                                    rel_prefix = f"{mm}ÂàÜ{ss}Áßí"
                                except Exception:
                                    pass
                except Exception:
                    structured_mode = False

        def _format_rel_compact(elapsed: float) -> str:
            try:
                if elapsed < 0:
                    elapsed = 0.0
                if elapsed < 1:
                    return f"+{int(elapsed * 1000)}ms"
                if elapsed < 60:
                    return f"+{elapsed:.1f}s"
                if elapsed < 3600:
                    m, s = divmod(int(elapsed), 60)
                    return f"+{m}:{s:02d}"
                if elapsed < 86400:
                    h, rem = divmod(int(elapsed), 3600)
                    m, s = divmod(rem, 60)
                    return f"+{h}h{m:02d}m"  # Áßí„ÅØÁúÅÁï•
                d, rem = divmod(int(elapsed), 86400)
                h, _ = divmod(rem, 3600)
                return f"+{d}d{h}h"
            except Exception:
                return "+0.0s"

        compact_mode = os.environ.get("COMPACT_REL_TIME") == "1"

        if structured_mode and parsed_msg is not None:
            # ISO or ÁèæÂú®ÊôÇÂàª fallback
            if iso_ts is None:
                iso_ts = time.strftime("%Y-%m-%d %H:%M:%S")
            if rel_prefix is None:
                try:
                    _elapsed = max(0, time.time() - self.start_time)
                    if compact_mode:
                        rel_prefix = _format_rel_compact(_elapsed)
                    else:
                        mm, ss = divmod(int(_elapsed), 60)
                        rel_prefix = f"{mm}ÂàÜ{ss}Áßí"
                except Exception:
                    rel_prefix = "0ÂàÜ0Áßí"
            line = f"[{iso_ts} | {rel_prefix}] {parsed_msg}"
        else:
            try:
                elapsed = max(0, time.time() - self.start_time)
                if compact_mode:
                    rel_prefix = _format_rel_compact(elapsed)
                else:
                    m, s = divmod(int(elapsed), 60)
            except Exception:
                rel_prefix = "0ÂàÜ0Áßí" if not compact_mode else "+0.0s"
            now_txt = time.strftime("%Y-%m-%d %H:%M:%S")
            if no_timestamp:
                line = str(msg)
            else:
                if compact_mode:
                    if not rel_prefix:
                        rel_prefix = "+0.0s"
                    line = f"[{now_txt} | {rel_prefix}] {msg}"
                else:
                    try:
                        # m,s „ÅåË®àÁÆóÊ∏à„Åø„Åß„Å™„ÅÑ„Ç±„Éº„Çπ„ÅØÂÜçË®àÁÆó
                        if "m" not in locals() or "s" not in locals():
                            _m, _s = divmod(int(max(0, time.time() - self.start_time)), 60)
                            m, s = _m, _s
                        line = f"[{now_txt} | {m}ÂàÜ{s}Áßí] {msg}"
                    except Exception:
                        line = f"[{now_txt} | 0ÂàÜ0Áßí] {msg}"
        self.log_lines.append(line)
        if _has_st_ctx() and self.progress_ui.show_overall:
            if self._should_display(str(msg)):
                try:
                    self.progress_ui.progress_area.text(line)
                except Exception:
                    pass
        if not forwarded_from_cli:
            self._echo_cli(line)
            try:
                _get_today_logger().info(str(msg))
            except Exception:
                pass

    def _should_display(self, msg: str) -> bool:
        if not self.progress_ui.show_overall:
            return False
        data_load_prefixes = (
            "üì¶ Âü∫Á§é„Éá„Éº„Çø„É≠„Éº„ÉâÈÄ≤Êçó",
            "üßÆ ÊåáÊ®ô„Éá„Éº„Çø„É≠„Éº„ÉâÈÄ≤Êçó",
            "üì¶ Âü∫Á§é„Éá„Éº„Çø„É≠„Éº„ÉâÂÆå‰∫Ü",
            "üßÆ ÊåáÊ®ô„Éá„Éº„Çø„É≠„Éº„ÉâÂÆå‰∫Ü",
            "üßÆ ÂÖ±ÊúâÊåáÊ®ô ÂâçË®àÁÆó",
        )
        # „Åì„Åì„ÅØÊØîËºÉÁöÑÈôêÂÆöÁöÑ„Å™„Ç≠„Éº„ÉØ„Éº„Éâ„ÅÆ„Åø„Å´„Åô„ÇãÔºàÈÅéÂâ∞Èô§Â§ñ„ÇíÈò≤Ê≠¢Ôºâ
        skip_keywords = (
            "batch time",
            "next batch size",
        )
        if msg.startswith(data_load_prefixes):
            return self.progress_ui.show_data_load
        # Áü≠ÊôÇÈñìÂÜÖ„ÅÆÂêå‰∏Ä„É≠„Ç∞„ÇíÊäëÊ≠¢Ôºà0.3Áßí‰ª•ÂÜÖ„ÅÆÈáçË§á„ÅØÁÑ°Ë¶ñÔºâ
        try:
            now = time.time()
            last = self._last_log.get(msg)
            if last is not None and (now - last) < 0.3:
                return False
            self._last_log[msg] = now
        except Exception:
            pass
        return not any(keyword in msg for keyword in skip_keywords)

    def _echo_cli(self, line: str) -> None:
        # Windows „Ç≥„É≥„ÇΩ„Éº„É´„Åß„ÅÆÊñáÂ≠óÂåñ„ÅëÁ∑©ÂíåÔºà‰ªªÊÑè„Éï„É©„Ç∞Ôºâ
        try:
            if os.name == "nt" and os.environ.get("FORCE_UTF8_CONSOLE") == "1":
                try:
                    if hasattr(sys.stdout, "reconfigure"):
                        # Êó¢„Å´ utf-8 „ÅÆÂ†¥Âêà„ÅØËß¶„Çâ„Å™„ÅÑ
                        if (getattr(sys.stdout, "encoding", "") or "").lower() not in (
                            "utf-8",
                            "utf8",
                        ):
                            sys.stdout.reconfigure(encoding="utf-8")  # type: ignore[attr-defined]
                except Exception:
                    pass
            # ÂàùÂõû„Éí„É≥„ÉàË°®Á§∫ÔºàÂåñ„Åë„ÇíÊ§úÁü•„Åß„Åç„Åù„ÅÜ„Å™„ÇâÔºâ
            if not getattr(self, "_encoding_hint_done", False) and os.name == "nt":
                setattr(self, "_encoding_hint_done", True)
                if os.environ.get("SUPPRESS_ENCODING_HINT") != "1":
                    enc = (getattr(sys.stdout, "encoding", "") or "").lower()
                    # Á∞°ÊòìÂà§ÂÆö: cp932 / ansi Á≥ª„ÅßÁµµÊñáÂ≠ó„ÅåÂê´„Åæ„Çå„Åù„ÅÜ„Å™Ë°å
                    if enc and "utf" not in enc and any(ch for ch in line if ord(ch) > 0x2600):
                        try:
                            print(
                                "[INFO] ÊñáÂ≠óÂåñ„Åë„Åô„ÇãÂ†¥Âêà„ÅØ 'chcp 65001' ÂÆüË°åÂæå„Å´ÂÜçË©¶Ë°å„Åó„Å¶„Åè„Å†„Åï„ÅÑ (SUPPRESS_ENCODING_HINT=1 „ÅßÈùûË°®Á§∫)",
                                flush=True,
                            )
                        except Exception:
                            pass
            try:
                print(line, flush=True)
                return
            except UnicodeEncodeError:
                try:
                    encoding = getattr(sys.stdout, "encoding", "") or "utf-8"
                    safe = line.encode(encoding, errors="replace").decode(
                        encoding, errors="replace"
                    )
                    print(safe, flush=True)
                    return
                except Exception:
                    pass
        except Exception:
            pass
        # ÊúÄÁµÇ„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ: ASCII ÁΩÆÊèõ
        try:
            fallback = line.encode("ascii", errors="replace").decode("ascii", errors="replace")
            print(fallback, flush=True)
        except Exception:
            pass


class RunCallbacks:
    """run_all_systems_today „Å∏Ê∏°„Åô„Ç≥„Éº„É´„Éê„ÉÉ„ÇØ„Çí„Åæ„Å®„ÇÅ„Çã„ÄÇ"""

    def __init__(
        self, logger: UILogger, progress_ui: ProgressUI, tracker: StageTracker
    ):  # noqa: E501
        self.logger = logger
        self.progress_ui = progress_ui
        self.tracker = tracker

    def ui_log(self, msg: str) -> None:
        self.logger.log(str(msg))

    def overall_progress(self, done: int, total: int, name: str) -> None:
        self.progress_ui.update(done, total, name)

    def per_system_progress(self, name: str, phase: str) -> None:
        self.tracker.update_progress(name, phase)

    def per_system_stage(
        self,
        name: str,
        value: int,
        filter_cnt: int | None = None,
        setup_cnt: int | None = None,
        cand_cnt: int | None = None,
        final_cnt: int | None = None,
    ) -> None:
        self.tracker.update_stage(
            name, value, filter_cnt, setup_cnt, cand_cnt, final_cnt
        )  # noqa: E501

    def per_system_exit(self, name: str, count: int) -> None:
        self.tracker.update_exit(name, count)

    def register_with_module(self) -> None:
        try:
            import scripts.run_all_systems_today as _run_today_mod

            # ÂÆâÂÖ®„Å™Â±ûÊÄß„Ç¢„ÇØ„Çª„ÇπÊñπÊ≥ï„Çí‰ΩøÁî®
            mod = _run_today_mod
            setattr(mod, "_PER_SYSTEM_STAGE", self.per_system_stage)
            setattr(mod, "_PER_SYSTEM_EXIT", self.per_system_exit)
            setattr(mod, "_SET_STAGE_UNIVERSE_TARGET", self.tracker.set_universe_target)
        except Exception:
            pass


@dataclass
class RunArtifacts:
    final_df: pd.DataFrame
    per_system: dict[str, pd.DataFrame]
    log_lines: list[str]
    total_elapsed: float
    stage_tracker: StageTracker
    logger: UILogger
    debug_mode: bool = False
    missing_report_path: Path | None = None
    missing_details: list[dict[str, Any]] | None = None


@dataclass
class ExitAnalysisResult:
    exits_today: pd.DataFrame
    planned: pd.DataFrame
    exit_counts: dict[str, int]
    error: str | None = None


def _indicator_requirements() -> dict[str, int]:
    """„Ç∑„Ç∞„Éä„É´Ë®àÁÆó„Åß‰ΩøÁî®„Åô„ÇãÊåáÊ®ôÊó•Êï∞„ÇíÂÆöÁæ©„Åô„Çã„ÄÇ"""

    return {
        "ROC200": int(200 * 1.1),
        "SMA25": int(25 * 1.1),
        "ATR20": int(20 * 1.1),
        "ADX7": int(7 * 1.1),
        "RETURN_6D": int(6 * 1.1),
        "Drop3D": int(3 * 1.1),
        "return_6d": int(6 * 1.1),
    }


def _rows_needed(indicator_days: dict[str, int]) -> int:
    if not indicator_days:
        return 0
    return max(indicator_days.values())


def _prepare_symbol_data(
    symbols: list[str],
    rows: int,
    logger: UILogger,
    *,
    debug_scan: bool = False,
) -> tuple[dict[str, pd.DataFrame], list[dict[str, Any]]]:
    cache_key = (tuple(symbols), rows)
    symbol_cache = st.session_state.get("today_symbol_cache")
    if (
        not debug_scan
        and isinstance(symbol_cache, dict)
        and symbol_cache.get("key") == cache_key
        and isinstance(symbol_cache.get("data"), dict)
    ):
        data_map = symbol_cache.get("data", {})
        try:
            count = len(data_map)
        except Exception:
            count = 0
        logger.log(f"üì¶ Âü∫Á§é„Éá„Éº„Çø„É≠„Éº„ÉâÂÜçÂà©Áî®: {count}/{len(symbols)}‰ª∂ (ÂâçÂõûÁµêÊûú„Çí‰ΩøÁî®)")
        return data_map, []

    logger.log(f"üì¶ Âü∫Á§é„Éá„Éº„Çø„É≠„Éº„ÉâÈñãÂßã: {len(symbols)} ÈäòÊüÑ (ÂøÖË¶ÅÊó•Êï∞‚âí{rows})")
    data_map, missing_details = _collect_symbol_data(
        symbols,
        rows=rows,
        log_fn=logger.log,
        debug_scan=debug_scan,
    )
    if not debug_scan:
        st.session_state["today_symbol_cache"] = {"key": cache_key, "data": data_map}
    return data_map, missing_details


def _save_missing_report(missing_details: list[dict[str, Any]]) -> Path | None:
    if not missing_details:
        return None
    try:
        base_dir = Path(settings.LOGS_DIR)
    except Exception:
        base_dir = Path("logs")
    target_dir = base_dir / "debug"
    try:
        target_dir.mkdir(parents=True, exist_ok=True)
    except Exception:
        pass
    try:
        timestamp = time.strftime("%Y%m%d_%H%M%S")
    except Exception:
        timestamp = str(int(time.time()))
    path = target_dir / f"rolling_cache_missing_{timestamp}.csv"
    try:
        try:
            settings2 = get_settings(create_dirs=True)
            round_dec = getattr(settings2.cache, "round_decimals", None)
        except Exception:
            round_dec = None
        try:
            out_df = round_dataframe(pd.DataFrame(missing_details), round_dec)
        except Exception:
            out_df = pd.DataFrame(missing_details)
        out_df.to_csv(path, index=False)
    except Exception:
        return None
    return path


def _store_run_results(
    final_df: pd.DataFrame, per_system: dict[str, pd.DataFrame]
) -> None:  # noqa: E501
    try:
        try:
            settings2 = get_settings(create_dirs=True)
            round_dec = getattr(settings2.cache, "round_decimals", None)
        except Exception:
            round_dec = None
        try:
            st.session_state["today_final_df"] = round_dataframe(final_df.copy(), round_dec)
        except Exception:
            st.session_state["today_final_df"] = final_df.copy()
        stored = {}
        for k, v in per_system.items():
            try:
                stored[k] = round_dataframe(v.copy(), round_dec)
            except Exception:
                stored[k] = v.copy()
        st.session_state["today_per_system"] = stored  # noqa: E501
    except Exception:
        pass


def _postprocess_results(
    final_df: pd.DataFrame, per_system: dict[str, pd.DataFrame]
) -> tuple[pd.DataFrame, dict[str, pd.DataFrame]]:
    final_df = final_df.reset_index(drop=True)
    per_system = {name: df.reset_index(drop=True) for name, df in per_system.items()}
    final_df = _sort_final_df(final_df)
    if final_df is not None and not final_df.empty:
        try:
            final_df.insert(0, "no", range(1, len(final_df) + 1))
        except Exception:
            pass
    return final_df, per_system


def _sort_final_df(final_df: pd.DataFrame) -> pd.DataFrame:
    if final_df is None or final_df.empty or "system" not in final_df.columns:
        return final_df
    try:
        tmp = final_df.copy()
        tmp["_system_no"] = (
            tmp["system"].astype(str).str.extract(r"(\d+)").fillna(0).astype(int)
        )  # noqa: E501
        sort_cols = [c for c in ["side", "_system_no"] if c in tmp.columns]
        tmp = tmp.sort_values(sort_cols, kind="stable").drop(
            columns=["_system_no"], errors="ignore"
        )
        return tmp.reset_index(drop=True)
    except Exception:
        return final_df


def _log_run_completion(
    final_df: pd.DataFrame, per_system: dict[str, pd.DataFrame], elapsed: float
) -> None:
    try:
        m, s = divmod(int(max(0, elapsed)), 60)
        final_n = 0 if final_df is None or final_df.empty else int(len(final_df))
        per_counts_lines: list[str] = []
        counts_map = {
            str(name).strip().lower(): 0 if df is None or df.empty else int(len(df))
            for name, df in per_system.items()
            if str(name).strip()
        }
        if counts_map:
            per_counts_lines = format_group_counts(counts_map)
        detail = (
            f" | Long/ShortÂà•: {', '.join(per_counts_lines)}" if per_counts_lines else ""
        )  # noqa: E501
        _get_today_logger().info(
            "‚úÖ Êú¨Êó•„ÅÆ„Ç∑„Ç∞„Éä„É´: „Ç∑„Ç∞„Éä„É´Ê§úÂá∫Âá¶ÁêÜÁµÇ‰∫Ü (ÁµåÈÅé %dÂàÜ%dÁßí, ÊúÄÁµÇÂÄôË£ú %d ‰ª∂)%s",
            m,
            s,
            final_n,
            detail,
        )
    except Exception:
        pass


def _build_per_system_logs(log_lines: list[str]) -> dict[str, list[str]]:
    per_system_logs: dict[str, list[str]] = {f"system{i}": [] for i in range(1, 8)}
    skip_keywords = (
        "üìä ÊåáÊ®ôË®àÁÆó",
        "‚è±Ô∏è „Éê„ÉÉ„ÉÅÊôÇÈñì",
        "üßÆ ÊåáÊ®ô„Éá„Éº„Çø",
        "üßÆ ÊåáÊ®ô„Éá„Éº„Çø„É≠„Éº„Éâ",
        "üßÆ ÂÖ±ÊúâÊåáÊ®ô„ÅÆÂâçË®àÁÆó",
        "üì¶ Âü∫Á§é„Éá„Éº„Çø„É≠„Éº„Éâ",
        "ÂÄôË£úÊäΩÂá∫",
        "„Ç§„É≥„Ç∏„Ç±„Éº„Çø„Éº",
        "indicator",
        "indicators",
        "batch time",
        "next batch size",
    )
    for ln in log_lines:
        try:
            if any(k in ln for k in skip_keywords):
                continue
        except Exception:
            pass
        ln_l = ln.lower()
        for i in range(1, 8):
            key = f"system{i}"
            tag_candidates = [f"[system{i}]", f" {key}:", f"{key}:", f" {key}Ôºö"]
            if any(tag in ln_l for tag in tag_candidates):
                per_system_logs[key].append(ln)
                break
    return per_system_logs


def _display_per_system_logs(per_system_logs: dict[str, list[str]]) -> None:
    if not per_system_logs:
        return
    if not any(per_system_logs[key] for key in per_system_logs):
        return
    tabs = st.tabs([f"system{i}" for i in range(1, 8)])
    for i, key in enumerate([f"system{i}" for i in range(1, 8)]):
        logs = per_system_logs.get(key, [])
        if not logs:
            continue
        with tabs[i]:
            st.text_area(
                label=f"„É≠„Ç∞Ôºà{key}Ôºâ",
                key=f"logs_{key}",
                value="\n".join(logs[-1000:]),
                height=380,
                disabled=True,
            )
            if key == "system2":
                _display_system2_filter_breakdown(logs)
            elif key == "system5":
                _display_system5_filter_breakdown(logs)


def _display_system2_filter_breakdown(logs: list[str]) -> None:
    try:
        detail_lines = [x for x in logs if ("„Éï„Ç£„É´„ÇøÂÜÖË®≥:" in x or "filter breakdown:" in x)]
        if not detail_lines:
            return
        last_line = str(detail_lines[-1])
        disp = last_line.split("] ", 1)[1] if "] " in last_line else last_line
        st.caption(disp)
    except Exception:
        pass


def _display_system5_filter_breakdown(logs: list[str]) -> None:
    try:
        detail_lines = [
            x for x in logs if ("system5ÂÜÖË®≥" in x and ("AvgVol50" in x or "avgvol50" in x))
        ]
        if not detail_lines:
            return
        last_line = str(detail_lines[-1])
        disp = last_line.split("] ", 1)[1] if "] " in last_line else last_line
        st.caption(disp)
    except Exception:
        pass


def _configure_today_logger_ui() -> None:
    try:
        mode_env = (os.environ.get("TODAY_SIGNALS_LOG_MODE") or "").strip().lower()
    except Exception:
        mode_env = ""
    sel_mode = "single" if mode_env == "single" else "dated"
    try:
        import scripts.run_all_systems_today as _run_today_mod

        _run_today_mod._configure_today_logger(mode=sel_mode)
        sel_path = getattr(_run_today_mod, "_LOG_FILE_PATH", None)
        if sel_path:
            st.caption(f"„É≠„Ç∞‰øùÂ≠òÂÖà: {sel_path}")
    except Exception:
        pass


def execute_today_signals(run_config: RunConfig) -> RunArtifacts:
    # ÂÆüË°åÈñãÂßãÊôÇ„ÅÆ„Éò„ÉÉ„ÉÄ„Éº„É°„ÉÉ„Çª„Éº„Ç∏„ÇíË°®Á§∫
    today = get_signal_target_trading_day().normalize()
    try:
        run_id = str(uuid.uuid4())[:8]
    except Exception:
        run_id = "--------"

    # ‰ªÆ„ÅÆlogger„Çí‰ΩúÊàê„Åó„Å¶„Éò„ÉÉ„ÉÄ„Éº„É°„ÉÉ„Çª„Éº„Ç∏„ÇíË°®Á§∫
    temp_start_time = time.time()
    temp_progress_ui = ProgressUI({})
    temp_logger = UILogger(temp_start_time, temp_progress_ui)

    # „Éò„ÉÉ„ÉÄ„Éº„É°„ÉÉ„Çª„Éº„Ç∏„ÅÆË°®Á§∫
    temp_logger.log(
        "####################################################################", no_timestamp=True
    )
    temp_logger.log("# üöÄüöÄüöÄ  Êú¨Êó•„ÅÆ„Ç∑„Ç∞„Éä„É´ ÂÆüË°åÈñãÂßã (Engine)  üöÄüöÄüöÄ", no_timestamp=True)

    # ÊôÇÂàª„Å®RUN-ID„ÄÅÈäòÊüÑÊï∞„ÅÆË°®Á§∫
    now_str = time.strftime("%Y-%m-%d %H:%M:%S")
    symbols_count = len(run_config.symbols) if run_config.symbols else 0
    temp_logger.log(
        f"# ‚è±Ô∏è {now_str} | ÈäòÊüÑÊï∞Ôºö{symbols_count}„ÄÄ| RUN-ID: {run_id}", no_timestamp=True
    )
    temp_logger.log(
        "####################################################################", no_timestamp=True
    )

    # Âñ∂Ê•≠Êó•„Å®Ê≥®ÊÑè‰∫ãÈ†Ö„ÅÆË°®Á§∫
    temp_logger.log(f"üìÖ ÂØæË±°Âñ∂Ê•≠Êó•ÔºàNYSEÔºâ: {today.date()}", no_timestamp=True)

    # „Éá„Éº„Çø„ÅÆÊñ∞„Åó„Åï„Çí„ÉÅ„Çß„ÉÉ„ÇØ„Åó„Å¶ÂøÖË¶Å„Å™Â†¥Âêà„ÅÆ„ÅøË≠¶Âëä„ÇíË°®Á§∫
    try:
        settings = get_settings()
        cm = CacheManager(settings)
        # SPY„Éá„Éº„Çø„Åß„Ç≠„É£„ÉÉ„Ç∑„É•„ÅÆÊñ∞„Åó„Åï„ÇíÁ¢∫Ë™ç
        spy_df = cm.read("SPY", "rolling")
        if spy_df is not None and not spy_df.empty:
            # last_cache_date„ÇíË®àÁÆó„Åô„Çã„Åü„ÇÅ„ÅÆÁ∞°Âçò„Å™ÂÆüË£Ö
            if "date" in spy_df.columns:
                last_date = pd.to_datetime(spy_df["date"]).max()
            elif spy_df.index.name == "date" or hasattr(spy_df.index, "date"):
                last_date = pd.to_datetime(spy_df.index).max()
            else:
                last_date = None

            if last_date is not None:
                last_cache_date = pd.Timestamp(last_date).normalize()
                days_behind = (today - last_cache_date).days
                if days_behind > 1:  # 1Âñ∂Ê•≠Êó•„Çà„ÇäÂè§„ÅÑÂ†¥Âêà„ÅÆ„ÅøË≠¶Âëä
                    temp_logger.log(
                        f"‚ÑπÔ∏è Ê≥®: „Ç≠„É£„ÉÉ„Ç∑„É•„Éá„Éº„Çø„Åå{days_behind}Êó•Âè§„ÅÑ„Åü„ÇÅ„ÄÅ"
                        "Áõ¥ËøëÂñ∂Ê•≠Êó•„Éô„Éº„Çπ„ÅßË®àÁÆó„Åó„Åæ„Åô„ÄÇ",
                        no_timestamp=True,
                    )
    except Exception:
        # „Ç®„É©„ÉºÊôÇ„ÅØÂæìÊù•ÈÄö„ÇäË≠¶Âëä„ÇíË°®Á§∫
        temp_logger.log(
            "‚ÑπÔ∏è Ê≥®: EODHD„ÅØÂΩìÊó•ÁµÇÂÄ§„ÅåÊú™ÂèçÊò†„ÅÆ„Åü„ÇÅ„ÄÅÁõ¥ËøëÂñ∂Ê•≠Êó•„Éô„Éº„Çπ„ÅßË®àÁÆó„Åó„Åæ„Åô„ÄÇ", no_timestamp=True
        )

    temp_logger.log("", no_timestamp=True)  # Á©∫Ë°å„ÇíËøΩÂä†

    # Êó¢Â≠ò„ÅÆÂá¶ÁêÜ„ÇíÁ∂ôÁ∂ö
    indicator_days = _indicator_requirements()
    max_days = _rows_needed(indicator_days)
    start_time = time.time()
    ui_vis_raw = st.session_state.get("ui_vis", {})
    ui_vis = ui_vis_raw if isinstance(ui_vis_raw, dict) else {}
    progress_ui = ProgressUI(ui_vis)
    stage_tracker = StageTracker(ui_vis, progress_ui)
    logger = UILogger(start_time, progress_ui)
    callbacks = RunCallbacks(logger, progress_ui, stage_tracker)
    callbacks.register_with_module()
    _configure_today_logger_ui()
    buffer_days = max(20, int(max_days * 0.15))
    rows_needed = max_days + buffer_days
    symbols_for_data = list(dict.fromkeys([*run_config.symbols, "SPY"]))
    progress_ui.set_label("ÂØæË±°Ë™≠„ÅøËæº„Åø")
    final_df: pd.DataFrame = pd.DataFrame()
    per_system: dict[str, pd.DataFrame] = {}
    debug_result: RunArtifacts | None = None
    with st.spinner("ÂÆüË°å‰∏≠... (ÁµåÈÅéÊôÇÈñìË°®Á§∫„ÅÇ„Çä)"):
        logger.log("‚ñ∂ Êú¨Êó•„ÅÆ„Ç∑„Ç∞„Éä„É´: „Ç∑„Ç∞„Éä„É´Ê§úÂá∫Âá¶ÁêÜÈñãÂßã")
        symbol_data_map, missing_details = _prepare_symbol_data(
            symbols_for_data,
            rows_needed,
            logger,
            debug_scan=run_config.scan_missing_only,
        )
        if run_config.scan_missing_only:
            total_elapsed = max(0.0, time.time() - start_time)
            report_path = _save_missing_report(missing_details)
            if missing_details:
                if report_path is not None:
                    logger.log(f"üß™ Ê¨†ÊêçÊ¥ó„ÅÑÂá∫„Åó: {len(missing_details)}‰ª∂ (CSV: {report_path})")
                else:
                    logger.log(f"üß™ Ê¨†ÊêçÊ¥ó„ÅÑÂá∫„Åó: {len(missing_details)}‰ª∂ (CSV‰øùÂ≠ò„Å´Â§±Êïó)")
            else:
                logger.log("üß™ Ê¨†ÊêçÊ¥ó„ÅÑÂá∫„Åó: Ê¨†Êêç„ÅØÊ§úÂá∫„Åï„Çå„Åæ„Åõ„Çì„Åß„Åó„Åü")
            stage_tracker.finalize_counts(pd.DataFrame(), {})
            debug_result = RunArtifacts(
                final_df=pd.DataFrame(),
                per_system={},
                log_lines=logger.log_lines,
                total_elapsed=total_elapsed,
                stage_tracker=stage_tracker,
                logger=logger,
                debug_mode=True,
                missing_report_path=report_path,
                missing_details=missing_details,
            )
        else:
            result = compute_today_signals(
                run_config.symbols,
                capital_long=run_config.capital_long,
                capital_short=run_config.capital_short,
                save_csv=run_config.save_csv,
                notify=run_config.notify,
                csv_name_mode=run_config.csv_name_mode,
                log_callback=callbacks.ui_log,
                progress_callback=callbacks.overall_progress,
                per_system_progress=callbacks.per_system_progress,
                symbol_data=symbol_data_map,
                parallel=run_config.run_parallel,
            )
            # ÂÆâÂÖ®„Å´ÁµêÊûú„ÇíËß£Èáà
            if isinstance(result, (tuple, list)) and len(result) == 2:
                result_pair: Sequence[Any] = tuple(result)
                maybe_df, maybe_system = result_pair
                if isinstance(maybe_df, pd.DataFrame) and isinstance(maybe_system, dict):
                    final_df = maybe_df
                    per_system = maybe_system
                else:
                    actual_types = (
                        f"df={type(maybe_df).__name__}, system={type(maybe_system).__name__}"
                    )
                    logger.log(f"‚ö†Ô∏è compute_today_signals „ÅÆÊàª„ÇäÂÄ§Âûã„Åå‰∏çÊ≠£: {actual_types}")
            else:
                result_info = f"type={type(result).__name__}, len={len(result) if hasattr(result, '__len__') else 'N/A'}"
                logger.log(f"‚ö†Ô∏è compute_today_signals „ÅÆÊàª„ÇäÂÄ§ÊßãÈÄ†„Åå‰∏çÊ≠£: {result_info}")

    if debug_result is not None:
        return debug_result
    total_elapsed = max(0.0, time.time() - start_time)
    stage_tracker.finalize_counts(final_df, per_system)
    _store_run_results(final_df, per_system)
    return RunArtifacts(
        final_df=final_df,
        per_system=per_system,
        log_lines=logger.log_lines,
        total_elapsed=total_elapsed,
        stage_tracker=stage_tracker,
        logger=logger,
    )


def analyze_exit_candidates(paper_mode: bool) -> ExitAnalysisResult:
    """ÁèæÂú®‰øùÊúâ‰∏≠„Éù„Ç∏„Ç∑„Éß„É≥„ÅÆÊâã‰ªïËàû„ÅÑ‰∫àÂÆö„ÇíÊé®ÂÆö„Åô„Çã„ÄÇ

    ÂΩπÂâ≤:
      1. ‰øùÊúâ„Éù„Ç∏„Ç∑„Éß„É≥ÂèñÂæó
      2. „Ç®„É≥„Éà„É™„ÉºÊó•Ë£úÂÆåÔºà„É≠„Éº„Ç´„É´‚Üí‰∏çË∂≥ÂàÜ Alpaca ÂèñÂæó‚Üí‰øùÂ≠òÔºâ
      3. „Ç∑„Çπ„ÉÜ„É†Âà§ÂÆö & Strategy „Ç§„É≥„Çπ„Çø„É≥„ÇπÁîüÊàê
      4. „Çπ„Éà„É©„ÉÜ„Ç∏„Éº exit „É≠„Ç∏„ÉÉ„ÇØ„ÇíÁî®„ÅÑÊú¨Êó•/Â∞ÜÊù• exit „ÇíÂàÜÈ°û
    """

    exits_today_rows: list[dict[str, Any]] = []
    planned_rows: list[dict[str, Any]] = []
    exit_counts: dict[str, int] = {f"system{i}": 0 for i in range(1, 8)}
    try:
        client_tmp = ba.get_client(paper=paper_mode)
        try:
            positions = list(client_tmp.get_all_positions())
        except Exception:
            positions = []

        # 1) „Ç®„É≥„Éà„É™„ÉºÊó•„Éû„ÉÉ„ÉóË™≠„ÅøËæº„Åø
        raw_entry_map = load_entry_dates()
        entry_map: dict[str, str] = {}
        for k, v in raw_entry_map.items():
            try:
                entry_map[str(k).upper()] = str(v)
            except Exception:
                continue

        # 2) ‰∏çË∂≥„Ç®„É≥„Éà„É™„ÉºÊó•„ÅÆË£úÂÆå
        missing = [
            str(getattr(p, "symbol", "")).upper()
            for p in positions
            if str(getattr(p, "symbol", "")).upper()
            and str(getattr(p, "symbol", "")).upper() not in entry_map
        ]
        if missing:
            try:
                fetched = fetch_entry_dates_from_alpaca(client_tmp, missing)
            except Exception:
                fetched = None
            if fetched:
                for sym, ts in fetched.items():
                    if sym not in entry_map:
                        try:
                            entry_map[sym] = pd.Timestamp(ts).strftime("%Y-%m-%d")
                        except Exception:
                            continue
                try:
                    save_entry_dates(entry_map)
                except Exception:
                    pass

        symbol_system_map = _load_symbol_system_map(Path("data/symbol_system_map.json"))
        latest_trading_day = _latest_trading_day()
        strategy_classes = STRATEGY_CLASS_MAP

        # 3) ÂêÑ„Éù„Ç∏„Ç∑„Éß„É≥Ëß£Êûê
        for pos in positions:
            result = _evaluate_position_for_exit(
                pos,
                entry_map,
                symbol_system_map,
                latest_trading_day,
                strategy_classes,
            )
            if result is None:
                continue
            system, _pos_side, _qty, exit_when, row_base, exit_today = result
            when_val = str(exit_when or "").strip()
            when_lower = when_val.lower()
            when_display = when_lower or when_val
            if exit_today:
                exit_counts[system] = exit_counts.get(system, 0) + 1
                if when_lower == "tomorrow_open":
                    planned_rows.append(row_base | {"when": when_display})
                else:
                    exits_today_rows.append(row_base | {"when": when_display})
            else:
                planned_rows.append(row_base | {"when": when_display})

        exits_today_df = pd.DataFrame(exits_today_rows)
        planned_df = pd.DataFrame(planned_rows)
        return ExitAnalysisResult(
            exits_today=exits_today_df, planned=planned_df, exit_counts=exit_counts
        )
    except Exception as exc:
        return ExitAnalysisResult(
            exits_today=pd.DataFrame(),
            planned=pd.DataFrame(),
            exit_counts=exit_counts,
            error=str(exc),
        )


def _load_symbol_system_map(path: Path) -> dict[str, str]:
    try:
        if path.exists():
            data = json.loads(path.read_text(encoding="utf-8"))
            if isinstance(data, dict):
                return {str(k).upper(): str(v).lower() for k, v in data.items()}
    except Exception:
        pass
    return {}


def _latest_trading_day() -> pd.Timestamp | None:
    calendar_day: pd.Timestamp | None = None
    try:
        calendar_day = get_latest_nyse_trading_day()
    except Exception:
        calendar_day = None

    price_day: pd.Timestamp | None = None
    try:
        spy_df = load_price("SPY", cache_profile="rolling")
        if spy_df is not None and not spy_df.empty:
            price_raw = pd.Timestamp(spy_df.index[-1])
            try:
                price_raw = price_raw.tz_localize(None)
            except (TypeError, ValueError, AttributeError):
                try:
                    price_raw = price_raw.tz_convert(None)
                except Exception:
                    pass
            price_day = pd.Timestamp(price_raw).normalize()
    except Exception:
        price_day = None

    if calendar_day is not None and price_day is not None:
        return max(calendar_day, price_day)
    return calendar_day or price_day


STRATEGY_CLASS_MAP: dict[str, Callable[[], Any]] = {
    "system1": System1Strategy,
    "system2": System2Strategy,
    "system3": System3Strategy,
    "system4": System4Strategy,
    "system5": System5Strategy,
    "system6": System6Strategy,
}


## ‰∫íÊèõÁî®Èñ¢Êï∞„ÅØÂâäÈô§ÔºàÁõ¥Êé• STRATEGY_CLASS_MAP „ÇíÂèÇÁÖß„Åô„ÇãÂÆüË£Ö„Å∏ÁßªË°åÊ∏à„ÅøÔºâ


def _evaluate_position_for_exit(
    pos: Any,
    entry_map: dict[str, Any],
    symbol_system_map: dict[str, str],
    latest_trading_day: pd.Timestamp | None,
    strategy_classes: dict[str, Callable[[], Any]],
) -> tuple[str, str, int, str, dict[str, Any], bool] | None:
    try:
        sym = str(getattr(pos, "symbol", "")).upper()
        if not sym:
            return None
        qty = int(abs(float(getattr(pos, "qty", 0)) or 0))
        if qty <= 0:
            return None
        pos_side = str(getattr(pos, "side", "")).lower()
        system = symbol_system_map.get(sym, "").lower()
        if not system:
            if sym == "SPY" and pos_side == "short":
                system = "system7"
            else:
                return None
        if system == "system7":
            return None
        entry_date_str = entry_map.get(sym)
        if not entry_date_str:
            return None
        entry_dt = pd.to_datetime(entry_date_str).normalize()
        df_price = load_price(sym, cache_profile="full")
        if df_price is None or df_price.empty:
            return None
        df = df_price.copy(deep=False)
        if "Date" in df.columns:
            df.index = pd.Index(pd.to_datetime(df["Date"]).dt.normalize())
        else:
            df.index = pd.Index(pd.to_datetime(df.index).normalize())
        if latest_trading_day is None and len(df.index) > 0:
            latest_trading_day = pd.to_datetime(df.index[-1]).normalize()
        entry_idx = _find_entry_index(df.index, entry_dt)
        if entry_idx < 0:
            return None
        strategy_cls = strategy_classes.get(system)
        if strategy_cls is None:
            return None
        strategy = strategy_cls()
        prev_close = float(df.iloc[int(max(0, entry_idx - 1))]["Close"])
        entry_price, stop_price = _entry_and_stop_prices(
            system, strategy, df, entry_idx, prev_close
        )
        if entry_price is None or stop_price is None:
            return None
        _apply_strategy_state(system, strategy, df, entry_idx, prev_close)
        # exit_price„Çí‰ΩøÁî®„Åô„Çã„Çà„ÅÜ„Å´‰øÆÊ≠£
        exit_price, exit_date = strategy.compute_exit(
            df, int(entry_idx), float(entry_price), float(stop_price)
        )
        # exit_price„Çí„Éó„É≠„Éë„ÉÜ„Ç£„Å´ËøΩÂä†
        today_norm = pd.to_datetime(df.index[-1]).normalize()
        if latest_trading_day is not None:
            today_norm = latest_trading_day
        is_today_exit, when = decide_exit_schedule(system, exit_date, today_norm)
        row_base = {
            "symbol": sym,
            "qty": qty,
            "position_side": pos_side,
            "system": system,
            "exit_price": exit_price,  # ËøΩÂä†
        }
        return system, pos_side, qty, when, row_base, is_today_exit
    except Exception:
        return None


def _find_entry_index(index: pd.Index, entry_dt: pd.Timestamp) -> int:
    try:
        if entry_dt in index:
            arr = index.get_indexer([entry_dt])
        else:
            arr = index.get_indexer([entry_dt], method="bfill")
        if len(arr) and arr[0] >= 0:
            return int(arr[0])
    except Exception:
        pass
    return -1


def _entry_and_stop_prices(
    system: str,
    strategy: Any,
    df: pd.DataFrame,
    entry_idx: int,
    prev_close: float,
) -> tuple[float | None, float | None]:
    try:
        if system == "system1":
            entry_price = float(df.iloc[int(entry_idx)]["Open"])
            atr20 = float(df.iloc[int(max(0, entry_idx - 1))]["ATR20"])
            stop_mult = float(strategy.config.get("stop_atr_multiple", 5.0))
            return entry_price, entry_price - stop_mult * atr20
        if system == "system2":
            entry_price = float(df.iloc[int(entry_idx)]["Open"])
            atr = float(df.iloc[int(max(0, entry_idx - 1))]["ATR10"])
            stop_mult = float(strategy.config.get("stop_atr_multiple", 3.0))
            return entry_price, entry_price + stop_mult * atr
        if system == "system6":
            ratio = float(strategy.config.get("entry_price_ratio_vs_prev_close", 1.05))
            entry_price = round(prev_close * ratio, 2)
            atr = float(df.iloc[int(max(0, entry_idx - 1))]["ATR10"])
            stop_mult = float(strategy.config.get("stop_atr_multiple", 3.0))
            return entry_price, entry_price + stop_mult * atr
        if system == "system3":
            ratio = float(strategy.config.get("entry_price_ratio_vs_prev_close", 0.93))
            entry_price = round(prev_close * ratio, 2)
            atr = float(df.iloc[int(max(0, entry_idx - 1))]["ATR10"])
            stop_mult = float(strategy.config.get("stop_atr_multiple", 2.5))
            return entry_price, entry_price - stop_mult * atr
        if system == "system4":
            entry_price = float(df.iloc[int(entry_idx)]["Open"])
            atr40 = float(df.iloc[int(max(0, entry_idx - 1))]["ATR40"])
            stop_mult = float(strategy.config.get("stop_atr_multiple", 1.5))
            return entry_price, entry_price - stop_mult * atr40
        if system == "system5":
            ratio = float(strategy.config.get("entry_price_ratio_vs_prev_close", 0.97))
            entry_price = round(prev_close * ratio, 2)
            atr = float(df.iloc[int(max(0, entry_idx - 1))]["ATR10"])
            stop_mult = float(strategy.config.get("stop_atr_multiple", 3.0))
            return entry_price, entry_price - stop_mult * atr
    except Exception:
        return None, None
    return None, None


def _apply_strategy_state(
    system: str,
    strategy: Any,
    df: pd.DataFrame,
    entry_idx: int,
    prev_close: float,
) -> None:
    if system == "system5":
        try:
            atr = float(df.iloc[int(max(0, entry_idx - 1))]["ATR10"])
            strategy._last_entry_atr = atr
        except Exception:
            pass
    if system in {"system3", "system5", "system6"}:
        try:
            strategy._last_prev_close = prev_close
        except Exception:
            pass


def render_exit_candidates_section(
    trade_options: TradeOptions,
    stage_tracker: StageTracker,
    logger: UILogger,
    notify: bool,
) -> ExitAnalysisResult:
    st.subheader("‰ªäÊó•„ÅÆÊâã‰ªïËàû„ÅÑÂÄôË£úÔºàMOCÔºâ")
    result = analyze_exit_candidates(trade_options.paper_mode)
    if result.error:
        st.warning(f"Êâã‰ªïËàû„ÅÑÂÄôË£ú„ÅÆÊé®ÂÆö„Å´Â§±Êïó„Åó„Åæ„Åó„Åü: {result.error}")
        return result
    _display_exit_orders_table(result, trade_options, stage_tracker, logger, notify)
    _display_planned_exits_section(result)  # trade_optionsÂºïÊï∞„ÇíÂâäÈô§
    return result


def _display_exit_orders_table(
    result: ExitAnalysisResult,
    trade_options: TradeOptions,
    stage_tracker: StageTracker,
    logger: UILogger,
    notify: bool,
) -> None:
    if result.exits_today.empty:
        st.info("Êú¨Êó•Â§ßÂºï„Åë„Åß„ÅÆÊâã‰ªïËàû„ÅÑÂÄôË£ú„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ")
        return
    st.dataframe(result.exits_today, use_container_width=True)
    stage_tracker.apply_exit_counts(result.exit_counts)
    if st.button("Êú¨Êó•ÂàÜ„ÅÆÊâã‰ªïËàû„ÅÑÊ≥®ÊñáÔºàMOCÔºâ„ÇíÈÄÅ‰ø°"):
        from common.alpaca_order import submit_exit_orders_df

        res = submit_exit_orders_df(
            result.exits_today,
            paper=trade_options.paper_mode,
            tif="CLS",
            retries=int(trade_options.retries),
            delay=float(max(0.0, trade_options.delay)),
            log_callback=logger.log,
            notify=notify,
        )
        if res is not None and not res.empty:
            st.dataframe(res, use_container_width=True)


def _display_planned_exits_section(result: ExitAnalysisResult) -> None:  # trade_optionsÂºïÊï∞„ÇíÂâäÈô§
    if result.planned.empty:
        return
    st.caption("ÊòéÊó•Áô∫Ê≥®„Åô„ÇãÊâã‰ªïËàû„ÅÑË®àÁîªÔºà‰øùÂ≠ò‚Üí„Çπ„Ç±„Ç∏„É•„Éº„É©„ÅåÂÆüË°åÔºâ")
    st.dataframe(result.planned, use_container_width=True)
    planned_rows = [
        {str(k): v for k, v in row.items()} for row in result.planned.to_dict(orient="records")
    ]
    _auto_save_planned_exits(planned_rows, show_success=False)
    if st.button("Ë®àÁîª„Çí‰øùÂ≠òÔºàJSONLÔºâ"):
        _auto_save_planned_exits(planned_rows, show_success=True)
    st.write("")
    dry_run_plan = st.checkbox(
        "„Éâ„É©„Ç§„É©„É≥Ôºà‰∫àÁ¥ÑÈÄÅ‰ø°„Çí„ÉÜ„Çπ„Éà„Å®„Åó„Å¶ÂÆüË°åÔºâ",
        value=True,
        key="planned_exits_dry_run",
    )
    col_open, col_close = st.columns(2)
    with col_open:
        if st.button("‚è±Ô∏è ÂØÑ„ÇäÔºàOPGÔºâ‰∫àÁ¥Ñ„Çí‰ªä„Åô„ÅêÈÄÅ‰ø°", key="run_scheduler_open"):
            _run_planned_exit_scheduler("open", dry_run_plan)
    with col_close:
        if st.button("‚è±Ô∏è Âºï„ÅëÔºàCLSÔºâ‰∫àÁ¥Ñ„Çí‰ªä„Åô„ÅêÈÄÅ‰ø°", key="run_scheduler_close"):
            _run_planned_exit_scheduler("close", dry_run_plan)


def _auto_save_planned_exits(
    planned_rows: list[dict[str, Any]], show_success: bool
) -> None:  # noqa: E501
    plan_path = Path("data/planned_exits.jsonl")
    try:
        plan_path.parent.mkdir(parents=True, exist_ok=True)
        with plan_path.open("w", encoding="utf-8") as f:
            for row in planned_rows:
                f.write(json.dumps(row, ensure_ascii=False) + "\n")
        if show_success:
            st.success(f"‰øùÂ≠ò„Åó„Åæ„Åó„Åü: {plan_path}")
        else:
            st.caption(f"Ë®àÁîª„Çí‰øùÂ≠ò„Åó„Åæ„Åó„Åü: {plan_path}")
    except Exception as exc:
        if show_success:
            st.error(f"‰øùÂ≠ò„Å´Â§±Êïó: {exc}")
        else:
            st.error(f"Ë®àÁîª„ÅÆ‰øùÂ≠ò„Å´Â§±Êïó: {exc}")


def _run_planned_exit_scheduler(kind: str, dry_run: bool) -> None:
    try:
        from schedulers.next_day_exits import submit_planned_exits as _run_sched

        df_exec = _run_sched(kind, dry_run=dry_run)
        if df_exec is not None and not df_exec.empty:
            st.success(
                "ÂØÑ„ÇäÔºàOPGÔºâÂàÜ„ÅÆ‰∫àÁ¥ÑÈÄÅ‰ø°„ÇíÂÆüË°å„Åó„Åæ„Åó„Åü„ÄÇÁµêÊûú„ÇíË°®Á§∫„Åó„Åæ„Åô„ÄÇ"
                if kind == "open"
                else "Âºï„ÅëÔºàCLSÔºâÂàÜ„ÅÆ‰∫àÁ¥ÑÈÄÅ‰ø°„ÇíÂÆüË°å„Åó„Åæ„Åó„Åü„ÄÇÁµêÊûú„ÇíË°®Á§∫„Åó„Åæ„Åô„ÄÇ"
            )
            st.dataframe(df_exec, use_container_width=True)
        else:
            st.info(
                "ÂØÑ„ÇäÔºàOPGÔºâÂØæË±°„ÅÆ‰∫àÁ¥Ñ„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ"
                if kind == "open"
                else "Âºï„ÅëÔºàCLSÔºâÂØæË±°„ÅÆ‰∫àÁ¥Ñ„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ"
            )
    except Exception as exc:
        label = "ÂØÑ„ÇäÔºàOPGÔºâ" if kind == "open" else "Âºï„ÅëÔºàCLSÔºâ"
        st.error(f"{label}‰∫àÁ¥Ñ„ÅÆÂÆüË°å„Å´Â§±Êïó: {exc}")


def render_today_signals_results(
    artifacts: RunArtifacts,
    run_config: RunConfig,
    trade_options: TradeOptions,
) -> None:
    if artifacts.debug_mode:
        _render_missing_debug_results(artifacts)
        return
    final_df, per_system = _postprocess_results(
        artifacts.final_df, artifacts.per_system
    )  # noqa: E501
    artifacts.stage_tracker.finalize_counts(final_df, per_system)
    _show_total_elapsed(artifacts.total_elapsed)
    _log_run_completion(final_df, per_system, artifacts.total_elapsed)
    per_system_logs = _build_per_system_logs(artifacts.logger.log_lines)
    _display_per_system_logs(per_system_logs)
    render_exit_candidates_section(
        trade_options,
        artifacts.stage_tracker,
        artifacts.logger,
        run_config.notify,
    )
    _render_final_signals_section(
        final_df,
        per_system,
        run_config,
        trade_options,
        artifacts.logger,
    )
    _render_system_details(per_system, artifacts.stage_tracker, per_system_logs)
    _render_previous_results_section()
    _render_previous_run_logs(artifacts.log_lines)


def _render_missing_debug_results(artifacts: RunArtifacts) -> None:
    st.subheader("üß™ Ê¨†ÊêçÊ¥ó„ÅÑÂá∫„Åó„É¢„Éº„Éâ„ÅÆÁµêÊûú")
    details = artifacts.missing_details or []
    if details:
        st.write(f"Ê§úÂá∫„Åï„Çå„ÅüÈäòÊüÑ: {len(details)}‰ª∂")
        try:
            df_details = pd.DataFrame(details)
        except Exception:
            df_details = None
        if df_details is not None and not df_details.empty:
            st.dataframe(df_details, use_container_width=True)
        else:
            st.json(details)
    else:
        st.success("„É≠„Éº„É™„É≥„Ç∞„Ç≠„É£„ÉÉ„Ç∑„É•„ÅÆÊ¨†Êêç„ÅØÊ§úÂá∫„Åï„Çå„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ")
    report_path = artifacts.missing_report_path
    if report_path:
        path_obj = Path(report_path)
        st.info(f"„É¨„Éù„Éº„Éà: {path_obj}")
        try:
            data_bytes = path_obj.read_bytes()
        except Exception:
            data_bytes = None
        if data_bytes:
            st.download_button(
                "Ê¨†Êêç„É¨„Éù„Éº„Éà„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ",
                data=data_bytes,
                file_name=path_obj.name,
                mime="text/csv",
                key=f"missing_report_{int(time.time() * 1000)}",
            )
    st.info("„Åì„ÅÆ„É¢„Éº„Éâ„Åß„ÅØÂü∫Á§é„Éá„Éº„Çø„ÅÆÊ¨†ÊêçÁ¢∫Ë™ç„ÅÆ„Åø„ÇíÂÆüÊñΩ„Åó„Åæ„Åó„Åü„ÄÇ„Ç∑„Ç∞„Éä„É´Ë®àÁÆó„ÅØË°å„Å£„Å¶„ÅÑ„Åæ„Åõ„Çì„ÄÇ")
    _render_previous_results_section()
    _render_previous_run_logs(artifacts.log_lines)


def _show_total_elapsed(total_elapsed: float) -> None:
    total_elapsed = max(0.0, float(total_elapsed))
    m, s = divmod(int(total_elapsed), 60)
    st.info(f"Á∑èÁµåÈÅéÊôÇÈñì: {m}ÂàÜ{s}Áßí")


def _render_final_signals_section(
    final_df: pd.DataFrame,
    per_system: dict[str, pd.DataFrame],
    run_config: RunConfig,
    trade_options: TradeOptions,
    logger: UILogger,
) -> None:
    st.subheader("ÊúÄÁµÇÈÅ∏ÂÆöÈäòÊüÑ")
    if final_df is None or final_df.empty:
        st.info("Êú¨Êó•„ÅÆ„Ç∑„Ç∞„Éä„É´„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ")
        return
    _render_final_summary(final_df)
    st.dataframe(final_df, use_container_width=True)
    _render_skip_reports()
    _download_final_csv(final_df)
    st.session_state["today_shown_this_run"] = True
    if run_config.save_csv:
        _auto_save_final_results(final_df, per_system, run_config)
    if trade_options.do_trade:
        _execute_auto_trading(
            final_df,
            trade_options,
            run_config,
            logger,
        )


def _render_final_summary(final_df: pd.DataFrame) -> None:
    summary_lines: list[str] = []
    try:
        if "system" in final_df.columns:
            system_series = final_df["system"].astype(str).str.strip().str.lower()
            counts_map = system_series.value_counts().to_dict()
            values_map: dict[str, float] = {}
            if "position_value" in final_df.columns:
                values_series = (
                    final_df.assign(_system=system_series)[
                        ["_system", "position_value"]
                    ]  # noqa: E501
                    .groupby("_system")["position_value"]
                    .sum()
                )
                values_map = values_series.to_dict()
            if counts_map:
                if values_map:
                    summary_lines = format_group_counts_and_values(
                        counts_map, values_map
                    )  # noqa: E501
                else:
                    summary_lines = format_group_counts(counts_map)
    except Exception:
        summary_lines = []
    if summary_lines:
        font_css = (
            "font-family: 'Noto Sans JP', 'Meiryo', sans-serif; "
            "font-size: 1rem; letter-spacing: 0.02em;"
        )
        html_summary = " / ".join(summary_lines)
        st.markdown(
            f'<div style="{font_css}">„Çµ„Éû„É™„ÉºÔºàLong/ShortÂà•Ôºâ: {html_summary}</div>',
            unsafe_allow_html=True,
        )


def _render_skip_reports() -> None:
    try:
        settings2 = get_settings(create_dirs=True)
        results_dir = Path(getattr(settings2.outputs, "results_csv_dir", "results_csv"))
        skip_files = []
        for i in range(1, 8):
            name = f"system{i}"
            fp = results_dir / f"skip_summary_{name}.csv"
            if fp.exists() and fp.is_file():
                skip_files.append((name, fp))
        if skip_files:
            with st.expander("üß™ „Éá„Éº„Çø„Çπ„Ç≠„ÉÉ„Éó/„Ç∑„Éß„Éº„Éà‰∏çÂèØ„ÅÆÂÜÖË®≥CSVÔºàÊú¨Êó•Ôºâ", expanded=False):
                _render_skip_file_group(skip_files, "skip")
            detail_files = []
            for i in range(1, 8):
                name = f"system{i}"
                fpd = results_dir / f"skip_details_{name}.csv"
                if fpd.exists() and fpd.is_file():
                    detail_files.append((name, fpd))
            if detail_files:
                st.markdown("---")
                st.caption("„Çπ„Ç≠„ÉÉ„ÉóË©≥Á¥∞Ôºàsymbol√óreasonÔºâ")
                _render_skip_file_group(detail_files, "skipdet")
            shortable_files = []
            for i in (2, 6):
                name = f"system{i}"
                fp2 = results_dir / f"shortability_excluded_{name}.csv"
                if fp2.exists() and fp2.is_file():
                    shortable_files.append((name, fp2))
            if shortable_files:
                st.markdown("---")
                st.caption("„Ç∑„Éß„Éº„Éà‰∏çÂèØ„ÅßÈô§Â§ñ„Åï„Çå„ÅüÈäòÊüÑÔºàsystem2/6Ôºâ")
                _render_skip_file_group(shortable_files, "short_exc")
    except Exception:
        pass


def _render_skip_file_group(files: list[tuple[str, Path]], key_prefix: str) -> None:
    for name, path in files:
        cols = st.columns([4, 1])
        with cols[0]:
            try:
                df_skip = pd.read_csv(path)
            except Exception:
                df_skip = None
            st.caption(f"{name}: {path.name}")
            if df_skip is not None and not df_skip.empty:
                st.dataframe(df_skip, use_container_width=True)
            else:
                st.write("(Á©∫) ÂÜÖË®≥ÊÉÖÂ†±„ÅØË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ")
        with cols[1]:
            try:
                data_bytes = path.read_bytes()
            except Exception:
                data_bytes = None
            if data_bytes:
                st.download_button(
                    label=f"{name} CSV",
                    data=data_bytes,
                    file_name=path.name,
                    mime="text/csv",
                    key=f"dl_{key_prefix}_{name}_{int(time.time() * 1000)}",
                )


def _download_final_csv(final_df: pd.DataFrame) -> None:
    try:
        settings2 = get_settings(create_dirs=True)
        round_dec = getattr(settings2.cache, "round_decimals", None)
    except Exception:
        round_dec = None
    try:
        out_df = round_dataframe(final_df, round_dec)
    except Exception:
        out_df = final_df
    csv = out_df.to_csv(index=False).encode("utf-8")
    st.download_button(
        "ÊúÄÁµÇCSV„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ",
        data=csv,
        file_name="today_signals_final.csv",
        on_click=_reset_shown_flag,
    )


def _auto_save_final_results(
    final_df: pd.DataFrame,
    per_system: dict[str, pd.DataFrame],
    run_config: RunConfig,
) -> None:
    try:
        settings2 = get_settings(create_dirs=True)
        sig_dir = Path(settings2.outputs.signals_dir)
        sig_dir.mkdir(parents=True, exist_ok=True)
        now = datetime.now()
        ts = now.strftime("%Y-%m-%d")
        if run_config.csv_name_mode == "datetime":
            ts = now.strftime("%Y-%m-%d_%H%M")
        elif run_config.csv_name_mode == "runid":
            rid = st.session_state.get("last_run_id") or "RUN"
            ts = f"{now.strftime('%Y-%m-%d')}_{rid}"
        fp = sig_dir / f"today_signals_{ts}.csv"
        try:
            settings2 = get_settings(create_dirs=True)
            round_dec = getattr(settings2.cache, "round_decimals", None)
        except Exception:
            round_dec = None
        try:
            out_df = round_dataframe(final_df, round_dec)
        except Exception:
            out_df = final_df
        out_df.to_csv(fp, index=False)
        st.caption(f"Ëá™Âãï‰øùÂ≠ò: {fp}")
        for name, df in per_system.items():
            try:
                if df is None or df.empty:
                    continue
                fp_sys = sig_dir / f"signals_{name}_{ts}.csv"
                try:
                    out_df = round_dataframe(df, round_dec)
                except Exception:
                    out_df = df
                out_df.to_csv(fp_sys, index=False)
                st.caption(f"Ëá™Âãï‰øùÂ≠ò: {fp_sys}")
            except Exception as exc:
                st.warning(f"{name} „ÅÆËá™Âãï‰øùÂ≠ò„Å´Â§±Êïó: {exc}")
    except Exception as exc:
        st.warning(f"Ëá™Âãï‰øùÂ≠ò„Å´Â§±Êïó: {exc}")


def _execute_auto_trading(
    final_df: pd.DataFrame,
    trade_options: TradeOptions,
    run_config: RunConfig,
    logger: UILogger,
) -> None:
    st.divider()
    st.subheader("AlpacaËá™ÂãïÁô∫Ê≥®ÁµêÊûú")
    system_order_type = {
        "system1": "market",
        "system3": "market",
        "system4": "market",
        "system5": "market",
        "system2": "limit",
        "system6": "limit",
        "system7": "limit",
    }
    results_df = submit_orders_df(
        final_df,
        paper=trade_options.paper_mode,
        order_type=None,
        system_order_type=system_order_type,
        tif="DAY",
        retries=int(trade_options.retries),
        delay=float(max(0.0, trade_options.delay)),
        log_callback=logger.log,
        notify=run_config.notify,
    )
    if results_df is not None and not results_df.empty:
        st.dataframe(results_df, use_container_width=True)
        if trade_options.poll_status and any(
            results_df["order_id"].fillna("").astype(str)
        ):  # noqa: E501
            _poll_order_status(results_df, trade_options)
    if trade_options.update_bp_after:
        _update_buying_power(trade_options)


def _poll_order_status(results_df: pd.DataFrame, trade_options: TradeOptions) -> None:
    st.info("Ê≥®ÊñáÁä∂Ê≥Å„Çí10ÁßíÈñì„Éù„Éº„É™„É≥„Ç∞„Åó„Åæ„Åô...")
    try:
        client = ba.get_client(paper=trade_options.paper_mode)
    except Exception:
        client = None
    if client is None:
        return
    order_ids = [str(oid) for oid in results_df["order_id"].tolist() if oid]
    end = time.time() + 10
    last: dict[str, Any] = {}
    while time.time() < end:
        status_map = ba.get_orders_status_map(client, order_ids)
        if status_map != last:
            if status_map:
                st.caption("Ê≥®ÊñáÁä∂Ê≥Å„ÇíÊõ¥Êñ∞„Åó„Åæ„Åó„ÅüÔºàË©≥Á¥∞„ÅØ„É≠„Ç∞ÂèÇÁÖßÔºâ")
            last = status_map
        time.sleep(1.0)


def _update_buying_power(trade_options: TradeOptions) -> None:
    try:
        client2 = ba.get_client(paper=trade_options.paper_mode)
        acct = client2.get_account()
        bp_raw = getattr(acct, "buying_power", None)
        if bp_raw is None:
            bp_raw = getattr(acct, "cash", None)
        if bp_raw is not None:
            bp = float(bp_raw)
            st.session_state["today_cap_long"] = round(bp / 2.0, 2)
            st.session_state["today_cap_short"] = round(bp / 2.0, 2)
            st.success(
                "Á¥ÑÂÆöÂèçÊò†Âæå„ÅÆË≥áÈáë‰ΩôÂäõ„ÅßLong/Short„ÇíÂÜçË®≠ÂÆö„Åó„Åæ„Åó„Åü: "
                f"${st.session_state['today_cap_long']} / "
                f"${st.session_state['today_cap_short']}"
            )
        else:
            st.warning("AlpacaÂè£Â∫ßÊÉÖÂ†±: buying_power/cash„ÅåÂèñÂæó„Åß„Åç„Åæ„Åõ„ÇìÔºàÊõ¥Êñ∞„Å™„ÅóÔºâ")
    except Exception as exc:
        st.error(f"‰ΩôÂäõ„ÅÆËá™ÂãïÊõ¥Êñ∞„Å´Â§±Êïó: {exc}")


def _render_system_details(
    per_system: dict[str, pd.DataFrame],
    stage_tracker: StageTracker,
    per_system_logs: dict[str, list[str]] | None = None,
) -> None:
    with st.expander("„Ç∑„Çπ„ÉÜ„É†Âà•Ë©≥Á¥∞"):
        settings_local = get_settings(create_dirs=True)
        results_dir = Path(getattr(settings_local.outputs, "results_csv_dir", "results_csv"))
        shortable_excluded_map = {}
        for i in (2, 6):
            name = f"system{i}"
            fp = results_dir / f"shortability_excluded_{name}.csv"
            if fp.exists() and fp.is_file():
                try:
                    df_exc = pd.read_csv(fp)
                    if df_exc is not None and not df_exc.empty:
                        shortable_excluded_map[name] = set(df_exc["symbol"].astype(str).str.upper())
                except Exception:
                    pass
        system_order = [f"system{i}" for i in range(1, 8)]
        for name in system_order:
            st.markdown(f"#### {name}")
            display_metrics = stage_tracker.get_display_metrics(name)
            # Line length fix - split formatted string
            metrics_parts = [
                f"Tgt {StageTracker._format_value(display_metrics.get('target'))}",
                f"FILpass {StageTracker._format_value(display_metrics.get('filter'))}",
                f"STUpass {StageTracker._format_value(display_metrics.get('setup'))}",
                f"TRDlist {stage_tracker._format_trdlist(display_metrics.get('cand'))}",
                f"Entry {StageTracker._format_value(display_metrics.get('entry'))}",
                f"Exit {StageTracker._format_value(display_metrics.get('exit'))}",
            ]
            metrics_line = "  ".join(metrics_parts)
            st.caption(metrics_line)
            df = per_system.get(name)
            if df is None or df.empty:
                # Try to extract explicit zero-reason from per-system logs if available
                reason_text: str | None = None
                try:
                    if per_system_logs and name in per_system_logs:
                        logs = per_system_logs.get(name) or []
                        for ln in reversed(logs):
                            if not ln:
                                continue
                            m = re.search(r"ÂÄôË£ú0‰ª∂ÁêÜÁî±[:Ôºö]\s*(.+)$", ln)
                            if m:
                                reason_text = m.group(1).strip()
                                break
                            m2 = re.search(r"„Çª„ÉÉ„Éà„Ç¢„ÉÉ„Éó‰∏çÊàêÁ´ã[:Ôºö]\s*(.+)$", ln)
                            if m2:
                                reason_text = m2.group(1).strip()
                                break
                except Exception:
                    reason_text = None

                st.write("(Á©∫) ÂÄôË£ú„ÅØ0‰ª∂„Åß„Åô„ÄÇ")
                if reason_text:
                    st.info(f"ÂÄôË£ú0‰ª∂ÁêÜÁî±: {reason_text}")
                continue
            df_disp = df.copy()
            side_type = None
            if name in LONG_SYSTEMS:
                side_type = "long"
            elif name in SHORT_SYSTEMS:
                side_type = "short"
            if side_type and "side" in df_disp.columns:
                mask = df_disp["side"].str.lower() != side_type
                if mask.any():
                    fill_cols = [
                        col
                        for col in df_disp.columns
                        if col not in {"symbol", "side", "system"}  # noqa: E501
                    ]
                    if fill_cols:
                        df_disp.loc[:, fill_cols] = df_disp.loc[:, fill_cols].astype(
                            "object"
                        )  # noqa: E501
                        df_disp.loc[mask, fill_cols] = "-"
            if name in shortable_excluded_map:
                excluded_syms = shortable_excluded_map[name]
                if excluded_syms:
                    st.caption(f"üö´ „Ç∑„Éß„Éº„Éà‰∏çÂèØ„ÅßÈô§Â§ñ: {len(excluded_syms)}‰ª∂")
                    st.write(
                        f"<span style='color:red;font-size:0.95em;'>"
                        f"„Ç∑„Éß„Éº„Éà‰∏çÂèØ: {', '.join(sorted(excluded_syms)[:10])}"
                        f"{' ...' if len(excluded_syms) > 10 else ''}"  # noqa: E501
                        f"</span>",
                        unsafe_allow_html=True,
                    )
            st.dataframe(df_disp, use_container_width=True)


def _render_previous_results_section() -> None:
    try:
        if (not st.session_state.get("today_shown_this_run", False)) and (
            "today_final_df" in st.session_state
        ):
            prev_df = st.session_state.get("today_final_df")
            if prev_df is not None and not prev_df.empty:
                st.subheader("ÂâçÂõû„ÅÆÊúÄÁµÇÈÅ∏ÂÆöÈäòÊüÑÔºàÂÜçË°®Á§∫Ôºâ")
                st.dataframe(prev_df, use_container_width=True)
                try:
                    settings2 = get_settings(create_dirs=True)
                    round_dec = getattr(settings2.cache, "round_decimals", None)
                except Exception:
                    round_dec = None
                try:
                    prev_out = round_dataframe(prev_df, round_dec)
                except Exception:
                    prev_out = prev_df
                csv_prev = prev_out.to_csv(index=False).encode("utf-8")
                st.download_button(
                    "ÊúÄÁµÇCSV„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„ÉâÔºàÂâçÂõûÔºâ",
                    data=csv_prev,
                    file_name="today_signals_final_prev.csv",
                    key="download_prev_final",
                    on_click=_reset_shown_flag,
                )
                prev_per = st.session_state.get("today_per_system", {})
                if isinstance(prev_per, dict):
                    with st.expander("ÂâçÂõû„ÅÆ„Ç∑„Çπ„ÉÜ„É†Âà•CSV", expanded=False):
                        for name, df in prev_per.items():
                            if df is None or df.empty:
                                continue
                            st.markdown(f"#### {name}")
                            st.dataframe(df, use_container_width=True)
    except Exception:
        pass


def _render_previous_run_logs(log_lines: list[str]) -> None:
    prev_msgs = [line for line in log_lines if line and ("(ÂâçÂõûÁµêÊûú) system" in line)]
    if not prev_msgs:
        return

    def _parse_prev_line(ln: str) -> tuple[str, int, str, str]:
        ts = ln.split("] ", 1)[0].strip("[")
        m = re.search(r"\(ÂâçÂõûÁµêÊûú\) (system\d+):\s*(\d+)", ln)
        sys = m.group(1) if m else "system999"
        cnt = int(m.group(2)) if m else 0
        return sys, cnt, ts, ln

    parsed = [_parse_prev_line(x) for x in prev_msgs]
    order = {f"system{i}": i for i in range(1, 8)}
    parsed.sort(key=lambda t: order.get(t[0], 999))
    lines_sorted = [f"{p[2]} | {p[0]}: {p[1]}‰ª∂\n{p[3]}" for p in parsed]
    with st.expander("ÂâçÂõûÁµêÊûúÔºàsystemÂà•Ôºâ", expanded=False):
        st.text("\n\n".join(lines_sorted))


def _log_and_notify(
    message: str,
    notifier: Callable[[str], None] | None,
    log_callback: Callable[[str], None] | None,
    level: int = logging.INFO,
):
    """Log to both logger and optional callbacks."""

    _get_today_logger().log(level, message)
    if notifier:
        try:
            notifier(message)
        except Exception as e:
            _get_today_logger().warning("Notifier failed: %s", e)
    if log_callback:
        try:
            log_callback(message)
        except Exception as e:
            _get_today_logger().warning("Log callback failed: %s", e)


# =============================================================================
# „É°„Ç§„É≥ UI ÂÆüË°åÈÉ®ÂàÜ
# =============================================================================

with st.sidebar:
    st.header("„É¶„Éã„Éê„Éº„Çπ")
    universe: list[str] = []
    try:
        import common.universe as univ
        from common.symbol_universe import build_symbol_universe_from_settings

        logger = logging.getLogger("today_signals.ui")
        universe = build_symbol_universe_from_settings(settings, logger=logger)
    except Exception as exc:
        universe = []
        st.warning(f"NASDAQ/EODHD„ÅÆÈäòÊüÑÂèñÂæó„Å´Â§±Êïó„Åó„Åæ„Åó„Åü: {exc}")

    if not universe:
        try:
            import common.universe as univ

            universe = univ.load_universe_file()
        except Exception:
            pass

    if not universe:
        try:
            import common.universe as univ

            universe = univ.build_universe_from_cache(limit=None)
            univ.save_universe_file(universe)
        except Exception:
            universe = []

    all_syms = universe

    # ‰ªªÊÑè„ÅÆ‰ª∂Êï∞„Åß„É¶„Éã„Éê„Éº„Çπ„ÇíÂà∂Èôê„Åô„Çã„ÉÜ„Çπ„ÉàÁî®„Ç™„Éó„Ç∑„Éß„É≥
    limit_max = max(1, len(all_syms))
    test_limit = st.number_input(
        "ÈäòÊüÑÊï∞ (0„ÅØÂÖ®ÈäòÊüÑ)",
        min_value=0,
        max_value=limit_max,
        value=0,
        step=1,
    )
    syms = all_syms[: int(test_limit)] if test_limit else all_syms

    # „Çª„ÉÉ„Ç∑„Éß„É≥Áä∂ÊÖã„Å´‰øùÂ≠ò
    st.session_state["universe_symbols"] = syms

    st.write(f"ÈäòÊüÑÊï∞: {len(syms)}")
    st.write(", ".join(syms[:10]) + (" ..." if len(syms) > 10 else ""))

    # AlpacaÊú™Á¥ÑÂÆöÊ≥®ÊñáË°®Á§∫
    st.header("AlpacaÊ≥®ÊñáÁä∂Ê≥Å")

    # „Éá„Éê„ÉÉ„Ç∞ÊÉÖÂ†±„ÅÆË°®Á§∫
    with st.expander("üîß „Éá„Éê„ÉÉ„Ç∞ÊÉÖÂ†±"):
        st.write("broker_alpaca „É¢„Ç∏„É•„Éº„É´Â±ûÊÄß:")
        ba_attrs = [attr for attr in dir(ba) if not attr.startswith("_")]
        for attr in sorted(ba_attrs):
            if attr == "get_open_orders":
                st.write(f"‚úÖ {attr}: {type(getattr(ba, attr))}")
            elif callable(getattr(ba, attr)):
                st.write(f"üìù {attr}: {type(getattr(ba, attr))}")
            else:
                st.write(f"üì¶ {attr}: {type(getattr(ba, attr))}")

        st.write(f"get_open_orders Â≠òÂú®Á¢∫Ë™ç: {hasattr(ba, 'get_open_orders')}")
        if hasattr(ba, "get_open_orders"):
            st.write(f"get_open_orders Âûã: {type(ba.get_open_orders)}")
            st.write(f"get_open_orders docstring: {ba.get_open_orders.__doc__}")

    if st.button("üìã Êú™Á¥ÑÂÆöÊ≥®Êñá„ÇíË°®Á§∫"):
        try:
            paper_mode = st.session_state.get("paper_mode", True)

            # „Éá„Éê„ÉÉ„Ç∞: „É¢„Ç∏„É•„Éº„É´Áä∂ÊÖã„ÅÆÁ¢∫Ë™ç
            st.info(f"broker_alpaca „É¢„Ç∏„É•„Éº„É´: {ba}")
            st.info(f"get_open_orders Â≠òÂú®: {hasattr(ba, 'get_open_orders')}")

            if not hasattr(ba, "get_open_orders"):
                st.error("get_open_orders Èñ¢Êï∞„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì")
                available_funcs = [
                    attr
                    for attr in dir(ba)
                    if callable(getattr(ba, attr)) and not attr.startswith("_")
                ]
                st.write("Âà©Áî®ÂèØËÉΩ„Å™Èñ¢Êï∞:")
                st.write(available_funcs)
                st.stop()

            client = ba.get_client(paper=paper_mode)
            orders = ba.get_open_orders(client)
            if orders:
                orders_data = []
                for order in orders:
                    orders_data.append(
                        {
                            "Ê≥®ÊñáID": order.id,
                            "ÈäòÊüÑ": order.symbol,
                            "„Çµ„Ç§„Éâ": order.side,
                            "Êï∞Èáè": order.qty,
                            "Ê≥®Êñá‰æ°Ê†º": getattr(order, "limit_price", "Market"),
                            "Ê≥®Êñá„Çø„Ç§„Éó": order.order_type,
                            "Áä∂Ê≥Å": order.status,
                            "‰ΩúÊàêÊó•ÊôÇ": order.created_at,
                        }
                    )
                orders_df = pd.DataFrame(orders_data)
                st.dataframe(orders_df, use_container_width=True)
            else:
                st.info("Êú™Á¥ÑÂÆöÊ≥®Êñá„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì")
        except Exception as e:
            st.error(f"Ê≥®ÊñáÂèñÂæó„Ç®„É©„Éº: {e}")
            st.error(f"„Ç®„É©„ÉºË©≥Á¥∞: {type(e).__name__}")
            import traceback

            st.code(traceback.format_exc())

    st.header("Ë≥áÁî£")
    # „Éá„Éï„Ç©„É´„ÉàÂÄ§„ÇíË®≠ÂÆö
    if "today_cap_long" not in st.session_state:
        st.session_state["today_cap_long"] = 10000.0
    if "today_cap_short" not in st.session_state:
        st.session_state["today_cap_short"] = 10000.0

    # AlpacaË≥áÁî£ÂèñÂæó„Éú„Çø„É≥„ÇíËøΩÂä†
    if st.button("üí∞ Alpaca„Åã„ÇâÁèæÂú®„ÅÆË≥áÁî£„ÇíÂèñÂæó"):
        try:
            # Êé•Á∂öÂâç„ÅÆ‰∫ãÂâç„ÉÅ„Çß„ÉÉ„ÇØ
            api_key = os.environ.get("APCA_API_KEY_ID")
            api_secret = os.environ.get("APCA_API_SECRET_KEY")

            if not api_key or not api_secret:
                st.error("‚ùå Alpaca APIË™çË®ºÊÉÖÂ†±„ÅåË®≠ÂÆö„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì")
                st.info("Áí∞Â¢ÉÂ§âÊï∞ APCA_API_KEY_ID „Å® APCA_API_SECRET_KEY „ÇíË®≠ÂÆö„Åó„Å¶„Åè„Å†„Åï„ÅÑ")
            else:
                # „Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÊé•Á∂ö„ÉÜ„Çπ„Éà
                with st.spinner("Alpaca„Çµ„Éº„Éê„Éº„Å´Êé•Á∂ö‰∏≠..."):
                    client = ba.get_client(paper=st.session_state.get("paper_mode", True))
                    acct = client.get_account()

                equity = getattr(acct, "equity", None)
                cash = getattr(acct, "cash", None)
                buying_power = getattr(acct, "buying_power", None)

                if equity is not None:
                    equity_val = float(equity)
                    st.success(f"‚úÖ Á∑èË≥áÁî£: ${equity_val:,.2f}")
                if cash is not None:
                    cash_val = float(cash)
                    st.info(f"üíµ ÁèæÈáëÊÆãÈ´ò: ${cash_val:,.2f}")
                if buying_power is not None:
                    bp_val = float(buying_power)
                    st.info(f"üöÄ Ë≤∑‰ªò‰ΩôÂäõ: ${bp_val:,.2f}")

                    # Ë≤∑‰ªò‰ΩôÂäõ„ÇíÂçäÂàÜ„Åö„Å§„É≠„É≥„Ç∞„Éª„Ç∑„Éß„Éº„Éà„Å´ÈÖçÂàÜ
                    half_bp = round(bp_val / 2.0, 2)
                    st.session_state["today_cap_long"] = half_bp
                    st.session_state["today_cap_short"] = half_bp
                    st.success("Ë≥áÈáëÈÖçÂàÜ„ÇíÊõ¥Êñ∞„Åó„Åæ„Åó„Åü:")
                    st.success(f"„É≠„É≥„Ç∞ `${half_bp:,.2f}` / „Ç∑„Éß„Éº„Éà `${half_bp:,.2f}`")
                else:
                    st.warning("Ë≤∑‰ªò‰ΩôÂäõ„ÅåÂèñÂæó„Åß„Åç„Åæ„Åõ„Çì„Åß„Åó„Åü")

        except Exception as exc:
            ERROR_MSG = str(exc)
            if "getaddrinfo failed" in ERROR_MSG or "Failed to resolve" in ERROR_MSG:
                st.error("üåê „Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÊé•Á∂ö„Ç®„É©„Éº")
                st.error("- „Ç§„É≥„Çø„Éº„Éç„ÉÉ„ÉàÊé•Á∂ö„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ")
                st.error("- DNS„Çµ„Éº„Éê„ÉºË®≠ÂÆö„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ")
                st.error("- „Éï„Ç°„Ç§„Ç¢„Ç¶„Ç©„Éº„É´/„Éó„É≠„Ç≠„Ç∑Ë®≠ÂÆö„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ")
                with st.expander("Ë©≥Á¥∞„Ç®„É©„ÉºÊÉÖÂ†±"):
                    st.code(ERROR_MSG)
            elif "HTTPSConnectionPool" in ERROR_MSG:
                st.error("üîí HTTPSÊé•Á∂ö„Ç®„É©„Éº")
                st.error("- SSLË®ºÊòéÊõ∏„ÅÆÂïèÈ°å„ÅÆÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„Åô")
                st.error("- „Éó„É≠„Ç≠„Ç∑Ë®≠ÂÆö„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ")
                with st.expander("Ë©≥Á¥∞„Ç®„É©„ÉºÊÉÖÂ†±"):
                    st.code(ERROR_MSG)
            elif "401" in ERROR_MSG or "403" in ERROR_MSG:
                st.error("üîë APIË™çË®º„Ç®„É©„Éº")
                st.error("- API „Ç≠„Éº„Å®„Ç∑„Éº„ÇØ„É¨„ÉÉ„Éà„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ")
                st.error("- API„Ç≠„Éº„ÅÆÊ®©Èôê„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ")
            else:
                st.error(f"‚ùå AlpacaË≥áÁî£ÂèñÂæó„Ç®„É©„Éº: {ERROR_MSG}")
                st.info("üí° „Ç™„Éï„É©„Ç§„É≥Áí∞Â¢É„Åß„ÅØÊâãÂãï„ÅßË≥áÈáë„ÇíË®≠ÂÆö„Åó„Å¶„Åè„Å†„Åï„ÅÑ")

    col1, col2 = st.columns(2)
    with col1:
        cap_long = st.number_input(
            "„É≠„É≥„Ç∞Ë≥áÊú¨ (USD)",
            min_value=0.0,
            step=100.0,
            key="today_cap_long",
        )
    with col2:
        cap_short = st.number_input(
            "„Ç∑„Éß„Éº„ÉàË≥áÊú¨ (USD)",
            min_value=0.0,
            step=100.0,
            key="today_cap_short",
        )

    st.header("„Ç™„Éó„Ç∑„Éß„É≥")
    save_csv = st.checkbox("CSV„Éï„Ç°„Ç§„É´„Çí‰øùÂ≠ò", value=True, key="save_csv")

    # CSV„Éï„Ç°„Ç§„É´Âêç„ÅÆÂΩ¢ÂºèÈÅ∏ÊäûÔºàdate/datetime/runidÔºâ
    st.session_state.setdefault("csv_name_mode", "date")
    csv_name_mode = st.selectbox(
        "CSV„Éï„Ç°„Ç§„É´Âêç",
        options=["date", "datetime", "runid"],
        index=["date", "datetime", "runid"].index(
            str(st.session_state.get("csv_name_mode", "date"))
        ),
        help="date=YYYY-MM-DD / datetime=YYYY-MM-DD_HHMM / runid=YYYY-MM-DD_RUNID",
        key="csv_name_mode",
    )

    # Êó¢ÂÆö„Åß‰∏¶ÂàóÂÆüË°å„ÇíONÔºàWindows„Åß„ÇÇÊúâÂäπÂåñÔºâ
    import platform

    is_windows = platform.system().lower().startswith("win")
    RUN_PARALLEL_DEFAULT = True
    run_parallel = st.checkbox(
        "‰∏¶ÂàóÂÆüË°åÔºà„Ç∑„Çπ„ÉÜ„É†Ê®™Êñ≠Ôºâ", value=RUN_PARALLEL_DEFAULT, key="run_parallel"
    )

    st.header("„Éá„Éê„ÉÉ„Ç∞")
    scan_missing_only = st.checkbox(
        "üß™ Ê¨†ÊêçÊ¥ó„ÅÑÂá∫„Åó„É¢„Éº„ÉâÔºà„É≠„Éº„É™„É≥„Ç∞„Ç≠„É£„ÉÉ„Ç∑„É•Ôºâ",
        key="today_scan_missing_only",
        help="rolling „Ç≠„É£„ÉÉ„Ç∑„É•„Åã„Çâ„ÅÆË™≠„ÅøËæº„ÅøÊôÇ„Å´Ê¨†Êêç„ÇíÊ§úÂá∫„Åó„ÄÅCSV„Å´Êõ∏„ÅçÂá∫„Åó„Å¶ÁµÇ‰∫Ü„Åó„Åæ„Åô„ÄÇ",
    )
    if scan_missing_only:
        st.caption("‚Äª „Åì„ÅÆ„É¢„Éº„Éâ„Åß„ÅØ„Ç∑„Ç∞„Éä„É´Ë®àÁÆó„ÇíË°å„ÅÑ„Åæ„Åõ„Çì„ÄÇÊ¨†Êêç„É¨„Éù„Éº„Éà„ÅÆ„ÅøÂá∫Âäõ„Åó„Åæ„Åô„ÄÇ")

    # ÈÄöÁü•ÔºàSlack Bot TokenÔºâË®≠ÂÆöÔºà„ÉÅ„É£„É≥„Éç„É´ÊåáÂÆö„Éï„Ç©„Éº„É†„ÅØÂªÉÊ≠¢Ôºâ
    st.header("ÈÄöÁü•Ë®≠ÂÆöÔºàSlack Bot TokenÔºâ")
    st.session_state.setdefault("use_slack_notify", True)
    use_slack_notify = st.checkbox(
        "SlackÈÄöÁü•„ÇíÊúâÂäπÂåñÔºàBot TokenÔºâ",
        key="use_slack_notify",
        help="Áí∞Â¢ÉÂ§âÊï∞ SLACK_BOT_TOKEN „ÅåË®≠ÂÆöÊ∏à„Åø„Åß„ÅÇ„ÇãÂâçÊèêÔºàÈÄöÁü•ÂÖà„ÅØÊó¢ÂÆöÂÄ§„Çí‰ΩøÁî®Ôºâ„ÄÇ",
    )
    # Á∞°Êòì„Éò„É´„Çπ„ÉÅ„Çß„ÉÉ„ÇØË°®Á§∫
    try:
        has_token = bool(os.environ.get("SLACK_BOT_TOKEN", "").strip())
        st.caption("„Éà„Éº„ÇØ„É≥: " + ("Ê§úÂá∫Ê∏à„Åø" if has_token else "Êú™Ë®≠ÂÆöÔºà.env„ÇíË®≠ÂÆö„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºâ"))
    except Exception:
        pass

    # ‰∏¶ÂàóÂÆüË°å„ÅÆË©≥Á¥∞Ë®≠ÂÆö„ÅØÂâäÈô§ÔºàÂàùÊúü„Éá„Éï„Ç©„É´„ÉàÊåôÂãï„Å´Êàª„ÅôÔºâ
    st.header("AlpacaËá™ÂãïÁô∫Ê≥®")
    paper_mode = st.checkbox("„Éö„Éº„Éë„Éº„Éà„É¨„Éº„Éâ„Çí‰ΩøÁî®", value=True, key="paper_mode")
    retries = st.number_input("„É™„Éà„É©„Ç§ÂõûÊï∞", min_value=0, max_value=5, value=2, key="retries")
    delay = st.number_input(
        "Áô∫Ê≥®ÈñìÈöî (Áßí)", min_value=0.0, max_value=10.0, value=1.0, step=0.1, key="delay"
    )
    poll_status = st.checkbox("Ê≥®ÊñáÁä∂Ê≥Å„Çí„Éù„Éº„É™„É≥„Ç∞", value=False, key="poll_status")
    do_trade = st.checkbox("ÂÆüÈöõ„Å´Áô∫Ê≥®„Åô„Çã", value=False, key="do_trade")
    update_bp_after = st.checkbox("Á¥ÑÂÆöÂæå„Å´‰ΩôÂäõ„ÇíÊõ¥Êñ∞", value=False, key="update_bp_after")

# „É°„Ç§„É≥ÂÆüË°åÈÉ®ÂàÜ
# „Éá„Éï„Ç©„É´„ÉàÂÄ§„ÇíË®≠ÂÆöÔºà„Çµ„Ç§„Éâ„Éê„Éº„ÅåÊú™ÂÆüË°å„ÅÆÂ†¥ÂêàÔºâ
syms = st.session_state.get("universe_symbols", [])
cap_long = st.session_state.get("today_cap_long", 10000.0)
cap_short = st.session_state.get("today_cap_short", 10000.0)
save_csv = st.session_state.get("save_csv", True)
csv_name_mode = st.session_state.get("csv_name_mode", "date")
use_slack_notify = st.session_state.get("use_slack_notify", True)
run_parallel = st.session_state.get("run_parallel", True)
scan_missing_only = st.session_state.get("today_scan_missing_only", False)
paper_mode = st.session_state.get("paper_mode", True)
retries = st.session_state.get("retries", 2)
delay = st.session_state.get("delay", 1.0)
poll_status = st.session_state.get("poll_status", False)
do_trade = st.session_state.get("do_trade", False)
update_bp_after = st.session_state.get("update_bp_after", False)

run_config = RunConfig(
    symbols=syms,
    capital_long=float(cap_long),
    capital_short=float(cap_short),
    save_csv=save_csv,
    csv_name_mode=csv_name_mode,
    notify=use_slack_notify,
    run_parallel=run_parallel,
    scan_missing_only=scan_missing_only,
)

trade_options = TradeOptions(
    paper_mode=bool(paper_mode),
    retries=int(retries),
    delay=float(delay),
    poll_status=bool(poll_status),
    do_trade=bool(do_trade),
    update_bp_after=bool(update_bp_after),
)

# Ë°®Á§∫Âà∂Âæ°„ÅØÂõ∫ÂÆöÔºà„ÉÅ„Çß„ÉÉ„ÇØ„Éú„ÉÉ„ÇØ„Çπ„ÅØÂªÉÊ≠¢Ôºâ
st.session_state["ui_vis"] = {
    "overall_progress": True,
    "per_system_progress": True,
    "data_load_progress_lines": True,
    "previous_results": True,
    "system_details": True,
}

st.subheader("‰øùÊúâ„Éù„Ç∏„Ç∑„Éß„É≥„Å®Âà©Áõä‰øùË≠∑Âà§ÂÆö")

if st.button("üîç Alpaca„Åã„Çâ‰øùÊúâ„Éù„Ç∏„Ç∑„Éß„É≥ÂèñÂæó"):
    try:
        client = ba.get_client(paper=paper_mode)
        positions = client.get_all_positions()
        st.session_state["positions_df"] = evaluate_positions(positions)
        st.success("„Éù„Ç∏„Ç∑„Éß„É≥„ÇíÂèñÂæó„Åó„Åæ„Åó„Åü")
    except Exception as e:
        st.error(f"„Éù„Ç∏„Ç∑„Éß„É≥ÂèñÂæó„Ç®„É©„Éº: {e}")

if "positions_df" in st.session_state:
    positions_df = st.session_state["positions_df"]
    if not positions_df.empty:
        try:
            summary_table = _build_position_summary_table(positions_df)
            if not summary_table.empty:
                st.caption("‰øùÊúâ„Éù„Ç∏„Ç∑„Éß„É≥ÔºàSystem √ó SideÂà•Ôºâ")
                st.dataframe(summary_table, use_container_width=True)
        except Exception:
            pass

        # Ë°®Á§∫Áî®„Å´„Ç´„É©„É†„ÇíÊó•Êú¨Ë™ûÂåñ
        df_disp = positions_df.copy()
        rename_map = {
            "symbol": "ÈäòÊüÑ",
            "system": "„Ç∑„Çπ„ÉÜ„É†",
            "side": "„Çµ„Ç§„Éâ",
            "qty": "Êï∞Èáè",
            "entry_date": "ÂèñÂæóÊó•",
            "holding_days": "‰øùÊúâÊó•Êï∞",
            "avg_entry_price": "Âπ≥ÂùáÂèñÂæóÂçò‰æ°",
            "current_price": "ÁèæÂú®ÂÄ§",
            "unrealized_pl": "Âê´„ÅøÊêçÁõä",
            "unrealized_plpc_percent": "Âê´„ÅøÊêçÁõäÁéá(%)",
            "judgement": "Âà§ÂÆö",
            "next_action": "Ê¨°„ÅÆ„Ç¢„ÇØ„Ç∑„Éß„É≥ÁõÆÂÆâ",
            "rule_summary": "Âà©Á¢∫/ÊêçÂàá„Çä„É´„Éº„É´Ê¶ÇË¶Å",
        }
        df_disp = df_disp.rename(columns=rename_map)
        display_cols = [
            "ÈäòÊüÑ",
            "„Ç∑„Çπ„ÉÜ„É†",
            "„Çµ„Ç§„Éâ",
            "Êï∞Èáè",
            "ÂèñÂæóÊó•",
            "‰øùÊúâÊó•Êï∞",
            "Âπ≥ÂùáÂèñÂæóÂçò‰æ°",
            "ÁèæÂú®ÂÄ§",
            "Âê´„ÅøÊêçÁõä",
            "Âê´„ÅøÊêçÁõäÁéá(%)",
            "Âà§ÂÆö",
            "Ê¨°„ÅÆ„Ç¢„ÇØ„Ç∑„Éß„É≥ÁõÆÂÆâ",
            "Âà©Á¢∫/ÊêçÂàá„Çä„É´„Éº„É´Ê¶ÇË¶Å",
        ]
        df_disp = df_disp[[col for col in display_cols if col in df_disp.columns]]
        st.dataframe(df_disp, use_container_width=True)

        # ÊâãÂãïÊâã‰ªïËàû„ÅÑÊ©üËÉΩ
        st.subheader("üéØ ÊâãÂãïÊâã‰ªïËàû„ÅÑ")
        st.caption("ÈÅ∏Êäû„Åó„ÅüÈäòÊüÑ„ÇíÊâãÂãï„ÅßÊâã‰ªïËàû„ÅÑÊ≥®Êñá„Åó„Åæ„Åô")

        # Êâã‰ªïËàû„ÅÑÂØæË±°„ÅÆÈÅ∏Êäû
        if not positions_df.empty:
            symbols_list = positions_df["symbol"].tolist()
            selected_symbols: list[str] = st.multiselect(
                "Êâã‰ªïËàû„ÅÑ„Åô„ÇãÈäòÊüÑ„ÇíÈÅ∏Êäû:", options=symbols_list, key="manual_exit_symbols"
            )

            if selected_symbols:
                exit_type = st.selectbox(
                    "Êâã‰ªïËàû„ÅÑ„Çø„Ç§„Éó:",
                    ["MOC (Â§ßÂºï„Åë)", "OPG (ÂØÑ„Çä‰ªò„Åç)", "Market (ÊàêË°å)"],
                    key="exit_type",
                )

                col1, col2 = st.columns(2)
                with col1:
                    if st.button("üöÄ ÈÅ∏ÊäûÈäòÊüÑ„ÅÆÊâã‰ªïËàû„ÅÑÊ≥®Êñá„ÇíÈÄÅ‰ø°", type="primary"):
                        try:
                            # ÈÅ∏Êäû„Åï„Çå„ÅüÈäòÊüÑ„ÅÆ„Éù„Ç∏„Ç∑„Éß„É≥ÊÉÖÂ†±„ÇíÂèñÂæó
                            selected_positions = positions_df[
                                positions_df["symbol"].isin(selected_symbols)
                            ].copy()

                            # Êâã‰ªïËàû„ÅÑÊ≥®Êñá„ÅÆÂÆüË°å
                            exit_orders = []
                            for _, row in selected_positions.iterrows():
                                exit_orders.append(
                                    {
                                        "symbol": row["symbol"],
                                        "side": (
                                            "sell" if str(row["side"]).lower() == "long" else "buy"
                                        ),
                                        "qty": abs(float(row["qty"])),
                                        "order_type": (
                                            "market" if "Market" in exit_type else "limit"
                                        ),
                                        "time_in_force": (
                                            "cls"
                                            if "MOC" in exit_type
                                            else ("opg" if "OPG" in exit_type else "day")
                                        ),
                                    }
                                )

                            if exit_orders:
                                exit_df = pd.DataFrame(exit_orders)
                                results = submit_orders_df(
                                    exit_df,
                                    paper=paper_mode,
                                    tif="DAY",
                                    retries=int(retries),
                                    delay=float(delay),
                                )

                                if results is not None and not results.empty:
                                    st.success(
                                        f"{len(selected_symbols)}ÈäòÊüÑ„ÅÆÊâã‰ªïËàû„ÅÑÊ≥®Êñá„ÇíÈÄÅ‰ø°„Åó„Åæ„Åó„Åü"
                                    )
                                    st.dataframe(results, use_container_width=True)
                                else:
                                    st.warning("Ê≥®ÊñáÈÄÅ‰ø°ÁµêÊûú„ÅåÂèñÂæó„Åß„Åç„Åæ„Åõ„Çì„Åß„Åó„Åü")

                        except Exception as e:
                            st.error(f"Êâã‰ªïËàû„ÅÑÊ≥®Êñá„Ç®„É©„Éº: {e}")

                with col2:
                    if st.button("üìä Êâã‰ªïËàû„ÅÑÂΩ±Èüø„Çí‰∫ãÂâçÁ¢∫Ë™ç"):
                        if selected_symbols:
                            selected_positions = positions_df[
                                positions_df["symbol"].isin(selected_symbols)
                            ].copy()
                            total_pl = selected_positions["unrealized_pl"].astype(float).sum()
                            st.info(f"ÈÅ∏ÊäûÈäòÊüÑ„ÅÆÂêàË®àÂê´„ÅøÊêçÁõä: ${total_pl:,.2f}")
                            st.dataframe(
                                selected_positions[
                                    ["symbol", "side", "qty", "unrealized_pl", "judgement"]
                                ],
                                use_container_width=True,
                            )

if st.button("‚ñ∂ Êú¨Êó•„ÅÆ„Ç∑„Ç∞„Éä„É´ÂÆüË°å", type="primary"):
    artifacts = execute_today_signals(run_config)
    render_today_signals_results(artifacts, run_config, trade_options)
else:
    _render_previous_results_section()
