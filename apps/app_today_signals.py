from __future__ import annotations

import importlib
import json
import logging
import os
import re
import sys
import time
import uuid
from collections.abc import Callable, Sequence
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from datetime import datetime, timezone, tzinfo
from pathlib import Path
from threading import Lock
from typing import TYPE_CHECKING, Any

try:
    from zoneinfo import ZoneInfo

    def get_zoneinfo(name: str) -> tzinfo:
        return ZoneInfo(name)

except ImportError:
    # Python < 3.9 or Windows without zoneinfo, use UTC as fallback
    def get_zoneinfo(name: str) -> tzinfo:
        _ = name
        return timezone.utc


import pandas as pd
import streamlit as st
from streamlit.runtime.scriptrunner import get_script_run_ctx

# ãƒšãƒ¼ã‚¸è¨­å®šã‚’æœ€åˆã«å®Ÿè¡Œ
st.set_page_config(page_title="æœ¬æ—¥ã®ã‚·ã‚°ãƒŠãƒ«", layout="wide")

# sys.pathã‚’æ­£ã—ãè¨­å®šã—ã¦ã‹ã‚‰import
try:
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãŒsys.pathã«ãªã„å ´åˆã®äº‹å‰å‡¦ç†
    project_root = Path(__file__).parent.parent.resolve()
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))

    # scriptsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚‚è¿½åŠ 
    scripts_dir = project_root / "scripts"
    if str(scripts_dir) not in sys.path:
        sys.path.insert(0, str(scripts_dir))
except Exception:
    pass

from common import broker_alpaca as ba
from common.alpaca_order import submit_orders_df
from common.cache_format import round_dataframe
from common.cache_manager import CacheManager
from common.data_loader import load_price
from common.exit_planner import decide_exit_schedule
from common.notifier import create_notifier
from common.position_age import (
    fetch_entry_dates_from_alpaca,
    load_entry_dates,
    save_entry_dates,
)
from common.profit_protection import evaluate_positions
from common.stage_metrics import (
    DEFAULT_SYSTEM_ORDER,
    GLOBAL_STAGE_METRICS,
    StageMetricsStore,
    StageSnapshot,
)
from common.system_groups import format_group_counts, format_group_counts_and_values
from common.today_signals import LONG_SYSTEMS, SHORT_SYSTEMS
from common.today_signals import run_all_systems_today as compute_today_signals
from common.utils_spy import get_latest_nyse_trading_day, get_signal_target_trading_day
from config.settings import get_settings
from strategies.system1_strategy import System1Strategy
from strategies.system2_strategy import System2Strategy
from strategies.system3_strategy import System3Strategy
from strategies.system4_strategy import System4Strategy
from strategies.system5_strategy import System5Strategy
from strategies.system6_strategy import System6Strategy

# æ¡ä»¶ä»˜ãã‚¤ãƒ³ãƒãƒ¼ãƒˆ - alpaca.trading.requests ã¯å®Ÿè¡Œæ™‚ã®ã¿å¿…è¦
AlpacaTradingRequests: Any | None = None


def _import_alpaca_requests():
    """Runtime-safe importer for `alpaca.trading.requests`.

    Returns the module or None if not importable.
    """
    try:
        return importlib.import_module("alpaca.trading.requests")
    except ImportError:
        return None


# å®Ÿè¡Œæ™‚ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è©¦ã¿ã‚‹
if not TYPE_CHECKING:
    AlpacaTradingRequests = _import_alpaca_requests()


def _running_in_streamlit() -> bool:
    try:
        if get_script_run_ctx(suppress_warning=True) is not None:
            return True
    except Exception:
        pass
    try:
        flag = (os.environ.get("STREAMLIT_SERVER_ENABLED") or "").strip().lower()
        if flag in {"1", "true", "yes"}:
            return True
    except Exception:
        pass
    try:
        argv_text = " ".join(sys.argv).lower()
        if "streamlit" in argv_text:
            return True
    except Exception:
        pass
    return False


_IS_STREAMLIT_RUNTIME = _running_in_streamlit()

if not _IS_STREAMLIT_RUNTIME:
    if __name__ == "__main__":
        print(
            "ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯Streamlitã§å®Ÿè¡Œã—ã¦ãã ã•ã„: `streamlit run apps/dashboards/app_today_signals.py`"
        )
        raise SystemExit

try:
    # Streamlit ã®å®Ÿè¡Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæœ‰ç„¡ã‚’åˆ¤å®šï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰å¤–ã‹ã‚‰ã® UI å‘¼ã³å‡ºã—ã‚’é˜²ãï¼‰
    def _has_st_ctx() -> bool:
        if not _IS_STREAMLIT_RUNTIME:
            return False
        try:
            return get_script_run_ctx() is not None
        except Exception:
            return False

except Exception:

    def _has_st_ctx() -> bool:
        return _IS_STREAMLIT_RUNTIME


# Streamlit checkbox ã®é‡è¤‡IDå¯¾ç­–ï¼ˆkeyæœªæŒ‡å®šæ™‚ã«è‡ªå‹•ã§ä¸€æ„ã‚­ãƒ¼ã‚’ä»˜ä¸ï¼‰
try:
    # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å±æ€§ã‚’å®‰å…¨ã«å‡¦ç†
    original_checkbox = getattr(st, "checkbox", None)

    if original_checkbox is not None and callable(original_checkbox):

        def _unique_checkbox(label, *args, **kwargs):
            if "key" not in kwargs:
                base = f"auto_cb_{abs(hash(str(label))) % 10**8}"
                count_key = f"_{base}_cnt"
                try:
                    cnt = int(st.session_state.get(count_key, 0)) + 1
                except Exception:
                    cnt = 1
                st.session_state[count_key] = cnt
                kwargs["key"] = f"{base}_{cnt}"
            # å¿µã®ãŸã‚å‘¼ã³å‡ºã—å‰ã«å†åº¦ãƒã‚§ãƒƒã‚¯
            if callable(original_checkbox):
                return original_checkbox(label, *args, **kwargs)
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…ƒã®é–¢æ•°ã‚’ç›´æ¥å‘¼ã³å‡ºã—
                return st.checkbox(label, *args, **kwargs)

        # å…ƒã®ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã‚’ä¿å­˜ã—ã¦æ–°ã—ã„é–¢æ•°ã‚’è¨­å®š
        setattr(st, "_orig_checkbox", original_checkbox)
        setattr(st, "checkbox", _unique_checkbox)
except Exception:
    # å¤±æ•—ã—ã¦ã‚‚å¾“æ¥å‹•ä½œã®ã¾ã¾é€²ã‚ã‚‹
    pass

st.title("ğŸ“ˆ æœ¬æ—¥ã®ã‚·ã‚°ãƒŠãƒ«ï¼ˆå…¨ã‚·ã‚¹ãƒ†ãƒ ï¼‰")

settings = get_settings(create_dirs=True)
notifier = create_notifier(platform="slack", fallback=True)
# ã“ã®å®Ÿè¡Œãƒ«ãƒ¼ãƒ—ã§çµæœã‚’è¡¨ç¤ºã—ãŸã‹ã®ãƒ•ãƒ©ã‚°ï¼ˆä¿å­˜ãƒœã‚¿ãƒ³ç­‰ã§ã®ãƒªãƒ©ãƒ³å¯¾ç­–ï¼‰
st.session_state.setdefault("today_shown_this_run", False)


def _reset_shown_flag() -> None:
    """ãƒªãƒ©ãƒ³å¾Œã®å‰å›çµæœå†è¡¨ç¤ºã‚’æœ‰åŠ¹ã«ã™ã‚‹ãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆã™ã‚‹ã€‚"""
    st.session_state["today_shown_this_run"] = False


def _build_position_summary_table(df: pd.DataFrame) -> pd.DataFrame:
    """sideÃ—system åˆ¥ã®ä¿æœ‰ä»¶æ•°ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆã™ã‚‹ã€‚"""

    if df.empty:
        return pd.DataFrame()

    work = df.copy()
    allowed_systems = {
        *(s.lower() for s in LONG_SYSTEMS),
        *(s.lower() for s in SHORT_SYSTEMS),
    }

    def _norm_side(value: Any) -> str | None:
        if isinstance(value, str):
            side = value.strip().lower()
            if side in {"long", "short"}:
                return side
        return None

    def _norm_system(value: Any) -> str | None:
        if isinstance(value, str):
            system = value.strip().lower()
            if system in allowed_systems:
                return system
        return None

    work["side_norm"] = work["side"].map(_norm_side)
    work["system_norm"] = work["system"].map(_norm_system)

    invalid_side_mask = work["side_norm"].isna()
    if invalid_side_mask.any():
        invalid_values = sorted(
            {str(v) for v in work.loc[invalid_side_mask, "side"].tolist()}
        )  # noqa: E501
        raise ValueError(f"æœªå¯¾å¿œã®sideãŒå«ã¾ã‚Œã¦ã„ã¾ã™: {invalid_values}")

    invalid_system_mask = work["system_norm"].isna()
    if invalid_system_mask.any():
        invalid_values = sorted(
            {str(v) for v in work.loc[invalid_system_mask, "system"].tolist()}
        )  # noqa: E501
        raise ValueError(f"æœªå¯¾å¿œã®systemãŒå«ã¾ã‚Œã¦ã„ã¾ã™: {invalid_values}")

    long_conflict_mask = (work["side_norm"] == "long") & (
        ~work["system_norm"].isin(LONG_SYSTEMS)
    )  # noqa: E501
    if long_conflict_mask.any():
        conflict = sorted({str(v) for v in work.loc[long_conflict_mask, "system"].tolist()})
        raise ValueError(f"Longã‚µã‚¤ãƒ‰ã«æƒ³å®šå¤–ã®systemãŒå«ã¾ã‚Œã¦ã„ã¾ã™: {conflict}")

    short_conflict_mask = (work["side_norm"] == "short") & (
        ~work["system_norm"].isin(SHORT_SYSTEMS)
    )
    if short_conflict_mask.any():
        conflict = sorted({str(v) for v in work.loc[short_conflict_mask, "system"].tolist()})
        raise ValueError(f"Shortã‚µã‚¤ãƒ‰ã«æƒ³å®šå¤–ã®systemãŒå«ã¾ã‚Œã¦ã„ã¾ã™: {conflict}")

    def _sorted_systems(systems: set[str]) -> list[str]:
        def _key(name: str) -> tuple[int, int | str]:
            base = name.strip().lower()
            if base.startswith("system"):
                suffix = base[6:]
                if suffix.isdigit():
                    return (0, int(suffix))
            return (1, base)

        return sorted({s.strip().lower() for s in systems if s}, key=_key)

    long_order = _sorted_systems(LONG_SYSTEMS)
    short_order = _sorted_systems(SHORT_SYSTEMS)
    system_columns: list[str] = []
    for name in [*long_order, *short_order]:
        if name and name not in system_columns:
            system_columns.append(name)
    columns_all = [*system_columns, "åˆè¨ˆ"]

    def _format_system_label(name: str) -> str:
        base = name.strip().lower()
        if base.startswith("system"):
            suffix = base[6:]
            if suffix.isdigit():
                return f"System{int(suffix)}"
        return name

    def _build_row(side_key: str, allowed: list[str]) -> dict[str, int]:
        subset = work[work["side_norm"] == side_key]
        counts = subset["system_norm"].value_counts()
        row = {col: 0 for col in columns_all}
        for system_name in allowed:
            row[system_name] = int(counts.get(system_name, 0))
        row["åˆè¨ˆ"] = int(counts.sum())
        return row

    summary_rows: list[dict[str, int]] = []
    index_labels: list[str] = []

    summary_rows.append(_build_row("long", long_order))
    index_labels.append("Long")
    summary_rows.append(_build_row("short", short_order))
    index_labels.append("Short")

    summary = pd.DataFrame(summary_rows, index=index_labels)
    summary = summary.reindex(columns=columns_all, fill_value=0)

    rename_map = {name: _format_system_label(name) for name in system_columns}
    rename_map["åˆè¨ˆ"] = "åˆè¨ˆ"
    summary = summary.rename(columns=rename_map)

    summary.index.name = "side"
    summary.columns.name = None

    return summary.astype(int)


def _normalize_price_history(df: pd.DataFrame, rows: int) -> pd.DataFrame | None:
    """ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã‚’UIç”¨ã«æ­£è¦åŒ–ã™ã‚‹ã€‚"""

    try:
        work = df.copy()
    except Exception:
        return None

    try:
        work.columns = [str(col) for col in work.columns]
    except Exception:
        work = pd.DataFrame(work)
        work.columns = [str(col) for col in work.columns]

    lower_map = {col.lower(): col for col in work.columns}

    # æ—¥ä»˜åˆ—ã‚’æ±ºå®šï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯ index ã‹ã‚‰ç”Ÿæˆï¼‰
    date_col = lower_map.get("date")
    if date_col is not None:
        work["date"] = pd.to_datetime(work[date_col], errors="coerce")
    else:
        try:
            idx = pd.to_datetime(work.index, errors="coerce")
            work = work.assign(date=idx)
        except Exception:
            return None

    work = work.dropna(subset=["date"]).sort_values("date")

    rename_src = {
        "open": "open",
        "high": "high",
        "low": "low",
        "close": "close",
        "volume": "volume",
        "adj close": "adjusted_close",
        "adjclose": "adjusted_close",
    }
    for key, target in rename_src.items():
        col = lower_map.get(key)
        if col is not None:
            work.rename(columns={col: target}, inplace=True)

    try:
        work.columns = [str(col).lower() for col in work.columns]
    except Exception:
        work.columns = [str(col) for col in work.columns]

    if "date" not in work.columns or "close" not in work.columns:
        return None

    # `date` ã‚’å…ˆé ­ã«ç¶­æŒã—ã¤ã¤æ—¢çŸ¥ã‚«ãƒ©ãƒ ã‚’å„ªå…ˆè¡¨ç¤º
    known_order = ["date", "open", "high", "low", "close", "volume", "adjusted_close"]
    ordered: list[str] = []
    for col in known_order:
        if col in work.columns:
            ordered.append(col)
    if hasattr(work, "columns") and isinstance(work.columns, pd.Index):
        for col in list(work.columns):
            if col not in ordered:
                ordered.append(col)
        work = work.loc[:, ordered]
    else:
        # work.columns ãŒå­˜åœ¨ã—ãªã„å ´åˆã‚„åå¾©ã§ããªã„å ´åˆã¯ç©ºDataFrameã‚’è¿”ã™
        return pd.DataFrame()

    if rows > 0:
        work = work.tail(rows)

    return work.reset_index(drop=True)


_ROLLING_REQUIRED_COLUMNS = [
    "date",
    "open",
    "high",
    "low",
    "close",
    "volume",
    "sma25",
    "sma50",
    "sma100",
    "sma150",
    "sma200",
    "atr20",
    "roc200",
]

_ROLLING_IMPORTANT_COLUMNS = [
    "ema20",
    "ema50",
    "atr10",
    "atr14",
    "atr40",
    "atr50",
    "adx7",
    "rsi3",
    "rsi14",
    "hv50",
    "return_6d",
    "drop3d",
]

_ROLLING_NAN_THRESHOLD = 0.20
_ROLLING_RECENT_WINDOW = 120
_ROLLING_RECENT_STRICT_WINDOW = 30
_ROLLING_RECENT_STRICT_THRESHOLD = 0.0

# Per-column lookback (rows required before values become available).
# When computing NaN ratios we exclude the initial warm-up rows for indicators
# that naturally produce NaN for the first `lookback-1` rows (e.g. ROC200,
# SMA100). Keys are lower-cased to match `col_map` usage.
_ROLLING_COLUMN_LOOKBACK: dict[str, int] = {
    # price / basic
    "date": 0,
    "open": 0,
    "high": 0,
    "low": 0,
    "close": 0,
    "volume": 0,
    # SMAs
    "sma25": 25,
    "sma50": 50,
    "sma100": 100,
    "sma150": 150,
    "sma200": 200,
    # ATR / ROC
    "atr20": 20,
    "roc200": 200,
    # common optional indicators
    "ema20": 20,
    "ema50": 50,
    "atr10": 10,
    "atr14": 14,
    "atr40": 40,
    "atr50": 50,
    "adx7": 7,
    "rsi3": 3,
    "rsi14": 14,
    "hv50": 50,
    "return_6d": 6,
    "drop3d": 3,
}

# When True, emit a single info log summarizing how many indicator columns
# were skipped because their series length was shorter than the configured
# lookback (useful for debugging false-positive NaN warnings).
_ROLLING_DEBUG_LOG_SKIPPED = False


def _has_recent_valid_window(numeric: pd.Series) -> bool:
    """Return True if recent rows provide enough non-NaN coverage."""

    if numeric.empty:
        return False

    recent_len = int(min(len(numeric), _ROLLING_RECENT_WINDOW))
    if recent_len <= 0:
        return False
    recent = numeric.iloc[-recent_len:]
    try:
        recent_ratio = float(recent.isna().mean())
    except Exception:
        recent_ratio = 1.0
    if recent_ratio <= _ROLLING_NAN_THRESHOLD:
        return True

    strict_len = int(min(len(numeric), _ROLLING_RECENT_STRICT_WINDOW))
    if strict_len <= 0:
        return False
    strict_recent = recent.iloc[-strict_len:]
    try:
        strict_ratio = float(strict_recent.isna().mean())
    except Exception:
        strict_ratio = 1.0
    return strict_ratio <= _ROLLING_RECENT_STRICT_THRESHOLD


def _analyze_rolling_cache(df: pd.DataFrame | None) -> tuple[bool, dict[str, Any]]:
    if df is None or df.empty:
        return False, {"status": "rolling_missing"}
    try:
        columns = list(df.columns)
    except Exception:
        columns = []
    col_map = {str(col).lower(): col for col in columns}
    missing_required = [col for col in _ROLLING_REQUIRED_COLUMNS if col not in col_map]
    missing_optional = [col for col in _ROLLING_IMPORTANT_COLUMNS if col not in col_map]
    nan_required: list[tuple[str, float]] = []
    nan_optional: list[tuple[str, float]] = []
    skipped_lookback_count = 0
    for name in {*_ROLLING_REQUIRED_COLUMNS, *_ROLLING_IMPORTANT_COLUMNS}:
        actual = col_map.get(name)
        if actual is None:
            continue
        try:
            numeric = pd.to_numeric(df[actual], errors="coerce")
        except Exception:
            continue
        # Exclude initial warm-up rows for indicators that naturally produce NaNs
        # by using a per-column lookback. If the series is shorter than lookback,
        # the indicator cannot be computed yet â€” treat it as "not applicable"
        # for NaN-warning purposes (do not flag as NaNéå¤š).
        lookback = _ROLLING_COLUMN_LOOKBACK.get(name, 0)
        try:
            # If a lookback is defined but the series is too short, skip this
            # column entirely (it's not a problem â€” the indicator simply
            # couldn't have been computed yet).
            if lookback and len(numeric) <= lookback:
                skipped_lookback_count += 1
                # mark as not-applicable by continuing to next column
                continue
            if lookback and len(numeric) > lookback:
                # exclude the first (lookback - 1) rows from the recent window
                # so only rows where the indicator could exist are counted.
                effective_start = max(
                    0,
                    len(numeric) - _ROLLING_RECENT_WINDOW,
                    lookback - 1 - (len(numeric) - _ROLLING_RECENT_WINDOW),
                )
                eval_series = numeric.iloc[effective_start:]
                # If eval_series is empty fallback to full-series ratio
                if len(eval_series) > 0:
                    ratio = float(eval_series.isna().mean())
                else:
                    ratio = float(numeric.isna().mean())
            else:
                ratio = float(numeric.isna().mean())
        except Exception:
            ratio = 1.0
        if ratio > _ROLLING_NAN_THRESHOLD:
            if name in _ROLLING_REQUIRED_COLUMNS:
                nan_required.append((name, ratio))
            else:
                nan_optional.append((name, ratio))
    issues: dict[str, Any] = {}
    fatal = False
    if missing_required:
        issues["missing_required"] = missing_required
        fatal = True
    if nan_required or nan_optional:
        issues["nan_columns"] = [*nan_required, *nan_optional]
    if nan_required:
        fatal = True
    if missing_optional:
        issues["missing_optional"] = missing_optional
    if fatal:
        issues.setdefault(
            "status",
            "missing_required" if missing_required else "nan_columns",
        )
        return False, issues
    if missing_optional:
        issues.setdefault("status", "missing_optional")
        return True, issues
    if nan_optional:
        issues.setdefault("status", "nan_optional")
        return True, issues
    # Optionally log a debug summary about skipped lookback-short columns
    if _ROLLING_DEBUG_LOG_SKIPPED and skipped_lookback_count:
        try:
            logger = logging.getLogger("today_signals")
            logger.info("lookbackæœªæº€ã§ã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸåˆ—ã¯%dä»¶ã§ã—ãŸ", skipped_lookback_count)
        except Exception:
            pass

    return True, {}


def _format_nan_columns(values: list[tuple[str, float]]) -> str:
    if not values:
        return ""
    return ", ".join(f"{name}:{ratio:.1%}" for name, ratio in values)


def _issues_to_note(issues: dict[str, Any]) -> str:
    if not issues:
        return ""
    parts: list[str] = []
    missing_required = issues.get("missing_required") or []
    if missing_required:
        parts.append("required=" + ", ".join(str(x) for x in missing_required))
    missing_optional = issues.get("missing_optional") or []
    if missing_optional:
        parts.append("optional=" + ", ".join(str(x) for x in missing_optional))
    nan_columns = issues.get("nan_columns") or []
    if nan_columns:
        parts.append("nan=" + _format_nan_columns(list(nan_columns)))
    return "; ".join(parts)


def _merge_note(base: str, addition: str) -> str:
    parts = [part for part in [base, addition] if part]
    return " / ".join(parts)


def _build_missing_detail(
    symbol: str,
    issues: dict[str, Any],
    rows_before: int,
) -> dict[str, Any]:
    missing_required = issues.get("missing_required") or []
    missing_optional = issues.get("missing_optional") or []
    nan_columns = issues.get("nan_columns") or []
    return {
        "symbol": symbol,
        "status": issues.get("status", "missing"),
        "missing_required": ", ".join(str(x) for x in missing_required),
        "missing_optional": ", ".join(str(x) for x in missing_optional),
        "nan_columns": _format_nan_columns(list(nan_columns)),
        "rows_before": int(rows_before),
        "rows_after": 0,
        "action": "",
        "resolved": False,
        "note": "",
    }


def _build_manual_rebuild_message(symbol: str, detail: dict[str, Any]) -> str:
    status = str(detail.get("status") or "rolling_missing")
    reason_map = {
        "rolling_missing": "rollingæœªç”Ÿæˆ",
        "missing_required": "å¿…é ˆåˆ—ä¸è¶³",
        "missing_optional": "ä»»æ„åˆ—ä¸è¶³",
        "nan_columns": "NaNéå¤š",
    }
    reason_label = reason_map.get(status, status)
    parts: list[str] = []
    rows_before = detail.get("rows_before")
    try:
        rows_val = int(rows_before) if rows_before is not None else None
    except Exception:
        rows_val = None
    if rows_val:
        parts.append(f"rows={rows_val}")
    missing_required = str(detail.get("missing_required") or "").strip()
    if missing_required:
        parts.append(f"å¿…é ˆ: {missing_required}")
    missing_optional = str(detail.get("missing_optional") or "").strip()
    if missing_optional:
        parts.append(f"ä»»æ„: {missing_optional}")
    nan_columns = str(detail.get("nan_columns") or "").strip()
    if nan_columns:
        parts.append(f"NaN: {nan_columns}")
    message = f"â›” rollingæœªæ•´å‚™: {symbol} ({reason_label})"
    if parts:
        message += " | " + ", ".join(parts)
    message += " ï¼ˆè‡ªå‹•ã‚¹ã‚­ãƒƒãƒ—æ¸ˆã¿ï¼‰"
    return message


def _log_manual_rebuild_notice(
    symbol: str,
    detail: dict[str, Any],
    log_fn: Callable[[str], None] | None = None,
) -> str:
    """rollingæœªæ•´å‚™ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡ºåŠ›ã€‚

    COMPACT_TODAY_LOGS=1 ã®å ´åˆ:
        - æ—§ä»•æ§˜: éŠ˜æŸ„ã”ã¨ã« "â›” rollingæœªæ•´å‚™: SYMBOL (...) ï¼ˆè‡ªå‹•ã‚¹ã‚­ãƒƒãƒ—æ¸ˆã¿ï¼‰" ã‚’é€æ¬¡å‡ºåŠ›ã—å¤§é‡ã«å†—é•·åŒ–
        - æ–°ä»•æ§˜: `common.cache_warnings.RollingIssueAggregator` ã¸ã‚«ãƒ†ã‚´ãƒª manual_rebuild ã¨ã—ã¦é›†ç´„
            * å…ˆé ­ N ä»¶ (ROLLING_ISSUES_VERBOSE_HEAD, æ—¢å®š=5) ã®ã¿ WARNING
            * ä»¥é™ã¯ DEBUG ã«ãƒ€ã‚¦ãƒ³ã‚°ãƒ¬ãƒ¼ãƒ‰ï¼ˆãƒ­ã‚°é‡å‰Šæ¸›ï¼‰
            * é›†ç´„ã‚µãƒãƒªãƒ¼ã¯ä»–ã‚«ãƒ†ã‚´ãƒªã¨åŒã˜ä»•çµ„ã¿ã§ INFO å‡ºåŠ›
    COMPACT_TODAY_LOGS!=1 ã®å ´åˆã¯å¾“æ¥é€šã‚Šå…¨æ–‡ã‚’ log_fn ã¸å‡ºåŠ›ã™ã‚‹ã€‚
    """
    message = _build_manual_rebuild_message(symbol, detail)

    compact_mode = os.getenv("COMPACT_TODAY_LOGS") == "1"
    if compact_mode:
        # æ—¢å­˜ aggregator ã‚’åˆ©ç”¨ã—ã¦ã‚«ãƒ†ã‚´ãƒª: manual_rebuild ã¨ã—ã¦ç™»éŒ²
        try:
            from common.cache_warnings import report_rolling_issue  # ãƒ­ãƒ¼ã‚«ãƒ« import (é…å»¶)

            # ä»£è¡¨çš„ãªç†ç”±ã‚’ status ã‹ã‚‰æŠ½å‡ºã—ã¦ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç¸®å°
            status = str(detail.get("status") or "manual_rebuild")
            report_rolling_issue("manual_rebuild", symbol, status)
        except Exception:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç›´æ¥ãƒ­ã‚°
            if log_fn:
                try:
                    log_fn(message)
                except Exception:
                    pass
        return message

    # éã‚³ãƒ³ãƒ‘ã‚¯ãƒˆãƒ¢ãƒ¼ãƒ‰: å¾“æ¥é€šã‚Šå…¨æ–‡ã‚’å‡ºåŠ›
    if log_fn is None:
        return message
    try:
        log_fn(message)
    except Exception:
        pass
    return message


def _collect_symbol_data(
    symbols: list[str],
    *,
    rows: int,
    log_fn: Callable[[str], None] | None = None,
    debug_scan: bool = False,
) -> tuple[dict[str, pd.DataFrame], list[dict[str, Any]]]:
    """æŒ‡å®šã‚·ãƒ³ãƒœãƒ«ã®æ ªä¾¡å±¥æ­´ã‚’ã¾ã¨ã‚ã¦å–å¾—ã—ã€æ¬ æã‚‚è¨˜éŒ²ã™ã‚‹ã€‚"""

    start_ts = time.time()
    total = len(symbols)
    if total == 0:
        return {}, []

    step = max(1, total // 20)
    fetched: dict[str, pd.DataFrame] = {}
    malformed: list[str] = []
    missing_details: list[dict[str, Any]] = []

    try:
        env_parallel = (os.environ.get("TODAY_PREFETCH_PARALLEL") or "").strip().lower()
    except Exception:
        env_parallel = ""
    try:
        env_threshold = int(os.environ.get("TODAY_PREFETCH_PARALLEL_THRESHOLD", "200"))
    except Exception:
        env_threshold = 200

    if env_parallel in {"1", "true", "yes"}:
        use_parallel = total > 1
    elif env_parallel in {"0", "false", "no"}:
        use_parallel = False
    else:
        use_parallel = total >= max(0, env_threshold)

    max_workers: int | None = None
    if use_parallel:
        try:
            env_workers_raw = (os.environ.get("TODAY_PREFETCH_MAX_WORKERS") or "").strip()
            if env_workers_raw:
                max_workers = int(env_workers_raw)
        except Exception:
            max_workers = None
        if max_workers is None:
            try:
                cfg_workers = getattr(settings.cache.rolling, "load_max_workers", None)
                if cfg_workers:
                    max_workers = int(cfg_workers)
            except Exception:
                pass
        if max_workers is None:
            cpu_count = os.cpu_count() or 4
            max_workers = max(4, cpu_count * 2)
        max_workers = max(1, min(int(max_workers), total))
        if log_fn:
            try:
                log_fn(f"ğŸ§µ åŸºç¤ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰(äº‹å‰ãƒã‚§ãƒƒã‚¯)ä¸¦åˆ—åŒ–: workers={max_workers}")
            except Exception:
                pass

    data_lock = Lock()
    missing_lock = Lock()
    malformed_lock = Lock()
    progress_lock = Lock()
    processed = 0

    def _emit_progress(current: int) -> None:
        if log_fn is None:
            return
        if current % step != 0 and current != total:
            return
        try:
            elapsed = int(max(0, time.time() - start_ts))
            minutes, seconds = divmod(elapsed, 60)
            log_fn(f"ğŸ“¦ åŸºç¤ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰é€²æ—: {current}/{total} | çµŒé {minutes}åˆ†{seconds}ç§’")
        except Exception:
            try:
                log_fn(f"ğŸ“¦ åŸºç¤ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰é€²æ—: {current}/{total}")
            except Exception:
                pass

    def _process_symbol(
        sym: str,
    ) -> tuple[str, pd.DataFrame | None, dict[str, Any] | None, str | None, bool]:
        manual_msg: str | None = None
        detail: dict[str, Any] | None = None
        malformed_flag = False
        try:
            df = load_price(sym, cache_profile="rolling")
        except Exception:
            df = None
        rows_before = 0 if df is None else int(len(df))
        ok, issues = _analyze_rolling_cache(df)
        if not ok:
            detail = _build_missing_detail(sym, issues, rows_before)
            if debug_scan:
                detail["action"] = "debug_scan"
                detail["note"] = _issues_to_note(issues)
                return sym, None, detail, None, False
            detail["action"] = "manual_rebuild_required"
            manual_note = _merge_note(
                _issues_to_note(issues),
                "è‡ªå‹•ã‚¹ã‚­ãƒƒãƒ—",
            )
            detail["note"] = manual_note
            manual_msg = _build_manual_rebuild_message(sym, detail)
            return sym, None, detail, manual_msg, False

        if df is None:
            malformed_flag = True
            return sym, None, None, None, malformed_flag

        norm = _normalize_price_history(df, rows)
        if norm is not None and not norm.empty:
            return sym, norm, None, None, False
        malformed_flag = True
        return sym, None, None, None, malformed_flag

    def _handle_result(
        result: tuple[str, pd.DataFrame | None, dict[str, Any] | None, str | None, bool],
    ) -> None:
        nonlocal processed
        sym, norm, detail, manual_msg, malformed_flag = result
        if norm is not None and not getattr(norm, "empty", True):
            with data_lock:
                fetched[sym] = norm
        elif malformed_flag:
            with malformed_lock:
                malformed.append(sym)
        if detail is not None:
            with missing_lock:
                missing_details.append(detail)
        if manual_msg and log_fn and not debug_scan:
            try:
                log_fn(manual_msg)
            except Exception:
                pass
        with progress_lock:
            processed += 1
            _emit_progress(processed)

    if use_parallel and max_workers and total > 1:
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {executor.submit(_process_symbol, sym): sym for sym in symbols}
            for fut in as_completed(futures):
                sym = futures[fut]
                try:
                    result = fut.result()
                except Exception:
                    result = (sym, None, None, None, True)
                _handle_result(result)
    else:
        for sym in symbols:
            result = _process_symbol(sym)
            _handle_result(result)

    if log_fn:
        try:
            elapsed = int(max(0, time.time() - start_ts))
            minutes, seconds = divmod(elapsed, 60)
            log_fn(f"ğŸ“¦ åŸºç¤ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰å®Œäº†: {len(fetched)}/{total} | æ‰€è¦ {minutes}åˆ†{seconds}ç§’")
        except Exception:
            pass
        manual_symbols = [
            detail["symbol"]
            for detail in missing_details
            if detail.get("action") == "manual_rebuild_required"
        ]
        if manual_symbols:
            sample = ", ".join(manual_symbols[:5])
            if len(manual_symbols) > 5:
                sample += f" ã»ã‹{len(manual_symbols) - 5}ä»¶"
            # ã‚ˆã‚Šè©³ç´°ãªçŠ¶æ³èª¬æ˜ã‚’è¿½åŠ 
            new_listings = [
                s for s in manual_symbols if len(s) <= 4 and s.isalpha()
            ]  # æ–°è¦ä¸Šå ´ã®å¯èƒ½æ€§
            try:
                base_msg = f"âš ï¸ rollingæœªæ•´å‚™: {len(manual_symbols)}éŠ˜æŸ„ â†’ æ‰‹å‹•ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–°ã—ã¦ãã ã•ã„ | ä¾‹: {sample}"
                if new_listings:
                    base_msg += f" (æ–°è¦ä¸Šå ´å«ã‚€å¯èƒ½æ€§: {len(new_listings)}ä»¶)"
                log_fn(base_msg)
            except Exception:
                pass
        if malformed:
            sample = ", ".join(malformed[:5])
            if len(malformed) > 5:
                sample += f" ã»ã‹{len(malformed) - 5}ä»¶"
            try:
                log_fn(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿æ•´å½¢ä¸å¯: {sample}")
            except Exception:
                pass
        if debug_scan:
            try:
                if missing_details:
                    log_fn(f"ğŸ§ª æ¬ ææ´—ã„å‡ºã—æ¤œå‡º: {len(missing_details)}ä»¶")
                else:
                    log_fn("ğŸ§ª æ¬ ææ´—ã„å‡ºã—: å•é¡Œã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            except Exception:
                pass

    return fetched, missing_details


def _get_today_logger() -> logging.Logger:
    """æœ¬æ—¥ã®ã‚·ã‚°ãƒŠãƒ«å®Ÿè¡Œç”¨ãƒ­ã‚¬ãƒ¼ã€‚

    - orchestrator(`scripts.run_all_systems_today`)ãŒè¨­å®šã—ãŸãƒ­ã‚°ãƒ‘ã‚¹ãŒã‚ã‚Œã°ãã‚Œã«åˆã‚ã›ã‚‹
    - ç„¡ã„å ´åˆã¯ `TODAY_SIGNALS_LOG_MODE`ï¼ˆsingle|datedï¼‰ã‚’å‚ç…§
    - æ—¢å®šã¯ datedï¼ˆJST: today_signals_YYYYMMDD_HHMM.logï¼‰
    """
    logger = logging.getLogger("today_signals")
    logger.setLevel(logging.INFO)
    try:
        logger.propagate = False
    except Exception:
        pass

    # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    try:
        log_dir = Path(settings.LOGS_DIR)
    except Exception:
        log_dir = Path("logs")
    try:
        log_dir.mkdir(parents=True, exist_ok=True)
    except Exception:
        pass

    # orchestrator å´ã®è¨­å®šã‚’æœ€å„ªå…ˆ
    log_path: Path | None = None
    # å‹•çš„ã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ã‚¨ãƒ©ãƒ¼ã‚’å›é¿
    try:
        import scripts.run_all_systems_today as _run_today_mod

        sel = getattr(_run_today_mod, "_LOG_FILE_PATH", None)
        if isinstance(sel, Path):
            log_path = sel
    except Exception:
        log_path = None

    # ç„¡ã‘ã‚Œã°ç’°å¢ƒå¤‰æ•°ã‚’è¦‹ã¦æ±ºå®š
    if log_path is None:
        try:
            mode_env = (os.environ.get("TODAY_SIGNALS_LOG_MODE") or "").strip().lower()
        except Exception:
            mode_env = ""
        if mode_env == "single":
            log_path = log_dir / "today_signals.log"
        else:
            try:
                jst_now = datetime.now(get_zoneinfo("Asia/Tokyo"))
            except Exception:
                jst_now = datetime.now(get_zoneinfo("UTC"))
            stamp = jst_now.strftime("%Y%m%d_%H%M")
            log_path = log_dir / f"today_signals_{stamp}.log"

    # æ—¢å­˜ã®ãƒãƒ³ãƒ‰ãƒ©ã‚’æ•´ç†ï¼ˆç•°ãªã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒãƒ³ãƒ‰ãƒ©ã¯é™¤å»ï¼‰
    try:
        for h in list(logger.handlers):
            try:
                if isinstance(h, logging.FileHandler):
                    base = getattr(h, "baseFilename", None)
                    if base and Path(base) != log_path:
                        logger.removeHandler(h)
                        try:
                            h.close()
                        except Exception:
                            pass
            except Exception:
                pass
    except Exception:
        pass

    # åŒä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‘ã‘ãŒæœªè¿½åŠ ãªã‚‰è¿½åŠ 
    has_handler = False
    for h in list(logger.handlers):
        try:
            if isinstance(h, logging.FileHandler) and getattr(h, "baseFilename", None):
                if Path(h.baseFilename) == log_path:
                    has_handler = True
                    break
        except Exception:
            continue
    if not has_handler:
        try:
            fh = logging.FileHandler(str(log_path), encoding="utf-8")
            fmt = logging.Formatter(
                "%(asctime)s [%(levelname)s] %(message)s", "%Y-%m-%d %H:%M:%S"
            )  # noqa: E501
            fh.setFormatter(fmt)
            logger.addHandler(fh)
        except Exception:
            pass
    return logger


@dataclass
class RunConfig:
    symbols: list[str]
    capital_long: float
    capital_short: float
    save_csv: bool
    csv_name_mode: str
    notify: bool
    run_parallel: bool
    scan_missing_only: bool = False


@dataclass
class TradeOptions:
    paper_mode: bool
    retries: int
    delay: float
    poll_status: bool
    do_trade: bool
    update_bp_after: bool


class ProgressUI:
    """å…¨ä½“é€²æ—ã¨ãƒ­ã‚°è¡¨ç¤ºã‚’ç®¡ç†ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼ã€‚"""

    def __init__(self, ui_vis: dict[str, Any]):
        self.show_overall = bool(ui_vis.get("overall_progress", True))
        self.show_data_load = bool(ui_vis.get("data_load_progress_lines", True))
        self.phase_title_area = st.empty()
        self.progress_area = st.empty()
        self.progress_bar = st.progress(0) if self.show_overall else None
        # progress_textã¯å‰Šé™¤ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã§è¡¨ç¤ºã™ã‚‹ãŸã‚ï¼‰
        self.phase_state: dict[str, Any] = {"percent": 0, "label": "å¯¾è±¡èª­ã¿è¾¼ã¿"}
        self._render_title()

    def set_label(self, label: str) -> None:
        if not self.show_overall:
            return
        self.phase_state["label"] = label
        self._render_title()

    def update(self, done: int, total: int, tag: str) -> None:
        if not self.show_overall or self.progress_bar is None:
            return
        total = max(1, int(total))
        ratio = min(max(int(done), 0), total) / total
        percent = int(ratio * 100)
        self.phase_state["percent"] = percent
        mapped = self._map_phase(tag)
        if mapped:
            self.phase_state["label"] = mapped
        try:
            self.progress_bar.progress(percent)
        except Exception:
            pass
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã¯å‰Šé™¤ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã§è¡¨ç¤ºã™ã‚‹ãŸã‚ï¼‰
        self._render_title()

    def update_label_for_stage(self, stage_value: int) -> None:
        if not self.show_overall:
            return
        if stage_value <= 0:
            label = "å¯¾è±¡æº–å‚™"
        elif stage_value < 10:
            label = "å¯¾è±¡èª­ã¿è¾¼ã¿"
        elif stage_value < 30:
            label = "ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼"
        elif stage_value < 60:
            label = "ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"
        elif stage_value < 90:
            label = "ãƒˆãƒ¬ãƒ¼ãƒ‰å€™è£œé¸å®š"
        else:
            label = "ã‚¨ãƒ³ãƒˆãƒªãƒ¼"
        self.set_label(label)

    def _render_title(self) -> None:
        if not self.show_overall:
            return
        try:
            percent = int(self.phase_state.get("percent", 0))
            label = str(self.phase_state.get("label", "å¯¾è±¡èª­ã¿è¾¼ã¿"))
            self.phase_title_area.markdown(f"## é€²æ— {percent}%: {label}ãƒ•ã‚§ãƒ¼ã‚º")
        except Exception:
            pass

    @staticmethod
    def _map_phase(tag: str) -> str:
        try:
            t = (tag or "").lower()
        except Exception:
            t = ""
        if t in {
            "init",
            "å¯¾è±¡èª­ã¿è¾¼ã¿:start",
            "load_basic:start",
            "load_basic",
            "load_indicators",
            "spx",
            "spy",
        }:
            return "å¯¾è±¡èª­ã¿è¾¼ã¿"
        if t in {"filter", "ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼"}:
            return "ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼"
        if t in {"run_strategies", "setup"} or t.startswith("system"):
            return "ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"
        if t in {"strategies_done", "tradeå€™è£œ", "ãƒˆãƒ¬ãƒ¼ãƒ‰å€™è£œé¸å®š"}:
            return "ãƒˆãƒ¬ãƒ¼ãƒ‰å€™è£œé¸å®š"
        if t in {"finalize", "done", "ã‚¨ãƒ³ãƒˆãƒªãƒ¼"}:
            return "ã‚¨ãƒ³ãƒˆãƒªãƒ¼"
        return "å¯¾è±¡èª­ã¿è¾¼ã¿"


class StageTracker:
    """ã‚·ã‚¹ãƒ†ãƒ åˆ¥ã®é€²æ—ã¨ä»¶æ•°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ç®¡ç†ã™ã‚‹ã€‚"""

    def __init__(self, ui_vis: dict[str, Any], progress_ui: ProgressUI):
        self.progress_ui = progress_ui
        self.show_ui = bool(ui_vis.get("per_system_progress", True)) and _has_st_ctx()
        self.bars: dict[str, Any] = {}
        self.stage_txt: dict[str, Any] = {}
        self.metrics_txt: dict[str, Any] = {}
        self.states: dict[str, int] = {}
        self.metrics_store = StageMetricsStore(DEFAULT_SYSTEM_ORDER)
        self.stage_counts = self.metrics_store.stage_counts
        # æœ€å¾Œã«å—ã‘å–ã£ãŸã‚¹ãƒ†ãƒ¼ã‚¸æƒ…å ±ã®ãƒ‡ãƒ‡ãƒ¥ãƒ¼ãƒ—ç”¨ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
        self._last_event: dict[str, tuple[int, int, int, int, int, float]] = {}
        self.universe_total: int | None = None
        self.universe_target: int | None = None
        if self.show_ui:
            sys_cols = st.columns(7)
            sys_labels = [f"System{i}" for i in range(1, 8)]
            for i, col in enumerate(sys_cols, start=1):
                key = f"system{i}"
                try:
                    col.caption(sys_labels[i - 1])
                    self.bars[key] = col.progress(0)
                    self.stage_txt[key] = col.empty()
                    self.metrics_txt[key] = col.empty()
                    self._render_metrics(key)
                except Exception:
                    self.show_ui = False
                    break
        self._initialize_from_store()

    def update_progress(self, name: str, phase: str) -> None:
        if not self.show_ui:
            return
        key = str(name).lower()
        progress_bar = self.bars.get(key)
        if progress_bar is None:
            return

        # ãƒ•ã‚§ãƒ¼ã‚ºã«å¿œã˜ãŸé©åˆ‡ãªå€¤ã‚’è¨­å®š
        if phase == "start":
            # é–‹å§‹æ™‚ã¯0%ã‹ã‚‰é–‹å§‹ï¼ˆãƒªã‚»ãƒƒãƒˆï¼‰
            value = 0
            self.states[key] = 0  # çŠ¶æ…‹ã‚‚ãƒªã‚»ãƒƒãƒˆ
        elif phase == "done":
            value = 100
        else:
            # ãã®ä»–ã®ãƒ•ã‚§ãƒ¼ã‚ºã§ã¯å®Ÿéš›ã®é€²æ—å€¤ã‚’å–å¾—
            try:
                snapshot = GLOBAL_STAGE_METRICS.get_snapshot(key)
                if snapshot is not None:
                    value = snapshot.progress
                else:
                    value = self.states.get(key, 0)
            except Exception:
                value = self.states.get(key, 0)

        value = max(0, min(100, int(value)))

        # é€šå¸¸æ™‚ã¯é€²æ—å¾Œé€€ã‚’é˜²ããŒã€é–‹å§‹æ™‚ï¼ˆphase="start"ï¼‰ã¯ãƒªã‚»ãƒƒãƒˆã‚’è¨±å¯
        if phase != "start":
            prev = int(self.states.get(key, 0))
            value = max(prev, value)

        self.states[key] = value

        try:
            progress_bar.progress(value)
            self.stage_txt[key].text(f"run {value}%" if value < 100 else "done 100%")
        except Exception:
            pass

    def _initialize_from_store(self) -> None:
        try:
            stored_target = GLOBAL_STAGE_METRICS.get_universe_target()
            if stored_target is not None:
                self.universe_target = int(stored_target)
        except Exception:
            pass
        try:
            snapshots = GLOBAL_STAGE_METRICS.all_snapshots()
        except Exception:
            snapshots = {}
        for sys_name, snapshot in snapshots.items():
            try:
                self._apply_snapshot(sys_name, snapshot)
            except Exception:
                continue

    def _apply_snapshot(self, name: str, snapshot: StageSnapshot) -> None:
        key = str(name).lower()
        counts = self._ensure_counts(key)

        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ•°ã®è¨­å®šï¼ˆå„ªå…ˆåº¦é †ã§è¨­å®šï¼‰
        if snapshot.target is not None:
            try:
                target_val = int(snapshot.target)
                counts["target"] = target_val
                self.universe_total = target_val
            except Exception:
                pass
        elif snapshot.filter_pass is not None and counts.get("target") is None:
            try:
                fallback_target = int(snapshot.filter_pass)
                counts["target"] = fallback_target
                if self.universe_total is None:
                    self.universe_total = fallback_target
            except Exception:
                pass

        # é€²æ—ãƒ‡ãƒ¼ã‚¿ã®è¨­å®š
        if snapshot.filter_pass is not None:
            try:
                counts["filter"] = int(snapshot.filter_pass)
                # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é€šéæ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãŒãªã‘ã‚Œã°ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ•°ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨ã—ã¦ä½¿ç”¨
                if counts.get("target") is None:
                    counts["target"] = int(snapshot.filter_pass)
                    if self.universe_total is None:
                        self.universe_total = int(snapshot.filter_pass)
            except Exception:
                pass
        if snapshot.setup_pass is not None:
            try:
                counts["setup"] = int(snapshot.setup_pass)
            except Exception:
                pass
        if snapshot.candidate_count is not None:
            try:
                counts["cand"] = self._clamp_trdlist(snapshot.candidate_count)
            except Exception:
                pass
        if snapshot.entry_count is not None:
            try:
                counts["entry"] = int(snapshot.entry_count)
            except Exception:
                pass
        if snapshot.exit_count is not None:
            try:
                counts["exit"] = int(snapshot.exit_count)
            except Exception:
                pass
        self._update_bar(key, snapshot.progress)
        self.progress_ui.update_label_for_stage(snapshot.progress)
        self._render_metrics(key)

    def update_stage(
        self,
        name: str,
        value: int,
        filter_cnt: int | None = None,
        setup_cnt: int | None = None,
        cand_cnt: int | None = None,
        final_cnt: int | None = None,
    ) -> None:
        key = str(name).lower()
        # çŸ­æ™‚é–“å†…ã«åŒä¸€å†…å®¹ã®æ›´æ–°ãŒæ¥ã‚‹ã¨ UI ãŒãƒ•ãƒ©ãƒƒã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ãŸã‚ã€
        # åŒä¸€ã‚·ã‚¹ãƒ†ãƒ ãƒ»åŒä¸€å€¤ãƒ»åŒä¸€ã‚«ã‚¦ãƒ³ãƒˆã®æ›´æ–°ã¯ 0.5 ç§’ä»¥å†…ã¯ç„¡è¦–ã™ã‚‹ã€‚
        try:
            last = self._last_event.get(key)
            cur_sig = (
                value,
                int(filter_cnt) if filter_cnt is not None else -1,
                int(setup_cnt) if setup_cnt is not None else -1,
                int(cand_cnt) if cand_cnt is not None else -1,
                int(final_cnt) if final_cnt is not None else -1,
                time.time(),
            )
            if last is not None:
                same = last[0:5] == cur_sig[0:5]
                recent = (cur_sig[5] - last[5]) < 0.5
                if same and recent:
                    return
            self._last_event[key] = cur_sig
        except Exception:
            pass
        snapshot: StageSnapshot | None
        try:
            snapshot = GLOBAL_STAGE_METRICS.record_stage(
                key,
                value,
                filter_cnt,
                setup_cnt,
                cand_cnt,
                final_cnt,
                emit_event=False,
            )
        except Exception:
            snapshot = None
        if snapshot is not None:
            self._apply_snapshot(key, snapshot)
            return
        counts = self._ensure_counts(key)
        if filter_cnt is not None:
            try:
                filter_val = int(filter_cnt)
            except Exception:
                filter_val = None
            if filter_val is not None:
                if value == 0:
                    counts["target"] = filter_val
                    self.universe_total = filter_val
                else:
                    counts["filter"] = filter_val
                    if counts.get("target") is None:
                        counts["target"] = (
                            self.universe_total if self.universe_total is not None else filter_val
                        )
                        if self.universe_total is None:
                            self.universe_total = filter_val
        if setup_cnt is not None:
            counts["setup"] = int(setup_cnt)
        if cand_cnt is not None:
            counts["cand"] = self._clamp_trdlist(cand_cnt)
        if final_cnt is not None:
            counts["entry"] = int(final_cnt)
        self._update_bar(key, value)
        self.progress_ui.update_label_for_stage(value)
        self._render_metrics(key)

    def set_universe_target(self, tgt: int | None) -> None:
        """å…¨ä½“ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ï¼ˆTgtï¼‰ã‚’è¨­å®šã€‚UI ã«å³æ™‚åæ˜ ã™ã‚‹ã€‚

        - å¼•æ•°ãŒ None ã®å ´åˆã¯æ—¢å®šå‹•ä½œï¼ˆå„ system ã® target/filter ã‚’è¡¨ç¤ºï¼‰ã«æˆ»ã‚‹ã€‚
        - æ•´æ•°ãŒä¸ãˆã‚‰ã‚ŒãŸå ´åˆã€å„ system ã®è¡¨ç¤ºä¸Šã® `Tgt` ã¯ã“ã®å€¤ã‚’è¡¨ç¤ºã™ã‚‹ã€‚
        """
        try:
            if tgt is None:
                self.universe_target = None
                self.universe_total = None
            else:
                self.universe_target = int(tgt)
                self.universe_total = int(tgt)
            GLOBAL_STAGE_METRICS.set_universe_target(self.universe_target)
        except Exception:
            self.universe_target = None
            self.universe_total = None
            try:
                GLOBAL_STAGE_METRICS.set_universe_target(None)
            except Exception:
                pass
        # å…¨ system ã®è¡¨ç¤ºã‚’æ›´æ–°
        self.refresh_all()

    def update_exit(self, name: str, count: int) -> None:
        key = str(name).lower()
        snapshot: StageSnapshot | None
        try:
            snapshot = GLOBAL_STAGE_METRICS.record_exit(key, count, emit_event=False)
        except Exception:
            snapshot = None
        if snapshot is not None:
            self._apply_snapshot(key, snapshot)
            return
        counts = self._ensure_counts(key)
        counts["exit"] = int(count)
        self._render_metrics(key)

    def finalize_counts(
        self, final_df: pd.DataFrame, per_system: dict[str, pd.DataFrame]
    ) -> None:  # noqa: E501
        for name, counts in self.stage_counts.items():
            snapshot: StageSnapshot | None
            try:
                snapshot = GLOBAL_STAGE_METRICS.get_snapshot(name)
            except Exception:
                snapshot = None
            if snapshot is not None:
                if counts.get("target") is None and snapshot.target is not None:
                    try:
                        counts["target"] = int(snapshot.target)
                        if self.universe_total is None:
                            self.universe_total = int(snapshot.target)
                    except Exception:
                        pass
                if counts.get("filter") is None and snapshot.filter_pass is not None:
                    try:
                        counts["filter"] = int(snapshot.filter_pass)
                    except Exception:
                        pass
                if counts.get("setup") is None and snapshot.setup_pass is not None:
                    try:
                        counts["setup"] = int(snapshot.setup_pass)
                    except Exception:
                        pass
                if counts.get("cand") is None and snapshot.candidate_count is not None:
                    try:
                        counts["cand"] = self._clamp_trdlist(snapshot.candidate_count)
                    except Exception:
                        pass
                if counts.get("entry") is None and snapshot.entry_count is not None:
                    try:
                        counts["entry"] = int(snapshot.entry_count)
                    except Exception:
                        pass
                if counts.get("exit") is None and snapshot.exit_count is not None:
                    try:
                        counts["exit"] = int(snapshot.exit_count)
                    except Exception:
                        pass
        try:
            system_series = (
                final_df["system"].astype(str).str.strip().str.lower()
                if "system" in final_df.columns
                else pd.Series(dtype=str)
            )
        except Exception:
            system_series = pd.Series(dtype=str)
        for name, counts in self.stage_counts.items():
            if counts.get("cand") is None:
                df_sys = per_system.get(name)
                if df_sys is None or df_sys.empty:
                    counts["cand"] = 0
                else:
                    counts["cand"] = self._clamp_trdlist(len(df_sys))
            if counts.get("entry") is None and not system_series.empty:
                try:
                    counts["entry"] = int((system_series == name).sum())
                except Exception:
                    counts["entry"] = counts.get("entry")
            if counts.get("target") is None:
                if self.universe_total is not None:
                    counts["target"] = self.universe_total
                elif counts.get("filter") is not None and counts.get("setup") is None:
                    counts["target"] = counts.get("filter")
            try:
                GLOBAL_STAGE_METRICS.record_stage(
                    name,
                    int(self.states.get(name, 100 if counts.get("entry") is not None else 0)),
                    counts.get("filter"),
                    counts.get("setup"),
                    counts.get("cand"),
                    counts.get("entry"),
                    emit_event=False,
                )
            except Exception:
                pass
        self.refresh_all()

    def apply_exit_counts(self, exit_counts: dict[str, int]) -> None:
        for name, cnt in exit_counts.items():
            if not cnt:
                continue
            snapshot: StageSnapshot | None
            try:
                snapshot = GLOBAL_STAGE_METRICS.record_exit(name, cnt, emit_event=False)
            except Exception:
                snapshot = None
            if snapshot is not None:
                self._apply_snapshot(name, snapshot)
            else:
                self._ensure_counts(name)["exit"] = int(cnt)
        self.refresh_all()

    def refresh_all(self) -> None:
        for name in self.metrics_store.systems():
            self._render_metrics(name)

    def _update_bar(self, key: str, value: int) -> None:
        if not self.show_ui:
            return
        progress_bar = self.bars.get(key)
        if progress_bar is None:
            return
        vv = max(0, min(100, int(value)))
        prev = int(self.states.get(key, 0))
        vv = max(prev, vv)
        self.states[key] = vv
        try:
            progress_bar.progress(vv)
            self.stage_txt[key].text(f"run {vv}%" if vv < 100 else "done 100%")
        except Exception:
            pass

    def _render_metrics(self, key: str) -> None:
        placeholder = self.metrics_txt.get(key)
        if placeholder is None:
            return
        display = self.metrics_store.get_display_metrics(key)
        target_value = (
            self.universe_target if self.universe_target is not None else display.get("target")
        )
        text = "  ".join(
            [
                f"Tgt {self._format_value(target_value)}",
                f"FILpass {self._format_value(display.get('filter'))}",
                f"STUpass {self._format_value(display.get('setup'))}",
                f"TRDlist {self._format_trdlist(display.get('cand'))}",
                f"Entry {self._format_value(display.get('entry'))}",
                f"Exit {self._format_value(display.get('exit'))}",
            ]
        )
        try:
            placeholder.text(text)
        except Exception:
            pass

    def get_display_metrics(self, name: str) -> dict[str, int | None]:
        key = str(name).lower()
        return self.metrics_store.get_display_metrics(key)

    def _ensure_counts(self, name: str) -> dict[str, int | None]:
        return self.metrics_store.ensure_display_metrics(name)

    @staticmethod
    def _format_value(value: Any) -> str:
        return "-" if value is None else str(value)

    @staticmethod
    def _clamp_trdlist(value: Any) -> int | None:
        return StageMetricsStore.clamp_trdlist(value)

    def _format_trdlist(self, value: Any) -> str:
        if value is None:
            return "-"
        try:
            return str(self._clamp_trdlist(value))
        except Exception:
            return "-"


class UILogger:
    """UIã¨ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ã®ä¸¡æ–¹ã¸ãƒ­ã‚°ã‚’æ›¸ãå‡ºã™ã€‚"""

    def __init__(self, start_time: float, progress_ui: ProgressUI):
        self.start_time = start_time
        self.progress_ui = progress_ui
        self.log_lines: list[str] = []
        # ãƒ­ã‚°ãƒ‡ãƒ‡ãƒ¥ãƒ¼ãƒ—ç”¨ï¼ˆçŸ­æ™‚é–“ã«åŒä¸€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒæ¥ãŸã‚‰æŠ‘æ­¢ï¼‰
        self._last_log: dict[str, float] = {}

    def log(self, msg: str, no_timestamp: bool = False) -> None:
        forwarded_from_cli = False
        try:
            import scripts.run_all_systems_today as _run_today_mod

            forwarding_flag = getattr(_run_today_mod, "_LOG_FORWARDING", None)
            if forwarding_flag is not None:
                forwarded_from_cli = bool(forwarding_flag.get())
        except Exception:
            forwarded_from_cli = False
        structured_mode = False
        parsed_msg: str | None = None
        iso_ts: str | None = None
        rel_prefix: str | None = None
        if not no_timestamp:
            # STRUCTURED_UI_LOGS=1 ã®ã¨ãã€ã‚¨ãƒ³ã‚¸ãƒ³å´ã‹ã‚‰æ¸¡ã•ã‚Œã‚‹ JSON å½¢å¼ã‚’å„ªå…ˆçš„ã«è§£é‡ˆ
            if os.environ.get("STRUCTURED_UI_LOGS") == "1":
                try:
                    import json as _json

                    if isinstance(msg, str) and msg.startswith("{") and '"msg"' in msg:
                        obj = _json.loads(msg)
                        # æœ€ä½é™ 'msg' ãŒã‚ã‚‹ã“ã¨
                        raw_inner = obj.get("msg")
                        if isinstance(raw_inner, str):
                            structured_mode = True
                            parsed_msg = raw_inner
                            # ISO æ™‚åˆ»
                            iso_candidate = obj.get("iso")
                            if isinstance(iso_candidate, str):
                                iso_ts = iso_candidate
                            # ç›¸å¯¾æ™‚é–“ï¼ˆã‚¨ãƒãƒƒã‚¯ã‚’ start_time ã¨ã®å·®åˆ†ã§è¨ˆç®—ï¼‰
                            ts_val = obj.get("ts")
                            if isinstance(ts_val, (int, float)):
                                try:
                                    rel_elapsed = max(0, (ts_val / 1000.0) - self.start_time)
                                    mm, ss = divmod(int(rel_elapsed), 60)
                                    rel_prefix = f"{mm}åˆ†{ss}ç§’"
                                except Exception:
                                    pass
                except Exception:
                    structured_mode = False

        def _format_rel_compact(elapsed: float) -> str:
            try:
                if elapsed < 0:
                    elapsed = 0.0
                if elapsed < 1:
                    return f"+{int(elapsed * 1000)}ms"
                if elapsed < 60:
                    return f"+{elapsed:.1f}s"
                if elapsed < 3600:
                    m, s = divmod(int(elapsed), 60)
                    return f"+{m}:{s:02d}"
                if elapsed < 86400:
                    h, rem = divmod(int(elapsed), 3600)
                    m, s = divmod(rem, 60)
                    return f"+{h}h{m:02d}m"  # ç§’ã¯çœç•¥
                d, rem = divmod(int(elapsed), 86400)
                h, _ = divmod(rem, 3600)
                return f"+{d}d{h}h"
            except Exception:
                return "+0.0s"

        compact_mode = os.environ.get("COMPACT_REL_TIME") == "1"

        if structured_mode and parsed_msg is not None:
            # ISO or ç¾åœ¨æ™‚åˆ» fallback
            if iso_ts is None:
                iso_ts = time.strftime("%Y-%m-%d %H:%M:%S")
            if rel_prefix is None:
                try:
                    _elapsed = max(0, time.time() - self.start_time)
                    if compact_mode:
                        rel_prefix = _format_rel_compact(_elapsed)
                    else:
                        mm, ss = divmod(int(_elapsed), 60)
                        rel_prefix = f"{mm}åˆ†{ss}ç§’"
                except Exception:
                    rel_prefix = "0åˆ†0ç§’"
            line = f"[{iso_ts} | {rel_prefix}] {parsed_msg}"
        else:
            try:
                elapsed = max(0, time.time() - self.start_time)
                if compact_mode:
                    rel_prefix = _format_rel_compact(elapsed)
                else:
                    m, s = divmod(int(elapsed), 60)
            except Exception:
                rel_prefix = "0åˆ†0ç§’" if not compact_mode else "+0.0s"
            now_txt = time.strftime("%Y-%m-%d %H:%M:%S")
            if no_timestamp:
                line = str(msg)
            else:
                if compact_mode:
                    if not rel_prefix:
                        rel_prefix = "+0.0s"
                    line = f"[{now_txt} | {rel_prefix}] {msg}"
                else:
                    try:
                        # m,s ãŒè¨ˆç®—æ¸ˆã¿ã§ãªã„ã‚±ãƒ¼ã‚¹ã¯å†è¨ˆç®—
                        if "m" not in locals() or "s" not in locals():
                            _m, _s = divmod(int(max(0, time.time() - self.start_time)), 60)
                            m, s = _m, _s
                        line = f"[{now_txt} | {m}åˆ†{s}ç§’] {msg}"
                    except Exception:
                        line = f"[{now_txt} | 0åˆ†0ç§’] {msg}"
        self.log_lines.append(line)
        if _has_st_ctx() and self.progress_ui.show_overall:
            if self._should_display(str(msg)):
                try:
                    self.progress_ui.progress_area.text(line)
                except Exception:
                    pass
        if not forwarded_from_cli:
            self._echo_cli(line)
            try:
                _get_today_logger().info(str(msg))
            except Exception:
                pass

    def _should_display(self, msg: str) -> bool:
        if not self.progress_ui.show_overall:
            return False
        data_load_prefixes = (
            "ğŸ“¦ åŸºç¤ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰é€²æ—",
            "ğŸ§® æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰é€²æ—",
            "ğŸ“¦ åŸºç¤ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰å®Œäº†",
            "ğŸ§® æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰å®Œäº†",
            "ğŸ§® å…±æœ‰æŒ‡æ¨™ å‰è¨ˆç®—",
        )
        # ã“ã“ã¯æ¯”è¼ƒçš„é™å®šçš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ã«ã™ã‚‹ï¼ˆéå‰°é™¤å¤–ã‚’é˜²æ­¢ï¼‰
        skip_keywords = (
            "batch time",
            "next batch size",
        )
        if msg.startswith(data_load_prefixes):
            return self.progress_ui.show_data_load
        # çŸ­æ™‚é–“å†…ã®åŒä¸€ãƒ­ã‚°ã‚’æŠ‘æ­¢ï¼ˆ0.3ç§’ä»¥å†…ã®é‡è¤‡ã¯ç„¡è¦–ï¼‰
        try:
            now = time.time()
            last = self._last_log.get(msg)
            if last is not None and (now - last) < 0.3:
                return False
            self._last_log[msg] = now
        except Exception:
            pass
        return not any(keyword in msg for keyword in skip_keywords)

    def _echo_cli(self, line: str) -> None:
        # Windows ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã§ã®æ–‡å­—åŒ–ã‘ç·©å’Œï¼ˆä»»æ„ãƒ•ãƒ©ã‚°ï¼‰
        try:
            if os.name == "nt" and os.environ.get("FORCE_UTF8_CONSOLE") == "1":
                try:
                    if hasattr(sys.stdout, "reconfigure"):
                        # æ—¢ã« utf-8 ã®å ´åˆã¯è§¦ã‚‰ãªã„
                        if (getattr(sys.stdout, "encoding", "") or "").lower() not in (
                            "utf-8",
                            "utf8",
                        ):
                            sys.stdout.reconfigure(encoding="utf-8")  # type: ignore[attr-defined]
                except Exception:
                    pass
            # åˆå›ãƒ’ãƒ³ãƒˆè¡¨ç¤ºï¼ˆåŒ–ã‘ã‚’æ¤œçŸ¥ã§ããã†ãªã‚‰ï¼‰
            if not getattr(self, "_encoding_hint_done", False) and os.name == "nt":
                setattr(self, "_encoding_hint_done", True)
                if os.environ.get("SUPPRESS_ENCODING_HINT") != "1":
                    enc = (getattr(sys.stdout, "encoding", "") or "").lower()
                    # ç°¡æ˜“åˆ¤å®š: cp932 / ansi ç³»ã§çµµæ–‡å­—ãŒå«ã¾ã‚Œãã†ãªè¡Œ
                    if enc and "utf" not in enc and any(ch for ch in line if ord(ch) > 0x2600):
                        try:
                            print(
                                "[INFO] æ–‡å­—åŒ–ã‘ã™ã‚‹å ´åˆã¯ 'chcp 65001' å®Ÿè¡Œå¾Œã«å†è©¦è¡Œã—ã¦ãã ã•ã„ (SUPPRESS_ENCODING_HINT=1 ã§éè¡¨ç¤º)",
                                flush=True,
                            )
                        except Exception:
                            pass
            try:
                print(line, flush=True)
                return
            except UnicodeEncodeError:
                try:
                    encoding = getattr(sys.stdout, "encoding", "") or "utf-8"
                    safe = line.encode(encoding, errors="replace").decode(
                        encoding, errors="replace"
                    )
                    print(safe, flush=True)
                    return
                except Exception:
                    pass
        except Exception:
            pass
        # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ASCII ç½®æ›
        try:
            fallback = line.encode("ascii", errors="replace").decode("ascii", errors="replace")
            print(fallback, flush=True)
        except Exception:
            pass


class RunCallbacks:
    """run_all_systems_today ã¸æ¸¡ã™ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ã¾ã¨ã‚ã‚‹ã€‚"""

    def __init__(
        self, logger: UILogger, progress_ui: ProgressUI, tracker: StageTracker
    ):  # noqa: E501
        self.logger = logger
        self.progress_ui = progress_ui
        self.tracker = tracker

    def ui_log(self, msg: str) -> None:
        self.logger.log(str(msg))

    def overall_progress(self, done: int, total: int, name: str) -> None:
        self.progress_ui.update(done, total, name)

    def per_system_progress(self, name: str, phase: str) -> None:
        self.tracker.update_progress(name, phase)

    def per_system_stage(
        self,
        name: str,
        value: int,
        filter_cnt: int | None = None,
        setup_cnt: int | None = None,
        cand_cnt: int | None = None,
        final_cnt: int | None = None,
    ) -> None:
        self.tracker.update_stage(
            name, value, filter_cnt, setup_cnt, cand_cnt, final_cnt
        )  # noqa: E501

    def per_system_exit(self, name: str, count: int) -> None:
        self.tracker.update_exit(name, count)

    def register_with_module(self) -> None:
        try:
            import scripts.run_all_systems_today as _run_today_mod

            # å®‰å…¨ãªå±æ€§ã‚¢ã‚¯ã‚»ã‚¹æ–¹æ³•ã‚’ä½¿ç”¨
            mod = _run_today_mod
            setattr(mod, "_PER_SYSTEM_STAGE", self.per_system_stage)
            setattr(mod, "_PER_SYSTEM_EXIT", self.per_system_exit)
            setattr(mod, "_SET_STAGE_UNIVERSE_TARGET", self.tracker.set_universe_target)
        except Exception:
            pass


@dataclass
class RunArtifacts:
    final_df: pd.DataFrame
    per_system: dict[str, pd.DataFrame]
    log_lines: list[str]
    total_elapsed: float
    stage_tracker: StageTracker
    logger: UILogger
    debug_mode: bool = False
    missing_report_path: Path | None = None
    missing_details: list[dict[str, Any]] | None = None


@dataclass
class ExitAnalysisResult:
    exits_today: pd.DataFrame
    planned: pd.DataFrame
    exit_counts: dict[str, int]
    error: str | None = None


def _indicator_requirements() -> dict[str, int]:
    """ã‚·ã‚°ãƒŠãƒ«è¨ˆç®—ã§ä½¿ç”¨ã™ã‚‹æŒ‡æ¨™æ—¥æ•°ã‚’å®šç¾©ã™ã‚‹ã€‚"""

    return {
        "ROC200": int(200 * 1.1),
        "SMA25": int(25 * 1.1),
        "ATR20": int(20 * 1.1),
        "ADX7": int(7 * 1.1),
        "RETURN_6D": int(6 * 1.1),
        "Drop3D": int(3 * 1.1),
        "return_6d": int(6 * 1.1),
    }


def _rows_needed(indicator_days: dict[str, int]) -> int:
    if not indicator_days:
        return 0
    return max(indicator_days.values())


def _prepare_symbol_data(
    symbols: list[str],
    rows: int,
    logger: UILogger,
    *,
    debug_scan: bool = False,
) -> tuple[dict[str, pd.DataFrame], list[dict[str, Any]]]:
    cache_key = (tuple(symbols), rows)
    symbol_cache = st.session_state.get("today_symbol_cache")
    if (
        not debug_scan
        and isinstance(symbol_cache, dict)
        and symbol_cache.get("key") == cache_key
        and isinstance(symbol_cache.get("data"), dict)
    ):
        data_map = symbol_cache.get("data", {})
        try:
            count = len(data_map)
        except Exception:
            count = 0
        logger.log(f"ğŸ“¦ åŸºç¤ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰å†åˆ©ç”¨: {count}/{len(symbols)}ä»¶ (å‰å›çµæœã‚’ä½¿ç”¨)")
        return data_map, []

    logger.log(f"ğŸ“¦ åŸºç¤ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {len(symbols)} éŠ˜æŸ„ (å¿…è¦æ—¥æ•°â‰’{rows})")
    data_map, missing_details = _collect_symbol_data(
        symbols,
        rows=rows,
        log_fn=logger.log,
        debug_scan=debug_scan,
    )
    if not debug_scan:
        st.session_state["today_symbol_cache"] = {"key": cache_key, "data": data_map}
    return data_map, missing_details


def _save_missing_report(missing_details: list[dict[str, Any]]) -> Path | None:
    if not missing_details:
        return None
    try:
        base_dir = Path(settings.LOGS_DIR)
    except Exception:
        base_dir = Path("logs")
    target_dir = base_dir / "debug"
    try:
        target_dir.mkdir(parents=True, exist_ok=True)
    except Exception:
        pass
    try:
        timestamp = time.strftime("%Y%m%d_%H%M%S")
    except Exception:
        timestamp = str(int(time.time()))
    path = target_dir / f"rolling_cache_missing_{timestamp}.csv"
    try:
        try:
            settings2 = get_settings(create_dirs=True)
            round_dec = getattr(settings2.cache, "round_decimals", None)
        except Exception:
            round_dec = None
        try:
            out_df = round_dataframe(pd.DataFrame(missing_details), round_dec)
        except Exception:
            out_df = pd.DataFrame(missing_details)
        out_df.to_csv(path, index=False)
    except Exception:
        return None
    return path


def _store_run_results(
    final_df: pd.DataFrame, per_system: dict[str, pd.DataFrame]
) -> None:  # noqa: E501
    try:
        try:
            settings2 = get_settings(create_dirs=True)
            round_dec = getattr(settings2.cache, "round_decimals", None)
        except Exception:
            round_dec = None
        try:
            st.session_state["today_final_df"] = round_dataframe(final_df.copy(), round_dec)
        except Exception:
            st.session_state["today_final_df"] = final_df.copy()
        stored = {}
        for k, v in per_system.items():
            try:
                stored[k] = round_dataframe(v.copy(), round_dec)
            except Exception:
                stored[k] = v.copy()
        st.session_state["today_per_system"] = stored  # noqa: E501
    except Exception:
        pass


def _postprocess_results(
    final_df: pd.DataFrame, per_system: dict[str, pd.DataFrame]
) -> tuple[pd.DataFrame, dict[str, pd.DataFrame]]:
    final_df = final_df.reset_index(drop=True)
    per_system = {name: df.reset_index(drop=True) for name, df in per_system.items()}
    final_df = _sort_final_df(final_df)
    if final_df is not None and not final_df.empty:
        try:
            final_df.insert(0, "no", range(1, len(final_df) + 1))
        except Exception:
            pass
    return final_df, per_system


def _sort_final_df(final_df: pd.DataFrame) -> pd.DataFrame:
    if final_df is None or final_df.empty or "system" not in final_df.columns:
        return final_df
    try:
        tmp = final_df.copy()
        tmp["_system_no"] = (
            tmp["system"].astype(str).str.extract(r"(\d+)").fillna(0).astype(int)
        )  # noqa: E501
        sort_cols = [c for c in ["side", "_system_no"] if c in tmp.columns]
        tmp = tmp.sort_values(sort_cols, kind="stable").drop(
            columns=["_system_no"], errors="ignore"
        )
        return tmp.reset_index(drop=True)
    except Exception:
        return final_df


def _log_run_completion(
    final_df: pd.DataFrame, per_system: dict[str, pd.DataFrame], elapsed: float
) -> None:
    try:
        m, s = divmod(int(max(0, elapsed)), 60)
        final_n = 0 if final_df is None or final_df.empty else int(len(final_df))
        per_counts_lines: list[str] = []
        counts_map = {
            str(name).strip().lower(): 0 if df is None or df.empty else int(len(df))
            for name, df in per_system.items()
            if str(name).strip()
        }
        if counts_map:
            per_counts_lines = format_group_counts(counts_map)
        detail = (
            f" | Long/Shortåˆ¥: {', '.join(per_counts_lines)}" if per_counts_lines else ""
        )  # noqa: E501
        _get_today_logger().info(
            "âœ… æœ¬æ—¥ã®ã‚·ã‚°ãƒŠãƒ«: ã‚·ã‚°ãƒŠãƒ«æ¤œå‡ºå‡¦ç†çµ‚äº† (çµŒé %dåˆ†%dç§’, æœ€çµ‚å€™è£œ %d ä»¶)%s",
            m,
            s,
            final_n,
            detail,
        )
    except Exception:
        pass


def _build_per_system_logs(log_lines: list[str]) -> dict[str, list[str]]:
    per_system_logs: dict[str, list[str]] = {f"system{i}": [] for i in range(1, 8)}
    skip_keywords = (
        "ğŸ“Š æŒ‡æ¨™è¨ˆç®—",
        "â±ï¸ ãƒãƒƒãƒæ™‚é–“",
        "ğŸ§® æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿",
        "ğŸ§® æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰",
        "ğŸ§® å…±æœ‰æŒ‡æ¨™ã®å‰è¨ˆç®—",
        "ğŸ“¦ åŸºç¤ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰",
        "å€™è£œæŠ½å‡º",
        "ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼",
        "indicator",
        "indicators",
        "batch time",
        "next batch size",
    )
    for ln in log_lines:
        try:
            if any(k in ln for k in skip_keywords):
                continue
        except Exception:
            pass
        ln_l = ln.lower()
        for i in range(1, 8):
            key = f"system{i}"
            tag_candidates = [f"[system{i}]", f" {key}:", f"{key}:", f" {key}ï¼š"]
            if any(tag in ln_l for tag in tag_candidates):
                per_system_logs[key].append(ln)
                break
    return per_system_logs


def _display_per_system_logs(per_system_logs: dict[str, list[str]]) -> None:
    if not per_system_logs:
        return
    if not any(per_system_logs[key] for key in per_system_logs):
        return
    tabs = st.tabs([f"system{i}" for i in range(1, 8)])
    for i, key in enumerate([f"system{i}" for i in range(1, 8)]):
        logs = per_system_logs.get(key, [])
        if not logs:
            continue
        with tabs[i]:
            st.text_area(
                label=f"ãƒ­ã‚°ï¼ˆ{key}ï¼‰",
                key=f"logs_{key}",
                value="\n".join(logs[-1000:]),
                height=380,
                disabled=True,
            )
            if key == "system2":
                _display_system2_filter_breakdown(logs)
            elif key == "system5":
                _display_system5_filter_breakdown(logs)


def _display_system2_filter_breakdown(logs: list[str]) -> None:
    try:
        detail_lines = [x for x in logs if ("ãƒ•ã‚£ãƒ«ã‚¿å†…è¨³:" in x or "filter breakdown:" in x)]
        if not detail_lines:
            return
        last_line = str(detail_lines[-1])
        disp = last_line.split("] ", 1)[1] if "] " in last_line else last_line
        st.caption(disp)
    except Exception:
        pass


def _display_system5_filter_breakdown(logs: list[str]) -> None:
    try:
        detail_lines = [
            x for x in logs if ("system5å†…è¨³" in x and ("AvgVol50" in x or "avgvol50" in x))
        ]
        if not detail_lines:
            return
        last_line = str(detail_lines[-1])
        disp = last_line.split("] ", 1)[1] if "] " in last_line else last_line
        st.caption(disp)
    except Exception:
        pass


def _configure_today_logger_ui() -> None:
    try:
        mode_env = (os.environ.get("TODAY_SIGNALS_LOG_MODE") or "").strip().lower()
    except Exception:
        mode_env = ""
    sel_mode = "single" if mode_env == "single" else "dated"
    try:
        import scripts.run_all_systems_today as _run_today_mod

        _run_today_mod._configure_today_logger(mode=sel_mode)
        sel_path = getattr(_run_today_mod, "_LOG_FILE_PATH", None)
        if sel_path:
            st.caption(f"ãƒ­ã‚°ä¿å­˜å…ˆ: {sel_path}")
    except Exception:
        pass


def execute_today_signals(run_config: RunConfig) -> RunArtifacts:
    # å®Ÿè¡Œé–‹å§‹æ™‚ã®ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
    today = get_signal_target_trading_day().normalize()
    try:
        run_id = str(uuid.uuid4())[:8]
    except Exception:
        run_id = "--------"

    # ä»®ã®loggerã‚’ä½œæˆã—ã¦ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
    temp_start_time = time.time()
    temp_progress_ui = ProgressUI({})
    temp_logger = UILogger(temp_start_time, temp_progress_ui)

    # ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è¡¨ç¤º
    temp_logger.log(
        "####################################################################", no_timestamp=True
    )
    temp_logger.log("# ğŸš€ğŸš€ğŸš€  æœ¬æ—¥ã®ã‚·ã‚°ãƒŠãƒ« å®Ÿè¡Œé–‹å§‹ (Engine)  ğŸš€ğŸš€ğŸš€", no_timestamp=True)

    # æ™‚åˆ»ã¨RUN-IDã€éŠ˜æŸ„æ•°ã®è¡¨ç¤º
    now_str = time.strftime("%Y-%m-%d %H:%M:%S")
    symbols_count = len(run_config.symbols) if run_config.symbols else 0
    temp_logger.log(
        f"# â±ï¸ {now_str} | éŠ˜æŸ„æ•°ï¼š{symbols_count}ã€€| RUN-ID: {run_id}", no_timestamp=True
    )
    temp_logger.log(
        "####################################################################", no_timestamp=True
    )

    # å–¶æ¥­æ—¥ã¨æ³¨æ„äº‹é …ã®è¡¨ç¤º
    temp_logger.log(f"ğŸ“… å¯¾è±¡å–¶æ¥­æ—¥ï¼ˆNYSEï¼‰: {today.date()}", no_timestamp=True)

    # ãƒ‡ãƒ¼ã‚¿ã®æ–°ã—ã•ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦å¿…è¦ãªå ´åˆã®ã¿è­¦å‘Šã‚’è¡¨ç¤º
    try:
        settings = get_settings()
        cm = CacheManager(settings)
        # SPYãƒ‡ãƒ¼ã‚¿ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æ–°ã—ã•ã‚’ç¢ºèª
        spy_df = cm.read("SPY", "rolling")
        if spy_df is not None and not spy_df.empty:
            # last_cache_dateã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã®ç°¡å˜ãªå®Ÿè£…
            if "date" in spy_df.columns:
                last_date = pd.to_datetime(spy_df["date"]).max()
            elif spy_df.index.name == "date" or hasattr(spy_df.index, "date"):
                last_date = pd.to_datetime(spy_df.index).max()
            else:
                last_date = None

            if last_date is not None:
                last_cache_date = pd.Timestamp(last_date).normalize()
                days_behind = (today - last_cache_date).days
                if days_behind > 1:  # 1å–¶æ¥­æ—¥ã‚ˆã‚Šå¤ã„å ´åˆã®ã¿è­¦å‘Š
                    temp_logger.log(
                        f"â„¹ï¸ æ³¨: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ãŒ{days_behind}æ—¥å¤ã„ãŸã‚ã€"
                        "ç›´è¿‘å–¶æ¥­æ—¥ãƒ™ãƒ¼ã‚¹ã§è¨ˆç®—ã—ã¾ã™ã€‚",
                        no_timestamp=True,
                    )
    except Exception:
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å¾“æ¥é€šã‚Šè­¦å‘Šã‚’è¡¨ç¤º
        temp_logger.log(
            "â„¹ï¸ æ³¨: EODHDã¯å½“æ—¥çµ‚å€¤ãŒæœªåæ˜ ã®ãŸã‚ã€ç›´è¿‘å–¶æ¥­æ—¥ãƒ™ãƒ¼ã‚¹ã§è¨ˆç®—ã—ã¾ã™ã€‚", no_timestamp=True
        )

    temp_logger.log("", no_timestamp=True)  # ç©ºè¡Œã‚’è¿½åŠ 

    # æ—¢å­˜ã®å‡¦ç†ã‚’ç¶™ç¶š
    indicator_days = _indicator_requirements()
    max_days = _rows_needed(indicator_days)
    start_time = time.time()
    ui_vis_raw = st.session_state.get("ui_vis", {})
    ui_vis = ui_vis_raw if isinstance(ui_vis_raw, dict) else {}
    progress_ui = ProgressUI(ui_vis)
    stage_tracker = StageTracker(ui_vis, progress_ui)
    logger = UILogger(start_time, progress_ui)
    callbacks = RunCallbacks(logger, progress_ui, stage_tracker)
    callbacks.register_with_module()
    _configure_today_logger_ui()
    buffer_days = max(20, int(max_days * 0.15))
    rows_needed = max_days + buffer_days
    symbols_for_data = list(dict.fromkeys([*run_config.symbols, "SPY"]))
    progress_ui.set_label("å¯¾è±¡èª­ã¿è¾¼ã¿")
    final_df: pd.DataFrame = pd.DataFrame()
    per_system: dict[str, pd.DataFrame] = {}
    debug_result: RunArtifacts | None = None
    with st.spinner("å®Ÿè¡Œä¸­... (çµŒéæ™‚é–“è¡¨ç¤ºã‚ã‚Š)"):
        logger.log("â–¶ æœ¬æ—¥ã®ã‚·ã‚°ãƒŠãƒ«: ã‚·ã‚°ãƒŠãƒ«æ¤œå‡ºå‡¦ç†é–‹å§‹")
        symbol_data_map, missing_details = _prepare_symbol_data(
            symbols_for_data,
            rows_needed,
            logger,
            debug_scan=run_config.scan_missing_only,
        )
        if run_config.scan_missing_only:
            total_elapsed = max(0.0, time.time() - start_time)
            report_path = _save_missing_report(missing_details)
            if missing_details:
                if report_path is not None:
                    logger.log(f"ğŸ§ª æ¬ ææ´—ã„å‡ºã—: {len(missing_details)}ä»¶ (CSV: {report_path})")
                else:
                    logger.log(f"ğŸ§ª æ¬ ææ´—ã„å‡ºã—: {len(missing_details)}ä»¶ (CSVä¿å­˜ã«å¤±æ•—)")
            else:
                logger.log("ğŸ§ª æ¬ ææ´—ã„å‡ºã—: æ¬ æã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            stage_tracker.finalize_counts(pd.DataFrame(), {})
            debug_result = RunArtifacts(
                final_df=pd.DataFrame(),
                per_system={},
                log_lines=logger.log_lines,
                total_elapsed=total_elapsed,
                stage_tracker=stage_tracker,
                logger=logger,
                debug_mode=True,
                missing_report_path=report_path,
                missing_details=missing_details,
            )
        else:
            result = compute_today_signals(
                run_config.symbols,
                capital_long=run_config.capital_long,
                capital_short=run_config.capital_short,
                save_csv=run_config.save_csv,
                notify=run_config.notify,
                csv_name_mode=run_config.csv_name_mode,
                log_callback=callbacks.ui_log,
                progress_callback=callbacks.overall_progress,
                per_system_progress=callbacks.per_system_progress,
                symbol_data=symbol_data_map,
                parallel=run_config.run_parallel,
            )
            # å®‰å…¨ã«çµæœã‚’è§£é‡ˆ
            if isinstance(result, (tuple, list)) and len(result) == 2:
                result_pair: Sequence[Any] = tuple(result)
                maybe_df, maybe_system = result_pair
                if isinstance(maybe_df, pd.DataFrame) and isinstance(maybe_system, dict):
                    final_df = maybe_df
                    per_system = maybe_system
                else:
                    actual_types = (
                        f"df={type(maybe_df).__name__}, system={type(maybe_system).__name__}"
                    )
                    logger.log(f"âš ï¸ compute_today_signals ã®æˆ»ã‚Šå€¤å‹ãŒä¸æ­£: {actual_types}")
            else:
                result_info = f"type={type(result).__name__}, len={len(result) if hasattr(result, '__len__') else 'N/A'}"
                logger.log(f"âš ï¸ compute_today_signals ã®æˆ»ã‚Šå€¤æ§‹é€ ãŒä¸æ­£: {result_info}")

    if debug_result is not None:
        return debug_result
    total_elapsed = max(0.0, time.time() - start_time)
    stage_tracker.finalize_counts(final_df, per_system)
    _store_run_results(final_df, per_system)
    return RunArtifacts(
        final_df=final_df,
        per_system=per_system,
        log_lines=logger.log_lines,
        total_elapsed=total_elapsed,
        stage_tracker=stage_tracker,
        logger=logger,
    )


def analyze_exit_candidates(paper_mode: bool) -> ExitAnalysisResult:
    """ç¾åœ¨ä¿æœ‰ä¸­ãƒã‚¸ã‚·ãƒ§ãƒ³ã®æ‰‹ä»•èˆã„äºˆå®šã‚’æ¨å®šã™ã‚‹ã€‚

    å½¹å‰²:
      1. ä¿æœ‰ãƒã‚¸ã‚·ãƒ§ãƒ³å–å¾—
      2. ã‚¨ãƒ³ãƒˆãƒªãƒ¼æ—¥è£œå®Œï¼ˆãƒ­ãƒ¼ã‚«ãƒ«â†’ä¸è¶³åˆ† Alpaca å–å¾—â†’ä¿å­˜ï¼‰
      3. ã‚·ã‚¹ãƒ†ãƒ åˆ¤å®š & Strategy ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆ
      4. ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ãƒ¼ exit ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç”¨ã„æœ¬æ—¥/å°†æ¥ exit ã‚’åˆ†é¡
    """

    exits_today_rows: list[dict[str, Any]] = []
    planned_rows: list[dict[str, Any]] = []
    exit_counts: dict[str, int] = {f"system{i}": 0 for i in range(1, 8)}
    try:
        client_tmp = ba.get_client(paper=paper_mode)
        try:
            positions = list(client_tmp.get_all_positions())
        except Exception:
            positions = []

        # 1) ã‚¨ãƒ³ãƒˆãƒªãƒ¼æ—¥ãƒãƒƒãƒ—èª­ã¿è¾¼ã¿
        raw_entry_map = load_entry_dates()
        entry_map: dict[str, str] = {}
        for k, v in raw_entry_map.items():
            try:
                entry_map[str(k).upper()] = str(v)
            except Exception:
                continue

        # 2) ä¸è¶³ã‚¨ãƒ³ãƒˆãƒªãƒ¼æ—¥ã®è£œå®Œ
        missing = [
            str(getattr(p, "symbol", "")).upper()
            for p in positions
            if str(getattr(p, "symbol", "")).upper()
            and str(getattr(p, "symbol", "")).upper() not in entry_map
        ]
        if missing:
            try:
                fetched = fetch_entry_dates_from_alpaca(client_tmp, missing)
            except Exception:
                fetched = None
            if fetched:
                for sym, ts in fetched.items():
                    if sym not in entry_map:
                        try:
                            entry_map[sym] = pd.Timestamp(ts).strftime("%Y-%m-%d")
                        except Exception:
                            continue
                try:
                    save_entry_dates(entry_map)
                except Exception:
                    pass

        symbol_system_map = _load_symbol_system_map(Path("data/symbol_system_map.json"))
        latest_trading_day = _latest_trading_day()
        strategy_classes = STRATEGY_CLASS_MAP

        # 3) å„ãƒã‚¸ã‚·ãƒ§ãƒ³è§£æ
        for pos in positions:
            result = _evaluate_position_for_exit(
                pos,
                entry_map,
                symbol_system_map,
                latest_trading_day,
                strategy_classes,
            )
            if result is None:
                continue
            system, _pos_side, _qty, exit_when, row_base, exit_today = result
            when_val = str(exit_when or "").strip()
            when_lower = when_val.lower()
            when_display = when_lower or when_val
            if exit_today:
                exit_counts[system] = exit_counts.get(system, 0) + 1
                if when_lower == "tomorrow_open":
                    planned_rows.append(row_base | {"when": when_display})
                else:
                    exits_today_rows.append(row_base | {"when": when_display})
            else:
                planned_rows.append(row_base | {"when": when_display})

        exits_today_df = pd.DataFrame(exits_today_rows)
        planned_df = pd.DataFrame(planned_rows)
        return ExitAnalysisResult(
            exits_today=exits_today_df, planned=planned_df, exit_counts=exit_counts
        )
    except Exception as exc:
        return ExitAnalysisResult(
            exits_today=pd.DataFrame(),
            planned=pd.DataFrame(),
            exit_counts=exit_counts,
            error=str(exc),
        )


def _load_symbol_system_map(path: Path) -> dict[str, str]:
    try:
        if path.exists():
            data = json.loads(path.read_text(encoding="utf-8"))
            if isinstance(data, dict):
                return {str(k).upper(): str(v).lower() for k, v in data.items()}
    except Exception:
        pass
    return {}


def _latest_trading_day() -> pd.Timestamp | None:
    calendar_day: pd.Timestamp | None = None
    try:
        calendar_day = get_latest_nyse_trading_day()
    except Exception:
        calendar_day = None

    price_day: pd.Timestamp | None = None
    try:
        spy_df = load_price("SPY", cache_profile="rolling")
        if spy_df is not None and not spy_df.empty:
            price_raw = pd.Timestamp(spy_df.index[-1])
            try:
                price_raw = price_raw.tz_localize(None)
            except (TypeError, ValueError, AttributeError):
                try:
                    price_raw = price_raw.tz_convert(None)
                except Exception:
                    pass
            price_day = pd.Timestamp(price_raw).normalize()
    except Exception:
        price_day = None

    if calendar_day is not None and price_day is not None:
        return max(calendar_day, price_day)
    return calendar_day or price_day


STRATEGY_CLASS_MAP: dict[str, Callable[[], Any]] = {
    "system1": System1Strategy,
    "system2": System2Strategy,
    "system3": System3Strategy,
    "system4": System4Strategy,
    "system5": System5Strategy,
    "system6": System6Strategy,
}


## äº’æ›ç”¨é–¢æ•°ã¯å‰Šé™¤ï¼ˆç›´æ¥ STRATEGY_CLASS_MAP ã‚’å‚ç…§ã™ã‚‹å®Ÿè£…ã¸ç§»è¡Œæ¸ˆã¿ï¼‰


def _evaluate_position_for_exit(
    pos: Any,
    entry_map: dict[str, Any],
    symbol_system_map: dict[str, str],
    latest_trading_day: pd.Timestamp | None,
    strategy_classes: dict[str, Callable[[], Any]],
) -> tuple[str, str, int, str, dict[str, Any], bool] | None:
    try:
        sym = str(getattr(pos, "symbol", "")).upper()
        if not sym:
            return None
        qty = int(abs(float(getattr(pos, "qty", 0)) or 0))
        if qty <= 0:
            return None
        pos_side = str(getattr(pos, "side", "")).lower()
        system = symbol_system_map.get(sym, "").lower()
        if not system:
            if sym == "SPY" and pos_side == "short":
                system = "system7"
            else:
                return None
        if system == "system7":
            return None
        entry_date_str = entry_map.get(sym)
        if not entry_date_str:
            return None
        entry_dt = pd.to_datetime(entry_date_str).normalize()
        df_price = load_price(sym, cache_profile="full")
        if df_price is None or df_price.empty:
            return None
        df = df_price.copy(deep=False)
        if "Date" in df.columns:
            df.index = pd.Index(pd.to_datetime(df["Date"]).dt.normalize())
        else:
            df.index = pd.Index(pd.to_datetime(df.index).normalize())
        if latest_trading_day is None and len(df.index) > 0:
            latest_trading_day = pd.to_datetime(df.index[-1]).normalize()
        entry_idx = _find_entry_index(df.index, entry_dt)
        if entry_idx < 0:
            return None
        strategy_cls = strategy_classes.get(system)
        if strategy_cls is None:
            return None
        strategy = strategy_cls()
        prev_close = float(df.iloc[int(max(0, entry_idx - 1))]["Close"])
        entry_price, stop_price = _entry_and_stop_prices(
            system, strategy, df, entry_idx, prev_close
        )
        if entry_price is None or stop_price is None:
            return None
        _apply_strategy_state(system, strategy, df, entry_idx, prev_close)
        # exit_priceã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«ä¿®æ­£
        exit_price, exit_date = strategy.compute_exit(
            df, int(entry_idx), float(entry_price), float(stop_price)
        )
        # exit_priceã‚’ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã«è¿½åŠ 
        today_norm = pd.to_datetime(df.index[-1]).normalize()
        if latest_trading_day is not None:
            today_norm = latest_trading_day
        is_today_exit, when = decide_exit_schedule(system, exit_date, today_norm)
        row_base = {
            "symbol": sym,
            "qty": qty,
            "position_side": pos_side,
            "system": system,
            "exit_price": exit_price,  # è¿½åŠ 
        }
        return system, pos_side, qty, when, row_base, is_today_exit
    except Exception:
        return None


def _find_entry_index(index: pd.Index, entry_dt: pd.Timestamp) -> int:
    try:
        if entry_dt in index:
            arr = index.get_indexer([entry_dt])
        else:
            arr = index.get_indexer([entry_dt], method="bfill")
        if len(arr) and arr[0] >= 0:
            return int(arr[0])
    except Exception:
        pass
    return -1


def _entry_and_stop_prices(
    system: str,
    strategy: Any,
    df: pd.DataFrame,
    entry_idx: int,
    prev_close: float,
) -> tuple[float | None, float | None]:
    try:
        if system == "system1":
            entry_price = float(df.iloc[int(entry_idx)]["Open"])
            atr20 = float(df.iloc[int(max(0, entry_idx - 1))]["ATR20"])
            stop_mult = float(strategy.config.get("stop_atr_multiple", 5.0))
            return entry_price, entry_price - stop_mult * atr20
        if system == "system2":
            entry_price = float(df.iloc[int(entry_idx)]["Open"])
            atr = float(df.iloc[int(max(0, entry_idx - 1))]["ATR10"])
            stop_mult = float(strategy.config.get("stop_atr_multiple", 3.0))
            return entry_price, entry_price + stop_mult * atr
        if system == "system6":
            ratio = float(strategy.config.get("entry_price_ratio_vs_prev_close", 1.05))
            entry_price = round(prev_close * ratio, 2)
            atr = float(df.iloc[int(max(0, entry_idx - 1))]["ATR10"])
            stop_mult = float(strategy.config.get("stop_atr_multiple", 3.0))
            return entry_price, entry_price + stop_mult * atr
        if system == "system3":
            ratio = float(strategy.config.get("entry_price_ratio_vs_prev_close", 0.93))
            entry_price = round(prev_close * ratio, 2)
            atr = float(df.iloc[int(max(0, entry_idx - 1))]["ATR10"])
            stop_mult = float(strategy.config.get("stop_atr_multiple", 2.5))
            return entry_price, entry_price - stop_mult * atr
        if system == "system4":
            entry_price = float(df.iloc[int(entry_idx)]["Open"])
            atr40 = float(df.iloc[int(max(0, entry_idx - 1))]["ATR40"])
            stop_mult = float(strategy.config.get("stop_atr_multiple", 1.5))
            return entry_price, entry_price - stop_mult * atr40
        if system == "system5":
            ratio = float(strategy.config.get("entry_price_ratio_vs_prev_close", 0.97))
            entry_price = round(prev_close * ratio, 2)
            atr = float(df.iloc[int(max(0, entry_idx - 1))]["ATR10"])
            stop_mult = float(strategy.config.get("stop_atr_multiple", 3.0))
            return entry_price, entry_price - stop_mult * atr
    except Exception:
        return None, None
    return None, None


def _apply_strategy_state(
    system: str,
    strategy: Any,
    df: pd.DataFrame,
    entry_idx: int,
    prev_close: float,
) -> None:
    if system == "system5":
        try:
            atr = float(df.iloc[int(max(0, entry_idx - 1))]["ATR10"])
            strategy._last_entry_atr = atr
        except Exception:
            pass
    if system in {"system3", "system5", "system6"}:
        try:
            strategy._last_prev_close = prev_close
        except Exception:
            pass


def render_exit_candidates_section(
    trade_options: TradeOptions,
    stage_tracker: StageTracker,
    logger: UILogger,
    notify: bool,
) -> ExitAnalysisResult:
    st.subheader("ä»Šæ—¥ã®æ‰‹ä»•èˆã„å€™è£œï¼ˆMOCï¼‰")
    result = analyze_exit_candidates(trade_options.paper_mode)
    if result.error:
        st.warning(f"æ‰‹ä»•èˆã„å€™è£œã®æ¨å®šã«å¤±æ•—ã—ã¾ã—ãŸ: {result.error}")
        return result
    _display_exit_orders_table(result, trade_options, stage_tracker, logger, notify)
    _display_planned_exits_section(result)  # trade_optionså¼•æ•°ã‚’å‰Šé™¤
    return result


def _display_exit_orders_table(
    result: ExitAnalysisResult,
    trade_options: TradeOptions,
    stage_tracker: StageTracker,
    logger: UILogger,
    notify: bool,
) -> None:
    if result.exits_today.empty:
        st.info("æœ¬æ—¥å¤§å¼•ã‘ã§ã®æ‰‹ä»•èˆã„å€™è£œã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        return
    st.dataframe(result.exits_today, use_container_width=True)
    stage_tracker.apply_exit_counts(result.exit_counts)
    if st.button("æœ¬æ—¥åˆ†ã®æ‰‹ä»•èˆã„æ³¨æ–‡ï¼ˆMOCï¼‰ã‚’é€ä¿¡"):
        from common.alpaca_order import submit_exit_orders_df

        res = submit_exit_orders_df(
            result.exits_today,
            paper=trade_options.paper_mode,
            tif="CLS",
            retries=int(trade_options.retries),
            delay=float(max(0.0, trade_options.delay)),
            log_callback=logger.log,
            notify=notify,
        )
        if res is not None and not res.empty:
            st.dataframe(res, use_container_width=True)


def _display_planned_exits_section(result: ExitAnalysisResult) -> None:  # trade_optionså¼•æ•°ã‚’å‰Šé™¤
    if result.planned.empty:
        return
    st.caption("æ˜æ—¥ç™ºæ³¨ã™ã‚‹æ‰‹ä»•èˆã„è¨ˆç”»ï¼ˆä¿å­˜â†’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãŒå®Ÿè¡Œï¼‰")
    st.dataframe(result.planned, use_container_width=True)
    planned_rows = [
        {str(k): v for k, v in row.items()} for row in result.planned.to_dict(orient="records")
    ]
    _auto_save_planned_exits(planned_rows, show_success=False)
    if st.button("è¨ˆç”»ã‚’ä¿å­˜ï¼ˆJSONLï¼‰"):
        _auto_save_planned_exits(planned_rows, show_success=True)
    st.write("")
    dry_run_plan = st.checkbox(
        "ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼ˆäºˆç´„é€ä¿¡ã‚’ãƒ†ã‚¹ãƒˆã¨ã—ã¦å®Ÿè¡Œï¼‰",
        value=True,
        key="planned_exits_dry_run",
    )
    col_open, col_close = st.columns(2)
    with col_open:
        if st.button("â±ï¸ å¯„ã‚Šï¼ˆOPGï¼‰äºˆç´„ã‚’ä»Šã™ãé€ä¿¡", key="run_scheduler_open"):
            _run_planned_exit_scheduler("open", dry_run_plan)
    with col_close:
        if st.button("â±ï¸ å¼•ã‘ï¼ˆCLSï¼‰äºˆç´„ã‚’ä»Šã™ãé€ä¿¡", key="run_scheduler_close"):
            _run_planned_exit_scheduler("close", dry_run_plan)


def _auto_save_planned_exits(
    planned_rows: list[dict[str, Any]], show_success: bool
) -> None:  # noqa: E501
    plan_path = Path("data/planned_exits.jsonl")
    try:
        plan_path.parent.mkdir(parents=True, exist_ok=True)
        with plan_path.open("w", encoding="utf-8") as f:
            for row in planned_rows:
                f.write(json.dumps(row, ensure_ascii=False) + "\n")
        if show_success:
            st.success(f"ä¿å­˜ã—ã¾ã—ãŸ: {plan_path}")
        else:
            st.caption(f"è¨ˆç”»ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {plan_path}")
    except Exception as exc:
        if show_success:
            st.error(f"ä¿å­˜ã«å¤±æ•—: {exc}")
        else:
            st.error(f"è¨ˆç”»ã®ä¿å­˜ã«å¤±æ•—: {exc}")


def _run_planned_exit_scheduler(kind: str, dry_run: bool) -> None:
    try:
        from schedulers.next_day_exits import submit_planned_exits as _run_sched

        df_exec = _run_sched(kind, dry_run=dry_run)
        if df_exec is not None and not df_exec.empty:
            st.success(
                "å¯„ã‚Šï¼ˆOPGï¼‰åˆ†ã®äºˆç´„é€ä¿¡ã‚’å®Ÿè¡Œã—ã¾ã—ãŸã€‚çµæœã‚’è¡¨ç¤ºã—ã¾ã™ã€‚"
                if kind == "open"
                else "å¼•ã‘ï¼ˆCLSï¼‰åˆ†ã®äºˆç´„é€ä¿¡ã‚’å®Ÿè¡Œã—ã¾ã—ãŸã€‚çµæœã‚’è¡¨ç¤ºã—ã¾ã™ã€‚"
            )
            st.dataframe(df_exec, use_container_width=True)
        else:
            st.info(
                "å¯„ã‚Šï¼ˆOPGï¼‰å¯¾è±¡ã®äºˆç´„ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
                if kind == "open"
                else "å¼•ã‘ï¼ˆCLSï¼‰å¯¾è±¡ã®äºˆç´„ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
            )
    except Exception as exc:
        label = "å¯„ã‚Šï¼ˆOPGï¼‰" if kind == "open" else "å¼•ã‘ï¼ˆCLSï¼‰"
        st.error(f"{label}äºˆç´„ã®å®Ÿè¡Œã«å¤±æ•—: {exc}")


def render_today_signals_results(
    artifacts: RunArtifacts,
    run_config: RunConfig,
    trade_options: TradeOptions,
) -> None:
    if artifacts.debug_mode:
        _render_missing_debug_results(artifacts)
        return
    final_df, per_system = _postprocess_results(
        artifacts.final_df, artifacts.per_system
    )  # noqa: E501
    artifacts.stage_tracker.finalize_counts(final_df, per_system)
    _show_total_elapsed(artifacts.total_elapsed)
    _log_run_completion(final_df, per_system, artifacts.total_elapsed)
    per_system_logs = _build_per_system_logs(artifacts.logger.log_lines)
    _display_per_system_logs(per_system_logs)
    render_exit_candidates_section(
        trade_options,
        artifacts.stage_tracker,
        artifacts.logger,
        run_config.notify,
    )
    _render_final_signals_section(
        final_df,
        per_system,
        run_config,
        trade_options,
        artifacts.logger,
    )
    _render_system_details(per_system, artifacts.stage_tracker, per_system_logs)
    _render_previous_results_section()
    _render_previous_run_logs(artifacts.log_lines)


def _render_missing_debug_results(artifacts: RunArtifacts) -> None:
    st.subheader("ğŸ§ª æ¬ ææ´—ã„å‡ºã—ãƒ¢ãƒ¼ãƒ‰ã®çµæœ")
    details = artifacts.missing_details or []
    if details:
        st.write(f"æ¤œå‡ºã•ã‚ŒãŸéŠ˜æŸ„: {len(details)}ä»¶")
        try:
            df_details = pd.DataFrame(details)
        except Exception:
            df_details = None
        if df_details is not None and not df_details.empty:
            st.dataframe(df_details, use_container_width=True)
        else:
            st.json(details)
    else:
        st.success("ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æ¬ æã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
    report_path = artifacts.missing_report_path
    if report_path:
        path_obj = Path(report_path)
        st.info(f"ãƒ¬ãƒãƒ¼ãƒˆ: {path_obj}")
        try:
            data_bytes = path_obj.read_bytes()
        except Exception:
            data_bytes = None
        if data_bytes:
            st.download_button(
                "æ¬ æãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=data_bytes,
                file_name=path_obj.name,
                mime="text/csv",
                key=f"missing_report_{int(time.time() * 1000)}",
            )
    st.info("ã“ã®ãƒ¢ãƒ¼ãƒ‰ã§ã¯åŸºç¤ãƒ‡ãƒ¼ã‚¿ã®æ¬ æç¢ºèªã®ã¿ã‚’å®Ÿæ–½ã—ã¾ã—ãŸã€‚ã‚·ã‚°ãƒŠãƒ«è¨ˆç®—ã¯è¡Œã£ã¦ã„ã¾ã›ã‚“ã€‚")
    _render_previous_results_section()
    _render_previous_run_logs(artifacts.log_lines)


def _show_total_elapsed(total_elapsed: float) -> None:
    total_elapsed = max(0.0, float(total_elapsed))
    m, s = divmod(int(total_elapsed), 60)
    st.info(f"ç·çµŒéæ™‚é–“: {m}åˆ†{s}ç§’")


def _render_final_signals_section(
    final_df: pd.DataFrame,
    per_system: dict[str, pd.DataFrame],
    run_config: RunConfig,
    trade_options: TradeOptions,
    logger: UILogger,
) -> None:
    st.subheader("æœ€çµ‚é¸å®šéŠ˜æŸ„")
    if final_df is None or final_df.empty:
        st.info("æœ¬æ—¥ã®ã‚·ã‚°ãƒŠãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        return
    _render_final_summary(final_df)
    st.dataframe(final_df, use_container_width=True)
    _render_skip_reports()
    _download_final_csv(final_df)
    st.session_state["today_shown_this_run"] = True
    if run_config.save_csv:
        _auto_save_final_results(final_df, per_system, run_config)
    if trade_options.do_trade:
        _execute_auto_trading(
            final_df,
            trade_options,
            run_config,
            logger,
        )


def _render_final_summary(final_df: pd.DataFrame) -> None:
    summary_lines: list[str] = []
    try:
        if "system" in final_df.columns:
            system_series = final_df["system"].astype(str).str.strip().str.lower()
            counts_map = system_series.value_counts().to_dict()
            values_map: dict[str, float] = {}
            if "position_value" in final_df.columns:
                values_series = (
                    final_df.assign(_system=system_series)[
                        ["_system", "position_value"]
                    ]  # noqa: E501
                    .groupby("_system")["position_value"]
                    .sum()
                )
                values_map = values_series.to_dict()
            if counts_map:
                if values_map:
                    summary_lines = format_group_counts_and_values(
                        counts_map, values_map
                    )  # noqa: E501
                else:
                    summary_lines = format_group_counts(counts_map)
    except Exception:
        summary_lines = []
    if summary_lines:
        font_css = (
            "font-family: 'Noto Sans JP', 'Meiryo', sans-serif; "
            "font-size: 1rem; letter-spacing: 0.02em;"
        )
        html_summary = " / ".join(summary_lines)
        st.markdown(
            f'<div style="{font_css}">ã‚µãƒãƒªãƒ¼ï¼ˆLong/Shortåˆ¥ï¼‰: {html_summary}</div>',
            unsafe_allow_html=True,
        )


def _render_skip_reports() -> None:
    try:
        settings2 = get_settings(create_dirs=True)
        results_dir = Path(getattr(settings2.outputs, "results_csv_dir", "results_csv"))
        skip_files = []
        for i in range(1, 8):
            name = f"system{i}"
            fp = results_dir / f"skip_summary_{name}.csv"
            if fp.exists() and fp.is_file():
                skip_files.append((name, fp))
        if skip_files:
            with st.expander("ğŸ§ª ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒƒãƒ—/ã‚·ãƒ§ãƒ¼ãƒˆä¸å¯ã®å†…è¨³CSVï¼ˆæœ¬æ—¥ï¼‰", expanded=False):
                _render_skip_file_group(skip_files, "skip")
            detail_files = []
            for i in range(1, 8):
                name = f"system{i}"
                fpd = results_dir / f"skip_details_{name}.csv"
                if fpd.exists() and fpd.is_file():
                    detail_files.append((name, fpd))
            if detail_files:
                st.markdown("---")
                st.caption("ã‚¹ã‚­ãƒƒãƒ—è©³ç´°ï¼ˆsymbolÃ—reasonï¼‰")
                _render_skip_file_group(detail_files, "skipdet")
            shortable_files = []
            for i in (2, 6):
                name = f"system{i}"
                fp2 = results_dir / f"shortability_excluded_{name}.csv"
                if fp2.exists() and fp2.is_file():
                    shortable_files.append((name, fp2))
            if shortable_files:
                st.markdown("---")
                st.caption("ã‚·ãƒ§ãƒ¼ãƒˆä¸å¯ã§é™¤å¤–ã•ã‚ŒãŸéŠ˜æŸ„ï¼ˆsystem2/6ï¼‰")
                _render_skip_file_group(shortable_files, "short_exc")
    except Exception:
        pass


def _render_skip_file_group(files: list[tuple[str, Path]], key_prefix: str) -> None:
    for name, path in files:
        cols = st.columns([4, 1])
        with cols[0]:
            try:
                df_skip = pd.read_csv(path)
            except Exception:
                df_skip = None
            st.caption(f"{name}: {path.name}")
            if df_skip is not None and not df_skip.empty:
                st.dataframe(df_skip, use_container_width=True)
            else:
                st.write("(ç©º) å†…è¨³æƒ…å ±ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        with cols[1]:
            try:
                data_bytes = path.read_bytes()
            except Exception:
                data_bytes = None
            if data_bytes:
                st.download_button(
                    label=f"{name} CSV",
                    data=data_bytes,
                    file_name=path.name,
                    mime="text/csv",
                    key=f"dl_{key_prefix}_{name}_{int(time.time() * 1000)}",
                )


def _download_final_csv(final_df: pd.DataFrame) -> None:
    try:
        settings2 = get_settings(create_dirs=True)
        round_dec = getattr(settings2.cache, "round_decimals", None)
    except Exception:
        round_dec = None
    try:
        out_df = round_dataframe(final_df, round_dec)
    except Exception:
        out_df = final_df
    csv = out_df.to_csv(index=False).encode("utf-8")
    st.download_button(
        "æœ€çµ‚CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=csv,
        file_name="today_signals_final.csv",
        on_click=_reset_shown_flag,
    )


def _auto_save_final_results(
    final_df: pd.DataFrame,
    per_system: dict[str, pd.DataFrame],
    run_config: RunConfig,
) -> None:
    try:
        settings2 = get_settings(create_dirs=True)
        sig_dir = Path(settings2.outputs.signals_dir)
        sig_dir.mkdir(parents=True, exist_ok=True)
        now = datetime.now()
        ts = now.strftime("%Y-%m-%d")
        if run_config.csv_name_mode == "datetime":
            ts = now.strftime("%Y-%m-%d_%H%M")
        elif run_config.csv_name_mode == "runid":
            rid = st.session_state.get("last_run_id") or "RUN"
            ts = f"{now.strftime('%Y-%m-%d')}_{rid}"
        fp = sig_dir / f"today_signals_{ts}.csv"
        try:
            settings2 = get_settings(create_dirs=True)
            round_dec = getattr(settings2.cache, "round_decimals", None)
        except Exception:
            round_dec = None
        try:
            out_df = round_dataframe(final_df, round_dec)
        except Exception:
            out_df = final_df
        out_df.to_csv(fp, index=False)
        st.caption(f"è‡ªå‹•ä¿å­˜: {fp}")
        for name, df in per_system.items():
            try:
                if df is None or df.empty:
                    continue
                fp_sys = sig_dir / f"signals_{name}_{ts}.csv"
                try:
                    out_df = round_dataframe(df, round_dec)
                except Exception:
                    out_df = df
                out_df.to_csv(fp_sys, index=False)
                st.caption(f"è‡ªå‹•ä¿å­˜: {fp_sys}")
            except Exception as exc:
                st.warning(f"{name} ã®è‡ªå‹•ä¿å­˜ã«å¤±æ•—: {exc}")
    except Exception as exc:
        st.warning(f"è‡ªå‹•ä¿å­˜ã«å¤±æ•—: {exc}")


def _execute_auto_trading(
    final_df: pd.DataFrame,
    trade_options: TradeOptions,
    run_config: RunConfig,
    logger: UILogger,
) -> None:
    st.divider()
    st.subheader("Alpacaè‡ªå‹•ç™ºæ³¨çµæœ")
    system_order_type = {
        "system1": "market",
        "system3": "market",
        "system4": "market",
        "system5": "market",
        "system2": "limit",
        "system6": "limit",
        "system7": "limit",
    }
    results_df = submit_orders_df(
        final_df,
        paper=trade_options.paper_mode,
        order_type=None,
        system_order_type=system_order_type,
        tif="DAY",
        retries=int(trade_options.retries),
        delay=float(max(0.0, trade_options.delay)),
        log_callback=logger.log,
        notify=run_config.notify,
    )
    if results_df is not None and not results_df.empty:
        st.dataframe(results_df, use_container_width=True)
        if trade_options.poll_status and any(
            results_df["order_id"].fillna("").astype(str)
        ):  # noqa: E501
            _poll_order_status(results_df, trade_options)
    if trade_options.update_bp_after:
        _update_buying_power(trade_options)


def _poll_order_status(results_df: pd.DataFrame, trade_options: TradeOptions) -> None:
    st.info("æ³¨æ–‡çŠ¶æ³ã‚’10ç§’é–“ãƒãƒ¼ãƒªãƒ³ã‚°ã—ã¾ã™...")
    try:
        client = ba.get_client(paper=trade_options.paper_mode)
    except Exception:
        client = None
    if client is None:
        return
    order_ids = [str(oid) for oid in results_df["order_id"].tolist() if oid]
    end = time.time() + 10
    last: dict[str, Any] = {}
    while time.time() < end:
        status_map = ba.get_orders_status_map(client, order_ids)
        if status_map != last:
            if status_map:
                st.caption("æ³¨æ–‡çŠ¶æ³ã‚’æ›´æ–°ã—ã¾ã—ãŸï¼ˆè©³ç´°ã¯ãƒ­ã‚°å‚ç…§ï¼‰")
            last = status_map
        time.sleep(1.0)


def _update_buying_power(trade_options: TradeOptions) -> None:
    try:
        client2 = ba.get_client(paper=trade_options.paper_mode)
        acct = client2.get_account()
        bp_raw = getattr(acct, "buying_power", None)
        if bp_raw is None:
            bp_raw = getattr(acct, "cash", None)
        if bp_raw is not None:
            bp = float(bp_raw)
            st.session_state["today_cap_long"] = round(bp / 2.0, 2)
            st.session_state["today_cap_short"] = round(bp / 2.0, 2)
            st.success(
                "ç´„å®šåæ˜ å¾Œã®è³‡é‡‘ä½™åŠ›ã§Long/Shortã‚’å†è¨­å®šã—ã¾ã—ãŸ: "
                f"${st.session_state['today_cap_long']} / "
                f"${st.session_state['today_cap_short']}"
            )
        else:
            st.warning("Alpacaå£åº§æƒ…å ±: buying_power/cashãŒå–å¾—ã§ãã¾ã›ã‚“ï¼ˆæ›´æ–°ãªã—ï¼‰")
    except Exception as exc:
        st.error(f"ä½™åŠ›ã®è‡ªå‹•æ›´æ–°ã«å¤±æ•—: {exc}")


def _render_system_details(
    per_system: dict[str, pd.DataFrame],
    stage_tracker: StageTracker,
    per_system_logs: dict[str, list[str]] | None = None,
) -> None:
    with st.expander("ã‚·ã‚¹ãƒ†ãƒ åˆ¥è©³ç´°"):
        settings_local = get_settings(create_dirs=True)
        results_dir = Path(getattr(settings_local.outputs, "results_csv_dir", "results_csv"))
        shortable_excluded_map = {}
        for i in (2, 6):
            name = f"system{i}"
            fp = results_dir / f"shortability_excluded_{name}.csv"
            if fp.exists() and fp.is_file():
                try:
                    df_exc = pd.read_csv(fp)
                    if df_exc is not None and not df_exc.empty:
                        shortable_excluded_map[name] = set(df_exc["symbol"].astype(str).str.upper())
                except Exception:
                    pass
        system_order = [f"system{i}" for i in range(1, 8)]
        for name in system_order:
            st.markdown(f"#### {name}")
            display_metrics = stage_tracker.get_display_metrics(name)
            # Line length fix - split formatted string
            metrics_parts = [
                f"Tgt {StageTracker._format_value(display_metrics.get('target'))}",
                f"FILpass {StageTracker._format_value(display_metrics.get('filter'))}",
                f"STUpass {StageTracker._format_value(display_metrics.get('setup'))}",
                f"TRDlist {stage_tracker._format_trdlist(display_metrics.get('cand'))}",
                f"Entry {StageTracker._format_value(display_metrics.get('entry'))}",
                f"Exit {StageTracker._format_value(display_metrics.get('exit'))}",
            ]
            metrics_line = "  ".join(metrics_parts)
            st.caption(metrics_line)
            df = per_system.get(name)
            if df is None or df.empty:
                # Try to extract explicit zero-reason from per-system logs if available
                reason_text: str | None = None
                try:
                    if per_system_logs and name in per_system_logs:
                        logs = per_system_logs.get(name) or []
                        for ln in reversed(logs):
                            if not ln:
                                continue
                            m = re.search(r"å€™è£œ0ä»¶ç†ç”±[:ï¼š]\s*(.+)$", ln)
                            if m:
                                reason_text = m.group(1).strip()
                                break
                            m2 = re.search(r"ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸æˆç«‹[:ï¼š]\s*(.+)$", ln)
                            if m2:
                                reason_text = m2.group(1).strip()
                                break
                except Exception:
                    reason_text = None

                st.write("(ç©º) å€™è£œã¯0ä»¶ã§ã™ã€‚")
                if reason_text:
                    st.info(f"å€™è£œ0ä»¶ç†ç”±: {reason_text}")
                continue
            df_disp = df.copy()
            side_type = None
            if name in LONG_SYSTEMS:
                side_type = "long"
            elif name in SHORT_SYSTEMS:
                side_type = "short"
            if side_type and "side" in df_disp.columns:
                mask = df_disp["side"].str.lower() != side_type
                if mask.any():
                    fill_cols = [
                        col
                        for col in df_disp.columns
                        if col not in {"symbol", "side", "system"}  # noqa: E501
                    ]
                    if fill_cols:
                        df_disp.loc[:, fill_cols] = df_disp.loc[:, fill_cols].astype(
                            "object"
                        )  # noqa: E501
                        df_disp.loc[mask, fill_cols] = "-"
            if name in shortable_excluded_map:
                excluded_syms = shortable_excluded_map[name]
                if excluded_syms:
                    st.caption(f"ğŸš« ã‚·ãƒ§ãƒ¼ãƒˆä¸å¯ã§é™¤å¤–: {len(excluded_syms)}ä»¶")
                    st.write(
                        f"<span style='color:red;font-size:0.95em;'>"
                        f"ã‚·ãƒ§ãƒ¼ãƒˆä¸å¯: {', '.join(sorted(excluded_syms)[:10])}"
                        f"{' ...' if len(excluded_syms) > 10 else ''}"  # noqa: E501
                        f"</span>",
                        unsafe_allow_html=True,
                    )
            st.dataframe(df_disp, use_container_width=True)


def _render_previous_results_section() -> None:
    try:
        if (not st.session_state.get("today_shown_this_run", False)) and (
            "today_final_df" in st.session_state
        ):
            prev_df = st.session_state.get("today_final_df")
            if prev_df is not None and not prev_df.empty:
                st.subheader("å‰å›ã®æœ€çµ‚é¸å®šéŠ˜æŸ„ï¼ˆå†è¡¨ç¤ºï¼‰")
                st.dataframe(prev_df, use_container_width=True)
                try:
                    settings2 = get_settings(create_dirs=True)
                    round_dec = getattr(settings2.cache, "round_decimals", None)
                except Exception:
                    round_dec = None
                try:
                    prev_out = round_dataframe(prev_df, round_dec)
                except Exception:
                    prev_out = prev_df
                csv_prev = prev_out.to_csv(index=False).encode("utf-8")
                st.download_button(
                    "æœ€çµ‚CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆå‰å›ï¼‰",
                    data=csv_prev,
                    file_name="today_signals_final_prev.csv",
                    key="download_prev_final",
                    on_click=_reset_shown_flag,
                )
                prev_per = st.session_state.get("today_per_system", {})
                if isinstance(prev_per, dict):
                    with st.expander("å‰å›ã®ã‚·ã‚¹ãƒ†ãƒ åˆ¥CSV", expanded=False):
                        for name, df in prev_per.items():
                            if df is None or df.empty:
                                continue
                            st.markdown(f"#### {name}")
                            st.dataframe(df, use_container_width=True)
    except Exception:
        pass


def _render_previous_run_logs(log_lines: list[str]) -> None:
    prev_msgs = [line for line in log_lines if line and ("(å‰å›çµæœ) system" in line)]
    if not prev_msgs:
        return

    def _parse_prev_line(ln: str) -> tuple[str, int, str, str]:
        ts = ln.split("] ", 1)[0].strip("[")
        m = re.search(r"\(å‰å›çµæœ\) (system\d+):\s*(\d+)", ln)
        sys = m.group(1) if m else "system999"
        cnt = int(m.group(2)) if m else 0
        return sys, cnt, ts, ln

    parsed = [_parse_prev_line(x) for x in prev_msgs]
    order = {f"system{i}": i for i in range(1, 8)}
    parsed.sort(key=lambda t: order.get(t[0], 999))
    lines_sorted = [f"{p[2]} | {p[0]}: {p[1]}ä»¶\n{p[3]}" for p in parsed]
    with st.expander("å‰å›çµæœï¼ˆsystemåˆ¥ï¼‰", expanded=False):
        st.text("\n\n".join(lines_sorted))


def _log_and_notify(
    message: str,
    notifier: Callable[[str], None] | None,
    log_callback: Callable[[str], None] | None,
    level: int = logging.INFO,
):
    """Log to both logger and optional callbacks."""

    _get_today_logger().log(level, message)
    if notifier:
        try:
            notifier(message)
        except Exception as e:
            _get_today_logger().warning("Notifier failed: %s", e)
    if log_callback:
        try:
            log_callback(message)
        except Exception as e:
            _get_today_logger().warning("Log callback failed: %s", e)


# =============================================================================
# ãƒ¡ã‚¤ãƒ³ UI å®Ÿè¡Œéƒ¨åˆ†
# =============================================================================

with st.sidebar:
    st.header("ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹")
    universe: list[str] = []
    try:
        import common.universe as univ
        from common.symbol_universe import build_symbol_universe_from_settings

        logger = logging.getLogger("today_signals.ui")
        universe = build_symbol_universe_from_settings(settings, logger=logger)
    except Exception as exc:
        universe = []
        st.warning(f"NASDAQ/EODHDã®éŠ˜æŸ„å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {exc}")

    if not universe:
        try:
            import common.universe as univ

            universe = univ.load_universe_file()
        except Exception:
            pass

    if not universe:
        try:
            import common.universe as univ

            universe = univ.build_universe_from_cache(limit=None)
            univ.save_universe_file(universe)
        except Exception:
            universe = []

    all_syms = universe

    # ä»»æ„ã®ä»¶æ•°ã§ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ã‚’åˆ¶é™ã™ã‚‹ãƒ†ã‚¹ãƒˆç”¨ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    limit_max = max(1, len(all_syms))
    test_limit = st.number_input(
        "éŠ˜æŸ„æ•° (0ã¯å…¨éŠ˜æŸ„)",
        min_value=0,
        max_value=limit_max,
        value=0,
        step=1,
    )
    syms = all_syms[: int(test_limit)] if test_limit else all_syms

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
    st.session_state["universe_symbols"] = syms

    st.write(f"éŠ˜æŸ„æ•°: {len(syms)}")
    st.write(", ".join(syms[:10]) + (" ..." if len(syms) > 10 else ""))

    # Alpacaæœªç´„å®šæ³¨æ–‡è¡¨ç¤º
    st.header("Alpacaæ³¨æ–‡çŠ¶æ³")

    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®è¡¨ç¤º
    with st.expander("ğŸ”§ ãƒ‡ãƒãƒƒã‚°æƒ…å ±"):
        st.write("broker_alpaca ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å±æ€§:")
        ba_attrs = [attr for attr in dir(ba) if not attr.startswith("_")]
        for attr in sorted(ba_attrs):
            if attr == "get_open_orders":
                st.write(f"âœ… {attr}: {type(getattr(ba, attr))}")
            elif callable(getattr(ba, attr)):
                st.write(f"ğŸ“ {attr}: {type(getattr(ba, attr))}")
            else:
                st.write(f"ğŸ“¦ {attr}: {type(getattr(ba, attr))}")

        st.write(f"get_open_orders å­˜åœ¨ç¢ºèª: {hasattr(ba, 'get_open_orders')}")
        if hasattr(ba, "get_open_orders"):
            st.write(f"get_open_orders å‹: {type(ba.get_open_orders)}")
            st.write(f"get_open_orders docstring: {ba.get_open_orders.__doc__}")

    if st.button("ğŸ“‹ æœªç´„å®šæ³¨æ–‡ã‚’è¡¨ç¤º"):
        try:
            paper_mode = st.session_state.get("paper_mode", True)

            # ãƒ‡ãƒãƒƒã‚°: ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«çŠ¶æ…‹ã®ç¢ºèª
            st.info(f"broker_alpaca ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«: {ba}")
            st.info(f"get_open_orders å­˜åœ¨: {hasattr(ba, 'get_open_orders')}")

            if not hasattr(ba, "get_open_orders"):
                st.error("get_open_orders é–¢æ•°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                available_funcs = [
                    attr
                    for attr in dir(ba)
                    if callable(getattr(ba, attr)) and not attr.startswith("_")
                ]
                st.write("åˆ©ç”¨å¯èƒ½ãªé–¢æ•°:")
                st.write(available_funcs)
                st.stop()

            client = ba.get_client(paper=paper_mode)
            orders = ba.get_open_orders(client)
            if orders:
                orders_data = []
                for order in orders:
                    orders_data.append(
                        {
                            "æ³¨æ–‡ID": order.id,
                            "éŠ˜æŸ„": order.symbol,
                            "ã‚µã‚¤ãƒ‰": order.side,
                            "æ•°é‡": order.qty,
                            "æ³¨æ–‡ä¾¡æ ¼": getattr(order, "limit_price", "Market"),
                            "æ³¨æ–‡ã‚¿ã‚¤ãƒ—": order.order_type,
                            "çŠ¶æ³": order.status,
                            "ä½œæˆæ—¥æ™‚": order.created_at,
                        }
                    )
                orders_df = pd.DataFrame(orders_data)
                st.dataframe(orders_df, use_container_width=True)
            else:
                st.info("æœªç´„å®šæ³¨æ–‡ã¯ã‚ã‚Šã¾ã›ã‚“")
        except Exception as e:
            st.error(f"æ³¨æ–‡å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            st.error(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {type(e).__name__}")
            import traceback

            st.code(traceback.format_exc())

    st.header("è³‡ç”£")
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
    if "today_cap_long" not in st.session_state:
        st.session_state["today_cap_long"] = 10000.0
    if "today_cap_short" not in st.session_state:
        st.session_state["today_cap_short"] = 10000.0

    # Alpacaè³‡ç”£å–å¾—ãƒœã‚¿ãƒ³ã‚’è¿½åŠ 
    if st.button("ğŸ’° Alpacaã‹ã‚‰ç¾åœ¨ã®è³‡ç”£ã‚’å–å¾—"):
        try:
            # æ¥ç¶šå‰ã®äº‹å‰ãƒã‚§ãƒƒã‚¯
            api_key = os.environ.get("APCA_API_KEY_ID")
            api_secret = os.environ.get("APCA_API_SECRET_KEY")

            if not api_key or not api_secret:
                st.error("âŒ Alpaca APIèªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
                st.info("ç’°å¢ƒå¤‰æ•° APCA_API_KEY_ID ã¨ APCA_API_SECRET_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„")
            else:
                # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šãƒ†ã‚¹ãƒˆ
                with st.spinner("Alpacaã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šä¸­..."):
                    client = ba.get_client(paper=st.session_state.get("paper_mode", True))
                    acct = client.get_account()

                equity = getattr(acct, "equity", None)
                cash = getattr(acct, "cash", None)
                buying_power = getattr(acct, "buying_power", None)

                if equity is not None:
                    equity_val = float(equity)
                    st.success(f"âœ… ç·è³‡ç”£: ${equity_val:,.2f}")
                if cash is not None:
                    cash_val = float(cash)
                    st.info(f"ğŸ’µ ç¾é‡‘æ®‹é«˜: ${cash_val:,.2f}")
                if buying_power is not None:
                    bp_val = float(buying_power)
                    st.info(f"ğŸš€ è²·ä»˜ä½™åŠ›: ${bp_val:,.2f}")

                    # è²·ä»˜ä½™åŠ›ã‚’åŠåˆ†ãšã¤ãƒ­ãƒ³ã‚°ãƒ»ã‚·ãƒ§ãƒ¼ãƒˆã«é…åˆ†
                    half_bp = round(bp_val / 2.0, 2)
                    st.session_state["today_cap_long"] = half_bp
                    st.session_state["today_cap_short"] = half_bp
                    st.success("è³‡é‡‘é…åˆ†ã‚’æ›´æ–°ã—ã¾ã—ãŸ:")
                    st.success(f"ãƒ­ãƒ³ã‚° `${half_bp:,.2f}` / ã‚·ãƒ§ãƒ¼ãƒˆ `${half_bp:,.2f}`")
                else:
                    st.warning("è²·ä»˜ä½™åŠ›ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")

        except Exception as exc:
            ERROR_MSG = str(exc)
            if "getaddrinfo failed" in ERROR_MSG or "Failed to resolve" in ERROR_MSG:
                st.error("ğŸŒ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚¨ãƒ©ãƒ¼")
                st.error("- ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„")
                st.error("- DNSã‚µãƒ¼ãƒãƒ¼è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„")
                st.error("- ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«/ãƒ—ãƒ­ã‚­ã‚·è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„")
                with st.expander("è©³ç´°ã‚¨ãƒ©ãƒ¼æƒ…å ±"):
                    st.code(ERROR_MSG)
            elif "HTTPSConnectionPool" in ERROR_MSG:
                st.error("ğŸ”’ HTTPSæ¥ç¶šã‚¨ãƒ©ãƒ¼")
                st.error("- SSLè¨¼æ˜æ›¸ã®å•é¡Œã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
                st.error("- ãƒ—ãƒ­ã‚­ã‚·è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„")
                with st.expander("è©³ç´°ã‚¨ãƒ©ãƒ¼æƒ…å ±"):
                    st.code(ERROR_MSG)
            elif "401" in ERROR_MSG or "403" in ERROR_MSG:
                st.error("ğŸ”‘ APIèªè¨¼ã‚¨ãƒ©ãƒ¼")
                st.error("- API ã‚­ãƒ¼ã¨ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„")
                st.error("- APIã‚­ãƒ¼ã®æ¨©é™ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
            else:
                st.error(f"âŒ Alpacaè³‡ç”£å–å¾—ã‚¨ãƒ©ãƒ¼: {ERROR_MSG}")
                st.info("ğŸ’¡ ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ç’°å¢ƒã§ã¯æ‰‹å‹•ã§è³‡é‡‘ã‚’è¨­å®šã—ã¦ãã ã•ã„")

    col1, col2 = st.columns(2)
    with col1:
        cap_long = st.number_input(
            "ãƒ­ãƒ³ã‚°è³‡æœ¬ (USD)",
            min_value=0.0,
            step=100.0,
            key="today_cap_long",
        )
    with col2:
        cap_short = st.number_input(
            "ã‚·ãƒ§ãƒ¼ãƒˆè³‡æœ¬ (USD)",
            min_value=0.0,
            step=100.0,
            key="today_cap_short",
        )

    st.header("ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
    save_csv = st.checkbox("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜", value=True, key="save_csv")

    # CSVãƒ•ã‚¡ã‚¤ãƒ«åã®å½¢å¼é¸æŠï¼ˆdate/datetime/runidï¼‰
    st.session_state.setdefault("csv_name_mode", "date")
    csv_name_mode = st.selectbox(
        "CSVãƒ•ã‚¡ã‚¤ãƒ«å",
        options=["date", "datetime", "runid"],
        index=["date", "datetime", "runid"].index(
            str(st.session_state.get("csv_name_mode", "date"))
        ),
        help="date=YYYY-MM-DD / datetime=YYYY-MM-DD_HHMM / runid=YYYY-MM-DD_RUNID",
        key="csv_name_mode",
    )

    # æ—¢å®šã§ä¸¦åˆ—å®Ÿè¡Œã‚’ONï¼ˆWindowsã§ã‚‚æœ‰åŠ¹åŒ–ï¼‰
    import platform

    is_windows = platform.system().lower().startswith("win")
    RUN_PARALLEL_DEFAULT = True
    run_parallel = st.checkbox(
        "ä¸¦åˆ—å®Ÿè¡Œï¼ˆã‚·ã‚¹ãƒ†ãƒ æ¨ªæ–­ï¼‰", value=RUN_PARALLEL_DEFAULT, key="run_parallel"
    )

    st.header("ãƒ‡ãƒãƒƒã‚°")
    scan_missing_only = st.checkbox(
        "ğŸ§ª æ¬ ææ´—ã„å‡ºã—ãƒ¢ãƒ¼ãƒ‰ï¼ˆãƒ­ãƒ¼ãƒªãƒ³ã‚°ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰",
        key="today_scan_missing_only",
        help="rolling ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ã®èª­ã¿è¾¼ã¿æ™‚ã«æ¬ æã‚’æ¤œå‡ºã—ã€CSVã«æ›¸ãå‡ºã—ã¦çµ‚äº†ã—ã¾ã™ã€‚",
    )
    if scan_missing_only:
        st.caption("â€» ã“ã®ãƒ¢ãƒ¼ãƒ‰ã§ã¯ã‚·ã‚°ãƒŠãƒ«è¨ˆç®—ã‚’è¡Œã„ã¾ã›ã‚“ã€‚æ¬ æãƒ¬ãƒãƒ¼ãƒˆã®ã¿å‡ºåŠ›ã—ã¾ã™ã€‚")

    # é€šçŸ¥ï¼ˆSlack Bot Tokenï¼‰è¨­å®šï¼ˆãƒãƒ£ãƒ³ãƒãƒ«æŒ‡å®šãƒ•ã‚©ãƒ¼ãƒ ã¯å»ƒæ­¢ï¼‰
    st.header("é€šçŸ¥è¨­å®šï¼ˆSlack Bot Tokenï¼‰")
    st.session_state.setdefault("use_slack_notify", True)
    use_slack_notify = st.checkbox(
        "Slacké€šçŸ¥ã‚’æœ‰åŠ¹åŒ–ï¼ˆBot Tokenï¼‰",
        key="use_slack_notify",
        help="ç’°å¢ƒå¤‰æ•° SLACK_BOT_TOKEN ãŒè¨­å®šæ¸ˆã¿ã§ã‚ã‚‹å‰æï¼ˆé€šçŸ¥å…ˆã¯æ—¢å®šå€¤ã‚’ä½¿ç”¨ï¼‰ã€‚",
    )
    # ç°¡æ˜“ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯è¡¨ç¤º
    try:
        has_token = bool(os.environ.get("SLACK_BOT_TOKEN", "").strip())
        st.caption("ãƒˆãƒ¼ã‚¯ãƒ³: " + ("æ¤œå‡ºæ¸ˆã¿" if has_token else "æœªè¨­å®šï¼ˆ.envã‚’è¨­å®šã—ã¦ãã ã•ã„ï¼‰"))
    except Exception:
        pass

    # ä¸¦åˆ—å®Ÿè¡Œã®è©³ç´°è¨­å®šã¯å‰Šé™¤ï¼ˆåˆæœŸãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæŒ™å‹•ã«æˆ»ã™ï¼‰
    st.header("Alpacaè‡ªå‹•ç™ºæ³¨")
    paper_mode = st.checkbox("ãƒšãƒ¼ãƒ‘ãƒ¼ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚’ä½¿ç”¨", value=True, key="paper_mode")
    retries = st.number_input("ãƒªãƒˆãƒ©ã‚¤å›æ•°", min_value=0, max_value=5, value=2, key="retries")
    delay = st.number_input(
        "ç™ºæ³¨é–“éš” (ç§’)", min_value=0.0, max_value=10.0, value=1.0, step=0.1, key="delay"
    )
    poll_status = st.checkbox("æ³¨æ–‡çŠ¶æ³ã‚’ãƒãƒ¼ãƒªãƒ³ã‚°", value=False, key="poll_status")
    do_trade = st.checkbox("å®Ÿéš›ã«ç™ºæ³¨ã™ã‚‹", value=False, key="do_trade")
    update_bp_after = st.checkbox("ç´„å®šå¾Œã«ä½™åŠ›ã‚’æ›´æ–°", value=False, key="update_bp_after")

# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨åˆ†
# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®šï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ãŒæœªå®Ÿè¡Œã®å ´åˆï¼‰
syms = st.session_state.get("universe_symbols", [])
cap_long = st.session_state.get("today_cap_long", 10000.0)
cap_short = st.session_state.get("today_cap_short", 10000.0)
save_csv = st.session_state.get("save_csv", True)
csv_name_mode = st.session_state.get("csv_name_mode", "date")
use_slack_notify = st.session_state.get("use_slack_notify", True)
run_parallel = st.session_state.get("run_parallel", True)
scan_missing_only = st.session_state.get("today_scan_missing_only", False)
paper_mode = st.session_state.get("paper_mode", True)
retries = st.session_state.get("retries", 2)
delay = st.session_state.get("delay", 1.0)
poll_status = st.session_state.get("poll_status", False)
do_trade = st.session_state.get("do_trade", False)
update_bp_after = st.session_state.get("update_bp_after", False)

run_config = RunConfig(
    symbols=syms,
    capital_long=float(cap_long),
    capital_short=float(cap_short),
    save_csv=save_csv,
    csv_name_mode=csv_name_mode,
    notify=use_slack_notify,
    run_parallel=run_parallel,
    scan_missing_only=scan_missing_only,
)

trade_options = TradeOptions(
    paper_mode=bool(paper_mode),
    retries=int(retries),
    delay=float(delay),
    poll_status=bool(poll_status),
    do_trade=bool(do_trade),
    update_bp_after=bool(update_bp_after),
)

# è¡¨ç¤ºåˆ¶å¾¡ã¯å›ºå®šï¼ˆãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã¯å»ƒæ­¢ï¼‰
st.session_state["ui_vis"] = {
    "overall_progress": True,
    "per_system_progress": True,
    "data_load_progress_lines": True,
    "previous_results": True,
    "system_details": True,
}

st.subheader("ä¿æœ‰ãƒã‚¸ã‚·ãƒ§ãƒ³ã¨åˆ©ç›Šä¿è­·åˆ¤å®š")

if st.button("ğŸ” Alpacaã‹ã‚‰ä¿æœ‰ãƒã‚¸ã‚·ãƒ§ãƒ³å–å¾—"):
    try:
        client = ba.get_client(paper=paper_mode)
        positions = client.get_all_positions()
        st.session_state["positions_df"] = evaluate_positions(positions)
        st.success("ãƒã‚¸ã‚·ãƒ§ãƒ³ã‚’å–å¾—ã—ã¾ã—ãŸ")
    except Exception as e:
        st.error(f"ãƒã‚¸ã‚·ãƒ§ãƒ³å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

if "positions_df" in st.session_state:
    positions_df = st.session_state["positions_df"]
    if not positions_df.empty:
        try:
            summary_table = _build_position_summary_table(positions_df)
            if not summary_table.empty:
                st.caption("ä¿æœ‰ãƒã‚¸ã‚·ãƒ§ãƒ³ï¼ˆSystem Ã— Sideåˆ¥ï¼‰")
                st.dataframe(summary_table, use_container_width=True)
        except Exception:
            pass

        # è¡¨ç¤ºç”¨ã«ã‚«ãƒ©ãƒ ã‚’æ—¥æœ¬èªåŒ–
        df_disp = positions_df.copy()
        rename_map = {
            "symbol": "éŠ˜æŸ„",
            "system": "ã‚·ã‚¹ãƒ†ãƒ ",
            "side": "ã‚µã‚¤ãƒ‰",
            "qty": "æ•°é‡",
            "entry_date": "å–å¾—æ—¥",
            "holding_days": "ä¿æœ‰æ—¥æ•°",
            "avg_entry_price": "å¹³å‡å–å¾—å˜ä¾¡",
            "current_price": "ç¾åœ¨å€¤",
            "unrealized_pl": "å«ã¿æç›Š",
            "unrealized_plpc_percent": "å«ã¿æç›Šç‡(%)",
            "judgement": "åˆ¤å®š",
            "next_action": "æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç›®å®‰",
            "rule_summary": "åˆ©ç¢º/æåˆ‡ã‚Šãƒ«ãƒ¼ãƒ«æ¦‚è¦",
        }
        df_disp = df_disp.rename(columns=rename_map)
        display_cols = [
            "éŠ˜æŸ„",
            "ã‚·ã‚¹ãƒ†ãƒ ",
            "ã‚µã‚¤ãƒ‰",
            "æ•°é‡",
            "å–å¾—æ—¥",
            "ä¿æœ‰æ—¥æ•°",
            "å¹³å‡å–å¾—å˜ä¾¡",
            "ç¾åœ¨å€¤",
            "å«ã¿æç›Š",
            "å«ã¿æç›Šç‡(%)",
            "åˆ¤å®š",
            "æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç›®å®‰",
            "åˆ©ç¢º/æåˆ‡ã‚Šãƒ«ãƒ¼ãƒ«æ¦‚è¦",
        ]
        df_disp = df_disp[[col for col in display_cols if col in df_disp.columns]]
        st.dataframe(df_disp, use_container_width=True)

        # æ‰‹å‹•æ‰‹ä»•èˆã„æ©Ÿèƒ½
        st.subheader("ğŸ¯ æ‰‹å‹•æ‰‹ä»•èˆã„")
        st.caption("é¸æŠã—ãŸéŠ˜æŸ„ã‚’æ‰‹å‹•ã§æ‰‹ä»•èˆã„æ³¨æ–‡ã—ã¾ã™")

        # æ‰‹ä»•èˆã„å¯¾è±¡ã®é¸æŠ
        if not positions_df.empty:
            symbols_list = positions_df["symbol"].tolist()
            selected_symbols: list[str] = st.multiselect(
                "æ‰‹ä»•èˆã„ã™ã‚‹éŠ˜æŸ„ã‚’é¸æŠ:", options=symbols_list, key="manual_exit_symbols"
            )

            if selected_symbols:
                exit_type = st.selectbox(
                    "æ‰‹ä»•èˆã„ã‚¿ã‚¤ãƒ—:",
                    ["MOC (å¤§å¼•ã‘)", "OPG (å¯„ã‚Šä»˜ã)", "Market (æˆè¡Œ)"],
                    key="exit_type",
                )

                col1, col2 = st.columns(2)
                with col1:
                    if st.button("ğŸš€ é¸æŠéŠ˜æŸ„ã®æ‰‹ä»•èˆã„æ³¨æ–‡ã‚’é€ä¿¡", type="primary"):
                        try:
                            # é¸æŠã•ã‚ŒãŸéŠ˜æŸ„ã®ãƒã‚¸ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’å–å¾—
                            selected_positions = positions_df[
                                positions_df["symbol"].isin(selected_symbols)
                            ].copy()

                            # æ‰‹ä»•èˆã„æ³¨æ–‡ã®å®Ÿè¡Œ
                            exit_orders = []
                            for _, row in selected_positions.iterrows():
                                exit_orders.append(
                                    {
                                        "symbol": row["symbol"],
                                        "side": (
                                            "sell" if str(row["side"]).lower() == "long" else "buy"
                                        ),
                                        "qty": abs(float(row["qty"])),
                                        "order_type": (
                                            "market" if "Market" in exit_type else "limit"
                                        ),
                                        "time_in_force": (
                                            "cls"
                                            if "MOC" in exit_type
                                            else ("opg" if "OPG" in exit_type else "day")
                                        ),
                                    }
                                )

                            if exit_orders:
                                exit_df = pd.DataFrame(exit_orders)
                                results = submit_orders_df(
                                    exit_df,
                                    paper=paper_mode,
                                    tif="DAY",
                                    retries=int(retries),
                                    delay=float(delay),
                                )

                                if results is not None and not results.empty:
                                    st.success(
                                        f"{len(selected_symbols)}éŠ˜æŸ„ã®æ‰‹ä»•èˆã„æ³¨æ–‡ã‚’é€ä¿¡ã—ã¾ã—ãŸ"
                                    )
                                    st.dataframe(results, use_container_width=True)
                                else:
                                    st.warning("æ³¨æ–‡é€ä¿¡çµæœãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")

                        except Exception as e:
                            st.error(f"æ‰‹ä»•èˆã„æ³¨æ–‡ã‚¨ãƒ©ãƒ¼: {e}")

                with col2:
                    if st.button("ğŸ“Š æ‰‹ä»•èˆã„å½±éŸ¿ã‚’äº‹å‰ç¢ºèª"):
                        if selected_symbols:
                            selected_positions = positions_df[
                                positions_df["symbol"].isin(selected_symbols)
                            ].copy()
                            total_pl = selected_positions["unrealized_pl"].astype(float).sum()
                            st.info(f"é¸æŠéŠ˜æŸ„ã®åˆè¨ˆå«ã¿æç›Š: ${total_pl:,.2f}")
                            st.dataframe(
                                selected_positions[
                                    ["symbol", "side", "qty", "unrealized_pl", "judgement"]
                                ],
                                use_container_width=True,
                            )

if st.button("â–¶ æœ¬æ—¥ã®ã‚·ã‚°ãƒŠãƒ«å®Ÿè¡Œ", type="primary"):
    artifacts = execute_today_signals(run_config)
    render_today_signals_results(artifacts, run_config, trade_options)
else:
    _render_previous_results_section()
