# ============================================================================
# ğŸ§  Context Note
# ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ System5ï¼ˆãƒ­ãƒ³ã‚° ãƒŸãƒ¼ãƒ³ãƒ»ãƒªãƒãƒ¼ã‚¸ãƒ§ãƒ³ é«˜ ADXï¼‰ã®ãƒ­ã‚¸ãƒƒã‚¯å°‚é–€
#
# å‰ææ¡ä»¶ï¼š
#   - é«˜ ADX ç’°å¢ƒï¼ˆADX7 > 35ï¼‰ã§ã®ãƒŸãƒ¼ãƒ³ãƒ»ãƒªãƒãƒ¼ã‚¸ãƒ§ãƒ³ç‹™ã„
#   - ATR_Pct ã«ã‚ˆã‚‹å¤‰å‹•æ€§ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆ> 2.5%ï¼‰
#   - RSI3 < 50 ã§éå£²ã‚Šç¢ºèª
#   - æŒ‡æ¨™ã¯ precomputed ã®ã¿ä½¿ç”¨ï¼ˆADX7 ã§ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼‰
#   - ãƒ•ãƒ­ãƒ¼: setup() â†’ rank() â†’ signals() ã®é †åºå®Ÿè¡Œ
#
# ãƒ­ã‚¸ãƒƒã‚¯å˜ä½ï¼š
#   setup()       â†’ ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ãƒã‚§ãƒƒã‚¯ï¼ˆADX7>55ã€ATR_Pct>2.5% ãªã©ï¼‰
#   rank()        â†’ ADX7 ã®é™é †ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆå¼·ã„ãƒˆãƒ¬ãƒ³ãƒ‰ç’°å¢ƒå„ªå…ˆï¼‰
#   signals()     â†’ ã‚¹ã‚³ã‚¢ä»˜ãã‚·ã‚°ãƒŠãƒ«æŠ½å‡º
#
# Copilot ã¸ï¼š
#   â†’ ADX é–¾å€¤ï¼ˆ35ï¼‰ã®å¤‰æ›´ã¯æ…é‡ã«ã€‚ä»–ã‚·ã‚¹ãƒ†ãƒ ã¨ã®ç«¶åˆæ¤œè¨¼å¿…é ˆ
#   â†’ RSI3 æ¡ä»¶ï¼ˆ< 50ï¼‰ã®å½¹å‰²ã¯ã€Œãƒªãƒãƒ¼ã‚¸ãƒ§ãƒ³ç’°å¢ƒç¢ºèªã€ã€‚ãƒ­ã‚¸ãƒƒã‚¯å¤‰æ›´ç¦æ­¢
#   â†’ ATR_Pct > 2.5% ã¯å¤‰å‹•æ€§ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã€‚ä¸‹é™å¤‰æ›´ã¯åˆ¶å¾¡ãƒ†ã‚¹ãƒˆã§ç¢ºèª
# ============================================================================

"""System5 core logic (Long mean-reversion with high ADX).

High ADX mean-reversion strategy:
- Indicators: adx7, atr10, dollarvolume20, atr_pct (precomputed only)
- Setup conditions: Close>=5, AvgVol50>500k, DV50>2.5M, ATR_Pct>2.5%,
    Close>SMA100+ATR10, ADX7>35, RSI3<50
- Candidate generation: ADX7 descending ranking by date, extract top_n
- Optimization: Removed all indicator calculations, using precomputed indicators only
"""

from __future__ import annotations

from collections.abc import Callable
from typing import Any, cast

import pandas as pd

from common.batch_processing import process_symbols_batch
from common.system_candidates_utils import (
    choose_mode_date_for_latest_only,
    normalize_candidates_by_date,
    normalize_dataframe_to_by_date,
    set_diagnostics_after_ranking,
)
from common.system_common import check_precomputed_indicators, get_total_days
from common.system_constants import SYSTEM5_REQUIRED_INDICATORS
from common.system_setup_predicates import validate_predicate_equivalence
from common.utils import get_cached_data

# ============================================================================
# System5 Strategy Constants
# ============================================================================
# Price & Volume filters
MIN_PRICE = 5.0  # Minimum closing price for candidates

# ADX-based filters
MIN_ADX = 35.0  # Minimum ADX7 for high trend strength environment
MIN_ADX_FULL_SCAN = 35.0  # ADX7 threshold for full-scan filtering

# Volatility filters
DEFAULT_ATR_PCT_THRESHOLD = 0.025  # 2.5% minimum ATR percentage

# Ranking parameters
DEFAULT_TOP_N = 20  # Default number of top candidates to extract


def format_atr_pct_threshold_label(threshold: float | None = None) -> str:
    """UI/ãƒ­ã‚°ç”¨ã®ATRé–¾å€¤ãƒ©ãƒ™ãƒ«ã‚’ä¸€å…ƒåŒ–ã€‚scripts/today ã‚„ today_signals ã§åˆ©ç”¨ã€‚"""
    actual_threshold = threshold if threshold is not None else DEFAULT_ATR_PCT_THRESHOLD
    return f"> {actual_threshold:.2%}"


# ============================================================================
# System5 Helper Functions
# ============================================================================


def _apply_filter_conditions(df: pd.DataFrame) -> pd.DataFrame:
    """Apply System5 filter conditions, preserving existing 'filter' column if present.

    Args:
        df: DataFrame with required indicators (Close, adx7, atr_pct)

    Returns:
        DataFrame with 'filter' column added/updated
    """
    result = df.copy()

    close = pd.to_numeric(result["Close"], errors="coerce")
    adx7 = pd.to_numeric(result["adx7"], errors="coerce")
    atr_pct = pd.to_numeric(result["atr_pct"], errors="coerce")

    computed_filter = (
        (close >= MIN_PRICE) & (adx7 > MIN_ADX) & (atr_pct > DEFAULT_ATR_PCT_THRESHOLD)
    ).fillna(False)

    if "filter" in result.columns:
        existing = (
            pd.Series(result["filter"], index=result.index).fillna(False).astype(bool)
        )
        computed_filter = computed_filter & existing

    result["filter"] = computed_filter.astype(bool)

    return result


def _apply_setup_conditions(df: pd.DataFrame) -> pd.DataFrame:
    """Apply System5 setup conditions, preserving existing 'setup' column if present.

    For System5, setup conditions are identical to filter conditions.

    Args:
        df: DataFrame with 'filter' column

    Returns:
        DataFrame with 'setup' column added/updated
    """
    result = df.copy()

    computed_setup = (
        pd.Series(result["filter"], index=result.index).fillna(False).astype(bool)
    )

    if "setup" in result.columns:
        existing = (
            pd.Series(result["setup"], index=result.index).fillna(False).astype(bool)
        )
        computed_setup = computed_setup & existing

    result["setup"] = computed_setup.astype(bool)

    return result


def _compute_indicators(symbol: str) -> tuple[str, pd.DataFrame | None]:
    """Check precomputed indicators and apply System5-specific filters.

    Args:
        symbol: Target symbol to process

    Returns:
        (symbol, processed DataFrame | None)
    """
    try:
        df = get_cached_data(symbol)
        if df is None or df.empty:
            return symbol, None

        # Check for required indicators
        missing_indicators = [
            col for col in SYSTEM5_REQUIRED_INDICATORS if col not in df.columns
        ]
        if missing_indicators:
            return symbol, None

        # Apply System5-specific filters and setup using helpers
        x = df.copy()
        x = _apply_filter_conditions(x)
        x = _apply_setup_conditions(x)

        return symbol, x

    except Exception:
        return symbol, None


def prepare_data_vectorized_system5(
    raw_data_dict: dict[str, pd.DataFrame] | None,
    *,
    progress_callback: Callable[[str], None] | None = None,
    log_callback: Callable[[str], None] | None = None,
    skip_callback: Callable[[str, str], None] | None = None,
    batch_size: int | None = None,
    reuse_indicators: bool = True,
    symbols: list[str] | None = None,
    use_process_pool: bool = False,
    max_workers: int | None = None,
    **kwargs: Any,
) -> dict[str, pd.DataFrame]:
    """System5 data preparation processing (high ADX mean-reversion strategy).

    Execute high-speed processing using precomputed indicators.

    Args:
        raw_data_dict: Raw data dictionary (None to fetch from cache)
        progress_callback: Progress reporting callback
        log_callback: Log output callback
        skip_callback: Error skip callback
        batch_size: Batch size
        reuse_indicators: Reuse existing indicators (for speed)
        symbols: Target symbol list
        use_process_pool: Process pool usage flag
        max_workers: Maximum worker count

    Returns:
        Processed data dictionary
    """
    # Fast path: reuse precomputed indicators
    if reuse_indicators and raw_data_dict:
        try:
            # Early check - verify required indicators exist
            valid_data_dict, error_symbols = check_precomputed_indicators(
                raw_data_dict, SYSTEM5_REQUIRED_INDICATORS, "System5", skip_callback
            )

            if valid_data_dict:
                # Apply System5-specific filters using helpers
                prepared_dict = {}
                for symbol, df in valid_data_dict.items():
                    x = df.copy()
                    x = _apply_filter_conditions(x)
                    x = _apply_setup_conditions(x)
                    prepared_dict[symbol] = x

                if log_callback:
                    log_callback(
                        f"System5: Fast-path processed {len(prepared_dict)} symbols"
                    )

                return prepared_dict

        except RuntimeError:
            # Re-raise error immediately if required indicators are missing
            raise
        except Exception:
            # Fall back to normal processing for other errors
            if log_callback:
                log_callback(
                    "System5: Fast-path failed, falling back to normal processing"
                )

    # Normal processing path: batch processing from symbol list
    if symbols:
        target_symbols = symbols
    elif raw_data_dict:
        target_symbols = list(raw_data_dict.keys())
    else:
        if log_callback:
            log_callback("System5: No symbols provided, returning empty dict")
        return {}

    if log_callback:
        log_callback(
            f"System5: Starting normal processing for {len(target_symbols)} symbols"
        )

    # Execute batch processing
    results, error_symbols = process_symbols_batch(
        target_symbols,
        _compute_indicators,
        batch_size=batch_size,
        use_process_pool=use_process_pool,
        max_workers=max_workers,
        progress_callback=progress_callback,
        log_callback=log_callback,
        skip_callback=skip_callback,
        system_name="System5",
    )
    try:
        validate_predicate_equivalence(results, "5", log_fn=log_callback)
    except Exception:
        pass
    return cast(dict[str, pd.DataFrame], results)


def generate_candidates_system5(
    prepared_dict: dict[str, pd.DataFrame],
    *,
    top_n: int | None = None,
    progress_callback: Callable[[str], None] | None = None,
    log_callback: Callable[[str], None] | None = None,
    batch_size: int | None = None,
    latest_only: bool = False,
    include_diagnostics: bool = False,
    diagnostics: dict[str, Any] | None = None,
    **kwargs: Any,
) -> (
    tuple[dict[pd.Timestamp, dict[str, dict[str, Any]]], pd.DataFrame | None]
    | tuple[
        dict[pd.Timestamp, dict[str, dict[str, Any]]],
        pd.DataFrame | None,
        dict[str, Any],
    ]
):
    """System5 candidate generation (ADX7 descending ranking).

    Args:
        prepared_dict: Prepared data dictionary
        top_n: Number of top entries to extract
        progress_callback: Progress reporting callback
        log_callback: Log output callback

    Returns:
        (Daily candidate dictionary, Integrated candidate DataFrame)
    """
    if diagnostics is None:
        diagnostics = {
            "ranking_source": None,
            "setup_predicate_count": 0,
            "ranked_top_n_count": 0,
            "predicate_only_pass_count": 0,
            "mismatch_flag": 0,
        }

    # Reset counters every invocation to avoid carrying stale values when dict is reused
    diagnostics["setup_predicate_count"] = 0
    diagnostics["ranked_top_n_count"] = 0
    diagnostics["predicate_only_pass_count"] = 0
    diagnostics["mismatch_flag"] = 0

    if not prepared_dict:
        if log_callback:
            log_callback("System5: No data provided for candidate generation")
        # ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã‚‚ latest_only / full_scan ã«å¿œã˜ã¦ ranking_source ã‚’è¨­å®š
        try:
            set_diagnostics_after_ranking(
                diagnostics,
                final_df=None,
                ranking_source=("latest_only" if latest_only else "full_scan"),
            )
        except Exception:
            diagnostics["ranking_source"] = (
                "latest_only" if latest_only else "full_scan"
            )
        return ({}, None, diagnostics) if include_diagnostics else ({}, None)

    if top_n is None:
        top_n = DEFAULT_TOP_N

    if latest_only:
        try:
            rows: list[dict] = []
            date_counter: dict[pd.Timestamp, int] = {}
            try:
                from common.system_setup_predicates import (
                    system5_setup_predicate as _s5_pred,
                )
            except Exception:
                _s5_pred = None
            for sym, df in prepared_dict.items():
                if df is None or df.empty:
                    continue
                last_row = df.iloc[-1]

                # Prefer precomputed setup column; fall back to predicate evaluation
                # or manual recomputation when needed
                setup_from_column = False
                setup_value_available = False
                try:
                    raw_setup = last_row.get("setup", None)
                    if raw_setup is not None and not pd.isna(raw_setup):
                        setup_value_available = True
                        if bool(raw_setup):
                            setup_from_column = True
                except Exception:
                    setup_from_column = False
                    setup_value_available = False

                predicate_pass = False
                predicate_evaluated = False
                if _s5_pred is not None:
                    try:
                        predicate_pass = bool(_s5_pred(last_row))
                        predicate_evaluated = True
                    except Exception:
                        predicate_pass = False
                        predicate_evaluated = False

                manual_pass = False
                if (
                    not setup_from_column
                    and not predicate_evaluated
                    and not setup_value_available
                ):
                    try:
                        close_val = last_row.get("Close", float("nan"))
                        adx_val = last_row.get("adx7", float("nan"))
                        atr_pct_val = last_row.get("atr_pct", float("nan"))
                        if (
                            pd.notna(close_val)
                            and pd.notna(adx_val)
                            and pd.notna(atr_pct_val)
                        ):
                            manual_pass = bool(
                                float(close_val) >= MIN_PRICE
                                and float(adx_val) > MIN_ADX
                                and float(atr_pct_val) > DEFAULT_ATR_PCT_THRESHOLD
                            )
                    except Exception:
                        manual_pass = False

                setup_ok = False
                setup_source = ""
                if setup_from_column:
                    setup_ok = True
                    setup_source = "column"
                    if predicate_evaluated and not predicate_pass:
                        diagnostics["mismatch_flag"] = 1
                elif predicate_pass:
                    setup_ok = True
                    setup_source = "predicate"
                    diagnostics["mismatch_flag"] = 1
                elif manual_pass:
                    setup_ok = True
                    setup_source = "manual"
                    diagnostics["mismatch_flag"] = 1

                if not setup_ok:
                    continue

                adx7_val = last_row.get("adx7", None)
                try:
                    if adx7_val is None or pd.isna(adx7_val):
                        continue
                except Exception:
                    continue
                dt = pd.Timestamp(str(df.index[-1]))
                date_counter[dt] = date_counter.get(dt, 0) + 1

                # ATR10ã‚’é…åˆ†è¨ˆç®—ç”¨ã«ä¿æŒ
                atr10_val = 0.0
                try:
                    atr10_raw = last_row.get("atr10")
                    if atr10_raw is not None and not pd.isna(atr10_raw):
                        atr10_val = float(atr10_raw)
                except Exception:
                    pass

                rows.append(
                    {
                        "symbol": sym,
                        "date": dt,
                        "adx7": adx7_val,
                        "atr_pct": last_row.get("atr_pct", 0),
                        "close": last_row.get("Close", 0),
                        "atr10": atr10_val,
                        "_setup_via": setup_source,
                        "_predicate_pass": bool(predicate_pass),
                        "_manual_pass": bool(manual_pass),
                    }
                )

            diagnostics["setup_unique_symbols"] = len(
                set(row["symbol"] for row in rows)
            )
            if not rows:
                if log_callback:
                    try:
                        samples: list[str] = []
                        taken = 0
                        for s_sym, s_df in prepared_dict.items():
                            if s_df is None or getattr(s_df, "empty", True):
                                continue
                            try:
                                s_last = s_df.iloc[-1]
                                s_dt = pd.to_datetime(str(s_df.index[-1])).normalize()
                                s_setup = bool(s_last.get("setup", False))
                                s_adx = s_last.get("adx7", float("nan"))
                                try:
                                    s_adx_f = float(s_adx)
                                except Exception:
                                    s_adx_f = float("nan")
                                samples.append(
                                    f"{s_sym}: date={s_dt.date()} "
                                    f"setup={s_setup} adx7={s_adx_f:.4f}"
                                )
                                taken += 1
                                if taken >= 2:
                                    break
                            except Exception:
                                continue
                        if samples:
                            log_callback(
                                (
                                    "System5: DEBUG latest_only 0 candidates. "
                                    + " | ".join(samples)
                                )
                            )
                    except Exception:
                        pass
                    log_callback("System5: latest_only fast-path produced 0 rows")
                # è¨ºæ–­ã®ä¸€è²«æ€§: 0ä»¶ã§ã‚‚ ranking_source ã‚’ latest_only ã«è¨­å®šï¼ˆlog_callback æœ‰ç„¡ã«é–¢ã‚ã‚‰ãšï¼‰
                try:
                    set_diagnostics_after_ranking(
                        diagnostics, final_df=None, ranking_source="latest_only"
                    )
                except Exception:
                    diagnostics["ranking_source"] = "latest_only"
                return ({}, None, diagnostics) if include_diagnostics else ({}, None)
            df_all = pd.DataFrame(rows)
            mode_date = choose_mode_date_for_latest_only(date_counter)
            if mode_date is not None:
                df_all = df_all[df_all["date"] == mode_date]
            df_all = df_all.sort_values("adx7", ascending=False, kind="stable").head(
                top_n
            )

            if "_setup_via" in df_all.columns:
                via_series = df_all["_setup_via"].fillna("").astype(str)
                diagnostics["setup_predicate_count"] = int((via_series != "").sum())

                if "_predicate_pass" in df_all.columns:
                    predicate_series = (
                        df_all["_predicate_pass"].fillna(False).astype(bool)
                    )
                else:
                    predicate_series = pd.Series(False, index=df_all.index)

                if "_manual_pass" in df_all.columns:
                    manual_series = df_all["_manual_pass"].fillna(False).astype(bool)
                else:
                    manual_series = pd.Series(False, index=df_all.index)

                predicate_only_mask = (via_series != "column") & (
                    predicate_series | manual_series
                )
                diagnostics["predicate_only_pass_count"] = int(
                    predicate_only_mask.sum()
                )
            else:
                diagnostics["setup_predicate_count"] = len(df_all)
                diagnostics["predicate_only_pass_count"] = 0

            diagnostics["setup_unique_symbols"] = int(df_all["symbol"].nunique())

            meta_cols = ["_setup_via", "_predicate_pass", "_manual_pass"]
            df_public = df_all.drop(
                columns=[c for c in meta_cols if c in df_all.columns]
            )

            # Feature flag: allow using Option-B finalize helper (non-breaking)
            use_option_b_utils = False
            try:
                if bool(kwargs.get("use_option_b_utils", False)):
                    use_option_b_utils = True
                else:
                    try:
                        from config.environment import get_env_config as _get_env

                        _env = _get_env()
                        v = getattr(_env, "enable_option_b_system5", None)
                        if v is not None and bool(v):
                            use_option_b_utils = True
                    except Exception:
                        use_option_b_utils = False
            except Exception:
                use_option_b_utils = False

            if use_option_b_utils:
                try:
                    from common.system_candidates_utils import (
                        finalize_ranking_and_diagnostics as _finalize_diag,
                    )

                    _finalize_diag(
                        diagnostics,
                        df_public,
                        ranking_source="latest_only",
                        extras=None,
                    )
                except Exception:
                    set_diagnostics_after_ranking(
                        diagnostics, final_df=df_public, ranking_source="latest_only"
                    )
            else:
                set_diagnostics_after_ranking(
                    diagnostics, final_df=df_public, ranking_source="latest_only"
                )
            diagnostics["top_n_requested"] = top_n

            # âœ… è¨ºæ–­æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯: ranked > setup ã¯è«–ç†ã‚¨ãƒ©ãƒ¼
            if diagnostics["ranked_top_n_count"] > diagnostics["setup_predicate_count"]:
                if log_callback:
                    ranked = diagnostics["ranked_top_n_count"]
                    setup = diagnostics["setup_predicate_count"]
                    log_callback(
                        f"System5: WARNING - ranked_top_n ({ranked}) > "
                        f"setup_predicate_count ({setup}). "
                        "Possible duplicate or logic error."
                    )
            by_date = normalize_dataframe_to_by_date(df_public)
            if log_callback:
                msg = (
                    f"System5: latest_only fast-path -> {len(df_public)} "
                    f"candidates (symbols={len(rows)})"
                )
                log_callback(msg)
            return (
                (by_date, df_public.copy(), diagnostics)
                if include_diagnostics
                else (by_date, df_public.copy())
            )
        except Exception as e:
            if log_callback:
                log_callback(f"System5: fast-path failed -> fallback ({e})")
            pass

    # Aggregate all dates
    all_dates_set: set[pd.Timestamp] = set()
    for df in prepared_dict.values():
        if df is not None and not df.empty:
            all_dates_set.update(df.index)

    if not all_dates_set:
        if log_callback:
            log_callback("System5: No valid dates found in data")
        return ({}, None, diagnostics) if include_diagnostics else ({}, None)
    all_dates = sorted(all_dates_set)

    candidates_by_date: dict[pd.Timestamp, list[dict[str, Any]]] = {}
    all_candidates: list[dict[str, Any]] = []

    if log_callback:
        log_callback(f"System5: Generating candidates for {len(all_dates)} dates")

    # Execute ADX7 ranking by date (descending - highest ADX7 first)
    for i, date in enumerate(all_dates):
        date_candidates = []

        for symbol, df in prepared_dict.items():
            try:
                if df is None or date not in df.index:
                    continue
                row = cast(pd.Series, df.loc[date])
                setup_val = bool(row.get("setup", False))
                from common.system_setup_predicates import (
                    system5_setup_predicate as _s5_pred,
                )

                pred_val = _s5_pred(row)
                # setup é€šéã¯æœ€çµ‚å€™è£œç¢ºå®šå¾Œã«ä¸€æ‹¬è¨ˆä¸Šï¼ˆã“ã“ã§ã¯åŠ ç®—ã—ãªã„ï¼‰
                if pred_val and not setup_val:
                    diagnostics["predicate_only_pass_count"] += 1
                    diagnostics["mismatch_flag"] = 1
                if not bool(setup_val):
                    continue
                adx7_val = cast(Any, row.get("adx7", 0))
                try:
                    if pd.isna(adx7_val) or float(adx7_val) <= MIN_ADX_FULL_SCAN:
                        continue
                except Exception:
                    continue

                date_candidates.append(
                    {
                        "symbol": symbol,
                        "date": date,
                        "adx7": adx7_val,
                        "atr_pct": row.get("atr_pct", 0),
                        "close": row.get("Close", 0),
                    }
                )

            except Exception:
                continue

        # Sort by ADX7 descending (highest first) and extract top_n
        if date_candidates:
            date_candidates.sort(key=lambda x: x["adx7"], reverse=True)
            top_candidates = date_candidates[:top_n]

            candidates_by_date[date] = top_candidates
            all_candidates.extend(top_candidates)

        # Progress reporting
        if progress_callback and (i + 1) % max(1, len(all_dates) // 10) == 0:
            progress_callback(f"Processed {i + 1}/{len(all_dates)} dates")

    # Create integrated DataFrame
    if all_candidates:
        candidates_df = pd.DataFrame(all_candidates)
        candidates_df["date"] = pd.to_datetime(candidates_df["date"])
        candidates_df = candidates_df.sort_values(
            ["date", "adx7"], ascending=[True, False]
        )
        diagnostics["ranking_source"] = "full_scan"
        # Feature flag: Option-B finalize helper
        use_option_b_utils = False
        try:
            if bool(kwargs.get("use_option_b_utils", False)):
                use_option_b_utils = True
            else:
                try:
                    from config.environment import get_env_config as _get_env

                    _env = _get_env()
                    v = getattr(_env, "enable_option_b_system5", None)
                    if v is not None and bool(v):
                        use_option_b_utils = True
                except Exception:
                    use_option_b_utils = False
        except Exception:
            use_option_b_utils = False

        if use_option_b_utils:
            try:
                from common.system_candidates_utils import (
                    finalize_ranking_and_diagnostics as _finalize_diag,
                )

                _finalize_diag(
                    diagnostics, candidates_df, ranking_source="full_scan", extras=None
                )
            except Exception:
                set_diagnostics_after_ranking(
                    diagnostics, final_df=candidates_df, ranking_source="full_scan"
                )
        else:
            set_diagnostics_after_ranking(
                diagnostics, final_df=candidates_df, ranking_source="full_scan"
            )
    else:
        candidates_df = None
        # Feature flag: Option-B finalize helper for empty case
        use_option_b_utils = False
        try:
            if bool(kwargs.get("use_option_b_utils", False)):
                use_option_b_utils = True
            else:
                try:
                    from config.environment import get_env_config as _get_env

                    _env = _get_env()
                    v = getattr(_env, "enable_option_b_system5", None)
                    if v is not None and bool(v):
                        use_option_b_utils = True
                except Exception:
                    use_option_b_utils = False
        except Exception:
            use_option_b_utils = False

        if use_option_b_utils:
            try:
                from common.system_candidates_utils import (
                    finalize_ranking_and_diagnostics as _finalize_diag,
                )

                _finalize_diag(
                    diagnostics, None, ranking_source="full_scan", extras=None
                )
            except Exception:
                set_diagnostics_after_ranking(
                    diagnostics, final_df=None, ranking_source="full_scan"
                )
        else:
            set_diagnostics_after_ranking(
                diagnostics, final_df=None, ranking_source="full_scan"
            )

    if log_callback:
        total_candidates = len(all_candidates)
        unique_dates = len(candidates_by_date)
        msg = (
            f"System5: Generated {total_candidates} candidates "
            f"across {unique_dates} dates"
        )
        log_callback(msg)

    normalized = normalize_candidates_by_date(candidates_by_date)
    return (
        (normalized, candidates_df, diagnostics)
        if include_diagnostics
        else (normalized, candidates_df)
    )


def get_total_days_system5(data_dict: dict[str, pd.DataFrame]) -> int:
    """Get total days count for System5 data.

    Args:
        data_dict: Data dictionary

    Returns:
        Maximum day count
    """
    # follow-imports è¨­å®šã«ã‚ˆã‚Šæˆ»ã‚Šå€¤ãŒ Any æ‰±ã„ã«ãªã‚‹ç’°å¢ƒå‘ã‘ã«æ˜ç¤ºå¤‰æ›
    return int(get_total_days(data_dict))


__all__ = [
    "prepare_data_vectorized_system5",
    "generate_candidates_system5",
    "get_total_days_system5",
]
