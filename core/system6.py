"""System6 core logic (Short mean-reversion momentum burst)."""

import time

import numpy as np
import pandas as pd
from ta.volatility import AverageTrueRange

from common.batch_processing import process_symbols_batch
from common.i18n import tr
from common.structured_logging import MetricsCollector
from common.utils import resolve_batch_size

# System6 configuration constants
MIN_PRICE = 5.0  # æœ€ä½ä¾¡æ ¼ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆãƒ‰ãƒ«ï¼‰
MIN_DOLLAR_VOLUME_50 = 10_000_000  # æœ€ä½ãƒ‰ãƒ«ãƒœãƒªãƒ¥ãƒ¼ãƒ 50æ—¥å¹³å‡ï¼ˆãƒ‰ãƒ«ï¼‰

# Shared metrics collector to avoid file handle leaks
_metrics = MetricsCollector()

SYSTEM6_BASE_COLUMNS = ["Open", "High", "Low", "Close", "Volume"]
SYSTEM6_FEATURE_COLUMNS = [
    "atr10",
    "dollarvolume50",
    "return_6d",
    "UpTwoDays",
    "filter",
    "setup",
]
SYSTEM6_ALL_COLUMNS = SYSTEM6_BASE_COLUMNS + SYSTEM6_FEATURE_COLUMNS
SYSTEM6_NUMERIC_COLUMNS = ["atr10", "dollarvolume50", "return_6d"]


def _compute_indicators_from_frame(df: pd.DataFrame) -> pd.DataFrame:
    # æŸ”è»Ÿãªåˆ—åãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆå¤§æ–‡å­—ãƒ»å°æ–‡å­—ä¸¡å¯¾å¿œï¼‰
    col_mapping = {}
    required_base_cols = ["Open", "High", "Low", "Close", "Volume"]

    for required_col in required_base_cols:
        if required_col in df.columns:
            col_mapping[required_col] = required_col
        elif required_col.lower() in df.columns:
            col_mapping[required_col] = required_col.lower()
        else:
            raise ValueError(f"missing column: {required_col} (or {required_col.lower()})")

    # å¿…è¦ãªåˆ—ã®ã¿ã‚’æŠ½å‡ºã—ã¦ã‚³ãƒ”ãƒ¼
    base_cols = [col_mapping[col] for col in required_base_cols]

    # æ—¥ä»˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ±ºå®šï¼ˆåˆ—å„ªå…ˆãƒ»ãªã‘ã‚Œã°æ—¢å­˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼‰
    date_series: pd.Series | None = None
    for date_col in ("Date", "date"):
        if date_col in df.columns:
            # ç„¡é™å€¤ã‚’NaNã«å¤‰æ›ã—ã¦ã‹ã‚‰æ—¥ä»˜å¤‰æ›
            date_col_data = df[date_col].replace([np.inf, -np.inf], np.nan)
            date_series = pd.to_datetime(date_col_data, errors="coerce")
            break

    if date_series is None:
        raw_index = df.index
        if isinstance(raw_index, pd.DatetimeIndex):
            date_series = pd.to_datetime(raw_index, errors="coerce")
        else:
            # ç„¡é™å€¤ã‚’NaNã«å¤‰æ›ã—ã¦ã‹ã‚‰æ—¥ä»˜å¤‰æ›
            index_data = pd.Series(raw_index).replace([np.inf, -np.inf], np.nan)
            date_series = pd.to_datetime(index_data, errors="coerce")

    if date_series is None:
        raise ValueError("missing date index")

    if isinstance(date_series, (pd.Index, pd.Series)):
        values = date_series.to_numpy()
    else:
        values = pd.to_datetime(date_series, errors="coerce").to_numpy()
    date_series = pd.Series(values, index=df.index)

    if getattr(date_series.dt, "tz", None) is not None:
        date_series = date_series.dt.tz_localize(None)

    valid_mask = date_series.notna()
    if not valid_mask.any():
        raise ValueError("invalid date index")

    if not valid_mask.all():
        date_series = date_series[valid_mask]
        x = df.loc[valid_mask, base_cols].copy()
    else:
        x = df.loc[:, base_cols].copy()

    # åˆ—åã‚’æ¨™æº–åŒ–ï¼ˆå¤§æ–‡å­—ã«çµ±ä¸€ï¼‰
    x.columns = required_base_cols

    if len(x) < 50:
        raise ValueError("insufficient rows")

    # æ­£è¦åŒ–ã—ãŸæ—¥ä»˜ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¨­å®š
    x.index = pd.Index(date_series, name="Date")

    try:
        # ğŸš€ ãƒ—ãƒªã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ãƒƒãƒ‰æŒ‡æ¨™ã‚’ä½¿ç”¨ï¼ˆã™ã¹ã¦ã®æŒ‡æ¨™ã‚’æœ€é©åŒ–ï¼‰
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å¯¾å¿œã®å•é¡Œã‚’å›é¿ã™ã‚‹ãŸã‚ã€.valuesã‚’ä½¿ç”¨ã—ã¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ç„¡è¦–

        # ATR10
        if "ATR10" in df.columns:
            x["atr10"] = df["ATR10"].values
        elif "atr10" in df.columns:
            x["atr10"] = df["atr10"].values
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆé€šå¸¸ã¯å®Ÿè¡Œã•ã‚Œãªã„ï¼‰
            _metrics.record_metric("system6_fallback_atr10", 1, "count")
            x["atr10"] = AverageTrueRange(
                x["High"], x["Low"], x["Close"], window=10
            ).average_true_range()

        # DollarVolume50
        if "DollarVolume50" in df.columns:
            x["dollarvolume50"] = df["DollarVolume50"].values
        elif "dollarvolume50" in df.columns:
            x["dollarvolume50"] = df["dollarvolume50"].values
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆé€šå¸¸ã¯å®Ÿè¡Œã•ã‚Œãªã„ï¼‰
            _metrics.record_metric("system6_fallback_dollarvolume50", 1, "count")
            x["dollarvolume50"] = (x["Close"] * x["Volume"]).rolling(50).mean()

        # Return_6D
        if "Return_6D" in df.columns:
            x["return_6d"] = df["Return_6D"].values
        elif "return_6d" in df.columns:
            x["return_6d"] = df["return_6d"].values
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆé€šå¸¸ã¯å®Ÿè¡Œã•ã‚Œãªã„ï¼‰
            _metrics.record_metric("system6_fallback_return_6d", 1, "count")
            x["return_6d"] = x["Close"].pct_change(6)

        # UpTwoDays
        if "UpTwoDays" in df.columns:
            x["UpTwoDays"] = df["UpTwoDays"].values
        elif "uptwodays" in df.columns:
            x["UpTwoDays"] = df["uptwodays"].values
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆé€šå¸¸ã¯å®Ÿè¡Œã•ã‚Œãªã„ï¼‰
            _metrics.record_metric("system6_fallback_uptwodays", 1, "count")
            x["UpTwoDays"] = (x["Close"] > x["Close"].shift(1)) & (
                x["Close"].shift(1) > x["Close"].shift(2)
            )

        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ¡ä»¶ï¼ˆè»½é‡ãªè«–ç†æ¼”ç®—ï¼‰
        x["filter"] = (x["Low"] >= MIN_PRICE) & (x["dollarvolume50"] > MIN_DOLLAR_VOLUME_50)
        x["setup"] = x["filter"] & (x["return_6d"] > 0.20) & x["UpTwoDays"]

    except Exception as exc:
        raise ValueError(f"calc_error: {type(exc).__name__}: {exc}") from exc

    # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã¨æœ€çµ‚çš„ãªã‚½ãƒ¼ãƒˆãƒ»é‡è¤‡é™¤å»ï¼ˆä¸€ç®‡æ‰€ã«çµ±åˆï¼‰
    x = x.dropna(subset=SYSTEM6_NUMERIC_COLUMNS)
    if x.empty:
        raise ValueError("insufficient rows")
    x = x.loc[~x.index.duplicated()].sort_index()
    return x


def prepare_data_vectorized_system6(
    raw_data_dict: dict[str, pd.DataFrame] | None,
    *,
    progress_callback=None,
    log_callback=None,
    skip_callback=None,
    batch_size: int | None = None,
    use_process_pool: bool = False,
    max_workers: int | None = None,
    **kwargs,
) -> dict[str, pd.DataFrame]:
    """System6 data preparation using standard batch processing pattern"""

    if not raw_data_dict:
        if log_callback:
            log_callback("System6: No raw data provided, returning empty dict")
        return {}

    target_symbols = list(raw_data_dict.keys())

    if log_callback:
        log_callback(f"System6: Starting processing for {len(target_symbols)} symbols")

    # Create a closure to pass raw_data_dict to the compute function
    def _compute_indicators_with_data(symbol: str) -> tuple[str, pd.DataFrame | None]:
        """Indicator calculation function that uses provided raw data"""
        df = raw_data_dict.get(symbol)
        if df is None or df.empty:
            return symbol, None

        try:
            prepared = _compute_indicators_from_frame(df)
            return symbol, prepared
        except Exception:
            return symbol, None

    # Execute batch processing using standard pattern
    results, error_symbols = process_symbols_batch(
        target_symbols,
        _compute_indicators_with_data,
        batch_size=batch_size,
        use_process_pool=use_process_pool,
        max_workers=max_workers,
        progress_callback=progress_callback,
        log_callback=log_callback,
        skip_callback=skip_callback,
        system_name="System6",
    )

    return results


def generate_candidates_system6(
    prepared_dict: dict[str, pd.DataFrame],
    *,
    top_n: int = 10,
    progress_callback=None,
    log_callback=None,
    skip_callback=None,
    batch_size: int | None = None,
) -> tuple[dict, pd.DataFrame | None]:
    candidates_by_date: dict[pd.Timestamp, list] = {}
    total = len(prepared_dict)

    if batch_size is None:
        try:
            from config.settings import get_settings

            batch_size = get_settings(create_dirs=False).data.batch_size
        except Exception:
            batch_size = 100
        # System6ã§ã¯éå¸¸ã«å¤§ããªãƒãƒƒãƒã‚µã‚¤ã‚ºã§é«˜é€Ÿå‡¦ç†ï¼ˆå€™è£œæŠ½å‡ºã¯è»½ã„å‡¦ç†ï¼‰
        batch_size = max(batch_size, 2000)  # æœ€å°2000ã«è¨­å®š
        batch_size = resolve_batch_size(total, batch_size)
    start_time = time.time()
    batch_start = time.time()
    processed, skipped = 0, 0
    skipped_missing_cols = 0
    filter_passed = 0  # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶é€šéæ•°
    setup_passed = 0  # ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ¡ä»¶é€šéæ•°
    buffer: list[str] = []

    # å‡¦ç†é–‹å§‹ã®ãƒ­ã‚°ã‚’è¿½åŠ 
    if log_callback:
        log_callback(
            f"ğŸ“Š System6 å€™è£œæŠ½å‡ºé–‹å§‹: {total}éŠ˜æŸ„ã‚’å‡¦ç†ä¸­... (ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size})"
        )

    for sym, df in prepared_dict.items():
        # featherã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯
        if df is None or df.empty:
            skipped += 1
            continue
        missing_cols = [c for c in SYSTEM6_ALL_COLUMNS if c not in df.columns]
        if missing_cols:
            skipped += 1
            skipped_missing_cols += 1
            continue
        if df[SYSTEM6_NUMERIC_COLUMNS].isnull().any().any():
            # NaNè­¦å‘Šã¯å€‹åˆ¥ã«å‡ºåŠ›ã›ãšã€çµ±è¨ˆã®ã¿è¨˜éŒ²
            pass

        # last_priceï¼ˆç›´è¿‘çµ‚å€¤ï¼‰ã‚’å–å¾—
        last_price = None
        if "Close" in df.columns and not df["Close"].empty:
            last_price = df["Close"].iloc[-1]

        # çµ±è¨ˆè¨ˆç®—ï¼šãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é€šéæ•°ã¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é€šéæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆç´¯ç©æ—¥æ•°ï¼‰
        if "filter" in df.columns:
            filter_passed += df["filter"].sum()  # å…¨æœŸé–“ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã‚’æº€ãŸã—ãŸæ—¥æ•°
        if "setup" in df.columns:
            setup_passed += df["setup"].sum()  # å…¨æœŸé–“ã§ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ¡ä»¶ã‚’æº€ãŸã—ãŸæ—¥æ•°

        try:
            if "setup" not in df.columns or not df["setup"].any():
                skipped += 1
                continue
            setup_days = df[df["setup"] == 1]
            if setup_days.empty:
                skipped += 1
                continue
            for date, row in setup_days.iterrows():
                # æ—¥ä»˜å¤‰æ›ã‚’ç°¡ç•¥åŒ–ï¼ˆå–¶æ¥­æ—¥è£œæ­£ãªã—ã§é«˜é€ŸåŒ–ï¼‰
                if isinstance(date, pd.Timestamp):
                    entry_date = date
                else:
                    entry_date = pd.Timestamp(date)

                rec = {
                    "symbol": sym,
                    "entry_date": entry_date,
                    "entry_price": last_price,
                    "return_6d": row["return_6d"],
                    "atr10": row["atr10"],
                }
                candidates_by_date.setdefault(entry_date, []).append(rec)
        except Exception:
            skipped += 1

        processed += 1
        buffer.append(sym)
        if progress_callback:
            try:
                progress_callback(processed, total)
            except Exception:
                pass
        if (processed % batch_size == 0 or processed == total) and log_callback:
            elapsed = time.time() - start_time
            remain = (elapsed / processed) * (total - processed) if processed else 0
            em, es = divmod(int(elapsed), 60)
            rm, rs = divmod(int(remain), 60)

            # System6ã®è©³ç´°çµ±è¨ˆã‚’è¨ˆç®—
            total_candidates = sum(len(cands) for cands in candidates_by_date.values())

            msg = tr(
                "ğŸ“Š System6 é€²æ—: {done}/{total} | "
                "ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é€šé: {filter_passed}æ—¥ | ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é€šé: {setup_passed}æ—¥ | "
                "å€™è£œ: {candidates}ä»¶\n"
                "â±ï¸ çµŒé: {em}m{es}s | æ®‹ã‚Š: ~{rm}m{rs}s | "
                "ã‚¹ã‚­ãƒƒãƒ—: {skipped}éŠ˜æŸ„ (åˆ—ä¸è¶³: {missing_cols}éŠ˜æŸ„)",
                done=processed,
                total=total,
                filter_passed=filter_passed,
                setup_passed=setup_passed,
                candidates=total_candidates,
                em=em,
                es=es,
                rm=rm,
                rs=rs,
                skipped=skipped,
                missing_cols=skipped_missing_cols,
            )
            if buffer:
                sample = ", ".join(buffer[:10])
                more = len(buffer) - len(buffer[:10])
                if more > 0:
                    sample = f"{sample}, ...(+{more} more)"
                msg += "\n" + tr("ğŸ” å‡¦ç†ä¸­éŠ˜æŸ„: {names}", names=sample)
            try:
                log_callback(msg)
            except Exception:
                pass

            # ãƒãƒƒãƒæ€§èƒ½è¨˜éŒ²
            batch_duration = time.time() - batch_start
            if batch_duration > 0:
                symbols_per_second = len(buffer) / batch_duration
                _metrics.record_metric(
                    "system6_candidates_batch_duration", batch_duration, "seconds"
                )
                _metrics.record_metric(
                    "system6_candidates_symbols_per_second", symbols_per_second, "rate"
                )

            batch_start = time.time()
            buffer.clear()

    limit_n = int(top_n)
    for date in list(candidates_by_date.keys()):
        rows = candidates_by_date.get(date, [])
        if not rows:
            candidates_by_date[date] = []
            continue
        df = pd.DataFrame(rows)
        if df.empty:
            candidates_by_date[date] = []
            continue
        df = df.sort_values("return_6d", ascending=False)
        total = len(df)
        df.loc[:, "rank"] = list(range(1, total + 1))
        df.loc[:, "rank_total"] = total
        limited = df.head(limit_n)
        candidates_by_date[date] = limited.to_dict("records")

    # å€™è£œæŠ½å‡ºã®é›†è¨ˆã‚µãƒãƒªãƒ¼ã¯ãƒ­ã‚°ã«ã®ã¿å‡ºåŠ›
    if skipped > 0 and log_callback:
        summary_lines = [f"âš ï¸ å€™è£œæŠ½å‡ºä¸­ã«ã‚¹ã‚­ãƒƒãƒ—: {skipped} ä»¶"]
        if skipped_missing_cols:
            summary_lines.append(f"  â””â”€ å¿…é ˆåˆ—æ¬ è½: {skipped_missing_cols} ä»¶")
        try:
            for line in summary_lines:
                log_callback(line)
        except Exception:
            pass

    # æœ€çµ‚ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
    total_candidates = sum(len(candidates) for candidates in candidates_by_date.values())
    unique_dates = len(candidates_by_date)
    _metrics.record_metric("system6_total_candidates", total_candidates, "count")
    _metrics.record_metric("system6_unique_entry_dates", unique_dates, "count")
    _metrics.record_metric("system6_processed_symbols_candidates", processed, "count")

    if log_callback:
        try:
            log_callback(
                f"ğŸ“Š System6 å€™è£œç”Ÿæˆå®Œäº†: {total_candidates}ä»¶ã®å€™è£œ "
                f"({unique_dates}æ—¥åˆ†, {processed}ã‚·ãƒ³ãƒœãƒ«å‡¦ç†)"
            )
        except Exception:
            pass

    return candidates_by_date, None


def get_total_days_system6(data_dict: dict[str, pd.DataFrame]) -> int:
    all_dates = set()
    for df in data_dict.values():
        if df is None or df.empty:
            continue
        if "Date" in df.columns:
            dates = pd.to_datetime(df["Date"]).dt.normalize()
        elif "date" in df.columns:
            dates = pd.to_datetime(df["date"]).dt.normalize()
        else:
            dates = pd.to_datetime(df.index).normalize()
        all_dates.update(dates)
    return len(all_dates)


__all__ = [
    "prepare_data_vectorized_system6",
    "generate_candidates_system6",
    "get_total_days_system6",
]
