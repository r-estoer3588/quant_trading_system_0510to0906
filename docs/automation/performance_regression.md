# Performance Regression Detection

## ğŸ¯ ç›®çš„

ã‚³ãƒ¼ãƒ‰å¤‰æ›´ã«ã‚ˆã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ£åŒ–ã‚’è‡ªå‹•æ¤œå‡ºã€‚

## ğŸ“‹ å®Ÿè£…ãƒ—ãƒ©ãƒ³

### Phase 1: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è‡ªå‹•å®Ÿè¡Œ

```python
# tools/auto_benchmark.py
"""
ã‚³ãƒŸãƒƒãƒˆå‰ã«ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œã—ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ£åŒ–ã‚’æ¤œå‡º
"""
import subprocess
import json
import sys
from pathlib import Path
from datetime import datetime

BENCHMARK_HISTORY = Path("benchmarks/history.jsonl")

def run_benchmark():
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
    result = subprocess.run(
        [
            sys.executable,
            "scripts/run_all_systems_today.py",
            "--test-mode", "mini",
            "--skip-external",
            "--benchmark"
        ],
        capture_output=True,
        text=True
    )

    if result.returncode != 0:
        print("âŒ Benchmark failed")
        return None

    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’èª­ã¿è¾¼ã¿
    benchmark_files = list(Path("results_csv_test").glob("benchmark_*.json"))
    if not benchmark_files:
        print("âš ï¸  No benchmark file found")
        return None

    latest_benchmark = max(benchmark_files, key=lambda p: p.stat().st_mtime)
    with open(latest_benchmark) as f:
        data = json.load(f)

    return data

def compare_with_baseline(current: dict, baseline: dict, threshold: float = 0.10):
    """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨æ¯”è¼ƒï¼ˆ10%ä»¥ä¸Šã®åŠ£åŒ–ã‚’æ¤œå‡ºï¼‰"""
    regressions = []

    for phase, current_time in current.get('phase_times', {}).items():
        baseline_time = baseline.get('phase_times', {}).get(phase, 0)

        if baseline_time > 0:
            regression_pct = (current_time - baseline_time) / baseline_time

            if regression_pct > threshold:
                regressions.append({
                    'phase': phase,
                    'baseline': baseline_time,
                    'current': current_time,
                    'regression_pct': regression_pct
                })

    return regressions

def get_baseline():
    """éå»7æ—¥é–“ã®ä¸­å¤®å€¤ã‚’ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã—ã¦ä½¿ç”¨"""
    if not BENCHMARK_HISTORY.exists():
        return None

    import pandas as pd

    # å±¥æ­´èª­ã¿è¾¼ã¿
    history = []
    with open(BENCHMARK_HISTORY) as f:
        for line in f:
            history.append(json.loads(line))

    if len(history) < 3:
        return None

    # ç›´è¿‘7æ—¥é–“
    recent = history[-7:]

    # ãƒ•ã‚§ãƒ¼ã‚ºã”ã¨ã®ä¸­å¤®å€¤ã‚’è¨ˆç®—
    baseline = {'phase_times': {}}

    df = pd.DataFrame([h['phase_times'] for h in recent])
    for col in df.columns:
        baseline['phase_times'][col] = df[col].median()

    return baseline

def save_benchmark(data: dict):
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’å±¥æ­´ã«ä¿å­˜"""
    BENCHMARK_HISTORY.parent.mkdir(parents=True, exist_ok=True)

    record = {
        'timestamp': datetime.now().isoformat(),
        'git_commit': subprocess.check_output(['git', 'rev-parse', 'HEAD']).decode().strip(),
        'phase_times': data.get('phase_times', {}),
        'total_time': data.get('total_time', 0)
    }

    with open(BENCHMARK_HISTORY, 'a') as f:
        f.write(json.dumps(record) + '\n')

def main():
    print("ğŸ” Running performance benchmark...")

    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    current = run_benchmark()
    if not current:
        return 1

    # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å–å¾—
    baseline = get_baseline()

    if baseline:
        # å›å¸°æ¤œå‡º
        regressions = compare_with_baseline(current, baseline, threshold=0.10)

        if regressions:
            print("\nâš ï¸  Performance regressions detected:\n")
            for reg in regressions:
                print(f"  â€¢ {reg['phase']}: {reg['regression_pct']:.1%} slower")
                print(f"    Baseline: {reg['baseline']:.2f}s â†’ Current: {reg['current']:.2f}s")

            print("\nâŒ Performance degraded by >10%. Review changes.")

            response = input("\nContinue with commit? (y/N): ")
            if response.lower() != 'y':
                return 1
        else:
            print("âœ… No performance regressions detected")
    else:
        print("â„¹ï¸  No baseline available (need 3+ historical runs)")

    # çµæœã‚’å±¥æ­´ã«ä¿å­˜
    save_benchmark(current)

    return 0

if __name__ == "__main__":
    sys.exit(main())
```

### Phase 2: pre-commit çµ±åˆ

```yaml
# .pre-commit-config.yaml ã«è¿½åŠ 
- repo: local
  hooks:
    - id: performance-check
      name: Performance Regression Check
      entry: python tools/auto_benchmark.py
      language: system
      pass_filenames: false
      files: ^(core|common|strategies)/.*\.py$
      stages: [pre-push]
```

### Phase 3: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å±¥æ­´å¯è¦–åŒ–

```python
# tools/visualize_benchmarks.py
"""
ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å±¥æ­´ã‚’ã‚°ãƒ©ãƒ•åŒ–
"""
import json
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path

def main():
    history_file = Path("benchmarks/history.jsonl")

    if not history_file.exists():
        print("No benchmark history found")
        return

    # å±¥æ­´èª­ã¿è¾¼ã¿
    history = []
    with open(history_file) as f:
        for line in f:
            history.append(json.loads(line))

    # DataFrame å¤‰æ›
    df = pd.DataFrame(history)
    df['timestamp'] = pd.to_datetime(df['timestamp'])

    # ãƒ•ã‚§ãƒ¼ã‚ºã”ã¨ã®ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³
    fig, ax = plt.subplots(figsize=(12, 6))

    phase_times = pd.DataFrame(df['phase_times'].tolist(), index=df['timestamp'])

    for col in phase_times.columns:
        ax.plot(phase_times.index, phase_times[col], marker='o', label=col)

    ax.set_xlabel('Date')
    ax.set_ylabel('Time (seconds)')
    ax.set_title('Performance Benchmark History')
    ax.legend()
    ax.grid(True, alpha=0.3)

    plt.xticks(rotation=45)
    plt.tight_layout()

    output = Path("benchmarks/performance_trend.png")
    plt.savefig(output, dpi=150)
    print(f"âœ… Chart saved: {output}")

if __name__ == "__main__":
    main()
```

## ğŸ”§ ä½¿ã„æ–¹

### è‡ªå‹•å®Ÿè¡Œï¼ˆpre-pushï¼‰

```powershell
git push origin branch0906

# â†’ è‡ªå‹•ã§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ & æ¯”è¼ƒ
```

### æ‰‹å‹•å®Ÿè¡Œ

```powershell
# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
python tools/auto_benchmark.py

# å±¥æ­´å¯è¦–åŒ–
python tools/visualize_benchmarks.py
```

## ğŸ“ˆ ãƒ¡ãƒªãƒƒãƒˆ

- âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ£åŒ–ã®æ—©æœŸæ¤œå‡º
- âœ… æœ€é©åŒ–ã®åŠ¹æœæ¸¬å®š
- âœ… é•·æœŸçš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰æŠŠæ¡
- âœ… ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®šã®è£œåŠ©
